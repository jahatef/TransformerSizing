bash: /fsx/home-jacob/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)
/fsx/home-jacob/write_hostfile.sh: line 7: /fsx/home-quentin/jacob/hostfiles/hosts_39454: Permission denied
1.13.1 

[2023-10-23 16:52:56,795] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-10-23 16:52:57,624] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.159.251, master_port=6000
[2023-10-23 16:52:57,625] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-10-23 16:53:00,582] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0386
Attention throughput (in TFLOP/s): 16.006
MLP duration (in seconds): 0.0362
MLP throughput (in TFLOP/s): 30.378
Transformer duration (in seconds): 0.0778
Transformer throughput (in TFLOP/s): 22.075
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0529
Attention throughput (in TFLOP/s): 12.045
MLP duration (in seconds): 0.0377
MLP throughput (in TFLOP/s): 30.096
Transformer duration (in seconds): 0.0946
Transformer throughput (in TFLOP/s): 18.730
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0542
Attention throughput (in TFLOP/s): 12.094
MLP duration (in seconds): 0.0389
MLP throughput (in TFLOP/s): 30.021
Transformer duration (in seconds): 0.0968
Transformer throughput (in TFLOP/s): 18.855
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 12.308
MLP duration (in seconds): 0.0402
MLP throughput (in TFLOP/s): 29.968
Transformer duration (in seconds): 0.0990
Transformer throughput (in TFLOP/s): 18.984
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0564
Attention throughput (in TFLOP/s): 12.308
MLP duration (in seconds): 0.0412
MLP throughput (in TFLOP/s): 30.126
Transformer duration (in seconds): 0.1013
Transformer throughput (in TFLOP/s): 19.095
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0569
Attention throughput (in TFLOP/s): 12.524
MLP duration (in seconds): 0.0426
MLP throughput (in TFLOP/s): 30.021
Transformer duration (in seconds): 0.1035
Transformer throughput (in TFLOP/s): 19.233
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0590
Attention throughput (in TFLOP/s): 12.426
MLP duration (in seconds): 0.0427
MLP throughput (in TFLOP/s): 30.832
Transformer duration (in seconds): 0.1048
Transformer throughput (in TFLOP/s): 19.549
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0598
Attention throughput (in TFLOP/s): 12.599
MLP duration (in seconds): 0.0440
MLP throughput (in TFLOP/s): 30.732
Transformer duration (in seconds): 0.1066
Transformer throughput (in TFLOP/s): 19.755
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0449
Attention throughput (in TFLOP/s): 17.204
MLP duration (in seconds): 0.0465
MLP throughput (in TFLOP/s): 29.935
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 22.791
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 13.014
MLP duration (in seconds): 0.0479
MLP throughput (in TFLOP/s): 29.860
Transformer duration (in seconds): 0.1132
Transformer throughput (in TFLOP/s): 19.645
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 16.573
MLP duration (in seconds): 0.0500
MLP throughput (in TFLOP/s): 29.415
Transformer duration (in seconds): 0.1024
Transformer throughput (in TFLOP/s): 22.316
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 13.377
MLP duration (in seconds): 0.0514
MLP throughput (in TFLOP/s): 29.385
Transformer duration (in seconds): 0.1181
Transformer throughput (in TFLOP/s): 19.857
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0504
Attention throughput (in TFLOP/s): 16.987
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 30.397
Transformer duration (in seconds): 0.1042
Transformer throughput (in TFLOP/s): 23.106
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 13.740
MLP duration (in seconds): 0.0541
MLP throughput (in TFLOP/s): 29.411
Transformer duration (in seconds): 0.1220
Transformer throughput (in TFLOP/s): 20.240
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 17.379
MLP duration (in seconds): 0.0538
MLP throughput (in TFLOP/s): 30.384
Transformer duration (in seconds): 0.1083
Transformer throughput (in TFLOP/s): 23.391
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0656
Attention throughput (in TFLOP/s): 14.071
MLP duration (in seconds): 0.0570
MLP throughput (in TFLOP/s): 29.402
Transformer duration (in seconds): 0.1268
Transformer throughput (in TFLOP/s): 20.494
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0510
Attention throughput (in TFLOP/s): 18.544
MLP duration (in seconds): 0.0567
MLP throughput (in TFLOP/s): 30.324
Transformer duration (in seconds): 0.1103
Transformer throughput (in TFLOP/s): 24.150
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0671
Attention throughput (in TFLOP/s): 14.430
MLP duration (in seconds): 0.0597
MLP throughput (in TFLOP/s): 29.504
Transformer duration (in seconds): 0.1309
Transformer throughput (in TFLOP/s): 20.841
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0549
Attention throughput (in TFLOP/s): 18.039
MLP duration (in seconds): 0.0612
MLP throughput (in TFLOP/s): 29.490
Transformer duration (in seconds): 0.1199
Transformer throughput (in TFLOP/s): 23.307
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0686
Attention throughput (in TFLOP/s): 14.777
MLP duration (in seconds): 0.0627
MLP throughput (in TFLOP/s): 29.484
Transformer duration (in seconds): 0.1357
Transformer throughput (in TFLOP/s): 21.104
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0563
Attention throughput (in TFLOP/s): 18.438
MLP duration (in seconds): 0.0624
MLP throughput (in TFLOP/s): 30.338
Transformer duration (in seconds): 0.1213
Transformer throughput (in TFLOP/s): 24.172
Transformer - MLP - Attention (in seconds): 0.0026
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 15.165
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 29.835
Transformer duration (in seconds): 0.1388
Transformer throughput (in TFLOP/s): 21.613
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 19.052
MLP duration (in seconds): 0.0641
MLP throughput (in TFLOP/s): 30.957
Transformer duration (in seconds): 0.1245
Transformer throughput (in TFLOP/s): 24.667
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0708
Attention throughput (in TFLOP/s): 15.670
MLP duration (in seconds): 0.0681
MLP throughput (in TFLOP/s): 29.851
Transformer duration (in seconds): 0.1431
Transformer throughput (in TFLOP/s): 21.956
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0567
Attention throughput (in TFLOP/s): 20.002
MLP duration (in seconds): 0.0675
MLP throughput (in TFLOP/s): 30.793
Transformer duration (in seconds): 0.1276
Transformer throughput (in TFLOP/s): 25.183
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0729
Attention throughput (in TFLOP/s): 15.891
MLP duration (in seconds): 0.0695
MLP throughput (in TFLOP/s): 30.595
Transformer duration (in seconds): 0.1462
Transformer throughput (in TFLOP/s): 22.471
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0610
Attention throughput (in TFLOP/s): 19.402
MLP duration (in seconds): 0.0729
MLP throughput (in TFLOP/s): 29.831
Transformer duration (in seconds): 0.1366
Transformer throughput (in TFLOP/s): 24.579
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0740
Attention throughput (in TFLOP/s): 16.331
MLP duration (in seconds): 0.0741
MLP throughput (in TFLOP/s): 29.980
Transformer duration (in seconds): 0.1521
Transformer throughput (in TFLOP/s): 22.557
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0615
Attention throughput (in TFLOP/s): 20.075
MLP duration (in seconds): 0.0733
MLP throughput (in TFLOP/s): 30.981
Transformer duration (in seconds): 0.1384
Transformer throughput (in TFLOP/s): 25.331
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0759
Attention throughput (in TFLOP/s): 16.603
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 29.934
Transformer duration (in seconds): 0.1580
Transformer throughput (in TFLOP/s): 22.668
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0630
Attention throughput (in TFLOP/s): 20.427
MLP duration (in seconds): 0.0757
MLP throughput (in TFLOP/s): 31.315
Transformer duration (in seconds): 0.1426
Transformer throughput (in TFLOP/s): 25.661
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0779
Attention throughput (in TFLOP/s): 16.858
MLP duration (in seconds): 0.0801
MLP throughput (in TFLOP/s): 30.247
Transformer duration (in seconds): 0.1622
Transformer throughput (in TFLOP/s): 23.027
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 21.411
MLP duration (in seconds): 0.0800
MLP throughput (in TFLOP/s): 30.933
Transformer duration (in seconds): 0.1463
Transformer throughput (in TFLOP/s): 26.072
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0816
Attention throughput (in TFLOP/s): 16.746
MLP duration (in seconds): 0.0833
MLP throughput (in TFLOP/s): 30.307
Transformer duration (in seconds): 0.1685
Transformer throughput (in TFLOP/s): 23.099
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 20.672
MLP duration (in seconds): 0.0819
MLP throughput (in TFLOP/s): 31.485
Transformer duration (in seconds): 0.1528
Transformer throughput (in TFLOP/s): 26.001
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0831
Attention throughput (in TFLOP/s): 17.099
MLP duration (in seconds): 0.0866
MLP throughput (in TFLOP/s): 30.378
Transformer duration (in seconds): 0.1743
Transformer throughput (in TFLOP/s): 23.254
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 21.045
MLP duration (in seconds): 0.0853
MLP throughput (in TFLOP/s): 31.453
Transformer duration (in seconds): 0.1582
Transformer throughput (in TFLOP/s): 26.132
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 17.157
MLP duration (in seconds): 0.0876
MLP throughput (in TFLOP/s): 31.266
Transformer duration (in seconds): 0.1780
Transformer throughput (in TFLOP/s): 23.689
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0710
Attention throughput (in TFLOP/s): 21.197
MLP duration (in seconds): 0.0896
MLP throughput (in TFLOP/s): 31.170
Transformer duration (in seconds): 0.1652
Transformer throughput (in TFLOP/s): 26.028
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0892
Attention throughput (in TFLOP/s): 17.204
MLP duration (in seconds): 0.0919
MLP throughput (in TFLOP/s): 30.978
Transformer duration (in seconds): 0.1850
Transformer throughput (in TFLOP/s): 23.693
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0711
Attention throughput (in TFLOP/s): 21.996
MLP duration (in seconds): 0.0923
MLP throughput (in TFLOP/s): 31.443
Transformer duration (in seconds): 0.1670
Transformer throughput (in TFLOP/s): 26.748
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0909
Attention throughput (in TFLOP/s): 17.515
MLP duration (in seconds): 0.0943
MLP throughput (in TFLOP/s): 31.370
Transformer duration (in seconds): 0.1888
Transformer throughput (in TFLOP/s): 24.112
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0749
Attention throughput (in TFLOP/s): 21.649
MLP duration (in seconds): 0.0959
MLP throughput (in TFLOP/s): 31.448
Transformer duration (in seconds): 0.1746
Transformer throughput (in TFLOP/s): 26.567
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0925
Attention throughput (in TFLOP/s): 17.857
MLP duration (in seconds): 0.0979
MLP throughput (in TFLOP/s): 31.381
Transformer duration (in seconds): 0.1948
Transformer throughput (in TFLOP/s): 24.250
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0763
Attention throughput (in TFLOP/s): 22.024
MLP duration (in seconds): 0.1000
MLP throughput (in TFLOP/s): 31.307
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 26.559
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0954
Attention throughput (in TFLOP/s): 17.935
MLP duration (in seconds): 0.1030
MLP throughput (in TFLOP/s): 30.966
Transformer duration (in seconds): 0.2026
Transformer throughput (in TFLOP/s): 24.190
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 22.066
MLP duration (in seconds): 0.1045
MLP throughput (in TFLOP/s): 31.090
Transformer duration (in seconds): 0.1875
Transformer throughput (in TFLOP/s): 26.619
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0964
Attention throughput (in TFLOP/s): 18.393
MLP duration (in seconds): 0.1053
MLP throughput (in TFLOP/s): 31.398
Transformer duration (in seconds): 0.2059
Transformer throughput (in TFLOP/s): 24.673
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0786
Attention throughput (in TFLOP/s): 22.945
MLP duration (in seconds): 0.1070
MLP throughput (in TFLOP/s): 31.465
Transformer duration (in seconds): 0.1906
Transformer throughput (in TFLOP/s): 27.129
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0985
Attention throughput (in TFLOP/s): 18.634
MLP duration (in seconds): 0.1098
MLP throughput (in TFLOP/s): 31.228
Transformer duration (in seconds): 0.2129
Transformer throughput (in TFLOP/s): 24.719
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0826
Attention throughput (in TFLOP/s): 22.610
MLP duration (in seconds): 0.1119
MLP throughput (in TFLOP/s): 31.188
Transformer duration (in seconds): 0.1990
Transformer throughput (in TFLOP/s): 26.906
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1016
Attention throughput (in TFLOP/s): 18.683
MLP duration (in seconds): 0.1146
MLP throughput (in TFLOP/s): 30.978
Transformer duration (in seconds): 0.2202
Transformer throughput (in TFLOP/s): 24.741
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0847
Attention throughput (in TFLOP/s): 22.780
MLP duration (in seconds): 0.1165
MLP throughput (in TFLOP/s): 30.998
Transformer duration (in seconds): 0.2050
Transformer throughput (in TFLOP/s): 27.039
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1027
Attention throughput (in TFLOP/s): 19.114
MLP duration (in seconds): 0.1188
MLP throughput (in TFLOP/s): 30.924
Transformer duration (in seconds): 0.2263
Transformer throughput (in TFLOP/s): 24.914
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0864
Attention throughput (in TFLOP/s): 23.095
MLP duration (in seconds): 0.1209
MLP throughput (in TFLOP/s): 30.923
Transformer duration (in seconds): 0.2121
Transformer throughput (in TFLOP/s): 27.034
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1057
Attention throughput (in TFLOP/s): 19.182
MLP duration (in seconds): 0.1231
MLP throughput (in TFLOP/s): 30.891
Transformer duration (in seconds): 0.2334
Transformer throughput (in TFLOP/s): 24.975
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0872
Attention throughput (in TFLOP/s): 23.631
MLP duration (in seconds): 0.1250
MLP throughput (in TFLOP/s): 30.932
Transformer duration (in seconds): 0.2162
Transformer throughput (in TFLOP/s): 27.415
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1071
Attention throughput (in TFLOP/s): 19.566
MLP duration (in seconds): 0.1274
MLP throughput (in TFLOP/s): 30.843
Transformer duration (in seconds): 0.2388
Transformer throughput (in TFLOP/s): 25.235
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0914
Attention throughput (in TFLOP/s): 23.301
MLP duration (in seconds): 0.1292
MLP throughput (in TFLOP/s): 30.913
Transformer duration (in seconds): 0.2247
Transformer throughput (in TFLOP/s): 27.249
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1094
Attention throughput (in TFLOP/s): 19.776
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 30.899
Transformer duration (in seconds): 0.2455
Transformer throughput (in TFLOP/s): 25.351
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0930
Attention throughput (in TFLOP/s): 23.633
MLP duration (in seconds): 0.1330
MLP throughput (in TFLOP/s): 31.044
Transformer duration (in seconds): 0.2292
Transformer throughput (in TFLOP/s): 27.594
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1103
Attention throughput (in TFLOP/s): 20.229
MLP duration (in seconds): 0.1337
MLP throughput (in TFLOP/s): 31.375
Transformer duration (in seconds): 0.2493
Transformer throughput (in TFLOP/s): 25.776
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0940
Attention throughput (in TFLOP/s): 24.100
MLP duration (in seconds): 0.1357
MLP throughput (in TFLOP/s): 31.403
Transformer duration (in seconds): 0.2340
Transformer throughput (in TFLOP/s): 27.897
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1128
Attention throughput (in TFLOP/s): 20.406
MLP duration (in seconds): 0.1381
MLP throughput (in TFLOP/s): 31.344
Transformer duration (in seconds): 0.2560
Transformer throughput (in TFLOP/s): 25.903
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0946
Attention throughput (in TFLOP/s): 24.689
MLP duration (in seconds): 0.1408
MLP throughput (in TFLOP/s): 31.231
Transformer duration (in seconds): 0.2403
Transformer throughput (in TFLOP/s): 28.029
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1202
Attention throughput (in TFLOP/s): 19.740
MLP duration (in seconds): 0.1423
MLP throughput (in TFLOP/s): 31.401
Transformer duration (in seconds): 0.2675
Transformer throughput (in TFLOP/s): 25.562
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1014
Attention throughput (in TFLOP/s): 23.750
MLP duration (in seconds): 0.1447
MLP throughput (in TFLOP/s): 31.357
Transformer duration (in seconds): 0.2517
Transformer throughput (in TFLOP/s): 27.590
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 19.731
MLP duration (in seconds): 0.1481
MLP throughput (in TFLOP/s): 31.105
Transformer duration (in seconds): 0.2779
Transformer throughput (in TFLOP/s): 25.372
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1042
Attention throughput (in TFLOP/s): 23.808
MLP duration (in seconds): 0.1494
MLP throughput (in TFLOP/s): 31.314
Transformer duration (in seconds): 0.2565
Transformer throughput (in TFLOP/s): 27.901
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1262
Attention throughput (in TFLOP/s): 19.937
MLP duration (in seconds): 0.1516
MLP throughput (in TFLOP/s): 31.319
Transformer duration (in seconds): 0.2828
Transformer throughput (in TFLOP/s): 25.687
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1061
Attention throughput (in TFLOP/s): 24.072
MLP duration (in seconds): 0.1548
MLP throughput (in TFLOP/s): 31.144
Transformer duration (in seconds): 0.2663
Transformer throughput (in TFLOP/s): 27.694
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1298
Attention throughput (in TFLOP/s): 19.966
MLP duration (in seconds): 0.1554
MLP throughput (in TFLOP/s): 31.491
Transformer duration (in seconds): 0.2897
Transformer throughput (in TFLOP/s): 25.831
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1065
Attention throughput (in TFLOP/s): 24.682
MLP duration (in seconds): 0.1583
MLP throughput (in TFLOP/s): 31.361
Transformer duration (in seconds): 0.2697
Transformer throughput (in TFLOP/s): 28.153
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1327
Attention throughput (in TFLOP/s): 20.089
MLP duration (in seconds): 0.1617
MLP throughput (in TFLOP/s): 31.149
Transformer duration (in seconds): 0.3002
Transformer throughput (in TFLOP/s): 25.663
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1121
Attention throughput (in TFLOP/s): 24.133
MLP duration (in seconds): 0.1648
MLP throughput (in TFLOP/s): 31.011
Transformer duration (in seconds): 0.2796
Transformer throughput (in TFLOP/s): 27.960
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1349
Attention throughput (in TFLOP/s): 20.328
MLP duration (in seconds): 0.1652
MLP throughput (in TFLOP/s): 31.392
Transformer duration (in seconds): 0.3050
Transformer throughput (in TFLOP/s): 25.993
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1133
Attention throughput (in TFLOP/s): 24.540
MLP duration (in seconds): 0.1680
MLP throughput (in TFLOP/s): 31.318
Transformer duration (in seconds): 0.2875
Transformer throughput (in TFLOP/s): 27.971
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1378
Attention throughput (in TFLOP/s): 20.470
MLP duration (in seconds): 0.1713
MLP throughput (in TFLOP/s): 31.155
Transformer duration (in seconds): 0.3146
Transformer throughput (in TFLOP/s): 25.929
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1169
Attention throughput (in TFLOP/s): 24.452
MLP duration (in seconds): 0.1752
MLP throughput (in TFLOP/s): 30.895
Transformer duration (in seconds): 0.2968
Transformer throughput (in TFLOP/s): 27.865
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1413
Attention throughput (in TFLOP/s): 20.508
MLP duration (in seconds): 0.1774
MLP throughput (in TFLOP/s): 30.935
Transformer duration (in seconds): 0.3234
Transformer throughput (in TFLOP/s): 25.935
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1167
Attention throughput (in TFLOP/s): 25.182
MLP duration (in seconds): 0.1796
MLP throughput (in TFLOP/s): 31.000
Transformer duration (in seconds): 0.3027
Transformer throughput (in TFLOP/s): 28.096
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1321
Attention throughput (in TFLOP/s): 22.539
MLP duration (in seconds): 0.1828
MLP throughput (in TFLOP/s): 30.877
Transformer duration (in seconds): 0.3196
Transformer throughput (in TFLOP/s): 26.974
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1223
Attention throughput (in TFLOP/s): 24.673
MLP duration (in seconds): 0.1839
MLP throughput (in TFLOP/s): 31.108
Transformer duration (in seconds): 0.3118
Transformer throughput (in TFLOP/s): 28.033
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1342
Attention throughput (in TFLOP/s): 22.792
MLP duration (in seconds): 0.1873
MLP throughput (in TFLOP/s): 30.977
Transformer duration (in seconds): 0.3253
Transformer throughput (in TFLOP/s): 27.231
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 25.015
MLP duration (in seconds): 0.1883
MLP throughput (in TFLOP/s): 31.225
Transformer duration (in seconds): 0.3183
Transformer throughput (in TFLOP/s): 28.209
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1363
Attention throughput (in TFLOP/s): 23.031
MLP duration (in seconds): 0.1911
MLP throughput (in TFLOP/s): 31.182
Transformer duration (in seconds): 0.3324
Transformer throughput (in TFLOP/s): 27.377
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1265
Attention throughput (in TFLOP/s): 25.145
MLP duration (in seconds): 0.1921
MLP throughput (in TFLOP/s): 31.446
Transformer duration (in seconds): 0.3240
Transformer throughput (in TFLOP/s): 28.463
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1390
Attention throughput (in TFLOP/s): 23.182
MLP duration (in seconds): 0.1958
MLP throughput (in TFLOP/s): 31.263
Transformer duration (in seconds): 0.3405
Transformer throughput (in TFLOP/s): 27.440
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1273
Attention throughput (in TFLOP/s): 25.632
MLP duration (in seconds): 0.1987
MLP throughput (in TFLOP/s): 31.205
Transformer duration (in seconds): 0.3325
Transformer throughput (in TFLOP/s): 28.465
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1417
Attention throughput (in TFLOP/s): 23.329
MLP duration (in seconds): 0.2008
MLP throughput (in TFLOP/s): 31.290
Transformer duration (in seconds): 0.3487
Transformer throughput (in TFLOP/s): 27.502
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1326
Attention throughput (in TFLOP/s): 25.258
MLP duration (in seconds): 0.2035
MLP throughput (in TFLOP/s): 31.290
Transformer duration (in seconds): 0.3396
Transformer throughput (in TFLOP/s): 28.610
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1438
Attention throughput (in TFLOP/s): 23.587
MLP duration (in seconds): 0.2044
MLP throughput (in TFLOP/s): 31.552
Transformer duration (in seconds): 0.3541
Transformer throughput (in TFLOP/s): 27.788
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1346
Attention throughput (in TFLOP/s): 25.515
MLP duration (in seconds): 0.2081
MLP throughput (in TFLOP/s): 31.398
Transformer duration (in seconds): 0.3491
Transformer throughput (in TFLOP/s): 28.552
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1472
Attention throughput (in TFLOP/s): 23.623
MLP duration (in seconds): 0.2120
MLP throughput (in TFLOP/s): 31.209
Transformer duration (in seconds): 0.3649
Transformer throughput (in TFLOP/s): 27.659
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1367
Attention throughput (in TFLOP/s): 25.750
MLP duration (in seconds): 0.2125
MLP throughput (in TFLOP/s): 31.542
Transformer duration (in seconds): 0.3553
Transformer throughput (in TFLOP/s): 28.771
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1493
Attention throughput (in TFLOP/s): 23.873
MLP duration (in seconds): 0.2168
MLP throughput (in TFLOP/s): 31.300
Transformer duration (in seconds): 0.3690
Transformer throughput (in TFLOP/s): 28.048
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1367
Attention throughput (in TFLOP/s): 26.390
MLP duration (in seconds): 0.2176
MLP throughput (in TFLOP/s): 31.585
Transformer duration (in seconds): 0.3613
Transformer throughput (in TFLOP/s): 29.007
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1526
Attention throughput (in TFLOP/s): 23.934
MLP duration (in seconds): 0.2862
MLP throughput (in TFLOP/s): 24.312
Transformer duration (in seconds): 0.4469
Transformer throughput (in TFLOP/s): 23.742
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 26.030
MLP duration (in seconds): 0.2828
MLP throughput (in TFLOP/s): 24.911
Transformer duration (in seconds): 0.4355
Transformer throughput (in TFLOP/s): 24.667
Transformer - MLP - Attention (in seconds): 0.0106
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1550
Attention throughput (in TFLOP/s): 24.139
MLP duration (in seconds): 0.2938
MLP throughput (in TFLOP/s): 24.279
Transformer duration (in seconds): 0.4573
Transformer throughput (in TFLOP/s): 23.777
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1455
Attention throughput (in TFLOP/s): 26.028
MLP duration (in seconds): 0.2980
MLP throughput (in TFLOP/s): 24.231
Transformer duration (in seconds): 0.4537
Transformer throughput (in TFLOP/s): 24.258
Transformer - MLP - Attention (in seconds): 0.0103
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1584
Attention throughput (in TFLOP/s): 24.182
MLP duration (in seconds): 0.3084
MLP throughput (in TFLOP/s): 23.698
Transformer duration (in seconds): 0.4736
Transformer throughput (in TFLOP/s): 23.520
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1490
Attention throughput (in TFLOP/s): 26.026
MLP duration (in seconds): 0.3073
MLP throughput (in TFLOP/s): 24.067
Transformer duration (in seconds): 0.4621
Transformer throughput (in TFLOP/s): 24.399
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1618
Attention throughput (in TFLOP/s): 24.247
MLP duration (in seconds): 0.3075
MLP throughput (in TFLOP/s): 24.348
Transformer duration (in seconds): 0.4796
Transformer throughput (in TFLOP/s): 23.786
Transformer - MLP - Attention (in seconds): 0.0104
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1506
Attention throughput (in TFLOP/s): 26.352
MLP duration (in seconds): 0.3117
MLP throughput (in TFLOP/s): 24.307
Transformer duration (in seconds): 0.4709
Transformer throughput (in TFLOP/s): 24.516
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1656
Attention throughput (in TFLOP/s): 24.246
MLP duration (in seconds): 0.3173
MLP throughput (in TFLOP/s): 24.166
Transformer duration (in seconds): 0.4862
Transformer throughput (in TFLOP/s): 24.026
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1549
Attention throughput (in TFLOP/s): 26.215
MLP duration (in seconds): 0.3150
MLP throughput (in TFLOP/s): 24.630
Transformer duration (in seconds): 0.4761
Transformer throughput (in TFLOP/s): 24.826
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1677
Attention throughput (in TFLOP/s): 24.494
MLP duration (in seconds): 0.3235
MLP throughput (in TFLOP/s): 24.265
Transformer duration (in seconds): 0.5022
Transformer throughput (in TFLOP/s): 23.809
Transformer - MLP - Attention (in seconds): 0.0110
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1567
Attention throughput (in TFLOP/s): 26.526
MLP duration (in seconds): 0.3314
MLP throughput (in TFLOP/s): 23.964
Transformer duration (in seconds): 0.4952
Transformer throughput (in TFLOP/s): 24.427
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1708
Attention throughput (in TFLOP/s): 24.609
MLP duration (in seconds): 0.3326
MLP throughput (in TFLOP/s): 24.154
Transformer duration (in seconds): 0.5113
Transformer throughput (in TFLOP/s): 23.933
Transformer - MLP - Attention (in seconds): 0.0079
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1613
Attention throughput (in TFLOP/s): 26.345
MLP duration (in seconds): 0.3343
MLP throughput (in TFLOP/s): 24.308
Transformer duration (in seconds): 0.5016
Transformer throughput (in TFLOP/s): 24.674
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1726
Attention throughput (in TFLOP/s): 24.903
MLP duration (in seconds): 0.2685
MLP throughput (in TFLOP/s): 30.619
Transformer duration (in seconds): 0.4484
Transformer throughput (in TFLOP/s): 27.919
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1619
Attention throughput (in TFLOP/s): 26.841
MLP duration (in seconds): 0.2734
MLP throughput (in TFLOP/s): 30.415
Transformer duration (in seconds): 0.4444
Transformer throughput (in TFLOP/s): 28.489
Transformer - MLP - Attention (in seconds): 0.0091
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1759
Attention throughput (in TFLOP/s): 24.982
MLP duration (in seconds): 0.2736
MLP throughput (in TFLOP/s): 30.743
Transformer duration (in seconds): 0.4555
Transformer throughput (in TFLOP/s): 28.110
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1656
Attention throughput (in TFLOP/s): 26.837
MLP duration (in seconds): 0.2781
MLP throughput (in TFLOP/s): 30.581
Transformer duration (in seconds): 0.4525
Transformer throughput (in TFLOP/s): 28.616
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 8, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1800
Attention throughput (in TFLOP/s): 24.962
MLP duration (in seconds): 0.2808
MLP throughput (in TFLOP/s): 30.633
Traceback (most recent call last):
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 478, in <module>
    benchmark_transformer(configuration, seq_length, train_batch_size)
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 423, in benchmark_transformer
    out = layer(inp, attention_mask)
  File "/fsx/home-jacob/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 856, in forward
    attention_output, attention_bias = self.attention(
  File "/fsx/home-jacob/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 685, in forward
    context_layer = self.attention(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 415, in attention
    matmul_result = torch.baddbmm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 39.56 GiB total capacity; 29.92 GiB already allocated; 1.97 GiB free; 36.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
