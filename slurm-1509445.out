
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
2
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-26 15:57:25,618] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-26 15:57:25,618] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 16, hidden_size: 24416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1526x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1526x2048): 85.617
Elapsed time for attention_prob_times_values (64x2048x2048x1526): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1526): 80.584

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2062.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1527x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1527x2048): 84.355
Elapsed time for attention_prob_times_values (64x2048x2048x1527): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1527): 73.480

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1952.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1528x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1528x2048): 86.256
Elapsed time for attention_prob_times_values (64x2048x2048x1528): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1528): 86.815

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2152.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1529x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1529x2048): 83.990
Elapsed time for attention_prob_times_values (64x2048x2048x1529): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1529): 73.307

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1948.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1530x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1530x2048): 84.662
Elapsed time for attention_prob_times_values (64x2048x2048x1530): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1530): 79.794

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2046.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1531x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1531x2048): 83.850
Elapsed time for attention_prob_times_values (64x2048x2048x1531): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1531): 73.174

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1947.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1532x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1532x2048): 85.263
Elapsed time for attention_prob_times_values (64x2048x2048x1532): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1532): 79.734

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2054.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1533x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1533x2048): 83.842
Elapsed time for attention_prob_times_values (64x2048x2048x1533): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1533): 73.272

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1951.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1534x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1534x2048): 84.599
Elapsed time for attention_prob_times_values (64x2048x2048x1534): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1534): 79.578

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2047.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1535x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1535x2048): 83.420
2.1.1+rocm5.6 

num_attention_heads: 32, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x761x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x761x2048): 75.685
Elapsed time for attention_prob_times_values (128x2048x2048x761): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x761): 65.981

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1747.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x762x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x762x2048): 76.584
Elapsed time for attention_prob_times_values (128x2048x2048x762): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x762): 68.643

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1796.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x763x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x763x2048): 75.934
Elapsed time for attention_prob_times_values (128x2048x2048x763): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x763): 66.124

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1756.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x764x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x764x2048): 77.265
Elapsed time for attention_prob_times_values (128x2048x2048x764): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x764): 68.741

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1809.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x765x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x765x2048): 75.838
Elapsed time for attention_prob_times_values (128x2048x2048x765): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x765): 66.259

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1761.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x766x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x766x2048): 76.449
Elapsed time for attention_prob_times_values (128x2048x2048x766): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x766): 68.946

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1808.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x767x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x767x2048): 75.796
Elapsed time for attention_prob_times_values (128x2048x2048x767): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x767): 65.472

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1754.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x768x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x768x2048): 86.825
Elapsed time for attention_prob_times_values (128x2048x2048x768): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x768): 96.732

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2287.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x769x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x769x2048): 77.608
Elapsed time for attention_prob_times_values (128x2048x2048x769): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x769): 61.683

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1720.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x770x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x770x2048): 78.645
Elapsed time for attention_prob_times_values (64x2048x2048x1535): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1535): 73.309

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1949.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1536x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1536x2048): 93.070
Elapsed time for attention_prob_times_values (64x2048x2048x1536): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1536): 90.874

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 2298.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1537x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1537x2048): 82.762
Elapsed time for attention_prob_times_values (64x2048x2048x1537): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1537): 68.164

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1870.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1538x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1538x2048): 83.903
Elapsed time for attention_prob_times_values (64x2048x2048x1538): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1538): 75.818

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1993.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1539x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1539x2048): 84.825
Elapsed time for attention_prob_times_values (64x2048x2048x1539): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1539): 70.238

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1924.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1540x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1540x2048): 84.274
Elapsed time for attention_prob_times_values (64x2048x2048x1540): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1540): 74.435

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1981.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1541x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1541x2048): 84.532
Elapsed time for attention_prob_times_values (64x2048x2048x1541): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1541): 70.293

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1924.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1542x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1542x2048): 85.625
Elapsed time for attention_prob_times_values (64x2048x2048x1542): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1542): 75.774

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2017.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1543x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1543x2048): 84.491
Elapsed time for attention_prob_times_values (64x2048x2048x1543): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1543): 70.439

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1929.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1544x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1544x2048): 86.539
Elapsed time for attention_prob_times_values (64x2048x2048x1544): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1544): 81.188

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2104.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Elapsed time for attention_prob_times_values (128x2048x2048x770): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x770): 65.178

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1786.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x771x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x771x2048): 77.332
Elapsed time for attention_prob_times_values (128x2048x2048x771): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x771): 62.188

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1729.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x772x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x772x2048): 79.104
Elapsed time for attention_prob_times_values (128x2048x2048x772): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x772): 65.337

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1798.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x773x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x773x2048): 77.614
Elapsed time for attention_prob_times_values (128x2048x2048x773): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x773): 62.362

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1739.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x774x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x774x2048): 78.436
Elapsed time for attention_prob_times_values (128x2048x2048x774): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x774): 65.294

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1794.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x775x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x775x2048): 75.531
Elapsed time for attention_prob_times_values (128x2048x2048x775): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x775): 62.463

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1724.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x776x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x776x2048): 78.827
Elapsed time for attention_prob_times_values (128x2048x2048x776): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x776): 82.886

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2040.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x777x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x777x2048): 76.376
Elapsed time for attention_prob_times_values (128x2048x2048x777): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x777): 62.346

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1735.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x778x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x778x2048): 77.324
Elapsed time for attention_prob_times_values (128x2048x2048x778): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x778): 65.272

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1791.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x779x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x779x2048): 76.537
Elapsed time for attention_prob_times_values (128x2048x2048x779): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x779): 62.392

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1742.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1545x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1545x2048): 84.140
Elapsed time for attention_prob_times_values (64x2048x2048x1545): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1545): 70.431

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1927.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1546x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1546x2048): 85.294
Elapsed time for attention_prob_times_values (64x2048x2048x1546): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1546): 75.893

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2020.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1547x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1547x2048): 84.263
Elapsed time for attention_prob_times_values (64x2048x2048x1547): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1547): 70.676

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1935.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1548x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1548x2048): 85.831
Elapsed time for attention_prob_times_values (64x2048x2048x1548): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1548): 75.943

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2029.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1549x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1549x2048): 84.197
Elapsed time for attention_prob_times_values (64x2048x2048x1549): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1549): 70.780

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1938.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1550x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1550x2048): 85.475
Elapsed time for attention_prob_times_values (64x2048x2048x1550): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1550): 75.889

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2027.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1551x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1551x2048): 84.265
Elapsed time for attention_prob_times_values (64x2048x2048x1551): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1551): 71.075

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1945.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1552x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1552x2048): 87.152
Elapsed time for attention_prob_times_values (64x2048x2048x1552): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1552): 81.897

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2132.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1553x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1553x2048): 83.824
Elapsed time for attention_prob_times_values (64x2048x2048x1553): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1553): 70.964

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1941.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x780x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x780x2048): 77.957
Elapsed time for attention_prob_times_values (128x2048x2048x780): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x780): 65.689

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1809.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x781x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x781x2048): 74.505
Elapsed time for attention_prob_times_values (128x2048x2048x781): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x781): 59.655

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1683.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x782x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x782x2048): 77.691
Elapsed time for attention_prob_times_values (128x2048x2048x782): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x782): 64.098

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1786.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x783x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x783x2048): 74.634
Elapsed time for attention_prob_times_values (128x2048x2048x783): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x783): 60.462

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1701.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x784x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x784x2048): 79.025
Elapsed time for attention_prob_times_values (128x2048x2048x784): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x784): 84.107

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2077.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x785x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x785x2048): 76.400
Elapsed time for attention_prob_times_values (128x2048x2048x785): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x785): 62.855

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1760.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x786x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x786x2048): 77.230
Elapsed time for attention_prob_times_values (128x2048x2048x786): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x786): 65.997

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1819.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x787x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x787x2048): 76.614
Elapsed time for attention_prob_times_values (128x2048x2048x787): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x787): 62.843

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1767.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x788x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x788x2048): 77.865
Elapsed time for attention_prob_times_values (128x2048x2048x788): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x788): 66.377

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1836.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1554x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1554x2048): 85.042
Elapsed time for attention_prob_times_values (64x2048x2048x1554): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1554): 76.088

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2030.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1555x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1555x2048): 83.976
Elapsed time for attention_prob_times_values (64x2048x2048x1555): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1555): 71.190

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1949.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1556x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1556x2048): 85.402
Elapsed time for attention_prob_times_values (64x2048x2048x1556): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1556): 76.189

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2038.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1557x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1557x2048): 84.183
Elapsed time for attention_prob_times_values (64x2048x2048x1557): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1557): 71.186

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1953.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1558x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1558x2048): 85.269
Elapsed time for attention_prob_times_values (64x2048x2048x1558): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1558): 76.276

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2040.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1559x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1559x2048): 84.346
Elapsed time for attention_prob_times_values (64x2048x2048x1559): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1559): 71.370

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1960.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1560x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1560x2048): 86.608
Elapsed time for attention_prob_times_values (64x2048x2048x1560): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1560): 81.893

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2136.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1561x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1561x2048): 83.866
Elapsed time for attention_prob_times_values (64x2048x2048x1561): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1561): 71.644

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1962.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1562x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1562x2048): 84.965
Elapsed time for attention_prob_times_values (64x2048x2048x1562): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1562): 76.568

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2046.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1563x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1563x2048): 84.059
Elapsed time for attention_prob_times_values (64x2048x2048x1563): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1563): 71.934

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1970.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1564x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1564x2048): 85.722
Elapsed time for attention_prob_times_values (64x2048x2048x1564): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1564): 76.953

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2063.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1565x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1565x2048): 84.187
Elapsed time for attention_prob_times_values (64x2048x2048x1565): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1565): 71.998

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1975.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1566x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1566x2048): 85.305
Elapsed time for attention_prob_times_values (64x2048x2048x1566): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1566): 76.804

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2058.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1567x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1567x2048): 84.289
Elapsed time for attention_prob_times_values (64x2048x2048x1567): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1567): 72.385

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1984.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1568x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1568x2048): 95.155
Elapsed time for attention_prob_times_values (64x2048x2048x1568): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1568): 84.001

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2275.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1569x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1569x2048): 85.698
Elapsed time for attention_prob_times_values (64x2048x2048x1569): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1569): 72.447

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2003.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1570x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1570x2048): 86.895
Elapsed time for attention_prob_times_values (64x2048x2048x1570): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1570): 77.121

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2086.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1571x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1571x2048): 85.449
Elapsed time for attention_prob_times_values (64x2048x2048x1571): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1571): 71.426

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1987.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1572x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1572x2048): 87.255
Elapsed time for attention_prob_times_values (64x2048x2048x1572): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1572): 77.405

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2097.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (128x2048x789x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x789x2048): 76.831
Elapsed time for attention_prob_times_values (128x2048x2048x789): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x789): 63.105

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1777.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x790x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x790x2048): 77.520
Elapsed time for attention_prob_times_values (128x2048x2048x790): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x790): 66.268

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1835.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x791x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x791x2048): 76.936
Elapsed time for attention_prob_times_values (128x2048x2048x791): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x791): 63.233

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1785.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x792x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x792x2048): 76.501
Elapsed time for attention_prob_times_values (128x2048x2048x792): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x792): 84.931

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2072.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x793x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x793x2048): 76.447
Elapsed time for attention_prob_times_values (128x2048x2048x793): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x793): 63.439

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1787.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x794x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x794x2048): 77.026
Elapsed time for attention_prob_times_values (128x2048x2048x794): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x794): 66.546

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1843.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x795x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x795x2048): 76.556
Elapsed time for attention_prob_times_values (128x2048x2048x795): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x795): 63.551

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1794.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x796x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x796x2048): 78.010
Elapsed time for attention_prob_times_values (128x2048x2048x796): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x796): 67.044

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1865.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x797x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x797x2048): 76.531
Elapsed time for attention_prob_times_values (128x2048x2048x797): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x797): 63.677

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1800.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x798x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x798x2048): 77.299
Elapsed time for attention_prob_times_values (128x2048x2048x798): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x798): 66.884

Attention duration (in seconds): 0.0239
========================================================================================================================
num_attention_heads: 16, hidden_size: 25168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1573x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1573x2048): 83.936
Elapsed time for attention_prob_times_values (64x2048x2048x1573): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1573): 70.557

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1960.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1574x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1574x2048): 86.470
Elapsed time for attention_prob_times_values (64x2048x2048x1574): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1574): 77.235

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2088.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1575x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1575x2048): 85.302
Elapsed time for attention_prob_times_values (64x2048x2048x1575): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1575): 72.048

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2000.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1576x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1576x2048): 87.502
Elapsed time for attention_prob_times_values (64x2048x2048x1576): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1576): 83.000

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2183.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1577x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1577x2048): 84.697
Elapsed time for attention_prob_times_values (64x2048x2048x1577): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1577): 72.124

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1997.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1578x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1578x2048): 85.811
Elapsed time for attention_prob_times_values (64x2048x2048x1578): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1578): 75.630

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2062.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1579x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1579x2048): 84.635
Elapsed time for attention_prob_times_values (64x2048x2048x1579): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1579): 72.094

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1998.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1580x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1580x2048): 86.553
Elapsed time for attention_prob_times_values (64x2048x2048x1580): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1580): 77.635

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2102.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1581x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1581x2048): 84.271
Elapsed time for attention_prob_times_values (64x2048x2048x1581): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1581): 71.879

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1994.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1582x2048): 0.0099
Attention throughput (in TFLOP/s): 1860.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x799x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x799x2048): 76.605
Elapsed time for attention_prob_times_values (128x2048x2048x799): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x799): 63.824

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1808.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x800x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x800x2048): 91.231
Elapsed time for attention_prob_times_values (128x2048x2048x800): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x800): 86.574

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2309.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x801x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x801x2048): 79.023
Elapsed time for attention_prob_times_values (128x2048x2048x801): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x801): 64.261

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 1845.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x802x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x802x2048): 79.777
Elapsed time for attention_prob_times_values (128x2048x2048x802): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x802): 67.188

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1901.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x803x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x803x2048): 78.821
Elapsed time for attention_prob_times_values (128x2048x2048x803): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x803): 64.261

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1847.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x804x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x804x2048): 80.379
Elapsed time for attention_prob_times_values (128x2048x2048x804): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x804): 67.607

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1918.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x805x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x805x2048): 78.622
Elapsed time for attention_prob_times_values (128x2048x2048x805): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x805): 64.301

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1850.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x806x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x806x2048): 79.460
Elapsed time for attention_prob_times_values (128x2048x2048x806): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x806): 67.373

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1909.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x807x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x807x2048): 78.471
Elapsed time for attention_prob_times_values (128x2048x2048x807): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x807): 64.526

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 1856.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1582x2048): 86.045
Elapsed time for attention_prob_times_values (64x2048x2048x1582): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1582): 77.577

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2098.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1583x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1583x2048): 85.094
Elapsed time for attention_prob_times_values (64x2048x2048x1583): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1583): 72.384

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2013.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1584x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1584x2048): 87.741
Elapsed time for attention_prob_times_values (64x2048x2048x1584): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1584): 84.023

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2210.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1585x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1585x2048): 84.678
Elapsed time for attention_prob_times_values (64x2048x2048x1585): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1585): 72.057

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2006.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1586x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1586x2048): 82.405
Elapsed time for attention_prob_times_values (64x2048x2048x1586): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1586): 75.378

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2029.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1587x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1587x2048): 84.532
Elapsed time for attention_prob_times_values (64x2048x2048x1587): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1587): 72.015

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2006.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1588x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1588x2048): 86.219
Elapsed time for attention_prob_times_values (64x2048x2048x1588): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1588): 74.586

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2064.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1589x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1589x2048): 81.593
Elapsed time for attention_prob_times_values (64x2048x2048x1589): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1589): 71.716

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1971.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1590x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1590x2048): 85.899
Elapsed time for attention_prob_times_values (64x2048x2048x1590): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1590): 77.834

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2110.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1591x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1591x2048): 81.255
Elapsed time for attention_prob_times_values (64x2048x2048x1591): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1591): 69.296

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1934.298
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x808x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x808x2048): 80.413
Elapsed time for attention_prob_times_values (128x2048x2048x808): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x808): 86.711

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2190.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x809x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x809x2048): 77.718
Elapsed time for attention_prob_times_values (128x2048x2048x809): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x809): 63.494

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 1836.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x810x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x810x2048): 77.301
Elapsed time for attention_prob_times_values (128x2048x2048x810): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x810): 67.564

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1897.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x811x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x811x2048): 77.844
Elapsed time for attention_prob_times_values (128x2048x2048x811): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x811): 63.791

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1847.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x812x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x812x2048): 78.335
Elapsed time for attention_prob_times_values (128x2048x2048x812): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x812): 66.636

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 1899.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x813x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x813x2048): 77.541
Elapsed time for attention_prob_times_values (128x2048x2048x813): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x813): 63.715

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1847.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x814x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x814x2048): 77.599
Elapsed time for attention_prob_times_values (128x2048x2048x814): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x814): 79.299

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2073.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x815x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x815x2048): 77.199
Elapsed time for attention_prob_times_values (128x2048x2048x815): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x815): 78.648

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2062.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x816x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x816x2048): 83.392
Elapsed time for attention_prob_times_values (128x2048x2048x816): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x816): 87.455

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2262.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x817x2048): 0.0114
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1592x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1592x2048): 87.104
Elapsed time for attention_prob_times_values (64x2048x2048x1592): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1592): 83.913

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2211.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1593x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1593x2048): 84.232
Elapsed time for attention_prob_times_values (64x2048x2048x1593): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1593): 72.171

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2012.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1594x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1594x2048): 85.565
Elapsed time for attention_prob_times_values (64x2048x2048x1594): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1594): 74.835

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2068.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1595x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1595x2048): 82.413
Elapsed time for attention_prob_times_values (64x2048x2048x1595): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1595): 72.183

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1994.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1596x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1596x2048): 86.151
Elapsed time for attention_prob_times_values (64x2048x2048x1596): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1596): 78.254

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2127.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1597x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1597x2048): 82.503
Elapsed time for attention_prob_times_values (64x2048x2048x1597): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1597): 70.208

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1968.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1598x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1598x2048): 85.602
Elapsed time for attention_prob_times_values (64x2048x2048x1598): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1598): 74.728

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2072.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1599x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1599x2048): 84.735
Elapsed time for attention_prob_times_values (64x2048x2048x1599): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1599): 72.090

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2024.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1600x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1600x2048): 94.472
Elapsed time for attention_prob_times_values (64x2048x2048x1600): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1600): 83.054

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2298.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x817x2048): 76.645
Elapsed time for attention_prob_times_values (128x2048x2048x817): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x817): 78.787

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2061.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x818x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x818x2048): 78.646
Elapsed time for attention_prob_times_values (128x2048x2048x818): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x818): 79.827

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2104.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x819x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x819x2048): 76.758
Elapsed time for attention_prob_times_values (128x2048x2048x819): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x819): 77.940

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2056.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x820x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x820x2048): 78.304
Elapsed time for attention_prob_times_values (128x2048x2048x820): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x820): 81.457

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2125.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x821x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x821x2048): 80.308
Elapsed time for attention_prob_times_values (128x2048x2048x821): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x821): 78.960

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2122.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x822x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x822x2048): 78.807
Elapsed time for attention_prob_times_values (128x2048x2048x822): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x822): 81.466

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2138.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x823x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x823x2048): 78.314
Elapsed time for attention_prob_times_values (128x2048x2048x823): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x823): 79.179

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2103.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x824x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x824x2048): 80.007
Elapsed time for attention_prob_times_values (128x2048x2048x824): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x824): 88.144

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2243.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x825x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x825x2048): 77.290
Elapsed time for attention_prob_times_values (128x2048x2048x825): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x825): 79.380

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2097.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x826x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x826x2048): 78.069
Elapsed time for attention_prob_times_values (128x2048x2048x826): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x826): 81.848

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2142.681
MLP duration (in seconds): 0.0000
num_attention_heads: 16, hidden_size: 25616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1601x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1601x2048): 82.983
Elapsed time for attention_prob_times_values (64x2048x2048x1601): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1601): 72.223

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2009.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1602x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1602x2048): 87.209
Elapsed time for attention_prob_times_values (64x2048x2048x1602): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1602): 78.221

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2146.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1603x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1603x2048): 81.803
Elapsed time for attention_prob_times_values (64x2048x2048x1603): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1603): 70.300

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1969.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1604x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1604x2048): 87.633
Elapsed time for attention_prob_times_values (64x2048x2048x1604): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1604): 74.861

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2104.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1605x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1605x2048): 85.757
Elapsed time for attention_prob_times_values (64x2048x2048x1605): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1605): 72.462

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2048.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1606x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1606x2048): 86.847
Elapsed time for attention_prob_times_values (64x2048x2048x1606): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1606): 73.887

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2083.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1607x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1607x2048): 81.323
Elapsed time for attention_prob_times_values (64x2048x2048x1607): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1607): 72.729

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2004.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1608x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1608x2048): 87.686
Elapsed time for attention_prob_times_values (64x2048x2048x1608): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1608): 84.458

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2247.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1609x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1609x2048): 82.447
Elapsed time for attention_prob_times_values (64x2048x2048x1609): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1609): 70.859

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1992.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1610x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1610x2048): 86.020
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x827x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x827x2048): 76.164
Elapsed time for attention_prob_times_values (128x2048x2048x827): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x827): 79.554

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2089.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x828x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x828x2048): 79.029
Elapsed time for attention_prob_times_values (128x2048x2048x828): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x828): 80.297

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2140.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x829x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x829x2048): 76.023
Elapsed time for attention_prob_times_values (128x2048x2048x829): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x829): 78.477

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2077.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x830x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x830x2048): 78.237
Elapsed time for attention_prob_times_values (128x2048x2048x830): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x830): 82.237

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2160.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x831x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x831x2048): 77.534
Elapsed time for attention_prob_times_values (128x2048x2048x831): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x831): 79.981

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2123.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x832x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x832x2048): 91.395
Elapsed time for attention_prob_times_values (128x2048x2048x832): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x832): 90.053

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2449.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x833x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x833x2048): 79.351
Elapsed time for attention_prob_times_values (128x2048x2048x833): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x833): 80.208

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2156.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x834x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x834x2048): 80.251
Elapsed time for attention_prob_times_values (128x2048x2048x834): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x834): 82.715

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2204.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x835x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x835x2048): 79.110
Elapsed time for attention_prob_times_values (128x2048x2048x835): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x835): 80.428

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2161.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (64x2048x2048x1610): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1610): 78.399

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2145.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1611x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1611x2048): 85.042
Elapsed time for attention_prob_times_values (64x2048x2048x1611): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1611): 68.248

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1981.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1612x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1612x2048): 84.285
Elapsed time for attention_prob_times_values (64x2048x2048x1612): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1612): 74.961

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2077.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1613x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1613x2048): 82.300
Elapsed time for attention_prob_times_values (64x2048x2048x1613): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1613): 72.957

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2026.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1614x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1614x2048): 86.013
Elapsed time for attention_prob_times_values (64x2048x2048x1614): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1614): 78.611

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2153.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1615x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1615x2048): 82.989
Elapsed time for attention_prob_times_values (64x2048x2048x1615): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1615): 69.504

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1984.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1616x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1616x2048): 83.582
Elapsed time for attention_prob_times_values (64x2048x2048x1616): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1616): 82.031

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2173.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1617x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1617x2048): 84.629
Elapsed time for attention_prob_times_values (64x2048x2048x1617): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1617): 71.992

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2043.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1618x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1618x2048): 85.673
Elapsed time for attention_prob_times_values (64x2048x2048x1618): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1618): 78.529

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2153.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1619x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1619x2048): 84.612
Elapsed time for attention_prob_times_values (64x2048x2048x1619): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1619): 73.441

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2067.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1620x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1620x2048): 86.433
Elapsed time for attention_prob_times_values (64x2048x2048x1620): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1620): 79.077

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2173.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1621x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1621x2048): 84.420
Elapsed time for attention_prob_times_values (64x2048x2048x1621): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1621): 71.679

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2041.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1622x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1622x2048): 85.517
Elapsed time for attention_prob_times_values (64x2048x2048x1622): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1622): 76.617

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2129.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1623x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1623x2048): 84.793
Elapsed time for attention_prob_times_values (64x2048x2048x1623): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1623): 73.684

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2078.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1624x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1624x2048): 87.186
Elapsed time for attention_prob_times_values (64x2048x2048x1624): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1624): 81.270

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2218.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1625x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1625x2048): 81.378
Elapsed time for attention_prob_times_values (64x2048x2048x1625): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1625): 72.319

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2021.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1626x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1626x2048): 85.489
Elapsed time for attention_prob_times_values (64x2048x2048x1626): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1626): 79.098

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2169.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1627x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1627x2048): 81.327
Elapsed time for attention_prob_times_values (64x2048x2048x1627): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1627): 71.463

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2010.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1628x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1628x2048): 86.083
Elapsed time for attention_prob_times_values (64x2048x2048x1628): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1628): 76.798

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2146.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 32, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x836x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x836x2048): 81.095
Elapsed time for attention_prob_times_values (128x2048x2048x836): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x836): 82.933

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2224.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x837x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x837x2048): 79.007
Elapsed time for attention_prob_times_values (128x2048x2048x837): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x837): 80.595

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2166.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x838x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x838x2048): 79.951
Elapsed time for attention_prob_times_values (128x2048x2048x838): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x838): 81.111

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2189.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x839x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x839x2048): 78.600
Elapsed time for attention_prob_times_values (128x2048x2048x839): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x839): 80.829

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2169.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x840x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x840x2048): 80.971
Elapsed time for attention_prob_times_values (128x2048x2048x840): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x840): 89.867

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2321.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x841x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x841x2048): 78.166
Elapsed time for attention_prob_times_values (128x2048x2048x841): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x841): 80.920

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2169.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x842x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x842x2048): 78.827
Elapsed time for attention_prob_times_values (128x2048x2048x842): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x842): 83.437

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2214.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x843x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x843x2048): 78.483
Elapsed time for attention_prob_times_values (128x2048x2048x843): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x843): 81.161

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2182.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x844x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x844x2048): 79.918
Elapsed time for attention_prob_times_values (128x2048x2048x844): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x844): 83.768

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2239.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x845x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x845x2048): 78.413
--------
Elapsed time for attention_key_query_prob (64x2048x1629x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1629x2048): 84.663
Elapsed time for attention_prob_times_values (64x2048x2048x1629): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1629): 73.006

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2074.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1630x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1630x2048): 85.422
Elapsed time for attention_prob_times_values (64x2048x2048x1630): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1630): 75.955

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2128.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1631x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1631x2048): 83.281
Elapsed time for attention_prob_times_values (64x2048x2048x1631): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1631): 74.414

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2081.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1632x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1632x2048): 96.344
Elapsed time for attention_prob_times_values (64x2048x2048x1632): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1632): 87.264

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2426.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1633x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1633x2048): 86.855
Elapsed time for attention_prob_times_values (64x2048x2048x1633): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1633): 72.182

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2090.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1634x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1634x2048): 87.531
Elapsed time for attention_prob_times_values (64x2048x2048x1634): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1634): 79.535

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2211.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1635x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1635x2048): 88.060
Elapsed time for attention_prob_times_values (64x2048x2048x1635): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1635): 71.570

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2096.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1636x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1636x2048): 85.361
Elapsed time for attention_prob_times_values (64x2048x2048x1636): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1636): 76.882

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2148.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1637x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1637x2048): 85.624
Elapsed time for attention_prob_times_values (64x2048x2048x1637): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1637): 73.034

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2095.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1638x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1638x2048): 86.734
Elapsed time for attention_prob_times_values (64x2048x2048x1638): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1638): 79.606

Attention duration (in seconds): 0.0212
Elapsed time for attention_prob_times_values (128x2048x2048x845): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x845): 81.353

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2188.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x846x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x846x2048): 79.363
Elapsed time for attention_prob_times_values (128x2048x2048x846): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x846): 82.531

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2220.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x847x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x847x2048): 76.744
Elapsed time for attention_prob_times_values (128x2048x2048x847): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x847): 81.750

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2174.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x848x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x848x2048): 80.675
Elapsed time for attention_prob_times_values (128x2048x2048x848): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x848): 88.597

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2322.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x849x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x849x2048): 76.491
Elapsed time for attention_prob_times_values (128x2048x2048x849): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x849): 81.950

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2178.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x850x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x850x2048): 78.882
Elapsed time for attention_prob_times_values (128x2048x2048x850): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x850): 84.192

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2244.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x851x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x851x2048): 78.094
Elapsed time for attention_prob_times_values (128x2048x2048x851): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x851): 81.286

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2198.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x852x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x852x2048): 78.853
Elapsed time for attention_prob_times_values (128x2048x2048x852): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x852): 84.454

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2253.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x853x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x853x2048): 78.260
Elapsed time for attention_prob_times_values (128x2048x2048x853): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x853): 82.057

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2215.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x854x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x854x2048): 78.011
Elapsed time for attention_prob_times_values (128x2048x2048x854): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x854): 84.527

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2246.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2207.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1639x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1639x2048): 85.116
Elapsed time for attention_prob_times_values (64x2048x2048x1639): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1639): 71.707

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2071.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1640x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1640x2048): 87.469
Elapsed time for attention_prob_times_values (64x2048x2048x1640): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1640): 80.977

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2239.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1641x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1641x2048): 85.008
Elapsed time for attention_prob_times_values (64x2048x2048x1641): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1641): 73.293

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2097.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1642x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1642x2048): 86.868
Elapsed time for attention_prob_times_values (64x2048x2048x1642): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1642): 79.797

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2217.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1643x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1643x2048): 85.507
Elapsed time for attention_prob_times_values (64x2048x2048x1643): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1643): 72.184

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2087.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1644x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1644x2048): 86.521
Elapsed time for attention_prob_times_values (64x2048x2048x1644): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1644): 79.511

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2211.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1645x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1645x2048): 87.852
Elapsed time for attention_prob_times_values (64x2048x2048x1645): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1645): 72.862

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2127.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1646x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1646x2048): 86.229
Elapsed time for attention_prob_times_values (64x2048x2048x1646): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1646): 80.041

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2218.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1647x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1647x2048): 85.383
Elapsed time for attention_prob_times_values (64x2048x2048x1647): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1647): 73.719

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2115.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x855x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x855x2048): 78.409
Elapsed time for attention_prob_times_values (128x2048x2048x855): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x855): 77.158

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2155.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x856x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x856x2048): 76.684
Elapsed time for attention_prob_times_values (128x2048x2048x856): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x856): 91.639

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2317.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x857x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x857x2048): 77.758
Elapsed time for attention_prob_times_values (128x2048x2048x857): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x857): 82.632

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2225.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x858x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x858x2048): 76.095
Elapsed time for attention_prob_times_values (128x2048x2048x858): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x858): 79.223

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2159.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x859x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x859x2048): 73.665
Elapsed time for attention_prob_times_values (128x2048x2048x859): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x859): 82.821

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2171.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x860x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x860x2048): 79.654
Elapsed time for attention_prob_times_values (128x2048x2048x860): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x860): 82.596

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2260.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x861x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x861x2048): 75.416
Elapsed time for attention_prob_times_values (128x2048x2048x861): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x861): 82.938

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2204.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x862x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x862x2048): 78.799
Elapsed time for attention_prob_times_values (128x2048x2048x862): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x862): 85.171

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2286.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x863x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x863x2048): 73.226
Elapsed time for attention_prob_times_values (128x2048x2048x863): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x863): 83.150

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2178.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
========================================================================================================================
num_attention_heads: 16, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1648x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1648x2048): 87.740
Elapsed time for attention_prob_times_values (64x2048x2048x1648): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1648): 87.126

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2338.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1649x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1649x2048): 84.928
Elapsed time for attention_prob_times_values (64x2048x2048x1649): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1649): 72.483

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2093.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1650x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1650x2048): 85.498
Elapsed time for attention_prob_times_values (64x2048x2048x1650): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1650): 79.642

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2208.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1651x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1651x2048): 84.899
Elapsed time for attention_prob_times_values (64x2048x2048x1651): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1651): 72.324

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2093.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1652x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1652x2048): 86.581
Elapsed time for attention_prob_times_values (64x2048x2048x1652): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1652): 79.993

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2229.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1653x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1653x2048): 85.017
Elapsed time for attention_prob_times_values (64x2048x2048x1653): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1653): 73.578

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2116.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1654x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1654x2048): 85.942
Elapsed time for attention_prob_times_values (64x2048x2048x1654): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1654): 80.446

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2230.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1655x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1655x2048): 84.926
Elapsed time for attention_prob_times_values (64x2048x2048x1655): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1655): 83.653

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2263.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1656x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1656x2048): 86.343
Elapsed time for attention_prob_times_values (64x2048x2048x1656): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1656): 79.995

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2231.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1657x2048): 0.0105
--------
Elapsed time for attention_key_query_prob (128x2048x864x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x864x2048): 92.340
Elapsed time for attention_prob_times_values (128x2048x2048x864): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x864): 90.126

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2554.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x865x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x865x2048): 75.192
Elapsed time for attention_prob_times_values (128x2048x2048x865): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x865): 78.291

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2150.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x866x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x866x2048): 77.586
Elapsed time for attention_prob_times_values (128x2048x2048x866): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x866): 85.540

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2283.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x867x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x867x2048): 78.784
Elapsed time for attention_prob_times_values (128x2048x2048x867): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x867): 82.592

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2265.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x868x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x868x2048): 79.259
Elapsed time for attention_prob_times_values (128x2048x2048x868): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x868): 85.826

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2317.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x869x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x869x2048): 79.329
Elapsed time for attention_prob_times_values (128x2048x2048x869): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x869): 80.122

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2244.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x870x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x870x2048): 78.381
Elapsed time for attention_prob_times_values (128x2048x2048x870): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x870): 85.839

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2309.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x871x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x871x2048): 79.140
Elapsed time for attention_prob_times_values (128x2048x2048x871): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x871): 81.802

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2270.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x872x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x872x2048): 79.275
Elapsed time for attention_prob_times_values (128x2048x2048x872): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x872): 93.151

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2419.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x873x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x873x2048): 78.390
Elapsed time for attention_prob_times_values (128x2048x2048x873): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x873): 82.106

Attention duration (in seconds): 0.0234
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1657x2048): 84.762
Elapsed time for attention_prob_times_values (64x2048x2048x1657): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1657): 84.388

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2274.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1658x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1658x2048): 85.542
Elapsed time for attention_prob_times_values (64x2048x2048x1658): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1658): 85.623

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2302.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1659x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1659x2048): 83.652
Elapsed time for attention_prob_times_values (64x2048x2048x1659): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1659): 84.399

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2262.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1660x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1660x2048): 86.244
Elapsed time for attention_prob_times_values (64x2048x2048x1660): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1660): 87.328

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2337.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1661x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1661x2048): 83.760
Elapsed time for attention_prob_times_values (64x2048x2048x1661): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1661): 84.613

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2269.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1662x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1662x2048): 85.262
Elapsed time for attention_prob_times_values (64x2048x2048x1662): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1662): 87.362

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2327.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1663x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1663x2048): 84.314
Elapsed time for attention_prob_times_values (64x2048x2048x1663): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1663): 83.730

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2267.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1664x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1664x2048): 92.743
Elapsed time for attention_prob_times_values (64x2048x2048x1664): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1664): 90.748

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2476.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1665x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1665x2048): 85.424
Elapsed time for attention_prob_times_values (64x2048x2048x1665): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1665): 79.795

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2229.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1666x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1666x2048): 85.882
Elapsed time for attention_prob_times_values (64x2048x2048x1666): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1666): 82.221

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2270.940
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 2268.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x874x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x874x2048): 77.762
Elapsed time for attention_prob_times_values (128x2048x2048x874): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x874): 86.168

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2314.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x875x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x875x2048): 78.667
Elapsed time for attention_prob_times_values (128x2048x2048x875): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x875): 81.370

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2267.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x876x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x876x2048): 78.057
Elapsed time for attention_prob_times_values (128x2048x2048x876): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x876): 86.426

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2327.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x877x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x877x2048): 78.691
Elapsed time for attention_prob_times_values (128x2048x2048x877): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x877): 83.630

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2303.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x878x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x878x2048): 77.601
Elapsed time for attention_prob_times_values (128x2048x2048x878): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x878): 86.426

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2325.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x879x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x879x2048): 78.753
Elapsed time for attention_prob_times_values (128x2048x2048x879): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x879): 82.937

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2300.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x880x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x880x2048): 80.516
Elapsed time for attention_prob_times_values (128x2048x2048x880): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x880): 94.327

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2475.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x881x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x881x2048): 78.212
Elapsed time for attention_prob_times_values (128x2048x2048x881): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x881): 80.988

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2270.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x882x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x882x2048): 75.504
Elapsed time for attention_prob_times_values (128x2048x2048x882): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x882): 83.655

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2267.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1667x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1667x2048): 85.663
Elapsed time for attention_prob_times_values (64x2048x2048x1667): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1667): 80.337

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2242.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1668x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1668x2048): 87.418
Elapsed time for attention_prob_times_values (64x2048x2048x1668): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1668): 82.668

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2299.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1669x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1669x2048): 85.639
Elapsed time for attention_prob_times_values (64x2048x2048x1669): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1669): 80.190

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2242.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1670x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1670x2048): 86.978
Elapsed time for attention_prob_times_values (64x2048x2048x1670): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1670): 82.663

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2296.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1671x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1671x2048): 85.607
Elapsed time for attention_prob_times_values (64x2048x2048x1671): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1671): 80.466

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2248.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1672x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1672x2048): 87.728
Elapsed time for attention_prob_times_values (64x2048x2048x1672): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1672): 80.945

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2283.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1673x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1673x2048): 85.119
Elapsed time for attention_prob_times_values (64x2048x2048x1673): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1673): 80.457

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2245.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1674x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1674x2048): 86.217
Elapsed time for attention_prob_times_values (64x2048x2048x1674): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1674): 82.792

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2293.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1675x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1675x2048): 85.286
Elapsed time for attention_prob_times_values (64x2048x2048x1675): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1675): 80.566

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2251.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1676x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1676x2048): 86.795
Elapsed time for attention_prob_times_values (64x2048x2048x1676): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1676): 82.991

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2306.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1677x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1677x2048): 85.328
Elapsed time for attention_prob_times_values (64x2048x2048x1677): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1677): 80.507

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2253.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1678x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1678x2048): 86.483
Elapsed time for attention_prob_times_values (64x2048x2048x1678): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1678): 82.975

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2305.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1679x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1679x2048): 85.356
Elapsed time for attention_prob_times_values (64x2048x2048x1679): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1679): 80.646

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2258.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1680x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1680x2048): 88.152
Elapsed time for attention_prob_times_values (64x2048x2048x1680): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1680): 82.313

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2319.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1681x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1681x2048): 84.636
Elapsed time for attention_prob_times_values (64x2048x2048x1681): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1681): 80.887

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2255.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1682x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1682x2048): 85.951
Elapsed time for attention_prob_times_values (64x2048x2048x1682): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1682): 83.011

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2304.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1683x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1683x2048): 84.961
Elapsed time for attention_prob_times_values (64x2048x2048x1683): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1683): 80.944

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2263.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1684x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1684x2048): 86.420
Elapsed time for attention_prob_times_values (64x2048x2048x1684): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1684): 83.274

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2316.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1685x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1685x2048): 84.963
========================================================================================================================
num_attention_heads: 32, hidden_size: 28256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x883x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x883x2048): 74.705
Elapsed time for attention_prob_times_values (128x2048x2048x883): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x883): 84.443

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2266.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x884x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x884x2048): 77.194
Elapsed time for attention_prob_times_values (128x2048x2048x884): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x884): 81.828

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2274.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x885x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x885x2048): 74.668
Elapsed time for attention_prob_times_values (128x2048x2048x885): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x885): 84.559

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2272.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x886x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x886x2048): 76.216
Elapsed time for attention_prob_times_values (128x2048x2048x886): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x886): 83.845

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2290.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x887x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x887x2048): 73.623
Elapsed time for attention_prob_times_values (128x2048x2048x887): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x887): 84.658

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2261.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x888x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x888x2048): 75.289
Elapsed time for attention_prob_times_values (128x2048x2048x888): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x888): 89.560

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2351.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x889x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x889x2048): 77.615
Elapsed time for attention_prob_times_values (128x2048x2048x889): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x889): 84.824

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2332.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x890x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x890x2048): 78.683
Elapsed time for attention_prob_times_values (128x2048x2048x890): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x890): 86.701

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2376.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x891x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x891x2048): 77.294
Elapsed time for attention_prob_times_values (128x2048x2048x891): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x891): 84.979

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2335.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x892x2048): 0.0121
Elapsed time for attention_prob_times_values (64x2048x2048x1685): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1685): 81.014

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2266.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1686x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1686x2048): 86.246
Elapsed time for attention_prob_times_values (64x2048x2048x1686): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1686): 83.254

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2316.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 26992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1687x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1687x2048): 85.167
Elapsed time for attention_prob_times_values (64x2048x2048x1687): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1687): 81.100

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2273.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1688x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1688x2048): 87.182
Elapsed time for attention_prob_times_values (64x2048x2048x1688): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1688): 81.811

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2310.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1689x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1689x2048): 84.785
Elapsed time for attention_prob_times_values (64x2048x2048x1689): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1689): 81.208

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2272.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1690x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1690x2048): 84.733
Elapsed time for attention_prob_times_values (64x2048x2048x1690): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1690): 83.331

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2302.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1691x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1691x2048): 84.821
Elapsed time for attention_prob_times_values (64x2048x2048x1691): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1691): 81.217

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2275.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1692x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1692x2048): 83.909
Elapsed time for attention_prob_times_values (64x2048x2048x1692): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1692): 80.527

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2254.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1693x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1693x2048): 84.767
Elapsed time for attention_prob_times_values (64x2048x2048x1693): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1693): 81.287

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2278.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1694x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1694x2048): 86.033
Elapsed time for attention_prob_times_values (64x2048x2048x1694): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1694): 83.073

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2321.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x892x2048): 79.429
Elapsed time for attention_prob_times_values (128x2048x2048x892): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x892): 86.414

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2390.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x893x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x893x2048): 77.728
Elapsed time for attention_prob_times_values (128x2048x2048x893): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x893): 85.132

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2348.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x894x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x894x2048): 78.734
Elapsed time for attention_prob_times_values (128x2048x2048x894): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x894): 87.025

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2392.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x895x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x895x2048): 77.028
Elapsed time for attention_prob_times_values (128x2048x2048x895): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x895): 85.263

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2344.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x896x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x896x2048): 88.472
Elapsed time for attention_prob_times_values (128x2048x2048x896): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x896): 97.066

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2684.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x897x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x897x2048): 78.105
Elapsed time for attention_prob_times_values (128x2048x2048x897): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x897): 74.145

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2208.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x898x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x898x2048): 80.436
Elapsed time for attention_prob_times_values (128x2048x2048x898): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x898): 74.540

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2248.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x899x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x899x2048): 72.881
Elapsed time for attention_prob_times_values (128x2048x2048x899): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x899): 72.426

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2113.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x900x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x900x2048): 81.157
Elapsed time for attention_prob_times_values (128x2048x2048x900): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x900): 74.866

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2268.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x901x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x901x2048): 75.441
Elapsed time for attention_prob_times_values (128x2048x2048x901): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x901): 76.969

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2221.632
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1695x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1695x2048): 84.572
Elapsed time for attention_prob_times_values (64x2048x2048x1695): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1695): 81.380

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2279.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1696x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1696x2048): 97.036
Elapsed time for attention_prob_times_values (64x2048x2048x1696): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1696): 84.328

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2481.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1697x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1697x2048): 82.659
Elapsed time for attention_prob_times_values (64x2048x2048x1697): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1697): 80.340

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2242.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1698x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1698x2048): 87.262
Elapsed time for attention_prob_times_values (64x2048x2048x1698): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1698): 83.806

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2353.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1699x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1699x2048): 86.056
Elapsed time for attention_prob_times_values (64x2048x2048x1699): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1699): 81.081

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2300.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1700x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1700x2048): 87.689
Elapsed time for attention_prob_times_values (64x2048x2048x1700): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1700): 83.816

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2362.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1701x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1701x2048): 85.803
Elapsed time for attention_prob_times_values (64x2048x2048x1701): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1701): 81.541

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2306.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1702x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1702x2048): 86.580
Elapsed time for attention_prob_times_values (64x2048x2048x1702): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1702): 83.679

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2348.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1703x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1703x2048): 85.759
Elapsed time for attention_prob_times_values (64x2048x2048x1703): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1703): 81.573

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2308.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x902x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x902x2048): 80.358
Elapsed time for attention_prob_times_values (128x2048x2048x902): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x902): 75.488

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2272.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x903x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x903x2048): 74.978
Elapsed time for attention_prob_times_values (128x2048x2048x903): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x903): 77.128

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2221.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x904x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x904x2048): 81.060
Elapsed time for attention_prob_times_values (128x2048x2048x904): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x904): 82.916

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2397.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x905x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x905x2048): 78.416
Elapsed time for attention_prob_times_values (128x2048x2048x905): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x905): 77.356

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2280.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x906x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x906x2048): 79.972
Elapsed time for attention_prob_times_values (128x2048x2048x906): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x906): 78.897

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2328.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x907x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x907x2048): 75.112
Elapsed time for attention_prob_times_values (128x2048x2048x907): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x907): 77.539

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2239.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x908x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x908x2048): 80.566
Elapsed time for attention_prob_times_values (128x2048x2048x908): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x908): 74.747

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2277.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x909x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x909x2048): 76.317
Elapsed time for attention_prob_times_values (128x2048x2048x909): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x909): 77.651

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2263.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x910x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x910x2048): 80.024
Elapsed time for attention_prob_times_values (128x2048x2048x910): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x910): 75.924

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2293.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (64x2048x1704x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1704x2048): 88.004
Elapsed time for attention_prob_times_values (64x2048x2048x1704): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1704): 82.043

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2345.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1705x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1705x2048): 85.309
Elapsed time for attention_prob_times_values (64x2048x2048x1705): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1705): 81.660

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2306.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1706x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1706x2048): 86.376
Elapsed time for attention_prob_times_values (64x2048x2048x1706): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1706): 84.223

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2358.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1707x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1707x2048): 85.464
Elapsed time for attention_prob_times_values (64x2048x2048x1707): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1707): 81.789

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2312.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1708x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1708x2048): 87.321
Elapsed time for attention_prob_times_values (64x2048x2048x1708): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1708): 84.329

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2375.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1709x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1709x2048): 85.355
Elapsed time for attention_prob_times_values (64x2048x2048x1709): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1709): 81.756

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2313.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1710x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1710x2048): 86.633
Elapsed time for attention_prob_times_values (64x2048x2048x1710): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1710): 84.278

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2368.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1711x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1711x2048): 85.897
Elapsed time for attention_prob_times_values (64x2048x2048x1711): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1711): 81.791

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2323.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1712x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1712x2048): 87.715
Elapsed time for attention_prob_times_values (64x2048x2048x1712): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1712): 83.854

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2379.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1713x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1713x2048): 85.301
Elapsed time for attention_prob_times_values (64x2048x2048x1713): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1713): 82.066

Attention duration (in seconds): 0.0220
num_attention_heads: 32, hidden_size: 29152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x911x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x911x2048): 75.466
Elapsed time for attention_prob_times_values (128x2048x2048x911): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x911): 77.816

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2257.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x912x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x912x2048): 81.426
Elapsed time for attention_prob_times_values (128x2048x2048x912): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x912): 84.597

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2447.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x913x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x913x2048): 78.770
Elapsed time for attention_prob_times_values (128x2048x2048x913): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x913): 78.088

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2316.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x914x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x914x2048): 80.023
Elapsed time for attention_prob_times_values (128x2048x2048x914): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x914): 77.058

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2321.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x915x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x915x2048): 75.019
Elapsed time for attention_prob_times_values (128x2048x2048x915): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x915): 78.220

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2266.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x916x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x916x2048): 80.203
Elapsed time for attention_prob_times_values (128x2048x2048x916): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x916): 77.052

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2328.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x917x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x917x2048): 76.782
Elapsed time for attention_prob_times_values (128x2048x2048x917): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x917): 78.306

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2299.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x918x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x918x2048): 79.833
Elapsed time for attention_prob_times_values (128x2048x2048x918): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x918): 78.941

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2356.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x919x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x919x2048): 78.488
Elapsed time for attention_prob_times_values (128x2048x2048x919): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x919): 78.502

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2332.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x920x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x920x2048): 80.197
Attention throughput (in TFLOP/s): 2322.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1714x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1714x2048): 86.253
Elapsed time for attention_prob_times_values (64x2048x2048x1714): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1714): 84.420

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2370.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1715x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1715x2048): 85.303
Elapsed time for attention_prob_times_values (64x2048x2048x1715): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1715): 82.146

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2326.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1716x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1716x2048): 87.239
Elapsed time for attention_prob_times_values (64x2048x2048x1716): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1716): 84.735

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2391.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1717x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1717x2048): 85.318
Elapsed time for attention_prob_times_values (64x2048x2048x1717): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1717): 82.023

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2327.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1718x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1718x2048): 86.354
Elapsed time for attention_prob_times_values (64x2048x2048x1718): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1718): 84.615

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2379.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1719x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1719x2048): 85.481
Elapsed time for attention_prob_times_values (64x2048x2048x1719): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1719): 81.442

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2323.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1720x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1720x2048): 87.524
Elapsed time for attention_prob_times_values (64x2048x2048x1720): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1720): 82.921

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2373.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1721x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1721x2048): 85.202
Elapsed time for attention_prob_times_values (64x2048x2048x1721): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1721): 82.360

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2336.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1722x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1722x2048): 86.175
Elapsed time for attention_prob_times_values (64x2048x2048x1722): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1722): 84.762

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2384.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1723x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1723x2048): 85.408
Elapsed time for attention_prob_times_values (64x2048x2048x1723): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1723): 82.571

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2344.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1724x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1724x2048): 86.240
Elapsed time for attention_prob_times_values (64x2048x2048x1724): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1724): 83.205

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2366.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1725x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1725x2048): 85.543
Elapsed time for attention_prob_times_values (64x2048x2048x1725): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1725): 82.558

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2348.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1726x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1726x2048): 85.805
Elapsed time for attention_prob_times_values (64x2048x2048x1726): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1726): 84.288

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2378.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1727x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1727x2048): 85.541
Elapsed time for attention_prob_times_values (64x2048x2048x1727): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1727): 82.694

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2353.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1728x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1728x2048): 96.257
Elapsed time for attention_prob_times_values (64x2048x2048x1728): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1728): 86.460

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2550.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1729x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1729x2048): 85.510
Elapsed time for attention_prob_times_values (64x2048x2048x1729): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1729): 82.660

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2355.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1730x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1730x2048): 87.842
Elapsed time for attention_prob_times_values (64x2048x2048x1730): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1730): 85.264

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2425.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1731x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1731x2048): 86.664
Elapsed time for attention_prob_times_values (64x2048x2048x1731): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1731): 82.731

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2374.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1732x2048): 0.0105
Elapsed time for attention_prob_times_values (128x2048x2048x920): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x920): 85.616

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2463.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x921x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x921x2048): 76.875
Elapsed time for attention_prob_times_values (128x2048x2048x921): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x921): 78.708

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2316.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x922x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x922x2048): 77.913
Elapsed time for attention_prob_times_values (128x2048x2048x922): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x922): 79.728

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2349.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x923x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x923x2048): 76.669
Elapsed time for attention_prob_times_values (128x2048x2048x923): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x923): 78.867

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2320.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x924x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x924x2048): 80.093
Elapsed time for attention_prob_times_values (128x2048x2048x924): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x924): 81.095

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2407.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x925x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x925x2048): 78.874
Elapsed time for attention_prob_times_values (128x2048x2048x925): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x925): 78.715

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2356.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x926x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x926x2048): 79.479
Elapsed time for attention_prob_times_values (128x2048x2048x926): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x926): 81.122

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2403.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x927x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x927x2048): 79.135
Elapsed time for attention_prob_times_values (128x2048x2048x927): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x927): 79.006

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2369.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x928x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x928x2048): 92.437
Elapsed time for attention_prob_times_values (128x2048x2048x928): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x928): 87.328

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2694.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x929x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x929x2048): 80.949
Elapsed time for attention_prob_times_values (128x2048x2048x929): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x929): 79.173

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2404.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1732x2048): 88.278
Elapsed time for attention_prob_times_values (64x2048x2048x1732): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1732): 85.377

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2435.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1733x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1733x2048): 86.671
Elapsed time for attention_prob_times_values (64x2048x2048x1733): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1733): 82.881

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2379.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1734x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1734x2048): 87.516
Elapsed time for attention_prob_times_values (64x2048x2048x1734): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1734): 85.278

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2426.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1735x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1735x2048): 86.384
Elapsed time for attention_prob_times_values (64x2048x2048x1735): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1735): 83.012

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2379.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1736x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1736x2048): 88.602
Elapsed time for attention_prob_times_values (64x2048x2048x1736): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1736): 83.696

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2420.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1737x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1737x2048): 85.974
Elapsed time for attention_prob_times_values (64x2048x2048x1737): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1737): 83.194

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2379.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1738x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1738x2048): 86.958
Elapsed time for attention_prob_times_values (64x2048x2048x1738): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1738): 85.478

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2427.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1739x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1739x2048): 86.039
Elapsed time for attention_prob_times_values (64x2048x2048x1739): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1739): 83.161

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2382.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1740x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1740x2048): 87.601
Elapsed time for attention_prob_times_values (64x2048x2048x1740): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1740): 85.680

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2441.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1741x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1741x2048): 82.444
Elapsed time for attention_prob_times_values (64x2048x2048x1741): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1741): 79.411

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2281.615
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x930x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x930x2048): 81.471
Elapsed time for attention_prob_times_values (128x2048x2048x930): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x930): 81.427

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2448.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x931x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x931x2048): 80.719
Elapsed time for attention_prob_times_values (128x2048x2048x931): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x931): 79.269

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2407.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x932x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x932x2048): 82.235
Elapsed time for attention_prob_times_values (128x2048x2048x932): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x932): 81.621

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2468.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x933x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x933x2048): 80.619
Elapsed time for attention_prob_times_values (128x2048x2048x933): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x933): 79.325

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2411.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x934x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x934x2048): 81.218
Elapsed time for attention_prob_times_values (128x2048x2048x934): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x934): 81.644

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2458.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x935x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x935x2048): 80.305
Elapsed time for attention_prob_times_values (128x2048x2048x935): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x935): 79.204

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2409.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x936x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x936x2048): 82.080
Elapsed time for attention_prob_times_values (128x2048x2048x936): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x936): 87.501

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2562.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 29984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x937x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x937x2048): 79.729
Elapsed time for attention_prob_times_values (128x2048x2048x937): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x937): 79.188

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2406.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x938x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x938x2048): 80.062
Elapsed time for attention_prob_times_values (128x2048x2048x938): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x938): 81.984

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2455.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1742x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1742x2048): 87.088
Elapsed time for attention_prob_times_values (64x2048x2048x1742): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1742): 85.647

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2437.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1743x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1743x2048): 86.163
Elapsed time for attention_prob_times_values (64x2048x2048x1743): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1743): 79.763

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2338.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1744x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1744x2048): 84.237
Elapsed time for attention_prob_times_values (64x2048x2048x1744): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1744): 85.254

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2393.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1745x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1745x2048): 85.660
Elapsed time for attention_prob_times_values (64x2048x2048x1745): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1745): 83.466

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2389.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1746x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1746x2048): 81.843
Elapsed time for attention_prob_times_values (64x2048x2048x1746): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1746): 81.717

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2312.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1747x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1747x2048): 85.739
Elapsed time for attention_prob_times_values (64x2048x2048x1747): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1747): 83.611

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2395.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1748x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1748x2048): 87.104
Elapsed time for attention_prob_times_values (64x2048x2048x1748): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1748): 81.983

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2391.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 27984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1749x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1749x2048): 81.811
Elapsed time for attention_prob_times_values (64x2048x2048x1749): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1749): 83.660

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2343.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1750x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1750x2048): 86.554
Elapsed time for attention_prob_times_values (64x2048x2048x1750): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1750): 85.881

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2443.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (128x2048x939x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x939x2048): 79.909
Elapsed time for attention_prob_times_values (128x2048x2048x939): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x939): 79.277

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2415.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x940x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x940x2048): 81.865
Elapsed time for attention_prob_times_values (128x2048x2048x940): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x940): 82.229

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2492.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x941x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x941x2048): 80.126
Elapsed time for attention_prob_times_values (128x2048x2048x941): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x941): 77.858

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2401.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x942x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x942x2048): 78.919
Elapsed time for attention_prob_times_values (128x2048x2048x942): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x942): 82.215

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2451.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x943x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x943x2048): 80.088
Elapsed time for attention_prob_times_values (128x2048x2048x943): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x943): 76.662

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2386.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x944x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x944x2048): 77.367
Elapsed time for attention_prob_times_values (128x2048x2048x944): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x944): 88.527

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2518.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x945x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x945x2048): 79.676
Elapsed time for attention_prob_times_values (128x2048x2048x945): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x945): 75.768

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2371.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x946x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x946x2048): 77.300
Elapsed time for attention_prob_times_values (128x2048x2048x946): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x946): 82.531

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2439.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x947x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x947x2048): 79.833
Elapsed time for attention_prob_times_values (128x2048x2048x947): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x947): 76.894

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2396.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x948x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x948x2048): 77.195
Elapsed time for attention_prob_times_values (128x2048x2048x948): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x948): 82.776

Attention duration (in seconds): 0.0255
num_attention_heads: 16, hidden_size: 28016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1751x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1751x2048): 83.156
Elapsed time for attention_prob_times_values (64x2048x2048x1751): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1751): 81.850

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2339.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1752x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1752x2048): 87.916
Elapsed time for attention_prob_times_values (64x2048x2048x1752): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1752): 84.311

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2442.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1753x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1753x2048): 85.521
Elapsed time for attention_prob_times_values (64x2048x2048x1753): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1753): 83.820

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2403.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1754x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1754x2048): 86.350
Elapsed time for attention_prob_times_values (64x2048x2048x1754): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1754): 86.085

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2449.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1755x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1755x2048): 85.571
Elapsed time for attention_prob_times_values (64x2048x2048x1755): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1755): 83.874

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2407.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1756x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1756x2048): 87.156
Elapsed time for attention_prob_times_values (64x2048x2048x1756): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1756): 86.172

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2464.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1757x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1757x2048): 85.559
Elapsed time for attention_prob_times_values (64x2048x2048x1757): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1757): 83.909

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2410.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1758x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1758x2048): 86.608
Elapsed time for attention_prob_times_values (64x2048x2048x1758): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1758): 86.082

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2458.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1759x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1759x2048): 85.361
Elapsed time for attention_prob_times_values (64x2048x2048x1759): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1759): 84.039

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2412.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1760x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1760x2048): 96.224
Attention throughput (in TFLOP/s): 2446.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x949x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x949x2048): 80.066
Elapsed time for attention_prob_times_values (128x2048x2048x949): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x949): 77.467

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2414.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x950x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x950x2048): 76.662
Elapsed time for attention_prob_times_values (128x2048x2048x950): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x950): 82.808

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2443.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x951x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x951x2048): 79.977
Elapsed time for attention_prob_times_values (128x2048x2048x951): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x951): 78.084

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2427.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x952x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x952x2048): 79.685
Elapsed time for attention_prob_times_values (128x2048x2048x952): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x952): 88.894

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2584.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x953x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x953x2048): 79.637
Elapsed time for attention_prob_times_values (128x2048x2048x953): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x953): 79.253

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2445.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x954x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x954x2048): 77.289
Elapsed time for attention_prob_times_values (128x2048x2048x954): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x954): 83.002

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2466.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x955x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x955x2048): 77.137
Elapsed time for attention_prob_times_values (128x2048x2048x955): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x955): 78.073

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2393.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x956x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x956x2048): 81.123
Elapsed time for attention_prob_times_values (128x2048x2048x956): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x956): 79.976

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2486.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x957x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x957x2048): 75.600
Elapsed time for attention_prob_times_values (128x2048x2048x957): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x957): 81.153

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2419.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (64x2048x2048x1760): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1760): 87.539

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2612.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1761x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1761x2048): 86.576
Elapsed time for attention_prob_times_values (64x2048x2048x1761): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1761): 83.894

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2429.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1762x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1762x2048): 89.043
Elapsed time for attention_prob_times_values (64x2048x2048x1762): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1762): 86.489

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2503.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1763x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1763x2048): 87.092
Elapsed time for attention_prob_times_values (64x2048x2048x1763): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1763): 83.754

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2437.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1764x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1764x2048): 89.246
Elapsed time for attention_prob_times_values (64x2048x2048x1764): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1764): 86.540

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2509.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1765x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1765x2048): 86.352
Elapsed time for attention_prob_times_values (64x2048x2048x1765): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1765): 84.083

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2434.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1766x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1766x2048): 86.907
Elapsed time for attention_prob_times_values (64x2048x2048x1766): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1766): 86.559

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2480.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1767x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1767x2048): 86.287
Elapsed time for attention_prob_times_values (64x2048x2048x1767): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1767): 84.013

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2435.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1768x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1768x2048): 87.527
Elapsed time for attention_prob_times_values (64x2048x2048x1768): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1768): 79.640

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2387.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1769x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1769x2048): 85.865
Elapsed time for attention_prob_times_values (64x2048x2048x1769): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1769): 84.297

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2436.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1770x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1770x2048): 86.052
Elapsed time for attention_prob_times_values (64x2048x2048x1770): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1770): 86.056

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2465.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1771x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1771x2048): 85.978
Elapsed time for attention_prob_times_values (64x2048x2048x1771): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1771): 84.322

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2441.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1772x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1772x2048): 87.445
Elapsed time for attention_prob_times_values (64x2048x2048x1772): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1772): 83.861

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2456.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1773x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1773x2048): 83.306
Elapsed time for attention_prob_times_values (64x2048x2048x1773): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1773): 84.257

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2404.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1774x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1774x2048): 86.902
Elapsed time for attention_prob_times_values (64x2048x2048x1774): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1774): 86.877

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2495.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1775x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1775x2048): 85.392
Elapsed time for attention_prob_times_values (64x2048x2048x1775): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1775): 84.134

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2435.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1776x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1776x2048): 88.480
Elapsed time for attention_prob_times_values (64x2048x2048x1776): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1776): 86.884

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2520.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1777x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1777x2048): 85.660
Elapsed time for attention_prob_times_values (64x2048x2048x1777): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1777): 84.001

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2439.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1778x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1778x2048): 86.760
Elapsed time for attention_prob_times_values (64x2048x2048x1778): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1778): 86.976

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2500.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
========================================================================================================================
num_attention_heads: 32, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x958x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x958x2048): 80.617
Elapsed time for attention_prob_times_values (128x2048x2048x958): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x958): 81.086

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2501.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x959x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x959x2048): 78.380
Elapsed time for attention_prob_times_values (128x2048x2048x959): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x959): 81.192

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2470.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x960x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x960x2048): 93.074
Elapsed time for attention_prob_times_values (128x2048x2048x960): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x960): 90.347

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2842.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x961x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x961x2048): 80.853
Elapsed time for attention_prob_times_values (128x2048x2048x961): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x961): 81.299

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2515.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x962x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x962x2048): 82.300
Elapsed time for attention_prob_times_values (128x2048x2048x962): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x962): 78.695

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2499.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x963x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x963x2048): 80.197
Elapsed time for attention_prob_times_values (128x2048x2048x963): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x963): 81.403

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2512.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x964x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x964x2048): 82.979
Elapsed time for attention_prob_times_values (128x2048x2048x964): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x964): 82.848

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2580.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x965x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x965x2048): 80.280
Elapsed time for attention_prob_times_values (128x2048x2048x965): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x965): 81.702

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2523.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x966x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x966x2048): 81.919
Elapsed time for attention_prob_times_values (128x2048x2048x966): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x966): 83.365

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2577.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x967x2048): 0.0129
--------
Elapsed time for attention_key_query_prob (64x2048x1779x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1779x2048): 85.775
Elapsed time for attention_prob_times_values (64x2048x2048x1779): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1779): 84.601

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2453.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1780x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1780x2048): 86.660
Elapsed time for attention_prob_times_values (64x2048x2048x1780): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1780): 87.214

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2504.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1781x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1781x2048): 85.860
Elapsed time for attention_prob_times_values (64x2048x2048x1781): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1781): 84.703

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2458.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1782x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1782x2048): 86.662
Elapsed time for attention_prob_times_values (64x2048x2048x1782): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1782): 85.945

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2489.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1783x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1783x2048): 85.870
Elapsed time for attention_prob_times_values (64x2048x2048x1783): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1783): 84.732

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2461.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1784x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1784x2048): 87.743
Elapsed time for attention_prob_times_values (64x2048x2048x1784): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1784): 79.188

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2403.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1785x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1785x2048): 84.123
Elapsed time for attention_prob_times_values (64x2048x2048x1785): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1785): 84.797

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2440.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1786x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1786x2048): 86.381
Elapsed time for attention_prob_times_values (64x2048x2048x1786): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1786): 87.235

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2509.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1787x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1787x2048): 85.598
Elapsed time for attention_prob_times_values (64x2048x2048x1787): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1787): 84.953

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2466.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1788x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1788x2048): 87.080
Elapsed time for attention_prob_times_values (64x2048x2048x1788): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1788): 87.483

Attention duration (in seconds): 0.0220
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x967x2048): 80.328
Elapsed time for attention_prob_times_values (128x2048x2048x967): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x967): 81.921

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2532.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x968x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x968x2048): 82.900
Elapsed time for attention_prob_times_values (128x2048x2048x968): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x968): 85.413

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2629.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x969x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x969x2048): 75.735
Elapsed time for attention_prob_times_values (128x2048x2048x969): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x969): 82.082

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2464.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x970x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x970x2048): 81.155
Elapsed time for attention_prob_times_values (128x2048x2048x970): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x970): 83.715

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2580.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x971x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x971x2048): 80.011
Elapsed time for attention_prob_times_values (128x2048x2048x971): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x971): 82.224

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2542.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x972x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x972x2048): 82.043
Elapsed time for attention_prob_times_values (128x2048x2048x972): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x972): 79.912

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2540.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x973x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x973x2048): 79.964
Elapsed time for attention_prob_times_values (128x2048x2048x973): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x973): 82.395

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2548.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x974x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x974x2048): 81.537
Elapsed time for attention_prob_times_values (128x2048x2048x974): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x974): 83.545

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2594.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x975x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x975x2048): 80.068
Elapsed time for attention_prob_times_values (128x2048x2048x975): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x975): 82.511

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2557.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x976x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x976x2048): 82.622
Elapsed time for attention_prob_times_values (128x2048x2048x976): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x976): 91.307

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2732.557
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 2525.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1789x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1789x2048): 85.284
Elapsed time for attention_prob_times_values (64x2048x2048x1789): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1789): 85.039

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2465.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1790x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1790x2048): 86.470
Elapsed time for attention_prob_times_values (64x2048x2048x1790): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1790): 87.554

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2520.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1791x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1791x2048): 85.528
Elapsed time for attention_prob_times_values (64x2048x2048x1791): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1791): 85.096

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2472.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1792x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1792x2048): 93.834
Elapsed time for attention_prob_times_values (64x2048x2048x1792): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1792): 90.702

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2674.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1793x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1793x2048): 86.252
Elapsed time for attention_prob_times_values (64x2048x2048x1793): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1793): 80.624

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2418.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1794x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1794x2048): 87.503
Elapsed time for attention_prob_times_values (64x2048x2048x1794): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1794): 83.269

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2477.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1795x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1795x2048): 85.989
Elapsed time for attention_prob_times_values (64x2048x2048x1795): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1795): 80.950

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2422.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1796x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1796x2048): 88.081
Elapsed time for attention_prob_times_values (64x2048x2048x1796): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1796): 83.431

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2490.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1797x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1797x2048): 86.495
Elapsed time for attention_prob_times_values (64x2048x2048x1797): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1797): 80.982

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2432.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x977x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x977x2048): 79.411
Elapsed time for attention_prob_times_values (128x2048x2048x977): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x977): 82.800

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2556.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x978x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x978x2048): 80.879
Elapsed time for attention_prob_times_values (128x2048x2048x978): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x978): 82.352

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2575.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x979x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x979x2048): 79.511
Elapsed time for attention_prob_times_values (128x2048x2048x979): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x979): 82.926

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2564.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x980x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x980x2048): 81.802
Elapsed time for attention_prob_times_values (128x2048x2048x980): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x980): 84.987

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2636.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x981x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x981x2048): 80.088
Elapsed time for attention_prob_times_values (128x2048x2048x981): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x981): 82.923

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2579.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x982x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x982x2048): 81.082
Elapsed time for attention_prob_times_values (128x2048x2048x982): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x982): 84.869

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2627.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x983x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x983x2048): 79.820
Elapsed time for attention_prob_times_values (128x2048x2048x983): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x983): 82.997

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2581.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x984x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x984x2048): 82.349
Elapsed time for attention_prob_times_values (128x2048x2048x984): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x984): 91.233

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2748.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x985x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x985x2048): 79.279
Elapsed time for attention_prob_times_values (128x2048x2048x985): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x985): 83.247

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2581.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 16, hidden_size: 28768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1798x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1798x2048): 87.389
Elapsed time for attention_prob_times_values (64x2048x2048x1798): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1798): 83.324

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2481.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1799x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1799x2048): 86.591
Elapsed time for attention_prob_times_values (64x2048x2048x1799): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1799): 81.094

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2437.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1800x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1800x2048): 88.187
Elapsed time for attention_prob_times_values (64x2048x2048x1800): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1800): 82.376

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2480.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1801x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1801x2048): 86.060
Elapsed time for attention_prob_times_values (64x2048x2048x1801): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1801): 81.250

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2435.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1802x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1802x2048): 86.906
Elapsed time for attention_prob_times_values (64x2048x2048x1802): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1802): 83.532

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2483.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1803x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1803x2048): 86.207
Elapsed time for attention_prob_times_values (64x2048x2048x1803): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1803): 81.353

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2441.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1804x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1804x2048): 87.499
Elapsed time for attention_prob_times_values (64x2048x2048x1804): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1804): 83.685

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2496.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1805x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1805x2048): 86.030
Elapsed time for attention_prob_times_values (64x2048x2048x1805): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1805): 81.368

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2442.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1806x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1806x2048): 87.008
Elapsed time for attention_prob_times_values (64x2048x2048x1806): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1806): 83.611

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2491.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1807x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1807x2048): 86.197
Elapsed time for attention_prob_times_values (64x2048x2048x1807): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1807): 81.519

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2449.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1808x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1808x2048): 88.496
Elapsed time for attention_prob_times_values (64x2048x2048x1808): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1808): 83.349

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2510.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1809x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1809x2048): 85.834
Elapsed time for attention_prob_times_values (64x2048x2048x1809): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1809): 81.615

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2448.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1810x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1810x2048): 86.807
Elapsed time for attention_prob_times_values (64x2048x2048x1810): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1810): 83.767

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2496.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1811x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1811x2048): 85.905
Elapsed time for attention_prob_times_values (64x2048x2048x1811): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1811): 81.632

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2452.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1812x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1812x2048): 87.614
Elapsed time for attention_prob_times_values (64x2048x2048x1812): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1812): 83.766

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2510.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1813x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1813x2048): 86.435
Elapsed time for attention_prob_times_values (64x2048x2048x1813): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1813): 81.660

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2462.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1814x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1814x2048): 87.119
Elapsed time for attention_prob_times_values (64x2048x2048x1814): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1814): 83.852

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2507.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1815x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1815x2048): 86.439
Elapsed time for attention_prob_times_values (64x2048x2048x1815): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1815): 81.709

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2466.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1816x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1816x2048): 88.032
Elapsed time for attention_prob_times_values (64x2048x2048x1816): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1816): 83.308

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2514.649
MLP duration (in seconds): 0.0000
num_attention_heads: 32, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x986x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x986x2048): 80.528
Elapsed time for attention_prob_times_values (128x2048x2048x986): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x986): 84.706

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2626.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x987x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x987x2048): 79.216
Elapsed time for attention_prob_times_values (128x2048x2048x987): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x987): 83.406

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2587.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x988x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x988x2048): 80.065
Elapsed time for attention_prob_times_values (128x2048x2048x988): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x988): 85.089

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2629.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x989x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x989x2048): 77.104
Elapsed time for attention_prob_times_values (128x2048x2048x989): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x989): 83.417

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2556.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x990x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x990x2048): 80.012
Elapsed time for attention_prob_times_values (128x2048x2048x990): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x990): 81.599

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2580.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x991x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x991x2048): 76.245
Elapsed time for attention_prob_times_values (128x2048x2048x991): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x991): 83.514

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2548.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x992x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x992x2048): 90.552
Elapsed time for attention_prob_times_values (128x2048x2048x992): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x992): 89.604

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2882.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x993x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x993x2048): 77.514
Elapsed time for attention_prob_times_values (128x2048x2048x993): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x993): 83.830

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2580.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x994x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x994x2048): 78.842
Elapsed time for attention_prob_times_values (128x2048x2048x994): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x994): 86.010

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2637.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x995x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x995x2048): 81.282
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1817x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1817x2048): 85.674
Elapsed time for attention_prob_times_values (64x2048x2048x1817): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1817): 81.827

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2460.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1818x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1818x2048): 86.608
Elapsed time for attention_prob_times_values (64x2048x2048x1818): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1818): 84.132

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2509.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1819x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1819x2048): 85.791
Elapsed time for attention_prob_times_values (64x2048x2048x1819): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1819): 81.888

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2465.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1820x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1820x2048): 87.221
Elapsed time for attention_prob_times_values (64x2048x2048x1820): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1820): 84.219

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2522.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1821x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1821x2048): 85.801
Elapsed time for attention_prob_times_values (64x2048x2048x1821): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1821): 82.040

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2470.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1822x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1822x2048): 86.652
Elapsed time for attention_prob_times_values (64x2048x2048x1822): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1822): 83.984

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2513.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1823x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1823x2048): 86.117
Elapsed time for attention_prob_times_values (64x2048x2048x1823): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1823): 82.217

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2480.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1824x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1824x2048): 96.126
Elapsed time for attention_prob_times_values (64x2048x2048x1824): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1824): 85.118

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2663.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1825x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1825x2048): 87.264
Elapsed time for attention_prob_times_values (64x2048x2048x1825): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1825): 82.288

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2500.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (128x2048x2048x995): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x995): 83.729

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2647.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x996x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x996x2048): 82.881
Elapsed time for attention_prob_times_values (128x2048x2048x996): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x996): 85.858

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2709.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x997x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x997x2048): 81.263
Elapsed time for attention_prob_times_values (128x2048x2048x997): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x997): 83.867

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2654.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x998x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x998x2048): 81.872
Elapsed time for attention_prob_times_values (128x2048x2048x998): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x998): 85.335

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2689.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x999x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x999x2048): 80.958
Elapsed time for attention_prob_times_values (128x2048x2048x999): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x999): 84.061

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2657.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1000x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1000x2048): 78.288
Elapsed time for attention_prob_times_values (128x2048x2048x1000): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1000): 89.964

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2700.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1001x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1001x2048): 77.406
Elapsed time for attention_prob_times_values (128x2048x2048x1001): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1001): 80.526

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2548.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1002x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1002x2048): 79.099
Elapsed time for attention_prob_times_values (128x2048x2048x1002): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1002): 82.003

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2601.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1003x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1003x2048): 80.588
Elapsed time for attention_prob_times_values (128x2048x2048x1003): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1003): 80.945

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2612.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1004x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1004x2048): 78.670
Elapsed time for attention_prob_times_values (128x2048x2048x1004): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1004): 86.716

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2670.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
num_attention_heads: 16, hidden_size: 29216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1826x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1826x2048): 88.452
Elapsed time for attention_prob_times_values (64x2048x2048x1826): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1826): 84.537

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2552.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1827x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1827x2048): 87.343
Elapsed time for attention_prob_times_values (64x2048x2048x1827): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1827): 82.258

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2503.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1828x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1828x2048): 88.807
Elapsed time for attention_prob_times_values (64x2048x2048x1828): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1828): 84.600

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2561.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1829x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1829x2048): 87.152
Elapsed time for attention_prob_times_values (64x2048x2048x1829): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1829): 82.290

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2503.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1830x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1830x2048): 87.667
Elapsed time for attention_prob_times_values (64x2048x2048x1830): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1830): 84.609

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2548.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1831x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1831x2048): 86.684
Elapsed time for attention_prob_times_values (64x2048x2048x1831): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1831): 80.252

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2467.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1832x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1832x2048): 86.693
Elapsed time for attention_prob_times_values (64x2048x2048x1832): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1832): 84.058

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2528.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1833x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1833x2048): 86.267
Elapsed time for attention_prob_times_values (64x2048x2048x1833): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1833): 82.421

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2498.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1834x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1834x2048): 83.716
Elapsed time for attention_prob_times_values (64x2048x2048x1834): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1834): 81.702

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2452.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1835x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1835x2048): 86.376
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1005x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1005x2048): 80.760
Elapsed time for attention_prob_times_values (128x2048x2048x1005): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1005): 80.769

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2617.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1006x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1006x2048): 77.409
Elapsed time for attention_prob_times_values (128x2048x2048x1006): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1006): 86.739

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2653.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1007x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1007x2048): 80.544
Elapsed time for attention_prob_times_values (128x2048x2048x1007): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1007): 83.946

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2669.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1008x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1008x2048): 82.914
Elapsed time for attention_prob_times_values (128x2048x2048x1008): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1008): 94.173

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2866.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1009x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1009x2048): 79.956
Elapsed time for attention_prob_times_values (128x2048x2048x1009): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1009): 84.025

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2665.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1010x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1010x2048): 80.704
Elapsed time for attention_prob_times_values (128x2048x2048x1010): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1010): 87.009

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2726.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1011x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1011x2048): 80.206
Elapsed time for attention_prob_times_values (128x2048x2048x1011): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1011): 83.870

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2672.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1012x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1012x2048): 81.535
Elapsed time for attention_prob_times_values (128x2048x2048x1012): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1012): 87.171

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2748.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1013x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1013x2048): 80.464
Elapsed time for attention_prob_times_values (128x2048x2048x1013): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1013): 84.531

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2692.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (64x2048x2048x1835): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1835): 82.465

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2503.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1836x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1836x2048): 88.097
Elapsed time for attention_prob_times_values (64x2048x2048x1836): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1836): 81.058

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2506.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1837x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1837x2048): 85.262
Elapsed time for attention_prob_times_values (64x2048x2048x1837): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1837): 82.485

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2490.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1838x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1838x2048): 87.448
Elapsed time for attention_prob_times_values (64x2048x2048x1838): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1838): 84.897

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2560.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1839x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1839x2048): 83.236
Elapsed time for attention_prob_times_values (64x2048x2048x1839): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1839): 81.368

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2446.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1840x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1840x2048): 88.950
Elapsed time for attention_prob_times_values (64x2048x2048x1840): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1840): 84.820

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2583.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1841x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1841x2048): 86.081
Elapsed time for attention_prob_times_values (64x2048x2048x1841): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1841): 79.063

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2453.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1842x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1842x2048): 85.764
Elapsed time for attention_prob_times_values (64x2048x2048x1842): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1842): 85.089

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2544.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1843x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1843x2048): 86.199
Elapsed time for attention_prob_times_values (64x2048x2048x1843): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1843): 82.756

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2516.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1844x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1844x2048): 84.011
Elapsed time for attention_prob_times_values (64x2048x2048x1844): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1844): 83.449

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2496.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
num_attention_heads: 32, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1014x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1014x2048): 80.717
Elapsed time for attention_prob_times_values (128x2048x2048x1014): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1014): 87.213

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2740.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1015x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1015x2048): 80.266
Elapsed time for attention_prob_times_values (128x2048x2048x1015): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1015): 84.008

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2686.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1016x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1016x2048): 81.123
Elapsed time for attention_prob_times_values (128x2048x2048x1016): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1016): 91.992

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2823.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1017x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1017x2048): 79.939
Elapsed time for attention_prob_times_values (128x2048x2048x1017): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1017): 84.425

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2692.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1018x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1018x2048): 80.558
Elapsed time for attention_prob_times_values (128x2048x2048x1018): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1018): 87.424

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2751.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1019x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1019x2048): 80.612
Elapsed time for attention_prob_times_values (128x2048x2048x1019): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1019): 84.846

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2715.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1020x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1020x2048): 84.620
Elapsed time for attention_prob_times_values (128x2048x2048x1020): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1020): 86.555

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2813.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1021x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1021x2048): 80.242
Elapsed time for attention_prob_times_values (128x2048x2048x1021): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1021): 83.922

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2699.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1022x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1022x2048): 79.331
Elapsed time for attention_prob_times_values (128x2048x2048x1022): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1022): 87.068

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2734.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1023x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1023x2048): 80.171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1845x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1845x2048): 86.395
Elapsed time for attention_prob_times_values (64x2048x2048x1845): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1845): 82.856

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2523.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1846x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1846x2048): 87.220
Elapsed time for attention_prob_times_values (64x2048x2048x1846): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1846): 82.036

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2523.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1847x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1847x2048): 84.985
Elapsed time for attention_prob_times_values (64x2048x2048x1847): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1847): 78.855

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2442.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1848x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1848x2048): 88.539
Elapsed time for attention_prob_times_values (64x2048x2048x1848): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1848): 84.775

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2587.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1849x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1849x2048): 81.770
Elapsed time for attention_prob_times_values (64x2048x2048x1849): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1849): 80.043

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2418.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1850x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1850x2048): 86.924
Elapsed time for attention_prob_times_values (64x2048x2048x1850): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1850): 85.484

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2577.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1851x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1851x2048): 86.196
Elapsed time for attention_prob_times_values (64x2048x2048x1851): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1851): 81.026

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2499.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1852x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1852x2048): 87.915
Elapsed time for attention_prob_times_values (64x2048x2048x1852): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1852): 81.958

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2539.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1853x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1853x2048): 86.391
Elapsed time for attention_prob_times_values (64x2048x2048x1853): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1853): 83.040

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2536.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (128x2048x2048x1023): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1023): 83.612

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2698.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1x2048): 1.017
Elapsed time for attention_prob_times_values (128x2048x2048x1): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1): 0.211

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 0.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x2x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x2x2048): 1.874
Elapsed time for attention_prob_times_values (128x2048x2048x2): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x2): 1.777

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 1.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x3x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x3x2048): 2.629
Elapsed time for attention_prob_times_values (128x2048x2048x3): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x3): 2.452

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 2.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x4x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x4x2048): 3.362
Elapsed time for attention_prob_times_values (128x2048x2048x4): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x4): 3.099

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 3.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x5x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x5x2048): 3.933
Elapsed time for attention_prob_times_values (128x2048x2048x5): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x5): 3.739

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 4.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x6x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x6x2048): 4.987
Elapsed time for attention_prob_times_values (128x2048x2048x6): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x6): 4.509

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 5.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x7x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x7x2048): 5.811
Elapsed time for attention_prob_times_values (128x2048x2048x7): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x7): 5.203

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 6.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x8x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x8x2048): 6.597
Elapsed time for attention_prob_times_values (128x2048x2048x8): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x8): 7.053

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 8.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x9x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x9x2048): 7.120
Elapsed time for attention_prob_times_values (128x2048x2048x9): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x9): 7.572

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 9.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x10x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x10x2048): 7.872
Elapsed time for attention_prob_times_values (128x2048x2048x10): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x10): 8.682

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 10.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x11x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x11x2048): 8.626
Elapsed time for attention_prob_times_values (128x2048x2048x11): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x11): 9.239

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 11.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x12x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x12x2048): 9.378
Elapsed time for attention_prob_times_values (128x2048x2048x12): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x12): 8.604

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 12.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x13x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x13x2048): 10.122
Elapsed time for attention_prob_times_values (128x2048x2048x13): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x13): 10.826

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 14.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x14x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x14x2048): 10.925
Elapsed time for attention_prob_times_values (128x2048x2048x14): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x14): 12.030

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 16.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x15x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x15x2048): 11.696
Elapsed time for attention_prob_times_values (128x2048x2048x15): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x15): 12.398

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 17.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x16x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x16x2048): 12.514
Elapsed time for attention_prob_times_values (128x2048x2048x16): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x16): 13.741

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 19.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x17x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x17x2048): 12.831
Elapsed time for attention_prob_times_values (128x2048x2048x17): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x17): 13.908

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 20.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x18x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x18x2048): 13.554
Elapsed time for attention_prob_times_values (128x2048x2048x18): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x18): 15.195

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 22.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x19x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x19x2048): 14.242
Elapsed time for attention_prob_times_values (128x2048x2048x19): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x19): 15.521

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 23.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x20x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x20x2048): 14.982
Elapsed time for attention_prob_times_values (128x2048x2048x20): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x20): 16.748

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 25.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x21x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x21x2048): 15.718
Elapsed time for attention_prob_times_values (128x2048x2048x21): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x21): 17.016

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 27.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x22x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x22x2048): 16.409
Elapsed time for attention_prob_times_values (128x2048x2048x22): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x22): 18.307

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 29.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x23x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x23x2048): 17.158
Elapsed time for attention_prob_times_values (128x2048x2048x23): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x23): 18.530

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 30.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x24x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x24x2048): 17.901
Elapsed time for attention_prob_times_values (128x2048x2048x24): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x24): 20.173

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 33.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x25x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x25x2048): 18.174
Elapsed time for attention_prob_times_values (128x2048x2048x25): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x25): 20.009

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 33.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x26x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x26x2048): 18.804
Elapsed time for attention_prob_times_values (128x2048x2048x26): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x26): 21.075

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 36.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x27x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x27x2048): 19.509
Elapsed time for attention_prob_times_values (128x2048x2048x27): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x27): 21.533

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 37.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x28x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x28x2048): 20.166
Elapsed time for attention_prob_times_values (128x2048x2048x28): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x28): 22.811

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 40.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
--------
Elapsed time for attention_key_query_prob (64x2048x1854x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1854x2048): 83.755
Elapsed time for attention_prob_times_values (64x2048x2048x1854): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1854): 85.726

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2539.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1855x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1855x2048): 86.627
Elapsed time for attention_prob_times_values (64x2048x2048x1855): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1855): 79.397

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2484.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1856x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1856x2048): 96.274
Elapsed time for attention_prob_times_values (64x2048x2048x1856): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1856): 88.149

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2760.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1857x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1857x2048): 87.549
Elapsed time for attention_prob_times_values (64x2048x2048x1857): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1857): 79.814

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2506.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1858x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1858x2048): 88.399
Elapsed time for attention_prob_times_values (64x2048x2048x1858): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1858): 84.781

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2599.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1859x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1859x2048): 84.615
Elapsed time for attention_prob_times_values (64x2048x2048x1859): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1859): 83.553

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2526.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1860x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1860x2048): 89.203
Elapsed time for attention_prob_times_values (64x2048x2048x1860): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1860): 81.797

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2565.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1861x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1861x2048): 87.155
Elapsed time for attention_prob_times_values (64x2048x2048x1861): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1861): 83.525

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2565.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1862x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1862x2048): 88.056
Elapsed time for attention_prob_times_values (64x2048x2048x1862): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1862): 85.838

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2616.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1863x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1863x2048): 87.048
Elapsed time for attention_prob_times_values (64x2048x2048x1863): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1863): 83.178

Attention duration (in seconds): 0.0235
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x29x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x29x2048): 20.820
Elapsed time for attention_prob_times_values (128x2048x2048x29): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x29): 22.905

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 41.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x30x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x30x2048): 21.610
Elapsed time for attention_prob_times_values (128x2048x2048x30): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x30): 24.191

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 44.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x31x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x31x2048): 22.228
Elapsed time for attention_prob_times_values (128x2048x2048x31): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x31): 24.403

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 45.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x32x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x32x2048): 39.811
Elapsed time for attention_prob_times_values (128x2048x2048x32): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x32): 26.188

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 63.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x33x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x33x2048): 26.300
Elapsed time for attention_prob_times_values (128x2048x2048x33): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x33): 25.955

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 53.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x34x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x34x2048): 26.469
Elapsed time for attention_prob_times_values (128x2048x2048x34): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x34): 26.939

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 55.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x35x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x35x2048): 25.510
Elapsed time for attention_prob_times_values (128x2048x2048x35): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x35): 27.447

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 55.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x36x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x36x2048): 26.133
Elapsed time for attention_prob_times_values (128x2048x2048x36): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x36): 28.534

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 57.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x37x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x37x2048): 26.000
Elapsed time for attention_prob_times_values (128x2048x2048x37): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x37): 28.889

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 59.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x38x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x38x2048): 26.828
Elapsed time for attention_prob_times_values (128x2048x2048x38): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x38): 30.003

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 61.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x39x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x39x2048): 27.038
Elapsed time for attention_prob_times_values (128x2048x2048x39): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x39): 30.220

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 63.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x40x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x40x2048): 27.925
Elapsed time for attention_prob_times_values (128x2048x2048x40): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x40): 32.140

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 67.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x41x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x41x2048): 27.547
Elapsed time for attention_prob_times_values (128x2048x2048x41): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x41): 31.494

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 67.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x42x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x42x2048): 28.380
Elapsed time for attention_prob_times_values (128x2048x2048x42): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x42): 32.545

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 70.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x43x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x43x2048): 28.796
Elapsed time for attention_prob_times_values (128x2048x2048x43): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x43): 32.640

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 71.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x44x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x44x2048): 29.704
Elapsed time for attention_prob_times_values (128x2048x2048x44): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x44): 34.077

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 75.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x45x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x45x2048): 30.185
Elapsed time for attention_prob_times_values (128x2048x2048x45): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x45): 33.924

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 76.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x46x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x46x2048): 31.134
Elapsed time for attention_prob_times_values (128x2048x2048x46): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x46): 35.522

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 80.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x47x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x47x2048): 31.326
Elapsed time for attention_prob_times_values (128x2048x2048x47): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x47): 35.408

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 82.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Attention throughput (in TFLOP/s): 2561.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1864x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1864x2048): 89.084
Elapsed time for attention_prob_times_values (64x2048x2048x1864): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1864): 84.870

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2618.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1865x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1865x2048): 87.033
Elapsed time for attention_prob_times_values (64x2048x2048x1865): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1865): 83.340

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2566.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1866x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1866x2048): 87.528
Elapsed time for attention_prob_times_values (64x2048x2048x1866): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1866): 85.791

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2613.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1867x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1867x2048): 86.885
Elapsed time for attention_prob_times_values (64x2048x2048x1867): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1867): 83.508

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2569.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1868x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1868x2048): 88.265
Elapsed time for attention_prob_times_values (64x2048x2048x1868): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1868): 86.060

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2630.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1869x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1869x2048): 86.817
Elapsed time for attention_prob_times_values (64x2048x2048x1869): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1869): 84.046

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2579.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1870x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1870x2048): 84.908
Elapsed time for attention_prob_times_values (64x2048x2048x1870): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1870): 85.975

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2581.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1871x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1871x2048): 86.777
Elapsed time for attention_prob_times_values (64x2048x2048x1871): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1871): 83.148

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2567.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1872x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1872x2048): 86.393
Elapsed time for attention_prob_times_values (64x2048x2048x1872): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1872): 86.342

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2612.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x48x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x48x2048): 32.709
Elapsed time for attention_prob_times_values (128x2048x2048x48): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x48): 38.092

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 87.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x49x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x49x2048): 32.863
Elapsed time for attention_prob_times_values (128x2048x2048x49): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x49): 36.970

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 88.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x50x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x50x2048): 33.694
Elapsed time for attention_prob_times_values (128x2048x2048x50): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x50): 37.954

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 91.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x51x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x51x2048): 34.281
Elapsed time for attention_prob_times_values (128x2048x2048x51): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x51): 37.945

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 93.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x52x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x52x2048): 34.444
Elapsed time for attention_prob_times_values (128x2048x2048x52): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x52): 39.673

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 96.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x53x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x53x2048): 35.610
Elapsed time for attention_prob_times_values (128x2048x2048x53): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x53): 39.175

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 99.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x54x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x54x2048): 36.395
Elapsed time for attention_prob_times_values (128x2048x2048x54): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x54): 40.942

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 103.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x55x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x55x2048): 36.898
Elapsed time for attention_prob_times_values (128x2048x2048x55): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x55): 40.375

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 104.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x56x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x56x2048): 37.783
Elapsed time for attention_prob_times_values (128x2048x2048x56): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x56): 43.854

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 111.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x57x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x57x2048): 37.139
Elapsed time for attention_prob_times_values (128x2048x2048x57): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x57): 42.083

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 109.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x58x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x58x2048): 37.889
Elapsed time for attention_prob_times_values (128x2048x2048x58): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x58): 43.695

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 114.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x59x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x59x2048): 38.298
Elapsed time for attention_prob_times_values (128x2048x2048x59): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x59): 43.332

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 115.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x60x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x60x2048): 39.043
Elapsed time for attention_prob_times_values (128x2048x2048x60): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x60): 45.179

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 120.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x61x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x61x2048): 39.403
Elapsed time for attention_prob_times_values (128x2048x2048x61): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x61): 44.403

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 121.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x62x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x62x2048): 40.095
Elapsed time for attention_prob_times_values (128x2048x2048x62): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x62): 46.506

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 126.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x63x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x63x2048): 40.509
Elapsed time for attention_prob_times_values (128x2048x2048x63): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x63): 45.689

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 127.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x64x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x64x2048): 56.021
Elapsed time for attention_prob_times_values (128x2048x2048x64): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x64): 50.031

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 158.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x65x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x65x2048): 42.691
Elapsed time for attention_prob_times_values (128x2048x2048x65): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x65): 33.092

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 113.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x66x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x66x2048): 43.494
Elapsed time for attention_prob_times_values (128x2048x2048x66): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x66): 34.450

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 117.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x67x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x67x2048): 43.155
Elapsed time for attention_prob_times_values (128x2048x2048x67): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x67): 33.957

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 117.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x68x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x68x2048): 44.153
Elapsed time for attention_prob_times_values (128x2048x2048x68): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x68): 35.429

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 122.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x69x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x69x2048): 43.854
Elapsed time for attention_prob_times_values (128x2048x2048x69): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x69): 34.919

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 122.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x70x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x70x2048): 44.908
Elapsed time for attention_prob_times_values (128x2048x2048x70): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x70): 36.403

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 128.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x71x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x71x2048): 44.571
Elapsed time for attention_prob_times_values (128x2048x2048x71): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x71): 35.811

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 127.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x72x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x72x2048): 45.955
Elapsed time for attention_prob_times_values (128x2048x2048x72): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x72): 35.747

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 130.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x73x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x73x2048): 44.290
Elapsed time for attention_prob_times_values (128x2048x2048x73): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x73): 36.456

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 131.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x74x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x74x2048): 45.697
Elapsed time for attention_prob_times_values (128x2048x2048x74): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x74): 38.114

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 137.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x75x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x75x2048): 42.583
Elapsed time for attention_prob_times_values (128x2048x2048x75): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x75): 37.352

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 133.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 16, hidden_size: 29968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1873x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1873x2048): 86.359
Elapsed time for attention_prob_times_values (64x2048x2048x1873): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1873): 84.357

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2583.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 29984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1874x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1874x2048): 84.651
Elapsed time for attention_prob_times_values (64x2048x2048x1874): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1874): 85.719

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2579.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1875x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1875x2048): 86.489
Elapsed time for attention_prob_times_values (64x2048x2048x1875): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1875): 83.989

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2581.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1876x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1876x2048): 85.419
Elapsed time for attention_prob_times_values (64x2048x2048x1876): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1876): 82.870

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2550.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1877x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1877x2048): 86.599
Elapsed time for attention_prob_times_values (64x2048x2048x1877): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1877): 83.993

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2586.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1878x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1878x2048): 87.404
Elapsed time for attention_prob_times_values (64x2048x2048x1878): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1878): 84.040

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2600.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1879x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1879x2048): 83.435
Elapsed time for attention_prob_times_values (64x2048x2048x1879): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1879): 83.742

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2537.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1880x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1880x2048): 88.918
Elapsed time for attention_prob_times_values (64x2048x2048x1880): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1880): 80.616

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2568.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1881x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1881x2048): 86.280
Elapsed time for attention_prob_times_values (64x2048x2048x1881): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1881): 81.561

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2548.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1882x2048): 0.0120
num_attention_heads: 32, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x76x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x76x2048): 46.555
Elapsed time for attention_prob_times_values (128x2048x2048x76): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x76): 39.175

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 143.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x77x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x77x2048): 46.268
Elapsed time for attention_prob_times_values (128x2048x2048x77): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x77): 38.208

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 142.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x78x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x78x2048): 47.394
Elapsed time for attention_prob_times_values (128x2048x2048x78): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x78): 39.675

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 148.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x79x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x79x2048): 47.218
Elapsed time for attention_prob_times_values (128x2048x2048x79): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x79): 39.020

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 148.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x80x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x80x2048): 48.780
Elapsed time for attention_prob_times_values (128x2048x2048x80): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x80): 37.683

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 148.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x81x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x81x2048): 46.625
Elapsed time for attention_prob_times_values (128x2048x2048x81): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x81): 39.868

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 151.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x82x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x82x2048): 45.424
Elapsed time for attention_prob_times_values (128x2048x2048x82): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x82): 41.639

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 154.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x83x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x83x2048): 47.400
Elapsed time for attention_prob_times_values (128x2048x2048x83): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x83): 40.732

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 157.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x84x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x84x2048): 47.984
Elapsed time for attention_prob_times_values (128x2048x2048x84): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x84): 42.515

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 163.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x85x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x85x2048): 47.947
Elapsed time for attention_prob_times_values (128x2048x2048x85): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x85): 41.510

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 162.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x86x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x86x2048): 48.724
Elapsed time for attention_prob_times_values (128x2048x2048x86): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x86): 43.309

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 169.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x87x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x87x2048): 48.958
Elapsed time for attention_prob_times_values (128x2048x2048x87): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x87): 42.432

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 169.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x88x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x88x2048): 49.820
Elapsed time for attention_prob_times_values (128x2048x2048x88): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x88): 42.619

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 172.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x89x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x89x2048): 48.420
Elapsed time for attention_prob_times_values (128x2048x2048x89): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x89): 43.206

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 172.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x90x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x90x2048): 49.226
Elapsed time for attention_prob_times_values (128x2048x2048x90): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x90): 45.026

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 179.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x91x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x91x2048): 49.211
Elapsed time for attention_prob_times_values (128x2048x2048x91): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x91): 44.072

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 178.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x92x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x92x2048): 50.022
Elapsed time for attention_prob_times_values (128x2048x2048x92): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x92): 45.894

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 185.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x93x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x93x2048): 50.004
Elapsed time for attention_prob_times_values (128x2048x2048x93): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x93): 44.838

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 184.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x94x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x94x2048): 50.784
Elapsed time for attention_prob_times_values (128x2048x2048x94): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x94): 45.751

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 189.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1882x2048): 84.545
Elapsed time for attention_prob_times_values (64x2048x2048x1882): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1882): 86.385

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2598.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1883x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1883x2048): 86.364
Elapsed time for attention_prob_times_values (64x2048x2048x1883): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1883): 83.700

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2586.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1884x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1884x2048): 88.082
Elapsed time for attention_prob_times_values (64x2048x2048x1884): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1884): 83.853

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2615.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1885x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1885x2048): 83.210
Elapsed time for attention_prob_times_values (64x2048x2048x1885): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1885): 83.366

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2536.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1886x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1886x2048): 87.178
Elapsed time for attention_prob_times_values (64x2048x2048x1886): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1886): 86.058

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2639.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1887x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1887x2048): 86.463
Elapsed time for attention_prob_times_values (64x2048x2048x1887): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1887): 82.060

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2566.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1888x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1888x2048): 92.424
Elapsed time for attention_prob_times_values (64x2048x2048x1888): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1888): 85.395

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2707.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1889x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1889x2048): 87.416
Elapsed time for attention_prob_times_values (64x2048x2048x1889): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1889): 84.074

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2615.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1890x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1890x2048): 88.335
Elapsed time for attention_prob_times_values (64x2048x2048x1890): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1890): 86.230

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2664.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1891x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1891x2048): 87.275
Elapsed time for attention_prob_times_values (64x2048x2048x1891): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1891): 82.151

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2585.359
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x95x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x95x2048): 50.692
Elapsed time for attention_prob_times_values (128x2048x2048x95): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x95): 45.496

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 190.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x96x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x96x2048): 65.467
Elapsed time for attention_prob_times_values (128x2048x2048x96): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x96): 47.153

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 219.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x97x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x97x2048): 51.515
Elapsed time for attention_prob_times_values (128x2048x2048x97): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x97): 46.577

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 197.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x98x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x98x2048): 52.392
Elapsed time for attention_prob_times_values (128x2048x2048x98): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x98): 48.272

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 204.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x99x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x99x2048): 51.989
Elapsed time for attention_prob_times_values (128x2048x2048x99): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x99): 47.316

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 202.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x100x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x100x2048): 53.322
Elapsed time for attention_prob_times_values (128x2048x2048x100): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x100): 49.123

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 210.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x101x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x101x2048): 52.604
Elapsed time for attention_prob_times_values (128x2048x2048x101): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x101): 48.272

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 209.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x102x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x102x2048): 53.489
Elapsed time for attention_prob_times_values (128x2048x2048x102): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x102): 49.937

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 216.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x103x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x103x2048): 53.233
Elapsed time for attention_prob_times_values (128x2048x2048x103): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x103): 48.659

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 214.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x104x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x104x2048): 54.595
Elapsed time for attention_prob_times_values (128x2048x2048x104): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x104): 49.532

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 220.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x105x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x105x2048): 52.966
Elapsed time for attention_prob_times_values (128x2048x2048x105): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x105): 49.641

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 219.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x106x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x106x2048): 52.529
Elapsed time for attention_prob_times_values (128x2048x2048x106): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x106): 50.346

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 221.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x107x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x107x2048): 53.501
Elapsed time for attention_prob_times_values (128x2048x2048x107): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x107): 50.134

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 224.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x108x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x108x2048): 54.804
Elapsed time for attention_prob_times_values (128x2048x2048x108): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x108): 52.328

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 234.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x109x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x109x2048): 54.266
Elapsed time for attention_prob_times_values (128x2048x2048x109): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x109): 50.944

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 231.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x110x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x110x2048): 55.364
Elapsed time for attention_prob_times_values (128x2048x2048x110): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x110): 52.899

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 240.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x111x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x111x2048): 55.001
Elapsed time for attention_prob_times_values (128x2048x2048x111): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x111): 51.547

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 237.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x112x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x112x2048): 57.124
Elapsed time for attention_prob_times_values (128x2048x2048x112): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x112): 53.427

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 248.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x113x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x113x2048): 54.821
Elapsed time for attention_prob_times_values (128x2048x2048x113): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x113): 52.314

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 242.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1892x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1892x2048): 86.295
Elapsed time for attention_prob_times_values (64x2048x2048x1892): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1892): 84.326

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2606.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1893x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1893x2048): 87.002
Elapsed time for attention_prob_times_values (64x2048x2048x1893): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1893): 83.513

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2605.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1894x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1894x2048): 87.983
Elapsed time for attention_prob_times_values (64x2048x2048x1894): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1894): 85.943

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2660.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1895x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1895x2048): 86.916
Elapsed time for attention_prob_times_values (64x2048x2048x1895): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1895): 83.565

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2608.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1896x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1896x2048): 85.170
Elapsed time for attention_prob_times_values (64x2048x2048x1896): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1896): 82.983

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2574.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1897x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1897x2048): 82.934
Elapsed time for attention_prob_times_values (64x2048x2048x1897): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1897): 83.578

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2550.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1898x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1898x2048): 87.634
Elapsed time for attention_prob_times_values (64x2048x2048x1898): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1898): 85.882

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2659.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1899x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1899x2048): 86.755
Elapsed time for attention_prob_times_values (64x2048x2048x1899): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1899): 83.761

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2614.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1900x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1900x2048): 88.259
Elapsed time for attention_prob_times_values (64x2048x2048x1900): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1900): 85.764

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2669.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x114x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x114x2048): 56.000
Elapsed time for attention_prob_times_values (128x2048x2048x114): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x114): 54.502

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 252.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x115x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x115x2048): 55.511
Elapsed time for attention_prob_times_values (128x2048x2048x115): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x115): 52.984

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 249.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x116x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x116x2048): 56.594
Elapsed time for attention_prob_times_values (128x2048x2048x116): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x116): 55.218

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 258.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x117x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x117x2048): 56.121
Elapsed time for attention_prob_times_values (128x2048x2048x117): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x117): 53.680

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 255.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x118x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x118x2048): 57.179
Elapsed time for attention_prob_times_values (128x2048x2048x118): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x118): 55.857

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 264.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x119x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x119x2048): 56.832
Elapsed time for attention_prob_times_values (128x2048x2048x119): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x119): 54.277

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 262.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x120x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x120x2048): 58.343
Elapsed time for attention_prob_times_values (128x2048x2048x120): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x120): 56.003

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 271.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x121x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x121x2048): 56.208
Elapsed time for attention_prob_times_values (128x2048x2048x121): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x121): 43.075

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 233.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x122x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x122x2048): 57.503
Elapsed time for attention_prob_times_values (128x2048x2048x122): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x122): 57.317

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 276.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x123x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x123x2048): 56.306
Elapsed time for attention_prob_times_values (128x2048x2048x123): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x123): 46.009

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 245.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x124x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x124x2048): 58.415
Elapsed time for attention_prob_times_values (128x2048x2048x124): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x124): 57.906

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 283.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x125x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x125x2048): 57.222
Elapsed time for attention_prob_times_values (128x2048x2048x125): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x125): 44.156

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 244.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x126x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x126x2048): 57.733
Elapsed time for attention_prob_times_values (128x2048x2048x126): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x126): 58.153

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 286.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x127x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x127x2048): 57.402
Elapsed time for attention_prob_times_values (128x2048x2048x127): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x127): 43.879

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 247.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x128x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x128x2048): 68.594
Elapsed time for attention_prob_times_values (128x2048x2048x128): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x128): 61.992

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 325.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x129x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x129x2048): 56.840
Elapsed time for attention_prob_times_values (128x2048x2048x129): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x129): 44.146

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 250.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x130x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x130x2048): 56.963
Elapsed time for attention_prob_times_values (128x2048x2048x130): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x130): 46.344

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 258.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x131x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x131x2048): 57.835
Elapsed time for attention_prob_times_values (128x2048x2048x131): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x131): 45.254

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 258.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x132x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x132x2048): 59.347
Elapsed time for attention_prob_times_values (128x2048x2048x132): 0.0030
num_attention_heads: 16, hidden_size: 30416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1901x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1901x2048): 86.273
Elapsed time for attention_prob_times_values (64x2048x2048x1901): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1901): 82.809

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2594.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1902x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1902x2048): 87.186
Elapsed time for attention_prob_times_values (64x2048x2048x1902): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1902): 86.130

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2661.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1903x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1903x2048): 86.868
Elapsed time for attention_prob_times_values (64x2048x2048x1903): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1903): 83.253

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2613.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1904x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1904x2048): 89.469
Elapsed time for attention_prob_times_values (64x2048x2048x1904): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1904): 87.538

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2721.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1905x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1905x2048): 86.480
Elapsed time for attention_prob_times_values (64x2048x2048x1905): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1905): 83.811

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2618.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1906x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1906x2048): 87.465
Elapsed time for attention_prob_times_values (64x2048x2048x1906): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1906): 85.873

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2667.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1907x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1907x2048): 86.587
Elapsed time for attention_prob_times_values (64x2048x2048x1907): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1907): 82.968

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2609.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1908x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1908x2048): 86.524
Elapsed time for attention_prob_times_values (64x2048x2048x1908): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1908): 86.322

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2662.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1909x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1909x2048): 85.785
Elapsed time for attention_prob_times_values (64x2048x2048x1909): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1909): 83.406

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2607.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1910x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1910x2048): 86.444
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x132): 47.703

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 271.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x133x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x133x2048): 58.187
Elapsed time for attention_prob_times_values (128x2048x2048x133): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x133): 45.923

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 264.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x134x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x134x2048): 59.586
Elapsed time for attention_prob_times_values (128x2048x2048x134): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x134): 48.240

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 276.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x135x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x135x2048): 57.749
Elapsed time for attention_prob_times_values (128x2048x2048x135): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x135): 46.686

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 269.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x136x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x136x2048): 59.413
Elapsed time for attention_prob_times_values (128x2048x2048x136): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x136): 45.633

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 271.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x137x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x137x2048): 58.800
Elapsed time for attention_prob_times_values (128x2048x2048x137): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x137): 46.717

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 274.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x138x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x138x2048): 59.800
Elapsed time for attention_prob_times_values (128x2048x2048x138): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x138): 49.302

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 287.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x139x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x139x2048): 59.191
Elapsed time for attention_prob_times_values (128x2048x2048x139): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x139): 47.392

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 281.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x140x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x140x2048): 60.719
Elapsed time for attention_prob_times_values (128x2048x2048x140): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x140): 50.231

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 295.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x141x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x141x2048): 60.054
Elapsed time for attention_prob_times_values (128x2048x2048x141): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x141): 48.099

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 288.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x142x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x142x2048): 61.168
Elapsed time for attention_prob_times_values (128x2048x2048x142): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x142): 50.542

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 300.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x143x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x143x2048): 60.437
Elapsed time for attention_prob_times_values (128x2048x2048x143): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x143): 48.745

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 295.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x144x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x144x2048): 62.853
Elapsed time for attention_prob_times_values (128x2048x2048x144): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x144): 49.190

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 303.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x145x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x145x2048): 60.054
Elapsed time for attention_prob_times_values (128x2048x2048x145): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x145): 49.296

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 299.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x146x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x146x2048): 61.589
Elapsed time for attention_prob_times_values (128x2048x2048x146): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x146): 51.761

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 312.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x147x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x147x2048): 60.854
Elapsed time for attention_prob_times_values (128x2048x2048x147): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x147): 49.814

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 306.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x148x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x148x2048): 62.206
Elapsed time for attention_prob_times_values (128x2048x2048x148): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x148): 52.508

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 320.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x149x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x149x2048): 61.276
Elapsed time for attention_prob_times_values (128x2048x2048x149): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x149): 50.484

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 313.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x150x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x150x2048): 62.748
Elapsed time for attention_prob_times_values (128x2048x2048x150): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x150): 52.959

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 326.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x151x2048): 0.0026
Elapsed time for attention_prob_times_values (64x2048x2048x1910): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1910): 85.902

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2657.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1911x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1911x2048): 85.533
Elapsed time for attention_prob_times_values (64x2048x2048x1911): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1911): 83.203

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2603.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1912x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1912x2048): 87.721
Elapsed time for attention_prob_times_values (64x2048x2048x1912): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1912): 87.427

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2703.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1913x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1913x2048): 86.187
Elapsed time for attention_prob_times_values (64x2048x2048x1913): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1913): 83.615

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2622.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1914x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1914x2048): 86.991
Elapsed time for attention_prob_times_values (64x2048x2048x1914): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1914): 86.202

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2676.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1915x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1915x2048): 86.316
Elapsed time for attention_prob_times_values (64x2048x2048x1915): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1915): 83.685

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2627.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1916x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1916x2048): 87.726
Elapsed time for attention_prob_times_values (64x2048x2048x1916): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1916): 86.263

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2691.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1917x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1917x2048): 86.060
Elapsed time for attention_prob_times_values (64x2048x2048x1917): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1917): 83.489

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2623.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1918x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1918x2048): 87.135
Elapsed time for attention_prob_times_values (64x2048x2048x1918): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1918): 86.582

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2689.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1919x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1919x2048): 86.390
Elapsed time for attention_prob_times_values (64x2048x2048x1919): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1919): 83.871

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2637.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x151x2048): 62.116
Elapsed time for attention_prob_times_values (128x2048x2048x151): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x151): 51.020

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 320.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x152x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x152x2048): 64.042
Elapsed time for attention_prob_times_values (128x2048x2048x152): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x152): 50.968

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 326.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x153x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x153x2048): 61.787
Elapsed time for attention_prob_times_values (128x2048x2048x153): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x153): 51.499

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 324.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x154x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x154x2048): 62.902
Elapsed time for attention_prob_times_values (128x2048x2048x154): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x154): 54.070

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 338.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x155x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x155x2048): 62.310
Elapsed time for attention_prob_times_values (128x2048x2048x155): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x155): 52.254

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 332.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x156x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x156x2048): 63.765
Elapsed time for attention_prob_times_values (128x2048x2048x156): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x156): 54.900

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 346.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x157x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x157x2048): 63.097
Elapsed time for attention_prob_times_values (128x2048x2048x157): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x157): 52.845

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 339.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x158x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x158x2048): 64.279
Elapsed time for attention_prob_times_values (128x2048x2048x158): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x158): 55.285

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 352.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x159x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x159x2048): 63.634
Elapsed time for attention_prob_times_values (128x2048x2048x159): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x159): 53.486

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 346.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x160x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x160x2048): 77.373
Elapsed time for attention_prob_times_values (128x2048x2048x160): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x160): 55.544

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 387.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x161x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x161x2048): 62.588
Elapsed time for attention_prob_times_values (128x2048x2048x161): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x161): 54.358

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 350.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x162x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x162x2048): 63.708
Elapsed time for attention_prob_times_values (128x2048x2048x162): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x162): 56.421

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 362.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x163x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x163x2048): 62.974
Elapsed time for attention_prob_times_values (128x2048x2048x163): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x163): 54.914

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 357.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x164x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x164x2048): 64.800
Elapsed time for attention_prob_times_values (128x2048x2048x164): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x164): 57.299

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 372.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x165x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x165x2048): 63.539
Elapsed time for attention_prob_times_values (128x2048x2048x165): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x165): 55.513

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 364.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x166x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x166x2048): 64.493
Elapsed time for attention_prob_times_values (128x2048x2048x166): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x166): 57.641

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 376.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x167x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x167x2048): 64.112
Elapsed time for attention_prob_times_values (128x2048x2048x167): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x167): 56.156

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 372.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x168x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x168x2048): 66.056
Elapsed time for attention_prob_times_values (128x2048x2048x168): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x168): 55.826

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 378.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x169x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x169x2048): 63.712
Elapsed time for attention_prob_times_values (128x2048x2048x169): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x169): 56.362

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 375.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1920x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1920x2048): 94.417
Elapsed time for attention_prob_times_values (64x2048x2048x1920): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1920): 91.078

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2874.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1921x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1921x2048): 87.326
Elapsed time for attention_prob_times_values (64x2048x2048x1921): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1921): 80.675

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2601.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1922x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1922x2048): 88.140
Elapsed time for attention_prob_times_values (64x2048x2048x1922): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1922): 83.514

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2661.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1923x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1923x2048): 86.846
Elapsed time for attention_prob_times_values (64x2048x2048x1923): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1923): 81.136

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2604.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1924x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1924x2048): 88.716
Elapsed time for attention_prob_times_values (64x2048x2048x1924): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1924): 83.418

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2670.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1925x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1925x2048): 87.142
Elapsed time for attention_prob_times_values (64x2048x2048x1925): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1925): 81.092

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2610.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1926x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1926x2048): 87.926
Elapsed time for attention_prob_times_values (64x2048x2048x1926): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1926): 83.405

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2661.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1927x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1927x2048): 87.093
Elapsed time for attention_prob_times_values (64x2048x2048x1927): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1927): 81.303

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2616.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1928x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1928x2048): 88.868
Elapsed time for attention_prob_times_values (64x2048x2048x1928): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1928): 70.295

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2443.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 32, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x170x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x170x2048): 64.987
Elapsed time for attention_prob_times_values (128x2048x2048x170): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x170): 58.884

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 390.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x171x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x171x2048): 64.380
Elapsed time for attention_prob_times_values (128x2048x2048x171): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x171): 56.772

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 382.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x172x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x172x2048): 65.612
Elapsed time for attention_prob_times_values (128x2048x2048x172): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x172): 59.756

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 398.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x173x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x173x2048): 64.760
Elapsed time for attention_prob_times_values (128x2048x2048x173): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x173): 57.700

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 390.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x174x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x174x2048): 66.129
Elapsed time for attention_prob_times_values (128x2048x2048x174): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x174): 60.020

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 405.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x175x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x175x2048): 65.470
Elapsed time for attention_prob_times_values (128x2048x2048x175): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x175): 58.186

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 398.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x176x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x176x2048): 68.502
Elapsed time for attention_prob_times_values (128x2048x2048x176): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x176): 59.130

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 412.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x177x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x177x2048): 65.025
Elapsed time for attention_prob_times_values (128x2048x2048x177): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x177): 58.864

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 403.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x178x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x178x2048): 66.482
Elapsed time for attention_prob_times_values (128x2048x2048x178): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x178): 60.938

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 417.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x179x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x179x2048): 65.551
Elapsed time for attention_prob_times_values (128x2048x2048x179): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x179): 59.393

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 410.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x180x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x180x2048): 66.962
Elapsed time for attention_prob_times_values (128x2048x2048x180): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x180): 61.853

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 426.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x181x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x181x2048): 66.036
Elapsed time for attention_prob_times_values (128x2048x2048x181): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x181): 59.692

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 417.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x182x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x182x2048): 67.367
Elapsed time for attention_prob_times_values (128x2048x2048x182): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x182): 62.090

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 432.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x183x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x183x2048): 66.685
Elapsed time for attention_prob_times_values (128x2048x2048x183): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x183): 60.259

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 425.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x184x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x184x2048): 68.843
Elapsed time for attention_prob_times_values (128x2048x2048x184): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x184): 60.561

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 434.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x185x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x185x2048): 66.245
Elapsed time for attention_prob_times_values (128x2048x2048x185): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x185): 60.797

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 429.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x186x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x186x2048): 67.478
Elapsed time for attention_prob_times_values (128x2048x2048x186): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x186): 63.168

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 444.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x187x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x187x2048): 66.770
Elapsed time for attention_prob_times_values (128x2048x2048x187): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x187): 61.414

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 437.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x188x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x188x2048): 68.264
Elapsed time for attention_prob_times_values (128x2048x2048x188): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x188): 63.579

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 452.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (64x2048x1929x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1929x2048): 85.905
Elapsed time for attention_prob_times_values (64x2048x2048x1929): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1929): 80.687

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2591.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1930x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1930x2048): 86.925
Elapsed time for attention_prob_times_values (64x2048x2048x1930): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1930): 82.904

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2644.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1931x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1931x2048): 85.455
Elapsed time for attention_prob_times_values (64x2048x2048x1931): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1931): 80.617

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2586.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1932x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1932x2048): 87.034
Elapsed time for attention_prob_times_values (64x2048x2048x1932): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1932): 82.704

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2645.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1933x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1933x2048): 85.856
Elapsed time for attention_prob_times_values (64x2048x2048x1933): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1933): 80.843

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2598.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1934x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1934x2048): 86.396
Elapsed time for attention_prob_times_values (64x2048x2048x1934): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1934): 82.640

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2637.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1935x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1935x2048): 86.260
Elapsed time for attention_prob_times_values (64x2048x2048x1935): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1935): 80.736

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2605.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1936x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1936x2048): 88.512
Elapsed time for attention_prob_times_values (64x2048x2048x1936): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1936): 78.586

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2601.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 30992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1937x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1937x2048): 85.300
Elapsed time for attention_prob_times_values (64x2048x2048x1937): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1937): 81.016

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2598.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1938x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1938x2048): 87.658
Elapsed time for attention_prob_times_values (64x2048x2048x1938): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1938): 84.020

Attention duration (in seconds): 0.0243
========================================================================================================================
num_attention_heads: 32, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x189x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x189x2048): 67.239
Elapsed time for attention_prob_times_values (128x2048x2048x189): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x189): 61.873

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 445.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x190x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x190x2048): 68.608
Elapsed time for attention_prob_times_values (128x2048x2048x190): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x190): 64.144

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 459.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x191x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x191x2048): 67.940
Elapsed time for attention_prob_times_values (128x2048x2048x191): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x191): 62.655

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 454.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x192x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x192x2048): 78.231
Elapsed time for attention_prob_times_values (128x2048x2048x192): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x192): 65.706

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 499.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x193x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x193x2048): 66.595
Elapsed time for attention_prob_times_values (128x2048x2048x193): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x193): 52.004

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 410.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x194x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x194x2048): 68.024
Elapsed time for attention_prob_times_values (128x2048x2048x194): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x194): 53.323

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 422.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x195x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x195x2048): 67.054
Elapsed time for attention_prob_times_values (128x2048x2048x195): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x195): 52.389

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 417.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x196x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x196x2048): 68.709
Elapsed time for attention_prob_times_values (128x2048x2048x196): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x196): 53.230

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 427.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x197x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x197x2048): 67.320
Elapsed time for attention_prob_times_values (128x2048x2048x197): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x197): 52.488

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 422.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x198x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x198x2048): 68.614
Elapsed time for attention_prob_times_values (128x2048x2048x198): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x198): 53.598

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 432.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x199x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x199x2048): 67.947
Elapsed time for attention_prob_times_values (128x2048x2048x199): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x199): 53.021

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 429.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x200x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x200x2048): 70.171
Elapsed time for attention_prob_times_values (128x2048x2048x200): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x200): 51.588

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 431.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x201x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x201x2048): 67.559
Elapsed time for attention_prob_times_values (128x2048x2048x201): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x201): 51.828

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 427.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x202x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x202x2048): 68.804
Elapsed time for attention_prob_times_values (128x2048x2048x202): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x202): 54.622

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 445.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x203x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x203x2048): 67.872
Elapsed time for attention_prob_times_values (128x2048x2048x203): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x203): 51.707

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 431.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x204x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x204x2048): 69.643
Elapsed time for attention_prob_times_values (128x2048x2048x204): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x204): 55.252

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 454.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x205x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x205x2048): 68.571
Elapsed time for attention_prob_times_values (128x2048x2048x205): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x205): 52.772

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 441.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x206x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x206x2048): 69.871
Elapsed time for attention_prob_times_values (128x2048x2048x206): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x206): 55.381

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 459.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x207x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x207x2048): 68.989
Elapsed time for attention_prob_times_values (128x2048x2048x207): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x207): 53.198

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 448.671
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 2683.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1939x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1939x2048): 86.788
Elapsed time for attention_prob_times_values (64x2048x2048x1939): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1939): 81.963

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2638.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1940x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1940x2048): 88.276
Elapsed time for attention_prob_times_values (64x2048x2048x1940): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1940): 84.105

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2697.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1941x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1941x2048): 86.868
Elapsed time for attention_prob_times_values (64x2048x2048x1941): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1941): 82.004

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2643.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1942x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1942x2048): 87.730
Elapsed time for attention_prob_times_values (64x2048x2048x1942): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1942): 84.101

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2691.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1943x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1943x2048): 86.866
Elapsed time for attention_prob_times_values (64x2048x2048x1943): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1943): 81.744

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2641.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1944x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1944x2048): 87.128
Elapsed time for attention_prob_times_values (64x2048x2048x1944): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1944): 78.625

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2593.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1945x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1945x2048): 86.432
Elapsed time for attention_prob_times_values (64x2048x2048x1945): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1945): 82.169

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2644.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1946x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1946x2048): 87.561
Elapsed time for attention_prob_times_values (64x2048x2048x1946): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1946): 83.720

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2688.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1947x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1947x2048): 85.740
Elapsed time for attention_prob_times_values (64x2048x2048x1947): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1947): 81.474

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2625.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x208x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x208x2048): 71.986
Elapsed time for attention_prob_times_values (128x2048x2048x208): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x208): 54.182

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 463.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x209x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x209x2048): 68.599
Elapsed time for attention_prob_times_values (128x2048x2048x209): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x209): 53.694

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 453.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x210x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x210x2048): 69.895
Elapsed time for attention_prob_times_values (128x2048x2048x210): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x210): 56.359

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 471.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x211x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x211x2048): 68.839
Elapsed time for attention_prob_times_values (128x2048x2048x211): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x211): 54.109

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 460.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x212x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x212x2048): 70.441
Elapsed time for attention_prob_times_values (128x2048x2048x212): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x212): 56.909

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 480.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x213x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x213x2048): 69.361
Elapsed time for attention_prob_times_values (128x2048x2048x213): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x213): 54.165

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 465.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x214x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x214x2048): 70.694
Elapsed time for attention_prob_times_values (128x2048x2048x214): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x214): 57.068

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 485.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x215x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x215x2048): 69.740
Elapsed time for attention_prob_times_values (128x2048x2048x215): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x215): 54.947

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 474.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x216x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x216x2048): 71.935
Elapsed time for attention_prob_times_values (128x2048x2048x216): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x216): 55.502

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 485.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x217x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x217x2048): 69.473
Elapsed time for attention_prob_times_values (128x2048x2048x217): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x217): 55.189

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 478.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x218x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x218x2048): 70.790
Elapsed time for attention_prob_times_values (128x2048x2048x218): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x218): 57.953

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 497.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x219x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x219x2048): 69.915
Elapsed time for attention_prob_times_values (128x2048x2048x219): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x219): 55.329

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 484.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x220x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x220x2048): 71.475
Elapsed time for attention_prob_times_values (128x2048x2048x220): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x220): 58.682

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 507.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x221x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x221x2048): 70.349
Elapsed time for attention_prob_times_values (128x2048x2048x221): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x221): 56.247

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 494.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x222x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x222x2048): 71.624
Elapsed time for attention_prob_times_values (128x2048x2048x222): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x222): 58.839

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 512.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x223x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x223x2048): 70.740
Elapsed time for attention_prob_times_values (128x2048x2048x223): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x223): 56.453

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 500.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x224x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x224x2048): 81.126
Elapsed time for attention_prob_times_values (128x2048x2048x224): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x224): 59.615

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 549.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x225x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x225x2048): 69.154
Elapsed time for attention_prob_times_values (128x2048x2048x225): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x225): 57.512

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 504.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x226x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x226x2048): 70.770
Elapsed time for attention_prob_times_values (128x2048x2048x226): 0.0041
========================================================================================================================
num_attention_heads: 16, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1948x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1948x2048): 88.260
Elapsed time for attention_prob_times_values (64x2048x2048x1948): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1948): 84.359

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2711.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1949x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1949x2048): 86.688
Elapsed time for attention_prob_times_values (64x2048x2048x1949): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1949): 82.200

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2654.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1950x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1950x2048): 87.738
Elapsed time for attention_prob_times_values (64x2048x2048x1950): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1950): 84.322

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2706.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1951x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1951x2048): 86.786
Elapsed time for attention_prob_times_values (64x2048x2048x1951): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1951): 82.308

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2660.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1952x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1952x2048): 95.349
Elapsed time for attention_prob_times_values (64x2048x2048x1952): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1952): 83.841

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2810.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1953x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1953x2048): 86.821
Elapsed time for attention_prob_times_values (64x2048x2048x1953): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1953): 77.380

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2578.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1954x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1954x2048): 82.779
Elapsed time for attention_prob_times_values (64x2048x2048x1954): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1954): 77.242

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2519.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1955x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1955x2048): 80.969
Elapsed time for attention_prob_times_values (64x2048x2048x1955): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1955): 82.410

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2576.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1956x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1956x2048): 89.305
Elapsed time for attention_prob_times_values (64x2048x2048x1956): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1956): 84.634

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2742.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1957x2048): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x226): 59.713

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 522.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x227x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x227x2048): 69.602
Elapsed time for attention_prob_times_values (128x2048x2048x227): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x227): 57.735

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 510.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x228x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x228x2048): 71.317
Elapsed time for attention_prob_times_values (128x2048x2048x228): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x228): 60.334

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 531.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x229x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x229x2048): 69.949
Elapsed time for attention_prob_times_values (128x2048x2048x229): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x229): 58.011

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 517.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x230x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x230x2048): 71.422
Elapsed time for attention_prob_times_values (128x2048x2048x230): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x230): 60.419

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 535.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x231x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x231x2048): 70.350
Elapsed time for attention_prob_times_values (128x2048x2048x231): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x231): 58.436

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 524.699
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x232x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x232x2048): 72.720
Elapsed time for attention_prob_times_values (128x2048x2048x232): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x232): 59.014

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 537.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x233x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x233x2048): 69.779
Elapsed time for attention_prob_times_values (128x2048x2048x233): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x233): 58.550

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 527.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x234x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x234x2048): 71.215
Elapsed time for attention_prob_times_values (128x2048x2048x234): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x234): 61.252

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 547.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x235x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x235x2048): 70.187
Elapsed time for attention_prob_times_values (128x2048x2048x235): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x235): 58.853

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 534.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x236x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x236x2048): 71.892
Elapsed time for attention_prob_times_values (128x2048x2048x236): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x236): 61.862

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 556.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x237x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x237x2048): 70.457
Elapsed time for attention_prob_times_values (128x2048x2048x237): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x237): 59.255

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 541.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x238x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x238x2048): 72.058
Elapsed time for attention_prob_times_values (128x2048x2048x238): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x238): 62.104

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 562.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x239x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x239x2048): 70.827
Elapsed time for attention_prob_times_values (128x2048x2048x239): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x239): 59.698

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 548.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x240x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x240x2048): 56.027
Elapsed time for attention_prob_times_values (128x2048x2048x240): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x240): 62.204

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 501.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x241x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x241x2048): 53.712
Elapsed time for attention_prob_times_values (128x2048x2048x241): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x241): 60.085

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 483.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x242x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x242x2048): 54.592
Elapsed time for attention_prob_times_values (128x2048x2048x242): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x242): 62.860

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 500.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x243x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x243x2048): 53.998
Elapsed time for attention_prob_times_values (128x2048x2048x243): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x243): 60.389

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 489.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x244x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x244x2048): 55.024
Elapsed time for attention_prob_times_values (128x2048x2048x244): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x244): 63.438

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 508.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x245x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1957x2048): 87.519
Elapsed time for attention_prob_times_values (64x2048x2048x1957): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1957): 82.425

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2680.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1958x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1958x2048): 88.535
Elapsed time for attention_prob_times_values (64x2048x2048x1958): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1958): 84.225

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2727.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1959x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1959x2048): 85.981
Elapsed time for attention_prob_times_values (64x2048x2048x1959): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1959): 82.086

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2654.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1960x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1960x2048): 89.296
Elapsed time for attention_prob_times_values (64x2048x2048x1960): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1960): 76.919

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2613.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1961x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1961x2048): 79.363
Elapsed time for attention_prob_times_values (64x2048x2048x1961): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1961): 77.659

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2483.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1962x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1962x2048): 88.010
Elapsed time for attention_prob_times_values (64x2048x2048x1962): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1962): 84.750

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2733.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1963x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1963x2048): 87.219
Elapsed time for attention_prob_times_values (64x2048x2048x1963): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1963): 82.537

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2686.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1964x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1964x2048): 89.054
Elapsed time for attention_prob_times_values (64x2048x2048x1964): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1964): 84.913

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2754.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1965x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1965x2048): 86.193
Elapsed time for attention_prob_times_values (64x2048x2048x1965): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1965): 81.489

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2655.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1966x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1966x2048): 88.373
Elapsed time for attention_prob_times_values (64x2048x2048x1966): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1966): 84.842

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2745.933
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x245x2048): 54.324
Elapsed time for attention_prob_times_values (128x2048x2048x245): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x245): 61.018

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 497.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x246x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x246x2048): 54.960
Elapsed time for attention_prob_times_values (128x2048x2048x246): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x246): 63.727

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 512.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x247x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x247x2048): 54.631
Elapsed time for attention_prob_times_values (128x2048x2048x247): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x247): 61.235

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 503.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x248x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x248x2048): 56.030
Elapsed time for attention_prob_times_values (128x2048x2048x248): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x248): 62.557

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 517.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x249x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x249x2048): 53.929
Elapsed time for attention_prob_times_values (128x2048x2048x249): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x249): 56.771

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 485.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x250x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x250x2048): 54.402
Elapsed time for attention_prob_times_values (128x2048x2048x250): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x250): 64.429

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 519.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x251x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x251x2048): 54.148
Elapsed time for attention_prob_times_values (128x2048x2048x251): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x251): 62.881

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 514.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x252x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x252x2048): 55.101
Elapsed time for attention_prob_times_values (128x2048x2048x252): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x252): 64.737

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 528.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x253x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x253x2048): 54.057
Elapsed time for attention_prob_times_values (128x2048x2048x253): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x253): 60.143

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 507.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x254x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x254x2048): 54.872
Elapsed time for attention_prob_times_values (128x2048x2048x254): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x254): 65.370

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 533.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1967x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1967x2048): 87.132
Elapsed time for attention_prob_times_values (64x2048x2048x1967): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1967): 82.665

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2692.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1968x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1968x2048): 89.320
Elapsed time for attention_prob_times_values (64x2048x2048x1968): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1968): 84.375

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2755.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1969x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1969x2048): 86.033
Elapsed time for attention_prob_times_values (64x2048x2048x1969): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1969): 81.630

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2661.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1970x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1970x2048): 87.865
Elapsed time for attention_prob_times_values (64x2048x2048x1970): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1970): 84.946

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2745.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1971x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1971x2048): 86.855
Elapsed time for attention_prob_times_values (64x2048x2048x1971): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1971): 82.832

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2696.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1972x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1972x2048): 88.590
Elapsed time for attention_prob_times_values (64x2048x2048x1972): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1972): 85.225

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2763.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1973x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1973x2048): 86.286
Elapsed time for attention_prob_times_values (64x2048x2048x1973): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1973): 82.899

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2691.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1974x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1974x2048): 87.915
Elapsed time for attention_prob_times_values (64x2048x2048x1974): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1974): 85.203

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2755.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1975x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1975x2048): 86.917
Elapsed time for attention_prob_times_values (64x2048x2048x1975): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1975): 82.935

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2704.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x255x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x255x2048): 53.921
Elapsed time for attention_prob_times_values (128x2048x2048x255): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x255): 59.708

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 508.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x256x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x256x2048): 77.180
Elapsed time for attention_prob_times_values (128x2048x2048x256): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x256): 69.781

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 659.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x257x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x257x2048): 57.089
Elapsed time for attention_prob_times_values (128x2048x2048x257): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x257): 53.560

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 499.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x258x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x258x2048): 57.610
Elapsed time for attention_prob_times_values (128x2048x2048x258): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x258): 57.430

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 521.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x259x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x259x2048): 57.108
Elapsed time for attention_prob_times_values (128x2048x2048x259): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x259): 54.476

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 507.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x260x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x260x2048): 58.019
Elapsed time for attention_prob_times_values (128x2048x2048x260): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x260): 57.177

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 525.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x261x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x261x2048): 57.055
Elapsed time for attention_prob_times_values (128x2048x2048x261): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x261): 54.994

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 512.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x262x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x262x2048): 57.637
Elapsed time for attention_prob_times_values (128x2048x2048x262): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x262): 57.665

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 529.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x263x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x263x2048): 56.838
Elapsed time for attention_prob_times_values (128x2048x2048x263): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x263): 55.592

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 518.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x264x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x264x2048): 58.026
Elapsed time for attention_prob_times_values (128x2048x2048x264): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x264): 54.487

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 519.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x265x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x265x2048): 55.899
Elapsed time for attention_prob_times_values (128x2048x2048x265): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x265): 55.432

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 516.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x266x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x266x2048): 56.414
Elapsed time for attention_prob_times_values (128x2048x2048x266): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x266): 58.060

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 532.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x267x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x267x2048): 56.237
Elapsed time for attention_prob_times_values (128x2048x2048x267): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x267): 55.809

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 523.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x268x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x268x2048): 57.213
Elapsed time for attention_prob_times_values (128x2048x2048x268): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x268): 58.977

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 544.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x269x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x269x2048): 56.611
Elapsed time for attention_prob_times_values (128x2048x2048x269): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x269): 56.439

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 531.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x270x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x270x2048): 57.265
Elapsed time for attention_prob_times_values (128x2048x2048x270): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x270): 59.062

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 548.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x271x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x271x2048): 56.780
Elapsed time for attention_prob_times_values (128x2048x2048x271): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x271): 56.509

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 536.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x272x2048): 0.0939
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x272x2048): 3.111
Elapsed time for attention_prob_times_values (128x2048x2048x272): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x272): 74.201

Attention duration (in seconds): 0.0978
Attention throughput (in TFLOP/s): 56.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0978
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x273x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x273x2048): 56.256
Elapsed time for attention_prob_times_values (128x2048x2048x273): 0.0052
num_attention_heads: 16, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1976x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1976x2048): 88.399
Elapsed time for attention_prob_times_values (64x2048x2048x1976): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1976): 80.552

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2686.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1977x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1977x2048): 86.815
Elapsed time for attention_prob_times_values (64x2048x2048x1977): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1977): 83.036

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2706.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1978x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1978x2048): 87.895
Elapsed time for attention_prob_times_values (64x2048x2048x1978): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1978): 85.242

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2761.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1979x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1979x2048): 87.010
Elapsed time for attention_prob_times_values (64x2048x2048x1979): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1979): 82.297

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2700.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1980x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1980x2048): 88.765
Elapsed time for attention_prob_times_values (64x2048x2048x1980): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1980): 85.552

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2782.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1981x2048): 0.1010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1981x2048): 10.526
Elapsed time for attention_prob_times_values (64x2048x2048x1981): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1981): 81.945

Attention duration (in seconds): 0.1140
Attention throughput (in TFLOP/s): 596.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1982x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1982x2048): 86.796
Elapsed time for attention_prob_times_values (64x2048x2048x1982): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1982): 78.449

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2634.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1983x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1983x2048): 87.072
Elapsed time for attention_prob_times_values (64x2048x2048x1983): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1983): 83.105

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2720.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1984x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1984x2048): 96.858
Elapsed time for attention_prob_times_values (64x2048x2048x1984): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1984): 92.235

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 3023.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1985x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1985x2048): 87.487
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x273): 56.905

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 539.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x274x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x274x2048): 57.029
Elapsed time for attention_prob_times_values (128x2048x2048x274): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x274): 59.598

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 557.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x275x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x275x2048): 56.470
Elapsed time for attention_prob_times_values (128x2048x2048x275): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x275): 57.319

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 545.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x276x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x276x2048): 57.429
Elapsed time for attention_prob_times_values (128x2048x2048x276): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x276): 60.472

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 567.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x277x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x277x2048): 56.844
Elapsed time for attention_prob_times_values (128x2048x2048x277): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x277): 57.632

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 552.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x278x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x278x2048): 57.371
Elapsed time for attention_prob_times_values (128x2048x2048x278): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x278): 60.428

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 570.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x279x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x279x2048): 57.197
Elapsed time for attention_prob_times_values (128x2048x2048x279): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x279): 58.247

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 560.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x280x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x280x2048): 58.502
Elapsed time for attention_prob_times_values (128x2048x2048x280): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x280): 75.645

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 643.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x281x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x281x2048): 56.328
Elapsed time for attention_prob_times_values (128x2048x2048x281): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x281): 58.648

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 562.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x282x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x282x2048): 56.821
Elapsed time for attention_prob_times_values (128x2048x2048x282): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x282): 61.155

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 578.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x283x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x283x2048): 56.726
Elapsed time for attention_prob_times_values (128x2048x2048x283): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x283): 58.872

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 568.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x284x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x284x2048): 57.843
Elapsed time for attention_prob_times_values (128x2048x2048x284): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x284): 61.920

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 590.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x285x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x285x2048): 56.987
Elapsed time for attention_prob_times_values (128x2048x2048x285): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x285): 59.192

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 575.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x286x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x286x2048): 57.627
Elapsed time for attention_prob_times_values (128x2048x2048x286): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x286): 61.936

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 593.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x287x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x287x2048): 57.245
Elapsed time for attention_prob_times_values (128x2048x2048x287): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x287): 59.768

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 582.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x288x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x288x2048): 77.933
Elapsed time for attention_prob_times_values (128x2048x2048x288): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x288): 77.214

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 775.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x289x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x289x2048): 60.817
Elapsed time for attention_prob_times_values (128x2048x2048x289): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x289): 60.343

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 607.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x290x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x290x2048): 61.644
Elapsed time for attention_prob_times_values (128x2048x2048x290): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x290): 62.526

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 624.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x291x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x291x2048): 60.896
Elapsed time for attention_prob_times_values (128x2048x2048x291): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x291): 60.494

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 612.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x292x2048): 0.0050
Elapsed time for attention_prob_times_values (64x2048x2048x1985): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1985): 79.263

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2662.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1986x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1986x2048): 89.190
Elapsed time for attention_prob_times_values (64x2048x2048x1986): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1986): 85.685

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2799.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1987x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1987x2048): 87.758
Elapsed time for attention_prob_times_values (64x2048x2048x1987): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1987): 83.152

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2736.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1988x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1988x2048): 89.077
Elapsed time for attention_prob_times_values (64x2048x2048x1988): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1988): 84.630

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2782.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1989x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1989x2048): 87.506
Elapsed time for attention_prob_times_values (64x2048x2048x1989): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1989): 83.131

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2735.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1990x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1990x2048): 88.585
Elapsed time for attention_prob_times_values (64x2048x2048x1990): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1990): 85.784

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2797.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1991x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1991x2048): 86.536
Elapsed time for attention_prob_times_values (64x2048x2048x1991): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1991): 82.601

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2713.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1992x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1992x2048): 89.875
Elapsed time for attention_prob_times_values (64x2048x2048x1992): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1992): 91.662

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2915.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1993x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1993x2048): 87.295
Elapsed time for attention_prob_times_values (64x2048x2048x1993): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1993): 83.527

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2743.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1994x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1994x2048): 87.232
Elapsed time for attention_prob_times_values (64x2048x2048x1994): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1994): 85.853

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2782.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x292x2048): 62.627
Elapsed time for attention_prob_times_values (128x2048x2048x292): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x292): 63.227

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 637.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x293x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x293x2048): 60.837
Elapsed time for attention_prob_times_values (128x2048x2048x293): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x293): 60.992

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 618.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x294x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x294x2048): 61.441
Elapsed time for attention_prob_times_values (128x2048x2048x294): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x294): 63.358

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 635.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x295x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x295x2048): 60.488
Elapsed time for attention_prob_times_values (128x2048x2048x295): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x295): 61.456

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 623.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x296x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x296x2048): 62.360
Elapsed time for attention_prob_times_values (128x2048x2048x296): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x296): 79.710

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 717.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x297x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x297x2048): 59.543
Elapsed time for attention_prob_times_values (128x2048x2048x297): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x297): 61.713

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 623.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x298x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x298x2048): 60.078
Elapsed time for attention_prob_times_values (128x2048x2048x298): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x298): 64.436

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 641.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x299x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x299x2048): 59.245
Elapsed time for attention_prob_times_values (128x2048x2048x299): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x299): 61.858

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 626.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x300x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x300x2048): 61.421
Elapsed time for attention_prob_times_values (128x2048x2048x300): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x300): 65.295

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 656.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x301x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x301x2048): 60.221
Elapsed time for attention_prob_times_values (128x2048x2048x301): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x301): 63.928

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 645.387
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1995x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1995x2048): 87.342
Elapsed time for attention_prob_times_values (64x2048x2048x1995): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1995): 83.586

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2748.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1996x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1996x2048): 88.898
Elapsed time for attention_prob_times_values (64x2048x2048x1996): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1996): 86.051

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2814.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1997x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1997x2048): 86.681
Elapsed time for attention_prob_times_values (64x2048x2048x1997): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1997): 83.706

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2742.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1998x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1998x2048): 88.154
Elapsed time for attention_prob_times_values (64x2048x2048x1998): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1998): 85.959

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2804.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 31984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1999x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1999x2048): 87.247
Elapsed time for attention_prob_times_values (64x2048x2048x1999): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1999): 83.569

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2751.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2000x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2000x2048): 90.036
Elapsed time for attention_prob_times_values (64x2048x2048x2000): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2000): 92.255

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2939.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2001x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2001x2048): 86.841
Elapsed time for attention_prob_times_values (64x2048x2048x2001): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2001): 83.797

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2752.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2002x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2002x2048): 87.836
Elapsed time for attention_prob_times_values (64x2048x2048x2002): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2002): 86.133

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2807.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2003x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2003x2048): 86.905
Elapsed time for attention_prob_times_values (64x2048x2048x2003): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2003): 83.767

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2755.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x302x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x302x2048): 60.675
Elapsed time for attention_prob_times_values (128x2048x2048x302): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x302): 66.233

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 661.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x303x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x303x2048): 60.119
Elapsed time for attention_prob_times_values (128x2048x2048x303): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x303): 63.966

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 648.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x304x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x304x2048): 61.721
Elapsed time for attention_prob_times_values (128x2048x2048x304): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x304): 76.175

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 716.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x305x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x305x2048): 59.595
Elapsed time for attention_prob_times_values (128x2048x2048x305): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x305): 64.525

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 652.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x306x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x306x2048): 60.366
Elapsed time for attention_prob_times_values (128x2048x2048x306): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x306): 66.888

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 670.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x307x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x307x2048): 59.993
Elapsed time for attention_prob_times_values (128x2048x2048x307): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x307): 64.376

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 657.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x308x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x308x2048): 61.540
Elapsed time for attention_prob_times_values (128x2048x2048x308): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x308): 67.000

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 681.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x309x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x309x2048): 60.437
Elapsed time for attention_prob_times_values (128x2048x2048x309): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x309): 65.620

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 670.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x310x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x310x2048): 60.936
Elapsed time for attention_prob_times_values (128x2048x2048x310): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x310): 68.053

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 687.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x311x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x311x2048): 60.601
Elapsed time for attention_prob_times_values (128x2048x2048x311): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x311): 65.871

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 676.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x312x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x312x2048): 62.270
Elapsed time for attention_prob_times_values (128x2048x2048x312): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x312): 84.346

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 770.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x313x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x313x2048): 59.867
Elapsed time for attention_prob_times_values (128x2048x2048x313): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x313): 65.763

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 675.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x314x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x314x2048): 60.384
Elapsed time for attention_prob_times_values (128x2048x2048x314): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x314): 67.626

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 689.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x315x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x315x2048): 60.262
Elapsed time for attention_prob_times_values (128x2048x2048x315): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x315): 66.350

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 684.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x316x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x316x2048): 61.661
Elapsed time for attention_prob_times_values (128x2048x2048x316): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x316): 67.959

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 703.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x317x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x317x2048): 60.442
Elapsed time for attention_prob_times_values (128x2048x2048x317): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x317): 67.129

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 693.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x318x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x318x2048): 61.018
Elapsed time for attention_prob_times_values (128x2048x2048x318): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x318): 68.573

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 706.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x319x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x319x2048): 60.409
Elapsed time for attention_prob_times_values (128x2048x2048x319): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x319): 67.407

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 698.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x320x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x320x2048): 78.851
--------
Elapsed time for attention_key_query_prob (64x2048x2004x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2004x2048): 88.606
Elapsed time for attention_prob_times_values (64x2048x2048x2004): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2004): 86.234

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2824.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2005x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2005x2048): 86.971
Elapsed time for attention_prob_times_values (64x2048x2048x2005): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2005): 83.852

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2760.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2006x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2006x2048): 87.873
Elapsed time for attention_prob_times_values (64x2048x2048x2006): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2006): 86.130

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2813.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2007x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2007x2048): 87.040
Elapsed time for attention_prob_times_values (64x2048x2048x2007): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2007): 83.732

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2762.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2008x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2008x2048): 89.606
Elapsed time for attention_prob_times_values (64x2048x2048x2008): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2008): 92.045

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2939.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2009x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2009x2048): 87.279
Elapsed time for attention_prob_times_values (64x2048x2048x2009): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2009): 83.915

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2771.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2010x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2010x2048): 87.763
Elapsed time for attention_prob_times_values (64x2048x2048x2010): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2010): 86.250

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2819.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2011x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2011x2048): 86.681
Elapsed time for attention_prob_times_values (64x2048x2048x2011): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2011): 83.852

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2763.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2012x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2012x2048): 88.406
Elapsed time for attention_prob_times_values (64x2048x2048x2012): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2012): 86.078

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2829.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2013x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2013x2048): 86.677
Elapsed time for attention_prob_times_values (64x2048x2048x2013): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2013): 83.959

Attention duration (in seconds): 0.0253
Elapsed time for attention_prob_times_values (128x2048x2048x320): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x320): 82.734

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 888.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x321x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x321x2048): 63.226
Elapsed time for attention_prob_times_values (128x2048x2048x321): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x321): 58.795

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 672.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x322x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x322x2048): 64.200
Elapsed time for attention_prob_times_values (128x2048x2048x322): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x322): 61.427

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 694.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x323x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x323x2048): 63.304
Elapsed time for attention_prob_times_values (128x2048x2048x323): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x323): 56.816

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 664.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x324x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x324x2048): 65.014
Elapsed time for attention_prob_times_values (128x2048x2048x324): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x324): 61.903

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 705.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x325x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x325x2048): 63.146
Elapsed time for attention_prob_times_values (128x2048x2048x325): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x325): 59.460

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 683.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x326x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x326x2048): 63.839
Elapsed time for attention_prob_times_values (128x2048x2048x326): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x326): 62.211

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 704.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x327x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x327x2048): 62.798
Elapsed time for attention_prob_times_values (128x2048x2048x327): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x327): 59.828

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 687.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x328x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x328x2048): 64.888
Elapsed time for attention_prob_times_values (128x2048x2048x328): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x328): 74.156

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 778.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x329x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x329x2048): 61.837
Elapsed time for attention_prob_times_values (128x2048x2048x329): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x329): 59.659

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 685.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 2768.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2014x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2014x2048): 87.762
Elapsed time for attention_prob_times_values (64x2048x2048x2014): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2014): 86.198

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2823.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2015x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2015x2048): 86.858
Elapsed time for attention_prob_times_values (64x2048x2048x2015): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2015): 83.845

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2771.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2016x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2016x2048): 96.763
Elapsed time for attention_prob_times_values (64x2048x2048x2016): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2016): 93.253

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 3086.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2017x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2017x2048): 87.593
Elapsed time for attention_prob_times_values (64x2048x2048x2017): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2017): 84.048

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2789.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2018x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2018x2048): 88.706
Elapsed time for attention_prob_times_values (64x2048x2048x2018): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2018): 86.453

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2848.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2019x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2019x2048): 87.462
Elapsed time for attention_prob_times_values (64x2048x2048x2019): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2019): 83.909

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2787.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2020x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2020x2048): 89.289
Elapsed time for attention_prob_times_values (64x2048x2048x2020): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2020): 86.453

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2860.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2021x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2021x2048): 87.568
Elapsed time for attention_prob_times_values (64x2048x2048x2021): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2021): 83.796

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2790.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2022x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2022x2048): 88.495
Elapsed time for attention_prob_times_values (64x2048x2048x2022): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2022): 86.456

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2850.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x330x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x330x2048): 62.547
Elapsed time for attention_prob_times_values (128x2048x2048x330): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x330): 62.714

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 708.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x331x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x331x2048): 62.252
Elapsed time for attention_prob_times_values (128x2048x2048x331): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x331): 59.110

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 687.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x332x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x332x2048): 63.856
Elapsed time for attention_prob_times_values (128x2048x2048x332): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x332): 63.326

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 723.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x333x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x333x2048): 62.339
Elapsed time for attention_prob_times_values (128x2048x2048x333): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x333): 58.329

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 687.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x334x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x334x2048): 63.151
Elapsed time for attention_prob_times_values (128x2048x2048x334): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x334): 63.418

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 723.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x335x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x335x2048): 62.493
Elapsed time for attention_prob_times_values (128x2048x2048x335): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x335): 59.808

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 700.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x336x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x336x2048): 64.225
Elapsed time for attention_prob_times_values (128x2048x2048x336): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x336): 76.075

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 800.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x337x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x337x2048): 61.683
Elapsed time for attention_prob_times_values (128x2048x2048x337): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x337): 59.851

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 700.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x338x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x338x2048): 62.560
Elapsed time for attention_prob_times_values (128x2048x2048x338): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x338): 62.998

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 725.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x339x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x339x2048): 62.173
Elapsed time for attention_prob_times_values (128x2048x2048x339): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x339): 61.162

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 714.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x340x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x340x2048): 63.352
Elapsed time for attention_prob_times_values (128x2048x2048x340): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x340): 64.481

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 742.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x341x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x341x2048): 62.301
Elapsed time for attention_prob_times_values (128x2048x2048x341): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x341): 60.720

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 716.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x342x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x342x2048): 63.011
Elapsed time for attention_prob_times_values (128x2048x2048x342): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x342): 63.831

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 741.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x343x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x343x2048): 62.713
Elapsed time for attention_prob_times_values (128x2048x2048x343): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x343): 61.804

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 729.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x344x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x344x2048): 64.525
Elapsed time for attention_prob_times_values (128x2048x2048x344): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x344): 77.676

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 828.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x345x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x345x2048): 61.791
Elapsed time for attention_prob_times_values (128x2048x2048x345): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x345): 60.155

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 718.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x346x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x346x2048): 62.305
Elapsed time for attention_prob_times_values (128x2048x2048x346): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x346): 62.987

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 739.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x347x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x347x2048): 61.992
Elapsed time for attention_prob_times_values (128x2048x2048x347): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x347): 59.974

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 722.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x348x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x348x2048): 63.462
Elapsed time for attention_prob_times_values (128x2048x2048x348): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x348): 61.912

Attention duration (in seconds): 0.0119
========================================================================================================================
num_attention_heads: 16, hidden_size: 32368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2023x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2023x2048): 88.264
Elapsed time for attention_prob_times_values (64x2048x2048x2023): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2023): 83.811

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2803.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2024x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2024x2048): 91.153
Elapsed time for attention_prob_times_values (64x2048x2048x2024): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2024): 92.620

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2997.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2025x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2025x2048): 88.704
Elapsed time for attention_prob_times_values (64x2048x2048x2025): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2025): 83.711

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2811.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2026x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2026x2048): 88.049
Elapsed time for attention_prob_times_values (64x2048x2048x2026): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2026): 86.319

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2846.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2027x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2027x2048): 87.272
Elapsed time for attention_prob_times_values (64x2048x2048x2027): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2027): 83.784

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2793.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2028x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2028x2048): 88.417
Elapsed time for attention_prob_times_values (64x2048x2048x2028): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2028): 86.529

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2858.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2029x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2029x2048): 87.140
Elapsed time for attention_prob_times_values (64x2048x2048x2029): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2029): 83.959

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2796.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2030x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2030x2048): 88.102
Elapsed time for attention_prob_times_values (64x2048x2048x2030): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2030): 86.482

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2855.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2031x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2031x2048): 87.179
Elapsed time for attention_prob_times_values (64x2048x2048x2031): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2031): 83.931

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2799.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2032x2048): 0.0121
Attention throughput (in TFLOP/s): 744.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x349x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x349x2048): 62.191
Elapsed time for attention_prob_times_values (128x2048x2048x349): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x349): 62.054

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 739.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x350x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x350x2048): 61.110
Elapsed time for attention_prob_times_values (128x2048x2048x350): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x350): 63.546

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 743.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x351x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x351x2048): 60.055
Elapsed time for attention_prob_times_values (128x2048x2048x351): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x351): 62.091

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 730.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x352x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x352x2048): 81.446
Elapsed time for attention_prob_times_values (128x2048x2048x352): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x352): 76.672

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 947.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x353x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x353x2048): 65.085
Elapsed time for attention_prob_times_values (128x2048x2048x353): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x353): 63.690

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 774.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x354x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x354x2048): 62.703
Elapsed time for attention_prob_times_values (128x2048x2048x354): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x354): 64.153

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 764.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x355x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x355x2048): 65.012
Elapsed time for attention_prob_times_values (128x2048x2048x355): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x355): 64.007

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 780.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x356x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x356x2048): 66.532
Elapsed time for attention_prob_times_values (128x2048x2048x356): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x356): 64.698

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 795.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x357x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x357x2048): 62.194
Elapsed time for attention_prob_times_values (128x2048x2048x357): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x357): 64.353

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 768.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x358x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x358x2048): 65.530
Elapsed time for attention_prob_times_values (128x2048x2048x358): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x358): 64.652

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 793.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x359x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x359x2048): 64.442
Elapsed time for attention_prob_times_values (128x2048x2048x359): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x359): 64.832

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 789.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x360x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x360x2048): 63.770
Elapsed time for attention_prob_times_values (128x2048x2048x360): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x360): 80.826

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 873.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x361x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x361x2048): 63.630
Elapsed time for attention_prob_times_values (128x2048x2048x361): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x361): 62.593

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 775.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x362x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x362x2048): 64.274
Elapsed time for attention_prob_times_values (128x2048x2048x362): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x362): 65.285

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 797.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x363x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x363x2048): 63.925
Elapsed time for attention_prob_times_values (128x2048x2048x363): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x363): 62.901

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 782.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x364x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x364x2048): 65.111
Elapsed time for attention_prob_times_values (128x2048x2048x364): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x364): 65.944

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 810.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x365x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x365x2048): 64.131
Elapsed time for attention_prob_times_values (128x2048x2048x365): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x365): 63.387

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 790.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x366x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x366x2048): 61.825
Elapsed time for attention_prob_times_values (128x2048x2048x366): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x366): 66.099

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 794.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x367x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x367x2048): 64.245
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2032x2048): 89.957
Elapsed time for attention_prob_times_values (64x2048x2048x2032): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2032): 93.537

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 3003.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2033x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2033x2048): 86.994
Elapsed time for attention_prob_times_values (64x2048x2048x2033): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2033): 84.074

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2801.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2034x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2034x2048): 87.897
Elapsed time for attention_prob_times_values (64x2048x2048x2034): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2034): 86.526

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2858.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2035x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2035x2048): 88.091
Elapsed time for attention_prob_times_values (64x2048x2048x2035): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2035): 84.033

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2821.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2036x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2036x2048): 88.826
Elapsed time for attention_prob_times_values (64x2048x2048x2036): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2036): 86.773

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2880.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2037x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2037x2048): 88.002
Elapsed time for attention_prob_times_values (64x2048x2048x2037): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2037): 83.979

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2821.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2038x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2038x2048): 89.809
Elapsed time for attention_prob_times_values (64x2048x2048x2038): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2038): 85.901

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2884.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2039x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2039x2048): 90.797
Elapsed time for attention_prob_times_values (64x2048x2048x2039): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2039): 80.996

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2813.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2040x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2040x2048): 88.969
Elapsed time for attention_prob_times_values (64x2048x2048x2040): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2040): 89.645

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 2935.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0245
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2041x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2041x2048): 90.648
Elapsed time for attention_prob_times_values (64x2048x2048x2041): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2041): 79.049

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2777.681
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (128x2048x2048x367): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x367): 63.766

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 798.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x368x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x368x2048): 63.592
Elapsed time for attention_prob_times_values (128x2048x2048x368): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x368): 82.895

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 899.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x369x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x369x2048): 60.967
Elapsed time for attention_prob_times_values (128x2048x2048x369): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x369): 63.779

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 781.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x370x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x370x2048): 64.469
Elapsed time for attention_prob_times_values (128x2048x2048x370): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x370): 66.627

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 823.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x371x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x371x2048): 63.629
Elapsed time for attention_prob_times_values (128x2048x2048x371): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x371): 64.479

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 806.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x372x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x372x2048): 61.807
Elapsed time for attention_prob_times_values (128x2048x2048x372): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x372): 67.089

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 812.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x373x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x373x2048): 63.503
Elapsed time for attention_prob_times_values (128x2048x2048x373): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x373): 64.884

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 812.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x374x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x374x2048): 62.319
Elapsed time for attention_prob_times_values (128x2048x2048x374): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x374): 67.207

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 820.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x375x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x375x2048): 62.101
Elapsed time for attention_prob_times_values (128x2048x2048x375): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x375): 64.831

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 806.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x376x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x376x2048): 65.805
Elapsed time for attention_prob_times_values (128x2048x2048x376): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x376): 84.642

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 944.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2042x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2042x2048): 89.558
Elapsed time for attention_prob_times_values (64x2048x2048x2042): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2042): 84.926

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 2868.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2043x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2043x2048): 87.961
Elapsed time for attention_prob_times_values (64x2048x2048x2043): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2043): 83.652

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2823.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2044x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2044x2048): 88.918
Elapsed time for attention_prob_times_values (64x2048x2048x2044): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2044): 86.681

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2891.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2045x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2045x2048): 86.722
Elapsed time for attention_prob_times_values (64x2048x2048x2045): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2045): 83.787

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2808.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2046x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2046x2048): 87.371
Elapsed time for attention_prob_times_values (64x2048x2048x2046): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2046): 86.637

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2868.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2047x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2047x2048): 87.079
Elapsed time for attention_prob_times_values (64x2048x2048x2047): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2047): 83.899

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2818.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x377x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x377x2048): 63.569
Elapsed time for attention_prob_times_values (128x2048x2048x377): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x377): 65.132

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 822.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x378x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x378x2048): 64.192
Elapsed time for attention_prob_times_values (128x2048x2048x378): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x378): 67.750

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 844.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x379x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x379x2048): 63.698
Elapsed time for attention_prob_times_values (128x2048x2048x379): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x379): 65.197

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 827.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x380x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x380x2048): 64.871
Elapsed time for attention_prob_times_values (128x2048x2048x380): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x380): 67.895

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 854.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x381x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x381x2048): 63.525
Elapsed time for attention_prob_times_values (128x2048x2048x381): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x381): 65.546

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 832.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x382x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x382x2048): 64.465
Elapsed time for attention_prob_times_values (128x2048x2048x382): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x382): 68.375

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 858.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x383x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x383x2048): 63.562
Elapsed time for attention_prob_times_values (128x2048x2048x383): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x383): 65.260

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 835.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x384x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x384x2048): 78.473
Elapsed time for attention_prob_times_values (128x2048x2048x384): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x384): 83.404

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1051.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x385x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x385x2048): 65.927
Elapsed time for attention_prob_times_values (128x2048x2048x385): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x385): 58.486

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 807.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x386x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x386x2048): 66.956
Elapsed time for attention_prob_times_values (128x2048x2048x386): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x386): 62.167

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 842.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x387x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x387x2048): 66.054
Elapsed time for attention_prob_times_values (128x2048x2048x387): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x387): 59.047

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 816.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x388x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x388x2048): 67.449
Elapsed time for attention_prob_times_values (128x2048x2048x388): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x388): 62.523

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 851.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x389x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x389x2048): 66.217
Elapsed time for attention_prob_times_values (128x2048x2048x389): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x389): 59.584

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 825.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x390x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x390x2048): 66.860
Elapsed time for attention_prob_times_values (128x2048x2048x390): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x390): 62.749

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 853.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x391x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x391x2048): 66.063
Elapsed time for attention_prob_times_values (128x2048x2048x391): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x391): 59.883

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 830.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x392x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x392x2048): 67.418
Elapsed time for attention_prob_times_values (128x2048x2048x392): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x392): 75.707

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 945.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x393x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x393x2048): 65.054
Elapsed time for attention_prob_times_values (128x2048x2048x393): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x393): 59.798

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 827.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x394x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x394x2048): 65.761
Elapsed time for attention_prob_times_values (128x2048x2048x394): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x394): 63.288

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 858.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x395x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x395x2048): 65.346
Elapsed time for attention_prob_times_values (128x2048x2048x395): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x395): 60.097

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 835.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x396x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x396x2048): 66.387
Elapsed time for attention_prob_times_values (128x2048x2048x396): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x396): 63.849

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 870.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x397x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x397x2048): 65.694
Elapsed time for attention_prob_times_values (128x2048x2048x397): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x397): 60.385

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 843.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x398x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x398x2048): 66.465
Elapsed time for attention_prob_times_values (128x2048x2048x398): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x398): 63.640

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 873.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x399x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x399x2048): 65.745
Elapsed time for attention_prob_times_values (128x2048x2048x399): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x399): 60.617

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 849.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x400x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x400x2048): 67.632
Elapsed time for attention_prob_times_values (128x2048x2048x400): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x400): 76.975

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 972.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x401x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x401x2048): 65.063
Elapsed time for attention_prob_times_values (128x2048x2048x401): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x401): 60.913

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 851.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x402x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x402x2048): 65.794
Elapsed time for attention_prob_times_values (128x2048x2048x402): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x402): 64.193

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 881.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x403x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x403x2048): 65.460
Elapsed time for attention_prob_times_values (128x2048x2048x403): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x403): 61.042

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 858.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x404x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x404x2048): 66.459
Elapsed time for attention_prob_times_values (128x2048x2048x404): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x404): 64.786

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 893.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x405x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x405x2048): 65.591
Elapsed time for attention_prob_times_values (128x2048x2048x405): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x405): 61.378

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 866.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x406x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x406x2048): 66.422
Elapsed time for attention_prob_times_values (128x2048x2048x406): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x406): 64.580

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 896.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x407x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x407x2048): 65.882
Elapsed time for attention_prob_times_values (128x2048x2048x407): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x407): 61.712

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 874.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x408x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x408x2048): 67.054
Elapsed time for attention_prob_times_values (128x2048x2048x408): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x408): 78.655

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 995.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x409x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x409x2048): 65.279
Elapsed time for attention_prob_times_values (128x2048x2048x409): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x409): 61.936

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 875.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x410x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x410x2048): 65.859
Elapsed time for attention_prob_times_values (128x2048x2048x410): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x410): 65.288

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 905.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x411x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x411x2048): 65.528
Elapsed time for attention_prob_times_values (128x2048x2048x411): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x411): 62.216

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 883.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x412x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x412x2048): 66.684
Elapsed time for attention_prob_times_values (128x2048x2048x412): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x412): 65.825

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 919.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x413x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x413x2048): 65.592
Elapsed time for attention_prob_times_values (128x2048x2048x413): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x413): 62.345

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 888.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x414x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x414x2048): 66.133
Elapsed time for attention_prob_times_values (128x2048x2048x414): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x414): 65.839

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 919.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x415x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x415x2048): 65.563
Elapsed time for attention_prob_times_values (128x2048x2048x415): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x415): 62.697

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 895.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x416x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x416x2048): 83.863
Elapsed time for attention_prob_times_values (128x2048x2048x416): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x416): 77.603

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1128.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x417x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x417x2048): 68.779
Elapsed time for attention_prob_times_values (128x2048x2048x417): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x417): 63.220

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 924.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x418x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x418x2048): 69.655
Elapsed time for attention_prob_times_values (128x2048x2048x418): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x418): 66.372

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 955.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x419x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x419x2048): 68.738
Elapsed time for attention_prob_times_values (128x2048x2048x419): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x419): 63.454

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 930.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x420x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x420x2048): 70.421
Elapsed time for attention_prob_times_values (128x2048x2048x420): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x420): 66.914

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 969.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x421x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x421x2048): 68.627
Elapsed time for attention_prob_times_values (128x2048x2048x421): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x421): 63.792

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 936.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x422x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x422x2048): 69.339
Elapsed time for attention_prob_times_values (128x2048x2048x422): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x422): 66.913

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 966.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x423x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x423x2048): 68.231
Elapsed time for attention_prob_times_values (128x2048x2048x423): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x423): 64.112

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 939.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x424x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x424x2048): 70.296
Elapsed time for attention_prob_times_values (128x2048x2048x424): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x424): 81.527

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1075.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x425x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x425x2048): 67.346
Elapsed time for attention_prob_times_values (128x2048x2048x425): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x425): 64.096

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 938.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x426x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x426x2048): 67.982
Elapsed time for attention_prob_times_values (128x2048x2048x426): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x426): 67.390

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 968.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x427x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x427x2048): 67.603
Elapsed time for attention_prob_times_values (128x2048x2048x427): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x427): 64.273

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 945.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x428x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x428x2048): 69.152
Elapsed time for attention_prob_times_values (128x2048x2048x428): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x428): 67.974

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 985.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x429x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x429x2048): 67.960
Elapsed time for attention_prob_times_values (128x2048x2048x429): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x429): 64.664

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 954.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x430x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x430x2048): 68.794
Elapsed time for attention_prob_times_values (128x2048x2048x430): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x430): 68.009

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 987.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x431x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x431x2048): 67.978
Elapsed time for attention_prob_times_values (128x2048x2048x431): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x431): 65.027

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 961.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x432x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x432x2048): 69.613
Elapsed time for attention_prob_times_values (128x2048x2048x432): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x432): 82.628

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1095.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x433x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x433x2048): 67.220
Elapsed time for attention_prob_times_values (128x2048x2048x433): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x433): 65.265

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 962.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x434x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x434x2048): 68.027
Elapsed time for attention_prob_times_values (128x2048x2048x434): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x434): 68.538

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 994.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x435x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x435x2048): 67.512
Elapsed time for attention_prob_times_values (128x2048x2048x435): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x435): 65.536

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 970.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x436x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x436x2048): 69.091
Elapsed time for attention_prob_times_values (128x2048x2048x436): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x436): 68.996

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1009.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x437x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x437x2048): 67.785
Elapsed time for attention_prob_times_values (128x2048x2048x437): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x437): 65.789

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 978.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x438x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x438x2048): 68.402
Elapsed time for attention_prob_times_values (128x2048x2048x438): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x438): 69.046

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1009.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x439x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x439x2048): 67.953
Elapsed time for attention_prob_times_values (128x2048x2048x439): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x439): 66.156

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 986.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x440x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x440x2048): 69.720
Elapsed time for attention_prob_times_values (128x2048x2048x440): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x440): 84.355

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1126.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x441x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x441x2048): 67.132
Elapsed time for attention_prob_times_values (128x2048x2048x441): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x441): 66.353

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 986.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x442x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x442x2048): 67.811
Elapsed time for attention_prob_times_values (128x2048x2048x442): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x442): 69.572

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1017.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x443x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x443x2048): 67.343
Elapsed time for attention_prob_times_values (128x2048x2048x443): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x443): 66.621

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 994.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x444x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x444x2048): 68.857
Elapsed time for attention_prob_times_values (128x2048x2048x444): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x444): 69.687

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1030.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x445x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x445x2048): 67.569
Elapsed time for attention_prob_times_values (128x2048x2048x445): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x445): 66.875

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1002.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x446x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x446x2048): 68.257
Elapsed time for attention_prob_times_values (128x2048x2048x446): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x446): 70.072

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1032.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x447x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x447x2048): 67.640
Elapsed time for attention_prob_times_values (128x2048x2048x447): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x447): 67.130

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1008.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x448x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x448x2048): 84.437
Elapsed time for attention_prob_times_values (128x2048x2048x448): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x448): 83.084

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1256.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x449x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x449x2048): 70.225
Elapsed time for attention_prob_times_values (128x2048x2048x449): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x449): 61.306

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 983.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x450x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x450x2048): 71.322
Elapsed time for attention_prob_times_values (128x2048x2048x450): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x450): 64.134

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1017.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x451x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x451x2048): 70.220
Elapsed time for attention_prob_times_values (128x2048x2048x451): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x451): 61.490

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 989.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x452x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x452x2048): 72.042
Elapsed time for attention_prob_times_values (128x2048x2048x452): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x452): 64.537

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1029.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x453x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x453x2048): 70.083
Elapsed time for attention_prob_times_values (128x2048x2048x453): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x453): 61.422

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 992.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x454x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x454x2048): 70.795
Elapsed time for attention_prob_times_values (128x2048x2048x454): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x454): 64.457

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1024.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x455x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x455x2048): 69.855
Elapsed time for attention_prob_times_values (128x2048x2048x455): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x455): 61.781

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 997.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x456x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x456x2048): 72.058
Elapsed time for attention_prob_times_values (128x2048x2048x456): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x456): 82.858

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1175.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x457x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x457x2048): 68.600
Elapsed time for attention_prob_times_values (128x2048x2048x457): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x457): 61.638

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 992.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x458x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x458x2048): 69.621
Elapsed time for attention_prob_times_values (128x2048x2048x458): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x458): 65.051

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1029.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x459x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x459x2048): 69.039
Elapsed time for attention_prob_times_values (128x2048x2048x459): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x459): 62.140

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1003.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x460x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x460x2048): 70.729
Elapsed time for attention_prob_times_values (128x2048x2048x460): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x460): 65.586

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1046.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x461x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x461x2048): 69.166
Elapsed time for attention_prob_times_values (128x2048x2048x461): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x461): 62.569

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1012.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x462x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x462x2048): 69.964
Elapsed time for attention_prob_times_values (128x2048x2048x462): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x462): 65.607

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1045.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x463x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x463x2048): 69.298
Elapsed time for attention_prob_times_values (128x2048x2048x463): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x463): 62.685

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1018.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x464x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x464x2048): 71.063
Elapsed time for attention_prob_times_values (128x2048x2048x464): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x464): 84.715

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1198.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x465x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x465x2048): 68.459
Elapsed time for attention_prob_times_values (128x2048x2048x465): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x465): 62.907

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1018.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x466x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x466x2048): 69.403
Elapsed time for attention_prob_times_values (128x2048x2048x466): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x466): 66.123

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1053.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x467x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x467x2048): 68.770
Elapsed time for attention_prob_times_values (128x2048x2048x467): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x467): 63.155

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1026.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x468x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x468x2048): 70.230
Elapsed time for attention_prob_times_values (128x2048x2048x468): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x468): 66.591

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1068.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x469x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x469x2048): 68.895
Elapsed time for attention_prob_times_values (128x2048x2048x469): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x469): 63.342

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1033.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x470x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x470x2048): 69.610
Elapsed time for attention_prob_times_values (128x2048x2048x470): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x470): 66.598

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1067.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x471x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x471x2048): 69.164
Elapsed time for attention_prob_times_values (128x2048x2048x471): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x471): 63.682

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1042.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x472x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x472x2048): 71.075
Elapsed time for attention_prob_times_values (128x2048x2048x472): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x472): 85.838

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1224.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x473x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x473x2048): 68.471
Elapsed time for attention_prob_times_values (128x2048x2048x473): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x473): 63.765

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1042.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x474x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x474x2048): 69.084
Elapsed time for attention_prob_times_values (128x2048x2048x474): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x474): 66.975

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1075.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x475x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x475x2048): 68.507
Elapsed time for attention_prob_times_values (128x2048x2048x475): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x475): 64.067

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1049.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x476x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x476x2048): 70.026
Elapsed time for attention_prob_times_values (128x2048x2048x476): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x476): 67.495

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1091.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x477x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x477x2048): 68.741
Elapsed time for attention_prob_times_values (128x2048x2048x477): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x477): 64.235

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1056.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x478x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x478x2048): 69.384
Elapsed time for attention_prob_times_values (128x2048x2048x478): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x478): 67.441

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1090.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x479x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x479x2048): 68.875
Elapsed time for attention_prob_times_values (128x2048x2048x479): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x479): 64.287

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1061.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x480x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x480x2048): 86.128
Elapsed time for attention_prob_times_values (128x2048x2048x480): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x480): 88.440

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1396.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x481x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x481x2048): 71.281
Elapsed time for attention_prob_times_values (128x2048x2048x481): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x481): 64.919

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1089.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x482x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x482x2048): 72.168
Elapsed time for attention_prob_times_values (128x2048x2048x482): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x482): 67.991

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1124.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x483x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x483x2048): 71.051
Elapsed time for attention_prob_times_values (128x2048x2048x483): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x483): 65.078

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1093.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x484x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x484x2048): 72.690
Elapsed time for attention_prob_times_values (128x2048x2048x484): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x484): 67.257

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1126.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x485x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x485x2048): 70.814
Elapsed time for attention_prob_times_values (128x2048x2048x485): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x485): 65.266

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1097.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x486x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x486x2048): 71.570
Elapsed time for attention_prob_times_values (128x2048x2048x486): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x486): 68.132

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1130.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x487x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x487x2048): 70.541
Elapsed time for attention_prob_times_values (128x2048x2048x487): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x487): 65.584

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1102.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x488x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x488x2048): 72.335
Elapsed time for attention_prob_times_values (128x2048x2048x488): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x488): 88.437

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1293.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x489x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x489x2048): 69.621
Elapsed time for attention_prob_times_values (128x2048x2048x489): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x489): 65.510

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1099.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x490x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x490x2048): 70.481
Elapsed time for attention_prob_times_values (128x2048x2048x490): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x490): 68.667

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1134.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x491x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x491x2048): 69.975
Elapsed time for attention_prob_times_values (128x2048x2048x491): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x491): 65.786

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1108.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x492x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x492x2048): 71.192
Elapsed time for attention_prob_times_values (128x2048x2048x492): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x492): 69.111

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1148.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x493x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x493x2048): 70.135
Elapsed time for attention_prob_times_values (128x2048x2048x493): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x493): 65.907

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1114.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x494x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x494x2048): 70.964
Elapsed time for attention_prob_times_values (128x2048x2048x494): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x494): 69.196

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1151.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x495x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x495x2048): 70.222
Elapsed time for attention_prob_times_values (128x2048x2048x495): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x495): 66.206

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1122.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x496x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x496x2048): 72.263
Elapsed time for attention_prob_times_values (128x2048x2048x496): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x496): 90.290

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1324.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x497x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x497x2048): 69.484
Elapsed time for attention_prob_times_values (128x2048x2048x497): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x497): 66.347

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1122.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x498x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x498x2048): 70.570
Elapsed time for attention_prob_times_values (128x2048x2048x498): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x498): 69.624

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1160.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x499x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x499x2048): 69.615
Elapsed time for attention_prob_times_values (128x2048x2048x499): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x499): 66.551

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1129.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x500x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x500x2048): 70.896
Elapsed time for attention_prob_times_values (128x2048x2048x500): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x500): 68.793

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1160.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x501x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x501x2048): 69.850
Elapsed time for attention_prob_times_values (128x2048x2048x501): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x501): 65.803

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1128.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x502x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x502x2048): 70.668
Elapsed time for attention_prob_times_values (128x2048x2048x502): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x502): 69.905

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1172.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x503x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x503x2048): 69.962
Elapsed time for attention_prob_times_values (128x2048x2048x503): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x503): 65.675

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1132.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x504x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x504x2048): 71.607
Elapsed time for attention_prob_times_values (128x2048x2048x504): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x504): 91.551

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1346.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x505x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x505x2048): 69.261
Elapsed time for attention_prob_times_values (128x2048x2048x505): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x505): 60.734

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1086.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x506x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x506x2048): 69.963
Elapsed time for attention_prob_times_values (128x2048x2048x506): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x506): 67.764

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1157.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x507x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x507x2048): 69.394
Elapsed time for attention_prob_times_values (128x2048x2048x507): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x507): 61.330

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1096.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x508x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x508x2048): 70.732
Elapsed time for attention_prob_times_values (128x2048x2048x508): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x508): 67.629

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1166.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x509x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x509x2048): 69.265
Elapsed time for attention_prob_times_values (128x2048x2048x509): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x509): 60.872

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1095.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x510x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x510x2048): 69.555
Elapsed time for attention_prob_times_values (128x2048x2048x510): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x510): 68.069

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1165.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x511x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x511x2048): 69.449
Elapsed time for attention_prob_times_values (128x2048x2048x511): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x511): 61.723

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1109.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x512x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x512x2048): 82.849
Elapsed time for attention_prob_times_values (128x2048x2048x512): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x512): 95.801

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1510.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x513x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x513x2048): 71.663
Elapsed time for attention_prob_times_values (128x2048x2048x513): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x513): 60.604

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1118.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x514x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x514x2048): 75.099
Elapsed time for attention_prob_times_values (128x2048x2048x514): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x514): 65.305

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1191.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x515x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x515x2048): 71.365
Elapsed time for attention_prob_times_values (128x2048x2048x515): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x515): 62.285

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1137.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x516x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x516x2048): 72.999
Elapsed time for attention_prob_times_values (128x2048x2048x516): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x516): 65.404

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1181.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x517x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x517x2048): 71.611
Elapsed time for attention_prob_times_values (128x2048x2048x517): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x517): 62.659

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1146.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x518x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x518x2048): 72.449
Elapsed time for attention_prob_times_values (128x2048x2048x518): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x518): 65.545

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1182.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x519x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x519x2048): 71.368
Elapsed time for attention_prob_times_values (128x2048x2048x519): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x519): 62.755

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1149.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x520x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x520x2048): 72.776
Elapsed time for attention_prob_times_values (128x2048x2048x520): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x520): 75.885

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1281.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x521x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x521x2048): 70.477
Elapsed time for attention_prob_times_values (128x2048x2048x521): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x521): 62.480

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1144.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x522x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x522x2048): 71.189
Elapsed time for attention_prob_times_values (128x2048x2048x522): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x522): 65.982

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1185.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x523x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x523x2048): 70.747
Elapsed time for attention_prob_times_values (128x2048x2048x523): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x523): 62.827

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1154.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x524x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x524x2048): 71.777
Elapsed time for attention_prob_times_values (128x2048x2048x524): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x524): 66.440

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1198.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x525x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x525x2048): 70.928
Elapsed time for attention_prob_times_values (128x2048x2048x525): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x525): 62.998

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1161.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x526x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x526x2048): 71.685
Elapsed time for attention_prob_times_values (128x2048x2048x526): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x526): 66.436

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1202.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x527x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x527x2048): 70.988
Elapsed time for attention_prob_times_values (128x2048x2048x527): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x527): 63.242

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1168.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x528x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x528x2048): 72.946
Elapsed time for attention_prob_times_values (128x2048x2048x528): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x528): 77.240

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1313.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x529x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x529x2048): 70.350
Elapsed time for attention_prob_times_values (128x2048x2048x529): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x529): 62.725

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1162.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x530x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x530x2048): 71.151
Elapsed time for attention_prob_times_values (128x2048x2048x530): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x530): 66.789

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1210.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x531x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x531x2048): 70.589
Elapsed time for attention_prob_times_values (128x2048x2048x531): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x531): 63.010

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1171.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x532x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x532x2048): 71.723
Elapsed time for attention_prob_times_values (128x2048x2048x532): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x532): 67.305

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1223.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x533x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x533x2048): 70.780
Elapsed time for attention_prob_times_values (128x2048x2048x533): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x533): 63.682

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1183.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x534x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x534x2048): 71.513
Elapsed time for attention_prob_times_values (128x2048x2048x534): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x534): 67.132

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1224.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x535x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x535x2048): 71.112
Elapsed time for attention_prob_times_values (128x2048x2048x535): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x535): 62.679

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1180.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x536x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x536x2048): 72.482
Elapsed time for attention_prob_times_values (128x2048x2048x536): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x536): 78.070

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1334.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x537x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x537x2048): 70.291
Elapsed time for attention_prob_times_values (128x2048x2048x537): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x537): 64.354

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1194.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x538x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x538x2048): 70.948
Elapsed time for attention_prob_times_values (128x2048x2048x538): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x538): 67.512

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1232.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x539x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x539x2048): 70.543
Elapsed time for attention_prob_times_values (128x2048x2048x539): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x539): 63.895

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1196.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x540x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x540x2048): 71.722
Elapsed time for attention_prob_times_values (128x2048x2048x540): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x540): 68.087

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1248.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x541x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x541x2048): 70.679
Elapsed time for attention_prob_times_values (128x2048x2048x541): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x541): 63.093

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1193.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x542x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x542x2048): 71.398
Elapsed time for attention_prob_times_values (128x2048x2048x542): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x542): 67.859

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1248.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x543x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x543x2048): 70.875
Elapsed time for attention_prob_times_values (128x2048x2048x543): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x543): 64.429

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1212.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x544x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x544x2048): 87.333
Elapsed time for attention_prob_times_values (128x2048x2048x544): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x544): 80.095

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1504.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x545x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x545x2048): 73.515
Elapsed time for attention_prob_times_values (128x2048x2048x545): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x545): 65.195

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1246.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x546x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x546x2048): 74.302
Elapsed time for attention_prob_times_values (128x2048x2048x546): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x546): 66.455

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1267.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x547x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x547x2048): 73.425
Elapsed time for attention_prob_times_values (128x2048x2048x547): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x547): 64.354

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1241.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x548x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x548x2048): 74.957
Elapsed time for attention_prob_times_values (128x2048x2048x548): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x548): 67.479

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1287.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x549x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x549x2048): 73.315
Elapsed time for attention_prob_times_values (128x2048x2048x549): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x549): 65.742

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1258.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x550x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x550x2048): 73.966
Elapsed time for attention_prob_times_values (128x2048x2048x550): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x550): 68.711

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1295.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x551x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x551x2048): 72.946
Elapsed time for attention_prob_times_values (128x2048x2048x551): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x551): 65.950

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1262.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x552x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x552x2048): 74.807
Elapsed time for attention_prob_times_values (128x2048x2048x552): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x552): 80.408

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1414.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x553x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x553x2048): 72.331
Elapsed time for attention_prob_times_values (128x2048x2048x553): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x553): 65.482

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1256.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x554x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x554x2048): 72.936
Elapsed time for attention_prob_times_values (128x2048x2048x554): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x554): 69.267

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1301.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x555x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x555x2048): 72.326
Elapsed time for attention_prob_times_values (128x2048x2048x555): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x555): 64.750

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1253.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x556x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x556x2048): 73.936
Elapsed time for attention_prob_times_values (128x2048x2048x556): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x556): 70.052

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1321.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x557x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x557x2048): 72.738
Elapsed time for attention_prob_times_values (128x2048x2048x557): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x557): 66.662

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1280.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x558x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x558x2048): 73.622
Elapsed time for attention_prob_times_values (128x2048x2048x558): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x558): 69.825

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1321.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x559x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x559x2048): 72.716
Elapsed time for attention_prob_times_values (128x2048x2048x559): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x559): 66.601

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1284.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x560x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x560x2048): 74.380
Elapsed time for attention_prob_times_values (128x2048x2048x560): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x560): 81.926

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1442.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x561x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x561x2048): 71.823
Elapsed time for attention_prob_times_values (128x2048x2048x561): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x561): 66.852

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1283.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x562x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x562x2048): 72.928
Elapsed time for attention_prob_times_values (128x2048x2048x562): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x562): 70.492

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1330.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x563x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x563x2048): 72.206
Elapsed time for attention_prob_times_values (128x2048x2048x563): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x563): 67.118

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1293.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x564x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x564x2048): 73.853
Elapsed time for attention_prob_times_values (128x2048x2048x564): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x564): 70.861

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1347.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x565x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x565x2048): 72.413
Elapsed time for attention_prob_times_values (128x2048x2048x565): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x565): 67.485

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1303.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x566x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x566x2048): 73.110
Elapsed time for attention_prob_times_values (128x2048x2048x566): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x566): 70.811

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1344.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x567x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x567x2048): 72.511
Elapsed time for attention_prob_times_values (128x2048x2048x567): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x567): 67.620

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1309.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x568x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x568x2048): 74.494
Elapsed time for attention_prob_times_values (128x2048x2048x568): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x568): 82.766

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1470.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x569x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x569x2048): 71.818
Elapsed time for attention_prob_times_values (128x2048x2048x569): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x569): 67.473

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1306.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x570x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x570x2048): 72.475
Elapsed time for attention_prob_times_values (128x2048x2048x570): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x570): 70.477

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1344.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x571x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x571x2048): 71.990
Elapsed time for attention_prob_times_values (128x2048x2048x571): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x571): 67.869

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1316.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x572x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x572x2048): 73.627
Elapsed time for attention_prob_times_values (128x2048x2048x572): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x572): 70.200

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1356.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x573x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x573x2048): 72.107
Elapsed time for attention_prob_times_values (128x2048x2048x573): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x573): 67.847

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1321.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x574x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x574x2048): 72.889
Elapsed time for attention_prob_times_values (128x2048x2048x574): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x574): 71.202

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1364.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x575x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x575x2048): 72.041
Elapsed time for attention_prob_times_values (128x2048x2048x575): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x575): 67.852

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1325.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x576x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x576x2048): 87.947
Elapsed time for attention_prob_times_values (128x2048x2048x576): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x576): 85.475

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1647.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x577x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x577x2048): 74.503
Elapsed time for attention_prob_times_values (128x2048x2048x577): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x577): 62.681

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1295.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x578x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x578x2048): 75.609
Elapsed time for attention_prob_times_values (128x2048x2048x578): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x578): 65.394

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1336.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x579x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x579x2048): 74.462
Elapsed time for attention_prob_times_values (128x2048x2048x579): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x579): 62.794

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1300.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x580x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x580x2048): 76.245
Elapsed time for attention_prob_times_values (128x2048x2048x580): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x580): 65.698

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1349.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x581x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x581x2048): 74.299
Elapsed time for attention_prob_times_values (128x2048x2048x581): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x581): 62.804

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1303.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x582x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x582x2048): 75.140
Elapsed time for attention_prob_times_values (128x2048x2048x582): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x582): 65.716

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1345.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x583x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x583x2048): 76.689
Elapsed time for attention_prob_times_values (128x2048x2048x583): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x583): 63.182

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1331.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x584x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x584x2048): 76.430
Elapsed time for attention_prob_times_values (128x2048x2048x584): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x584): 85.100

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1550.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x585x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x585x2048): 73.141
Elapsed time for attention_prob_times_values (128x2048x2048x585): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x585): 63.009

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1305.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x586x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x586x2048): 74.080
Elapsed time for attention_prob_times_values (128x2048x2048x586): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x586): 66.027

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1348.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x587x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x587x2048): 73.436
Elapsed time for attention_prob_times_values (128x2048x2048x587): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x587): 63.213

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1314.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x588x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x588x2048): 76.622
Elapsed time for attention_prob_times_values (128x2048x2048x588): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x588): 66.639

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1381.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x589x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x589x2048): 73.445
Elapsed time for attention_prob_times_values (128x2048x2048x589): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x589): 63.507

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1321.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x590x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x590x2048): 74.352
Elapsed time for attention_prob_times_values (128x2048x2048x590): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x590): 66.576

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1365.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x591x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x591x2048): 73.366
Elapsed time for attention_prob_times_values (128x2048x2048x591): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x591): 63.590

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1326.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x592x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x592x2048): 75.472
Elapsed time for attention_prob_times_values (128x2048x2048x592): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x592): 86.602

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1572.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x593x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x593x2048): 72.843
Elapsed time for attention_prob_times_values (128x2048x2048x593): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x593): 63.817

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1328.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x594x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x594x2048): 73.661
Elapsed time for attention_prob_times_values (128x2048x2048x594): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x594): 66.820

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1370.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x595x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x595x2048): 72.977
Elapsed time for attention_prob_times_values (128x2048x2048x595): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x595): 64.043

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1336.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x596x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x596x2048): 74.359
Elapsed time for attention_prob_times_values (128x2048x2048x596): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x596): 67.215

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1385.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x597x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x597x2048): 73.093
Elapsed time for attention_prob_times_values (128x2048x2048x597): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x597): 64.254

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1344.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x598x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x598x2048): 73.878
Elapsed time for attention_prob_times_values (128x2048x2048x598): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x598): 67.295

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1386.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x599x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x599x2048): 73.334
Elapsed time for attention_prob_times_values (128x2048x2048x599): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x599): 64.500

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1353.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x600x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x600x2048): 75.016
Elapsed time for attention_prob_times_values (128x2048x2048x600): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x600): 87.389

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1594.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x601x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x601x2048): 72.633
Elapsed time for attention_prob_times_values (128x2048x2048x601): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x601): 64.563

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1352.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x602x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x602x2048): 73.322
Elapsed time for attention_prob_times_values (128x2048x2048x602): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x602): 67.593

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1393.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x603x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x603x2048): 72.814
Elapsed time for attention_prob_times_values (128x2048x2048x603): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x603): 64.798

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1360.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x604x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x604x2048): 74.279
Elapsed time for attention_prob_times_values (128x2048x2048x604): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x604): 68.096

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1412.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x605x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x605x2048): 72.934
Elapsed time for attention_prob_times_values (128x2048x2048x605): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x605): 65.023

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1368.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x606x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x606x2048): 73.694
Elapsed time for attention_prob_times_values (128x2048x2048x606): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x606): 67.725

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1407.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x607x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x607x2048): 73.021
Elapsed time for attention_prob_times_values (128x2048x2048x607): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x607): 65.103

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1374.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x608x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x608x2048): 89.171
Elapsed time for attention_prob_times_values (128x2048x2048x608): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x608): 89.463

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1786.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x609x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x609x2048): 74.931
Elapsed time for attention_prob_times_values (128x2048x2048x609): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x609): 65.389

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1398.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x610x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x610x2048): 75.883
Elapsed time for attention_prob_times_values (128x2048x2048x610): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x610): 68.147

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1440.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x611x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x611x2048): 74.791
Elapsed time for attention_prob_times_values (128x2048x2048x611): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x611): 65.560

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1403.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x612x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x612x2048): 76.317
Elapsed time for attention_prob_times_values (128x2048x2048x612): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x612): 68.292

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1450.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x613x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x613x2048): 74.559
Elapsed time for attention_prob_times_values (128x2048x2048x613): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x613): 65.811

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1409.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x614x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x614x2048): 75.359
Elapsed time for attention_prob_times_values (128x2048x2048x614): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x614): 68.622

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1450.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x615x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x615x2048): 74.255
Elapsed time for attention_prob_times_values (128x2048x2048x615): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x615): 66.019

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1413.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x616x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x616x2048): 76.089
Elapsed time for attention_prob_times_values (128x2048x2048x616): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x616): 89.612

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1666.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x617x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x617x2048): 73.482
Elapsed time for attention_prob_times_values (128x2048x2048x617): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x617): 66.030

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1410.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x618x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x618x2048): 74.251
Elapsed time for attention_prob_times_values (128x2048x2048x618): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x618): 68.989

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1452.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x619x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x619x2048): 73.619
Elapsed time for attention_prob_times_values (128x2048x2048x619): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x619): 66.313

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1419.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x620x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x620x2048): 74.951
Elapsed time for attention_prob_times_values (128x2048x2048x620): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x620): 69.491

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1469.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x621x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x621x2048): 73.717
Elapsed time for attention_prob_times_values (128x2048x2048x621): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x621): 66.413

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1425.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x622x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x622x2048): 74.688
Elapsed time for attention_prob_times_values (128x2048x2048x622): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x622): 68.930

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1465.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x623x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x623x2048): 73.783
Elapsed time for attention_prob_times_values (128x2048x2048x623): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x623): 66.665

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1433.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x624x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x624x2048): 75.955
Elapsed time for attention_prob_times_values (128x2048x2048x624): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x624): 91.076

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1698.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x625x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x625x2048): 73.200
Elapsed time for attention_prob_times_values (128x2048x2048x625): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x625): 66.898

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1435.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x626x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x626x2048): 74.214
Elapsed time for attention_prob_times_values (128x2048x2048x626): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x626): 69.301

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1473.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x627x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x627x2048): 73.445
Elapsed time for attention_prob_times_values (128x2048x2048x627): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x627): 67.148

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1444.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x628x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x628x2048): 74.762
Elapsed time for attention_prob_times_values (128x2048x2048x628): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x628): 70.043

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1491.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x629x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x629x2048): 73.626
Elapsed time for attention_prob_times_values (128x2048x2048x629): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x629): 67.243

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1451.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x630x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x630x2048): 74.496
Elapsed time for attention_prob_times_values (128x2048x2048x630): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x630): 69.497

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1487.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x631x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x631x2048): 73.730
Elapsed time for attention_prob_times_values (128x2048x2048x631): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x631): 67.570

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1460.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x632x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x632x2048): 75.434
Elapsed time for attention_prob_times_values (128x2048x2048x632): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x632): 92.047

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1720.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x633x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x633x2048): 73.114
Elapsed time for attention_prob_times_values (128x2048x2048x633): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x633): 67.809

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1462.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x634x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x634x2048): 73.940
Elapsed time for attention_prob_times_values (128x2048x2048x634): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x634): 69.707

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1493.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x635x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x635x2048): 73.094
Elapsed time for attention_prob_times_values (128x2048x2048x635): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x635): 67.755

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1465.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x636x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x636x2048): 74.393
Elapsed time for attention_prob_times_values (128x2048x2048x636): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x636): 69.181

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1496.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x637x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x637x2048): 72.853
Elapsed time for attention_prob_times_values (128x2048x2048x637): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x637): 68.121

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1471.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x638x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x638x2048): 73.883
Elapsed time for attention_prob_times_values (128x2048x2048x638): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x638): 70.002

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1505.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x639x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x639x2048): 72.942
Elapsed time for attention_prob_times_values (128x2048x2048x639): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x639): 66.871

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1463.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x640x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x640x2048): 85.041
Elapsed time for attention_prob_times_values (128x2048x2048x640): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x640): 95.006

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1884.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x641x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x641x2048): 74.863
Elapsed time for attention_prob_times_values (128x2048x2048x641): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x641): 61.756

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1423.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x642x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x642x2048): 75.897
Elapsed time for attention_prob_times_values (128x2048x2048x642): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x642): 65.441

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1480.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x643x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x643x2048): 74.656
Elapsed time for attention_prob_times_values (128x2048x2048x643): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x643): 62.823

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1439.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x644x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x644x2048): 76.449
Elapsed time for attention_prob_times_values (128x2048x2048x644): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x644): 65.293

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1487.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x645x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x645x2048): 75.132
Elapsed time for attention_prob_times_values (128x2048x2048x645): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x645): 62.994

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1449.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x646x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x646x2048): 75.807
Elapsed time for attention_prob_times_values (128x2048x2048x646): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x646): 65.305

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1486.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x647x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x647x2048): 74.919
Elapsed time for attention_prob_times_values (128x2048x2048x647): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x647): 63.114

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1453.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x648x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x648x2048): 76.418
Elapsed time for attention_prob_times_values (128x2048x2048x648): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x648): 78.687

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1647.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x649x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x649x2048): 74.108
Elapsed time for attention_prob_times_values (128x2048x2048x649): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x649): 62.900

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1448.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x650x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x650x2048): 74.830
Elapsed time for attention_prob_times_values (128x2048x2048x650): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x650): 65.671

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1490.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x651x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x651x2048): 74.271
Elapsed time for attention_prob_times_values (128x2048x2048x651): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x651): 63.391

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1459.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x652x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x652x2048): 75.405
Elapsed time for attention_prob_times_values (128x2048x2048x652): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x652): 66.508

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1510.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x653x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x653x2048): 74.478
Elapsed time for attention_prob_times_values (128x2048x2048x653): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x653): 63.586

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1468.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x654x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x654x2048): 75.186
Elapsed time for attention_prob_times_values (128x2048x2048x654): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x654): 66.232

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1509.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x655x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x655x2048): 74.489
Elapsed time for attention_prob_times_values (128x2048x2048x655): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x655): 63.454

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1471.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x656x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x656x2048): 76.524
Elapsed time for attention_prob_times_values (128x2048x2048x656): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x656): 79.949

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1681.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x657x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x657x2048): 73.861
Elapsed time for attention_prob_times_values (128x2048x2048x657): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x657): 63.872

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1474.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x658x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x658x2048): 74.693
Elapsed time for attention_prob_times_values (128x2048x2048x658): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x658): 66.596

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1518.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x659x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x659x2048): 74.036
Elapsed time for attention_prob_times_values (128x2048x2048x659): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x659): 64.051

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1483.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x660x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x660x2048): 75.265
Elapsed time for attention_prob_times_values (128x2048x2048x660): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x660): 66.845

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1531.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x661x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x661x2048): 74.242
Elapsed time for attention_prob_times_values (128x2048x2048x661): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x661): 64.091

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1489.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x662x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x662x2048): 74.855
Elapsed time for attention_prob_times_values (128x2048x2048x662): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x662): 66.847

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1531.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x663x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x663x2048): 74.335
Elapsed time for attention_prob_times_values (128x2048x2048x663): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x663): 64.265

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1497.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x664x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x664x2048): 75.983
Elapsed time for attention_prob_times_values (128x2048x2048x664): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x664): 80.512

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1700.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x665x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x665x2048): 73.800
Elapsed time for attention_prob_times_values (128x2048x2048x665): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x665): 64.503

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1499.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x666x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x666x2048): 74.560
Elapsed time for attention_prob_times_values (128x2048x2048x666): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x666): 66.744

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1536.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x667x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x667x2048): 73.846
Elapsed time for attention_prob_times_values (128x2048x2048x667): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x667): 64.454

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1503.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x668x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x668x2048): 75.167
Elapsed time for attention_prob_times_values (128x2048x2048x668): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x668): 67.314

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1553.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x669x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x669x2048): 74.069
Elapsed time for attention_prob_times_values (128x2048x2048x669): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x669): 64.638

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1512.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x670x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x670x2048): 74.711
Elapsed time for attention_prob_times_values (128x2048x2048x670): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x670): 66.865

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1548.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x671x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x671x2048): 74.172
Elapsed time for attention_prob_times_values (128x2048x2048x671): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x671): 64.564

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1516.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x672x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x672x2048): 89.669
Elapsed time for attention_prob_times_values (128x2048x2048x672): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x672): 82.479

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1890.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x673x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x673x2048): 76.350
Elapsed time for attention_prob_times_values (128x2048x2048x673): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x673): 65.082

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1548.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x674x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x674x2048): 77.281
Elapsed time for attention_prob_times_values (128x2048x2048x674): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x674): 66.975

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1583.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x675x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x675x2048): 76.334
Elapsed time for attention_prob_times_values (128x2048x2048x675): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x675): 64.918

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1550.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x676x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x676x2048): 77.952
Elapsed time for attention_prob_times_values (128x2048x2048x676): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x676): 67.592

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1601.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x677x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x677x2048): 76.193
Elapsed time for attention_prob_times_values (128x2048x2048x677): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x677): 65.177

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1556.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x678x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x678x2048): 77.041
Elapsed time for attention_prob_times_values (128x2048x2048x678): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x678): 67.451

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1595.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x679x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x679x2048): 76.209
Elapsed time for attention_prob_times_values (128x2048x2048x679): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x679): 65.243

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1561.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x680x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x680x2048): 81.818
Elapsed time for attention_prob_times_values (128x2048x2048x680): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x680): 82.528

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1828.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x681x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x681x2048): 75.546
Elapsed time for attention_prob_times_values (128x2048x2048x681): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x681): 64.965

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1556.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x682x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x682x2048): 76.241
Elapsed time for attention_prob_times_values (128x2048x2048x682): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x682): 67.524

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1597.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x683x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x683x2048): 75.627
Elapsed time for attention_prob_times_values (128x2048x2048x683): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x683): 64.698

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1558.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x684x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x684x2048): 77.284
Elapsed time for attention_prob_times_values (128x2048x2048x684): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x684): 68.156

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1620.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x685x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x685x2048): 75.979
Elapsed time for attention_prob_times_values (128x2048x2048x685): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x685): 65.592

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1577.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x686x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x686x2048): 76.744
Elapsed time for attention_prob_times_values (128x2048x2048x686): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x686): 68.122

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1619.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x687x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x687x2048): 77.118
Elapsed time for attention_prob_times_values (128x2048x2048x687): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x687): 65.797

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1595.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x688x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x688x2048): 77.557
Elapsed time for attention_prob_times_values (128x2048x2048x688): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x688): 83.881

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1813.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x689x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x689x2048): 75.116
Elapsed time for attention_prob_times_values (128x2048x2048x689): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x689): 66.062

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1583.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x690x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x690x2048): 75.953
Elapsed time for attention_prob_times_values (128x2048x2048x690): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x690): 68.686

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1627.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x691x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x691x2048): 75.373
Elapsed time for attention_prob_times_values (128x2048x2048x691): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x691): 66.183

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1592.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x692x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x692x2048): 76.842
Elapsed time for attention_prob_times_values (128x2048x2048x692): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x692): 68.918

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1644.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x693x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x693x2048): 75.591
Elapsed time for attention_prob_times_values (128x2048x2048x693): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x693): 65.996

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1596.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x694x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x694x2048): 76.150
Elapsed time for attention_prob_times_values (128x2048x2048x694): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x694): 68.930

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1641.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x695x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x695x2048): 75.544
Elapsed time for attention_prob_times_values (128x2048x2048x695): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x695): 66.353

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1605.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x696x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x696x2048): 77.439
Elapsed time for attention_prob_times_values (128x2048x2048x696): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x696): 84.513

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1838.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x697x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x697x2048): 74.954
Elapsed time for attention_prob_times_values (128x2048x2048x697): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x697): 66.712

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1608.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x698x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x698x2048): 75.682
Elapsed time for attention_prob_times_values (128x2048x2048x698): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x698): 69.237

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1649.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x699x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x699x2048): 75.155
Elapsed time for attention_prob_times_values (128x2048x2048x699): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x699): 66.711

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1614.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x700x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x700x2048): 76.752
Elapsed time for attention_prob_times_values (128x2048x2048x700): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x700): 69.151

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1664.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x701x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x701x2048): 75.250
Elapsed time for attention_prob_times_values (128x2048x2048x701): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x701): 66.736

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1620.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x702x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x702x2048): 76.161
Elapsed time for attention_prob_times_values (128x2048x2048x702): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x702): 69.577

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1668.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x703x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x703x2048): 75.278
Elapsed time for attention_prob_times_values (128x2048x2048x703): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x703): 67.132

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1630.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x704x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x704x2048): 90.149
Elapsed time for attention_prob_times_values (128x2048x2048x704): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x704): 87.143

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 2038.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x705x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x705x2048): 77.427
Elapsed time for attention_prob_times_values (128x2048x2048x705): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x705): 64.028

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1614.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x706x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x706x2048): 78.356
Elapsed time for attention_prob_times_values (128x2048x2048x706): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x706): 66.752

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1662.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x707x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x707x2048): 77.358
Elapsed time for attention_prob_times_values (128x2048x2048x707): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x707): 64.055

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1618.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x708x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x708x2048): 78.954
Elapsed time for attention_prob_times_values (128x2048x2048x708): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x708): 67.277

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1680.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x709x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x709x2048): 77.144
Elapsed time for attention_prob_times_values (128x2048x2048x709): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x709): 64.212

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1622.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x710x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x710x2048): 77.887
Elapsed time for attention_prob_times_values (128x2048x2048x710): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x710): 67.114

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1671.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x711x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x711x2048): 76.889
Elapsed time for attention_prob_times_values (128x2048x2048x711): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x711): 64.388

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1627.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x712x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x712x2048): 78.832
Elapsed time for attention_prob_times_values (128x2048x2048x712): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x712): 86.330

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1916.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x713x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x713x2048): 76.119
Elapsed time for attention_prob_times_values (128x2048x2048x713): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x713): 63.588

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1613.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x714x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x714x2048): 76.907
slurmstepd: error: *** JOB 1509445 ON frontier03454 CANCELLED AT 2023-11-26T17:57:25 DUE TO TIME LIMIT ***
