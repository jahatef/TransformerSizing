
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-22 19:55:41,577] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 8, hidden_size: 23064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2883x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2883x2048): 71.321
Elapsed time for attention_prob_times_values (32x2048x2048x2883): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2883): 84.707

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1821.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2884x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2884x2048): 75.220
Elapsed time for attention_prob_times_values (32x2048x2048x2884): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2884): 92.875

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1955.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2885x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2885x2048): 71.155
Elapsed time for attention_prob_times_values (32x2048x2048x2885): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2885): 84.845

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1821.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2886x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2886x2048): 74.460
Elapsed time for attention_prob_times_values (32x2048x2048x2886): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2886): 91.485

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1933.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2887x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2887x2048): 71.196
Elapsed time for attention_prob_times_values (32x2048x2048x2887): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2887): 84.663

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1821.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2888x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2888x2048): 76.756
Elapsed time for attention_prob_times_values (32x2048x2048x2888): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2888): 66.540

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1679.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2889x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2889x2048): 70.472
Elapsed time for attention_prob_times_values (32x2048x2048x2889): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2889): 84.537

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1811.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2890x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2890x2048): 73.901
Elapsed time for attention_prob_times_values (32x2048x2048x2890): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2890): 91.836

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1931.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2891x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2891x2048): 70.949
Elapsed time for attention_prob_times_values (32x2048x2048x2891): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2891): 84.470

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1818.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2892x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2892x2048): 74.350
Elapsed time for attention_prob_times_values (32x2048x2048x2892): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2892): 92.674

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1946.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2893x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2893x2048): 70.650
Elapsed time for attention_prob_times_values (32x2048x2048x2893): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2893): 84.450

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1815.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2894x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2894x2048): 73.770
Elapsed time for attention_prob_times_values (32x2048x2048x2894): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2894): 92.347

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1936.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2895x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2895x2048): 70.524
Elapsed time for attention_prob_times_values (32x2048x2048x2895): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2895): 83.914

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1809.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2896x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2896x2048): 77.225
Elapsed time for attention_prob_times_values (32x2048x2048x2896): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2896): 69.781

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1732.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2897x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2897x2048): 70.423
Elapsed time for attention_prob_times_values (32x2048x2048x2897): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2897): 84.411

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1814.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2898x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2898x2048): 73.602
Elapsed time for attention_prob_times_values (32x2048x2048x2898): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2898): 92.351

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1936.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2899x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2899x2048): 70.611
Elapsed time for attention_prob_times_values (32x2048x2048x2899): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2899): 84.746

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1821.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2900x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2900x2048): 74.116
Elapsed time for attention_prob_times_values (32x2048x2048x2900): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2900): 92.856

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1950.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2901x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2901x2048): 70.611
Elapsed time for attention_prob_times_values (32x2048x2048x2901): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2901): 84.809

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1823.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2902x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2902x2048): 73.411
Elapsed time for attention_prob_times_values (32x2048x2048x2902): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2902): 92.452

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1937.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2903x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2903x2048): 70.147
Elapsed time for attention_prob_times_values (32x2048x2048x2903): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2903): 85.197

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1821.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2904x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2904x2048): 76.149
Elapsed time for attention_prob_times_values (32x2048x2048x2904): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2904): 66.452

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1681.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2905x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2905x2048): 70.251
Elapsed time for attention_prob_times_values (32x2048x2048x2905): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2905): 84.785

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1820.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2906x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2906x2048): 73.447
Elapsed time for attention_prob_times_values (32x2048x2048x2906): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2906): 92.397

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1939.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2907x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2907x2048): 70.174
Elapsed time for attention_prob_times_values (32x2048x2048x2907): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2907): 84.165

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1814.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2908x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2908x2048): 74.746
Elapsed time for attention_prob_times_values (32x2048x2048x2908): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2908): 92.741

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1963.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2909x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2909x2048): 70.185
Elapsed time for attention_prob_times_values (32x2048x2048x2909): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2909): 84.651

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1820.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2910x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2910x2048): 73.905
Elapsed time for attention_prob_times_values (32x2048x2048x2910): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2910): 92.173

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1947.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2911x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2911x2048): 69.900
Elapsed time for attention_prob_times_values (32x2048x2048x2911): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2911): 83.050

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1802.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2912x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2912x2048): 77.199
Elapsed time for attention_prob_times_values (32x2048x2048x2912): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2912): 77.465

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1836.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2913x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2913x2048): 69.844
Elapsed time for attention_prob_times_values (32x2048x2048x2913): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2913): 84.545

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1817.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2914x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2914x2048): 73.542
Elapsed time for attention_prob_times_values (32x2048x2048x2914): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2914): 91.371

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1936.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2915x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2915x2048): 69.538
Elapsed time for attention_prob_times_values (32x2048x2048x2915): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2915): 84.890

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1817.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2916x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2916x2048): 74.135
Elapsed time for attention_prob_times_values (32x2048x2048x2916): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2916): 92.236

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1954.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2917x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2917x2048): 70.051
Elapsed time for attention_prob_times_values (32x2048x2048x2917): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2917): 84.550

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1822.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2918x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2918x2048): 73.065
Elapsed time for attention_prob_times_values (32x2048x2048x2918): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2918): 91.051

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1929.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2919x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2919x2048): 69.811
Elapsed time for attention_prob_times_values (32x2048x2048x2919): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2919): 83.942

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1814.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2920x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2920x2048): 75.659
Elapsed time for attention_prob_times_values (32x2048x2048x2920): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2920): 66.922

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1691.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2921x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2921x2048): 69.453
Elapsed time for attention_prob_times_values (32x2048x2048x2921): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2921): 84.981

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1820.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2922x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2922x2048): 72.748
Elapsed time for attention_prob_times_values (32x2048x2048x2922): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2922): 92.327

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1939.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2923x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2923x2048): 69.957
Elapsed time for attention_prob_times_values (32x2048x2048x2923): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2923): 83.794

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1817.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2924x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2924x2048): 73.423
Elapsed time for attention_prob_times_values (32x2048x2048x2924): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2924): 93.135

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1957.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2925x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2925x2048): 69.743
Elapsed time for attention_prob_times_values (32x2048x2048x2925): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2925): 85.192

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1829.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2926x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2926x2048): 72.951
Elapsed time for attention_prob_times_values (32x2048x2048x2926): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2926): 92.933

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1950.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2927x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2927x2048): 69.468
Elapsed time for attention_prob_times_values (32x2048x2048x2927): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2927): 85.168

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1826.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2928x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2928x2048): 76.730
Elapsed time for attention_prob_times_values (32x2048x2048x2928): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2928): 71.120

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1762.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2929x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2929x2048): 69.533
Elapsed time for attention_prob_times_values (32x2048x2048x2929): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2929): 85.324

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1829.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2930x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2930x2048): 72.839
Elapsed time for attention_prob_times_values (32x2048x2048x2930): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2930): 92.723

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1949.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2931x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2931x2048): 69.472
Elapsed time for attention_prob_times_values (32x2048x2048x2931): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2931): 84.883

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1826.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2932x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2932x2048): 73.227
Elapsed time for attention_prob_times_values (32x2048x2048x2932): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2932): 93.506

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1963.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2933x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2933x2048): 69.932
Elapsed time for attention_prob_times_values (32x2048x2048x2933): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2933): 83.950

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1824.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2934x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2934x2048): 72.378
Elapsed time for attention_prob_times_values (32x2048x2048x2934): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2934): 92.589

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1943.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2935x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2935x2048): 69.544
Elapsed time for attention_prob_times_values (32x2048x2048x2935): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2935): 84.960

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1830.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2936x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2936x2048): 74.937
Elapsed time for attention_prob_times_values (32x2048x2048x2936): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2936): 66.283

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1683.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2937x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2937x2048): 69.751
Elapsed time for attention_prob_times_values (32x2048x2048x2937): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2937): 84.553

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1830.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2938x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2938x2048): 71.887
Elapsed time for attention_prob_times_values (32x2048x2048x2938): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2938): 91.741

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1930.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2939x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2939x2048): 69.634
Elapsed time for attention_prob_times_values (32x2048x2048x2939): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2939): 84.747

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1831.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2940x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2940x2048): 72.351
Elapsed time for attention_prob_times_values (32x2048x2048x2940): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2940): 93.102

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1951.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2941x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2941x2048): 68.861
Elapsed time for attention_prob_times_values (32x2048x2048x2941): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2941): 85.499

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1829.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2942x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2942x2048): 72.013
Elapsed time for attention_prob_times_values (32x2048x2048x2942): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2942): 92.396

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1941.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2943x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2943x2048): 68.782
Elapsed time for attention_prob_times_values (32x2048x2048x2943): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2943): 84.585

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1820.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2944x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2944x2048): 89.368
Elapsed time for attention_prob_times_values (32x2048x2048x2944): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2944): 91.164

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 2166.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2945x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2945x2048): 70.672
Elapsed time for attention_prob_times_values (32x2048x2048x2945): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2945): 84.857

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1851.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2946x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2946x2048): 74.056
Elapsed time for attention_prob_times_values (32x2048x2048x2946): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2946): 92.146

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1972.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2947x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2947x2048): 71.149
Elapsed time for attention_prob_times_values (32x2048x2048x2947): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2947): 85.662

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1867.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2948x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2948x2048): 74.506
Elapsed time for attention_prob_times_values (32x2048x2048x2948): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2948): 93.078

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1988.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2949x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2949x2048): 71.277
Elapsed time for attention_prob_times_values (32x2048x2048x2949): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2949): 84.809

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1861.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2950x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2950x2048): 74.553
Elapsed time for attention_prob_times_values (32x2048x2048x2950): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2950): 91.441

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1975.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2951x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2951x2048): 71.458
Elapsed time for attention_prob_times_values (32x2048x2048x2951): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2951): 84.754

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1865.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2952x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2952x2048): 76.420
Elapsed time for attention_prob_times_values (32x2048x2048x2952): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2952): 65.248

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1693.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2953x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2953x2048): 71.064
Elapsed time for attention_prob_times_values (32x2048x2048x2953): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2953): 85.011

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1863.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2954x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2954x2048): 73.990
Elapsed time for attention_prob_times_values (32x2048x2048x2954): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2954): 92.641

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1980.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2955x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2955x2048): 71.206
Elapsed time for attention_prob_times_values (32x2048x2048x2955): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2955): 85.755

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1874.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2956x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2956x2048): 74.621
Elapsed time for attention_prob_times_values (32x2048x2048x2956): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2956): 93.121

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1996.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2957x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2957x2048): 70.942
Elapsed time for attention_prob_times_values (32x2048x2048x2957): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2957): 83.963

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1853.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2958x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2958x2048): 73.961
Elapsed time for attention_prob_times_values (32x2048x2048x2958): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2958): 93.325

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1989.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2959x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2959x2048): 70.534
Elapsed time for attention_prob_times_values (32x2048x2048x2959): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2959): 85.297

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1862.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2960x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2960x2048): 77.766
Elapsed time for attention_prob_times_values (32x2048x2048x2960): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2960): 68.189

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1753.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2961x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2961x2048): 70.427
Elapsed time for attention_prob_times_values (32x2048x2048x2961): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2961): 85.550

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1864.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2962x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2962x2048): 73.806
Elapsed time for attention_prob_times_values (32x2048x2048x2962): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2962): 92.972

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1986.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2963x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2963x2048): 70.523
Elapsed time for attention_prob_times_values (32x2048x2048x2963): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2963): 85.632

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1867.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2964x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2964x2048): 74.237
Elapsed time for attention_prob_times_values (32x2048x2048x2964): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2964): 94.212

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2005.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2965x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2965x2048): 70.712
Elapsed time for attention_prob_times_values (32x2048x2048x2965): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2965): 85.853

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1873.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2966x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2966x2048): 73.408
Elapsed time for attention_prob_times_values (32x2048x2048x2966): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2966): 93.103

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1984.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2967x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2967x2048): 70.729
Elapsed time for attention_prob_times_values (32x2048x2048x2967): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2967): 85.869

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1875.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2968x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2968x2048): 75.921
Elapsed time for attention_prob_times_values (32x2048x2048x2968): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2968): 65.961

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1707.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2969x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2969x2048): 70.084
Elapsed time for attention_prob_times_values (32x2048x2048x2969): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2969): 86.277

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1871.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2970x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2970x2048): 73.556
Elapsed time for attention_prob_times_values (32x2048x2048x2970): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2970): 92.982

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1987.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2971x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2971x2048): 70.341
Elapsed time for attention_prob_times_values (32x2048x2048x2971): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2971): 85.832

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1871.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2972x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2972x2048): 74.229
Elapsed time for attention_prob_times_values (32x2048x2048x2972): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2972): 94.102

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2009.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2973x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2973x2048): 70.304
Elapsed time for attention_prob_times_values (32x2048x2048x2973): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2973): 86.033

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1874.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2974x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2974x2048): 73.618
Elapsed time for attention_prob_times_values (32x2048x2048x2974): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2974): 93.180

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1993.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2975x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2975x2048): 69.865
Elapsed time for attention_prob_times_values (32x2048x2048x2975): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2975): 86.260

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1871.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2976x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2976x2048): 77.367
Elapsed time for attention_prob_times_values (32x2048x2048x2976): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2976): 74.248

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1837.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2977x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2977x2048): 69.886
Elapsed time for attention_prob_times_values (32x2048x2048x2977): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2977): 86.092

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1871.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2978x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2978x2048): 73.392
Elapsed time for attention_prob_times_values (32x2048x2048x2978): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2978): 92.656

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1987.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2979x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2979x2048): 70.496
Elapsed time for attention_prob_times_values (32x2048x2048x2979): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2979): 85.510

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1875.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2980x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2980x2048): 73.862
Elapsed time for attention_prob_times_values (32x2048x2048x2980): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2980): 93.363

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2002.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2981x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2981x2048): 70.264
Elapsed time for attention_prob_times_values (32x2048x2048x2981): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2981): 84.272

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1861.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2982x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2982x2048): 73.007
Elapsed time for attention_prob_times_values (32x2048x2048x2982): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2982): 93.019

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1987.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2983x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2983x2048): 70.060
Elapsed time for attention_prob_times_values (32x2048x2048x2983): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2983): 85.715

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1873.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2984x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2984x2048): 75.614
Elapsed time for attention_prob_times_values (32x2048x2048x2984): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2984): 65.564

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1707.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2985x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2985x2048): 69.816
Elapsed time for attention_prob_times_values (32x2048x2048x2985): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2985): 85.886

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1873.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2986x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2986x2048): 72.913
Elapsed time for attention_prob_times_values (32x2048x2048x2986): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2986): 92.242

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1981.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2987x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2987x2048): 70.242
Elapsed time for attention_prob_times_values (32x2048x2048x2987): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2987): 85.100

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1872.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2988x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2988x2048): 73.766
Elapsed time for attention_prob_times_values (32x2048x2048x2988): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2988): 93.930

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2011.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2989x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2989x2048): 70.061
Elapsed time for attention_prob_times_values (32x2048x2048x2989): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2989): 86.012

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1880.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2990x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2990x2048): 73.418
Elapsed time for attention_prob_times_values (32x2048x2048x2990): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2990): 94.071

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2008.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2991x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2991x2048): 70.105
Elapsed time for attention_prob_times_values (32x2048x2048x2991): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2991): 87.354

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1895.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2992x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2992x2048): 76.876
Elapsed time for attention_prob_times_values (32x2048x2048x2992): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2992): 68.513

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1766.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2993x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2993x2048): 69.753
Elapsed time for attention_prob_times_values (32x2048x2048x2993): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2993): 86.565

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1883.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2994x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2994x2048): 72.824
Elapsed time for attention_prob_times_values (32x2048x2048x2994): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2994): 92.892

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1991.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2995x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2995x2048): 69.841
Elapsed time for attention_prob_times_values (32x2048x2048x2995): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2995): 86.200

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1882.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2996x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2996x2048): 73.394
Elapsed time for attention_prob_times_values (32x2048x2048x2996): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2996): 93.974

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2011.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2997x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2997x2048): 70.268
Elapsed time for attention_prob_times_values (32x2048x2048x2997): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2997): 86.034

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1888.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2998x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2998x2048): 72.750
Elapsed time for attention_prob_times_values (32x2048x2048x2998): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2998): 93.011

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1993.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 23992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x2999x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x2999x2048): 69.599
Elapsed time for attention_prob_times_values (32x2048x2048x2999): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x2999): 86.300

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1882.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3000x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3000x2048): 74.852
Elapsed time for attention_prob_times_values (32x2048x2048x3000): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3000): 65.473

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1706.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3001x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3001x2048): 69.445
Elapsed time for attention_prob_times_values (32x2048x2048x3001): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3001): 86.307

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1881.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3002x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3002x2048): 72.672
Elapsed time for attention_prob_times_values (32x2048x2048x3002): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3002): 93.114

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1996.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3003x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3003x2048): 69.676
Elapsed time for attention_prob_times_values (32x2048x2048x3003): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3003): 86.552

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1888.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3004x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3004x2048): 73.309
Elapsed time for attention_prob_times_values (32x2048x2048x3004): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3004): 93.372

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2009.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3005x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3005x2048): 69.800
Elapsed time for attention_prob_times_values (32x2048x2048x3005): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3005): 85.130

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1877.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3006x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3006x2048): 72.707
Elapsed time for attention_prob_times_values (32x2048x2048x3006): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3006): 93.143

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1999.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3007x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3007x2048): 69.615
Elapsed time for attention_prob_times_values (32x2048x2048x3007): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3007): 84.839

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1873.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3008x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3008x2048): 90.271
Elapsed time for attention_prob_times_values (32x2048x2048x3008): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3008): 83.964

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2131.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3009x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3009x2048): 71.434
Elapsed time for attention_prob_times_values (32x2048x2048x3009): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3009): 86.389

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1916.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3010x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3010x2048): 74.830
Elapsed time for attention_prob_times_values (32x2048x2048x3010): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3010): 92.947

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2032.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3011x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3011x2048): 71.497
Elapsed time for attention_prob_times_values (32x2048x2048x3011): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3011): 86.442

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1919.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3012x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3012x2048): 75.429
Elapsed time for attention_prob_times_values (32x2048x2048x3012): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3012): 93.316

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2046.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3013x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3013x2048): 71.355
Elapsed time for attention_prob_times_values (32x2048x2048x3013): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3013): 85.499

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1908.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3014x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3014x2048): 74.698
Elapsed time for attention_prob_times_values (32x2048x2048x3014): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3014): 92.791

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2031.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3015x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3015x2048): 71.483
Elapsed time for attention_prob_times_values (32x2048x2048x3015): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3015): 86.238

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1919.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3016x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3016x2048): 76.788
Elapsed time for attention_prob_times_values (32x2048x2048x3016): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3016): 65.443

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1735.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3017x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3017x2048): 71.093
Elapsed time for attention_prob_times_values (32x2048x2048x3017): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3017): 85.526

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1907.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3018x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3018x2048): 74.297
Elapsed time for attention_prob_times_values (32x2048x2048x3018): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3018): 93.126

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2031.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3019x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3019x2048): 71.498
Elapsed time for attention_prob_times_values (32x2048x2048x3019): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3019): 86.348

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1923.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3020x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3020x2048): 74.755
Elapsed time for attention_prob_times_values (32x2048x2048x3020): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3020): 92.821

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2036.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3021x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3021x2048): 71.133
Elapsed time for attention_prob_times_values (32x2048x2048x3021): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3021): 85.825

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1913.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3022x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3022x2048): 74.269
Elapsed time for attention_prob_times_values (32x2048x2048x3022): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3022): 92.705

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2029.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3023x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3023x2048): 70.862
Elapsed time for attention_prob_times_values (32x2048x2048x3023): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3023): 85.742

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1910.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3024x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3024x2048): 77.588
Elapsed time for attention_prob_times_values (32x2048x2048x3024): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3024): 68.965

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1798.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3025x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3025x2048): 70.501
Elapsed time for attention_prob_times_values (32x2048x2048x3025): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3025): 86.762

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1916.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3026x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3026x2048): 73.934
Elapsed time for attention_prob_times_values (32x2048x2048x3026): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3026): 92.387

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2023.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3027x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3027x2048): 70.762
Elapsed time for attention_prob_times_values (32x2048x2048x3027): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3027): 85.736

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1911.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3028x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3028x2048): 74.390
Elapsed time for attention_prob_times_values (32x2048x2048x3028): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3028): 92.915

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2037.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3029x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3029x2048): 70.934
Elapsed time for attention_prob_times_values (32x2048x2048x3029): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3029): 85.949

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1916.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3030x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3030x2048): 74.021
Elapsed time for attention_prob_times_values (32x2048x2048x3030): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3030): 92.444

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2028.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3031x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3031x2048): 70.631
Elapsed time for attention_prob_times_values (32x2048x2048x3031): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3031): 86.053

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1914.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3032x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3032x2048): 76.148
Elapsed time for attention_prob_times_values (32x2048x2048x3032): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3032): 65.853

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1743.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3033x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3033x2048): 70.539
Elapsed time for attention_prob_times_values (32x2048x2048x3033): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3033): 85.906

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1913.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3034x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3034x2048): 73.731
Elapsed time for attention_prob_times_values (32x2048x2048x3034): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3034): 93.015

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2032.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3035x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3035x2048): 70.721
Elapsed time for attention_prob_times_values (32x2048x2048x3035): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3035): 85.443

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1912.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3036x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3036x2048): 74.345
Elapsed time for attention_prob_times_values (32x2048x2048x3036): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3036): 93.236

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 2044.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3037x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3037x2048): 69.583
Elapsed time for attention_prob_times_values (32x2048x2048x3037): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3037): 84.860

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1890.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3038x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3038x2048): 74.123
Elapsed time for attention_prob_times_values (32x2048x2048x3038): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3038): 92.318

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2033.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3039x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3039x2048): 70.245
Elapsed time for attention_prob_times_values (32x2048x2048x3039): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3039): 86.382

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1917.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3040x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3040x2048): 77.873
Elapsed time for attention_prob_times_values (32x2048x2048x3040): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3040): 73.375

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1870.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3041x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3041x2048): 70.281
Elapsed time for attention_prob_times_values (32x2048x2048x3041): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3041): 86.713

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1922.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3042x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3042x2048): 73.678
Elapsed time for attention_prob_times_values (32x2048x2048x3042): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3042): 92.206

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2028.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3043x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3043x2048): 70.271
Elapsed time for attention_prob_times_values (32x2048x2048x3043): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3043): 85.514

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1911.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3044x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3044x2048): 74.035
Elapsed time for attention_prob_times_values (32x2048x2048x3044): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3044): 93.182

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2044.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3045x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3045x2048): 70.147
Elapsed time for attention_prob_times_values (32x2048x2048x3045): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3045): 86.183

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1917.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3046x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3046x2048): 73.544
Elapsed time for attention_prob_times_values (32x2048x2048x3046): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3046): 91.715

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2024.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3047x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3047x2048): 70.169
Elapsed time for attention_prob_times_values (32x2048x2048x3047): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3047): 85.995

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1916.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3048x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3048x2048): 75.579
Elapsed time for attention_prob_times_values (32x2048x2048x3048): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3048): 65.767

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1745.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3049x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3049x2048): 70.033
Elapsed time for attention_prob_times_values (32x2048x2048x3049): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3049): 85.895

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1915.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3050x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3050x2048): 73.165
Elapsed time for attention_prob_times_values (32x2048x2048x3050): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3050): 92.084

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2024.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3051x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3051x2048): 70.313
Elapsed time for attention_prob_times_values (32x2048x2048x3051): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3051): 86.141

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1922.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3052x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3052x2048): 73.551
Elapsed time for attention_prob_times_values (32x2048x2048x3052): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3052): 92.659

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2037.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3053x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3053x2048): 70.055
Elapsed time for attention_prob_times_values (32x2048x2048x3053): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3053): 85.577

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1914.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3054x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3054x2048): 73.395
Elapsed time for attention_prob_times_values (32x2048x2048x3054): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3054): 91.830

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2028.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3055x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3055x2048): 70.396
Elapsed time for attention_prob_times_values (32x2048x2048x3055): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3055): 86.442

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1929.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3056x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3056x2048): 77.859
Elapsed time for attention_prob_times_values (32x2048x2048x3056): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3056): 68.162

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1808.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3057x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3057x2048): 70.912
Elapsed time for attention_prob_times_values (32x2048x2048x3057): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3057): 85.504

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1929.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3058x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3058x2048): 73.960
Elapsed time for attention_prob_times_values (32x2048x2048x3058): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3058): 92.116

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2042.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3059x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3059x2048): 71.479
Elapsed time for attention_prob_times_values (32x2048x2048x3059): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3059): 86.140

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1945.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3060x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3060x2048): 74.988
Elapsed time for attention_prob_times_values (32x2048x2048x3060): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3060): 92.003

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2057.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3061x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3061x2048): 72.807
Elapsed time for attention_prob_times_values (32x2048x2048x3061): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3061): 86.319

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1967.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3062x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3062x2048): 73.459
Elapsed time for attention_prob_times_values (32x2048x2048x3062): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3062): 90.657

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2022.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3063x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3063x2048): 70.782
Elapsed time for attention_prob_times_values (32x2048x2048x3063): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3063): 86.636

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1942.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3064x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3064x2048): 75.631
Elapsed time for attention_prob_times_values (32x2048x2048x3064): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3064): 65.505

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1750.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3065x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3065x2048): 70.059
Elapsed time for attention_prob_times_values (32x2048x2048x3065): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3065): 86.532

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1931.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3066x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3066x2048): 72.904
Elapsed time for attention_prob_times_values (32x2048x2048x3066): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3066): 91.887

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2028.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3067x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3067x2048): 69.750
Elapsed time for attention_prob_times_values (32x2048x2048x3067): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3067): 86.096

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1923.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3068x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3068x2048): 71.490
Elapsed time for attention_prob_times_values (32x2048x2048x3068): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3068): 91.445

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2003.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3069x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3069x2048): 69.565
Elapsed time for attention_prob_times_values (32x2048x2048x3069): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3069): 85.963

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1920.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3070x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3070x2048): 72.323
Elapsed time for attention_prob_times_values (32x2048x2048x3070): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3070): 91.016

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2013.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3071x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3071x2048): 69.319
Elapsed time for attention_prob_times_values (32x2048x2048x3071): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3071): 86.823

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1926.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3072x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3072x2048): 89.663
Elapsed time for attention_prob_times_values (32x2048x2048x3072): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3072): 91.272

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2261.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3073x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3073x2048): 71.096
Elapsed time for attention_prob_times_values (32x2048x2048x3073): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3073): 79.392

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1875.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3074x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3074x2048): 74.546
Elapsed time for attention_prob_times_values (32x2048x2048x3074): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3074): 88.917

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2028.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3075x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3075x2048): 71.293
Elapsed time for attention_prob_times_values (32x2048x2048x3075): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3075): 81.967

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1908.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3076x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3076x2048): 73.250
Elapsed time for attention_prob_times_values (32x2048x2048x3076): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3076): 89.479

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2016.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3077x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3077x2048): 71.410
Elapsed time for attention_prob_times_values (32x2048x2048x3077): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3077): 80.736

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1897.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3078x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3078x2048): 74.809
Elapsed time for attention_prob_times_values (32x2048x2048x3078): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3078): 88.294

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2028.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3079x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3079x2048): 71.729
Elapsed time for attention_prob_times_values (32x2048x2048x3079): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3079): 81.645

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1913.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3080x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3080x2048): 77.352
Elapsed time for attention_prob_times_values (32x2048x2048x3080): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3080): 71.230

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1858.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3081x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3081x2048): 71.964
Elapsed time for attention_prob_times_values (32x2048x2048x3081): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3081): 81.007

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1910.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3082x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3082x2048): 75.135
Elapsed time for attention_prob_times_values (32x2048x2048x3082): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3082): 88.745

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 2040.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3083x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3083x2048): 72.880
Elapsed time for attention_prob_times_values (32x2048x2048x3083): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3083): 80.816

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1922.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3084x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3084x2048): 76.510
Elapsed time for attention_prob_times_values (32x2048x2048x3084): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3084): 88.342

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 2057.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3085x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3085x2048): 73.891
Elapsed time for attention_prob_times_values (32x2048x2048x3085): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3085): 80.102

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1929.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3086x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3086x2048): 74.879
Elapsed time for attention_prob_times_values (32x2048x2048x3086): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3086): 88.767

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2039.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3087x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3087x2048): 71.866
Elapsed time for attention_prob_times_values (32x2048x2048x3087): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3087): 80.345

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1905.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3088x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3088x2048): 79.292
Elapsed time for attention_prob_times_values (32x2048x2048x3088): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3088): 75.520

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1943.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3089x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3089x2048): 71.053
Elapsed time for attention_prob_times_values (32x2048x2048x3089): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3089): 81.722

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1910.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3090x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3090x2048): 74.197
Elapsed time for attention_prob_times_values (32x2048x2048x3090): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3090): 89.121

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2035.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3091x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3091x2048): 70.811
Elapsed time for attention_prob_times_values (32x2048x2048x3091): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3091): 80.238

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1891.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3092x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3092x2048): 74.226
Elapsed time for attention_prob_times_values (32x2048x2048x3092): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3092): 89.749

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2044.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3093x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3093x2048): 70.779
Elapsed time for attention_prob_times_values (32x2048x2048x3093): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3093): 81.243

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1903.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3094x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3094x2048): 74.026
Elapsed time for attention_prob_times_values (32x2048x2048x3094): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3094): 89.214

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2036.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3095x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3095x2048): 70.587
Elapsed time for attention_prob_times_values (32x2048x2048x3095): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3095): 80.358

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1892.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3096x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3096x2048): 76.104
Elapsed time for attention_prob_times_values (32x2048x2048x3096): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3096): 71.568

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1857.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3097x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3097x2048): 70.463
Elapsed time for attention_prob_times_values (32x2048x2048x3097): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3097): 81.643

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1905.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3098x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3098x2048): 74.083
Elapsed time for attention_prob_times_values (32x2048x2048x3098): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3098): 89.464

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2042.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3099x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3099x2048): 70.491
Elapsed time for attention_prob_times_values (32x2048x2048x3099): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3099): 81.266

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1903.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3100x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3100x2048): 74.429
Elapsed time for attention_prob_times_values (32x2048x2048x3100): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3100): 90.291

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2057.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3101x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3101x2048): 70.534
Elapsed time for attention_prob_times_values (32x2048x2048x3101): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3101): 82.242

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1915.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3102x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3102x2048): 74.038
Elapsed time for attention_prob_times_values (32x2048x2048x3102): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3102): 89.607

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2046.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3103x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3103x2048): 70.367
Elapsed time for attention_prob_times_values (32x2048x2048x3103): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3103): 81.677

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1908.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3104x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3104x2048): 77.965
Elapsed time for attention_prob_times_values (32x2048x2048x3104): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3104): 86.123

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2066.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3105x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3105x2048): 70.452
Elapsed time for attention_prob_times_values (32x2048x2048x3105): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3105): 82.172

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1916.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3106x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3106x2048): 73.930
Elapsed time for attention_prob_times_values (32x2048x2048x3106): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3106): 89.487

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2045.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3107x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3107x2048): 70.726
Elapsed time for attention_prob_times_values (32x2048x2048x3107): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3107): 82.020

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1919.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3108x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3108x2048): 73.972
Elapsed time for attention_prob_times_values (32x2048x2048x3108): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3108): 91.109

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 2064.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3109x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3109x2048): 69.668
Elapsed time for attention_prob_times_values (32x2048x2048x3109): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3109): 82.303

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1908.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3110x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3110x2048): 73.855
Elapsed time for attention_prob_times_values (32x2048x2048x3110): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3110): 89.733

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2049.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3111x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3111x2048): 70.425
Elapsed time for attention_prob_times_values (32x2048x2048x3111): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3111): 83.324

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1931.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3112x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3112x2048): 75.897
Elapsed time for attention_prob_times_values (32x2048x2048x3112): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3112): 71.777

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1867.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3113x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3113x2048): 70.481
Elapsed time for attention_prob_times_values (32x2048x2048x3113): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3113): 82.312

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1922.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3114x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3114x2048): 73.514
Elapsed time for attention_prob_times_values (32x2048x2048x3114): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3114): 89.729

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2046.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3115x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3115x2048): 70.714
Elapsed time for attention_prob_times_values (32x2048x2048x3115): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3115): 81.549

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1919.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3116x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3116x2048): 73.979
Elapsed time for attention_prob_times_values (32x2048x2048x3116): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3116): 91.400

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2072.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3117x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3117x2048): 70.479
Elapsed time for attention_prob_times_values (32x2048x2048x3117): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3117): 83.028

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1932.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3118x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3118x2048): 73.486
Elapsed time for attention_prob_times_values (32x2048x2048x3118): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3118): 89.786

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2049.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3119x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3119x2048): 70.224
Elapsed time for attention_prob_times_values (32x2048x2048x3119): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3119): 82.189

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1921.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3120x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3120x2048): 76.865
Elapsed time for attention_prob_times_values (32x2048x2048x3120): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3120): 73.604

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1908.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3121x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3121x2048): 70.270
Elapsed time for attention_prob_times_values (32x2048x2048x3121): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3121): 83.025

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1932.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3122x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3122x2048): 73.240
Elapsed time for attention_prob_times_values (32x2048x2048x3122): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3122): 89.569

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2046.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3123x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3123x2048): 70.453
Elapsed time for attention_prob_times_values (32x2048x2048x3123): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3123): 83.595

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1942.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 24992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3124x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3124x2048): 73.723
Elapsed time for attention_prob_times_values (32x2048x2048x3124): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3124): 90.557

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2064.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3125x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3125x2048): 70.749
Elapsed time for attention_prob_times_values (32x2048x2048x3125): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3125): 82.820

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1939.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3126x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3126x2048): 73.216
Elapsed time for attention_prob_times_values (32x2048x2048x3126): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3126): 89.826

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2050.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3127x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3127x2048): 70.479
Elapsed time for attention_prob_times_values (32x2048x2048x3127): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3127): 83.176

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1940.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3128x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3128x2048): 75.169
Elapsed time for attention_prob_times_values (32x2048x2048x3128): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3128): 70.856

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1855.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3129x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3129x2048): 70.381
Elapsed time for attention_prob_times_values (32x2048x2048x3129): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3129): 83.448

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1943.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3130x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3130x2048): 73.261
Elapsed time for attention_prob_times_values (32x2048x2048x3130): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3130): 89.815

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2054.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3131x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3131x2048): 69.946
Elapsed time for attention_prob_times_values (32x2048x2048x3131): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3131): 84.045

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1943.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3132x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3132x2048): 73.568
Elapsed time for attention_prob_times_values (32x2048x2048x3132): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3132): 92.652

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2088.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3133x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3133x2048): 69.730
Elapsed time for attention_prob_times_values (32x2048x2048x3133): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3133): 84.927

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1951.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3134x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3134x2048): 72.277
Elapsed time for attention_prob_times_values (32x2048x2048x3134): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3134): 90.460

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2047.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3135x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3135x2048): 69.596
Elapsed time for attention_prob_times_values (32x2048x2048x3135): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3135): 84.207

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1942.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3136x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3136x2048): 90.676
Elapsed time for attention_prob_times_values (32x2048x2048x3136): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3136): 89.551

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2297.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3137x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3137x2048): 71.509
Elapsed time for attention_prob_times_values (32x2048x2048x3137): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3137): 84.210

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1972.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3138x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3138x2048): 74.207
Elapsed time for attention_prob_times_values (32x2048x2048x3138): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3138): 90.251

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2078.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3139x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3139x2048): 71.832
Elapsed time for attention_prob_times_values (32x2048x2048x3139): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3139): 83.970

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1976.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3140x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3140x2048): 75.561
Elapsed time for attention_prob_times_values (32x2048x2048x3140): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3140): 90.649

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2104.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3141x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3141x2048): 71.924
Elapsed time for attention_prob_times_values (32x2048x2048x3141): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3141): 84.105

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1980.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3142x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3142x2048): 75.058
Elapsed time for attention_prob_times_values (32x2048x2048x3142): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3142): 89.823

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2089.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3143x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3143x2048): 71.928
Elapsed time for attention_prob_times_values (32x2048x2048x3143): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3143): 83.183

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1971.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3144x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3144x2048): 77.117
Elapsed time for attention_prob_times_values (32x2048x2048x3144): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3144): 72.024

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1903.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3145x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3145x2048): 71.566
Elapsed time for attention_prob_times_values (32x2048x2048x3145): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3145): 82.765

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1962.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3146x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3146x2048): 74.367
Elapsed time for attention_prob_times_values (32x2048x2048x3146): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3146): 90.242

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2085.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3147x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3147x2048): 71.901
Elapsed time for attention_prob_times_values (32x2048x2048x3147): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3147): 83.323

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1975.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3148x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3148x2048): 74.892
Elapsed time for attention_prob_times_values (32x2048x2048x3148): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3148): 90.488

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 2097.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3149x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3149x2048): 71.801
Elapsed time for attention_prob_times_values (32x2048x2048x3149): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3149): 83.166

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1973.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3150x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3150x2048): 74.643
Elapsed time for attention_prob_times_values (32x2048x2048x3150): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3150): 89.821

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2087.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3151x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3151x2048): 71.291
Elapsed time for attention_prob_times_values (32x2048x2048x3151): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3151): 83.387

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1969.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3152x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3152x2048): 77.923
Elapsed time for attention_prob_times_values (32x2048x2048x3152): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3152): 74.188

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1947.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3153x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3153x2048): 71.167
Elapsed time for attention_prob_times_values (32x2048x2048x3153): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3153): 83.112

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1965.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3154x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3154x2048): 74.236
Elapsed time for attention_prob_times_values (32x2048x2048x3154): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3154): 89.817

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2084.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3155x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3155x2048): 71.249
Elapsed time for attention_prob_times_values (32x2048x2048x3155): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3155): 83.704

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1974.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3156x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3156x2048): 74.779
Elapsed time for attention_prob_times_values (32x2048x2048x3156): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3156): 90.524

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2101.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3157x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3157x2048): 71.305
Elapsed time for attention_prob_times_values (32x2048x2048x3157): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3157): 83.194

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1970.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3158x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3158x2048): 74.243
Elapsed time for attention_prob_times_values (32x2048x2048x3158): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3158): 89.747

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2086.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3159x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3159x2048): 70.919
Elapsed time for attention_prob_times_values (32x2048x2048x3159): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3159): 83.517

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1969.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3160x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3160x2048): 76.326
Elapsed time for attention_prob_times_values (32x2048x2048x3160): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3160): 72.030

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1903.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3161x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3161x2048): 70.578
Elapsed time for attention_prob_times_values (32x2048x2048x3161): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3161): 83.422

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1964.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3162x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3162x2048): 74.204
Elapsed time for attention_prob_times_values (32x2048x2048x3162): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3162): 89.963

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2090.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3163x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3163x2048): 70.779
Elapsed time for attention_prob_times_values (32x2048x2048x3163): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3163): 82.591

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1959.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3164x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3164x2048): 74.852
Elapsed time for attention_prob_times_values (32x2048x2048x3164): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3164): 90.791

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 2110.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3165x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3165x2048): 70.884
Elapsed time for attention_prob_times_values (32x2048x2048x3165): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3165): 82.866

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1965.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3166x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3166x2048): 74.280
Elapsed time for attention_prob_times_values (32x2048x2048x3166): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3166): 89.942

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2093.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3167x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3167x2048): 70.438
Elapsed time for attention_prob_times_values (32x2048x2048x3167): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3167): 83.514

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1967.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3168x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3168x2048): 77.872
Elapsed time for attention_prob_times_values (32x2048x2048x3168): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3168): 88.783

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2136.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3169x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3169x2048): 70.167
Elapsed time for attention_prob_times_values (32x2048x2048x3169): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3169): 83.061

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1959.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3170x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3170x2048): 74.103
Elapsed time for attention_prob_times_values (32x2048x2048x3170): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3170): 89.764

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2091.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3171x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3171x2048): 70.956
Elapsed time for attention_prob_times_values (32x2048x2048x3171): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3171): 83.543

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1977.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3172x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3172x2048): 74.776
Elapsed time for attention_prob_times_values (32x2048x2048x3172): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3172): 89.993

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 2105.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3173x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3173x2048): 71.291
Elapsed time for attention_prob_times_values (32x2048x2048x3173): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3173): 82.981

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1977.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3174x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3174x2048): 74.394
Elapsed time for attention_prob_times_values (32x2048x2048x3174): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3174): 89.499

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2096.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3175x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3175x2048): 70.875
Elapsed time for attention_prob_times_values (32x2048x2048x3175): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3175): 82.282

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1965.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3176x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3176x2048): 76.402
Elapsed time for attention_prob_times_values (32x2048x2048x3176): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3176): 69.128

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1873.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3177x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3177x2048): 70.862
Elapsed time for attention_prob_times_values (32x2048x2048x3177): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3177): 82.328

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1966.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3178x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3178x2048): 73.841
Elapsed time for attention_prob_times_values (32x2048x2048x3178): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3178): 89.828

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2093.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3179x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3179x2048): 71.176
Elapsed time for attention_prob_times_values (32x2048x2048x3179): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3179): 84.213

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1993.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3180x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3180x2048): 74.379
Elapsed time for attention_prob_times_values (32x2048x2048x3180): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3180): 90.817

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2113.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3181x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3181x2048): 70.933
Elapsed time for attention_prob_times_values (32x2048x2048x3181): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3181): 82.479

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1971.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3182x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3182x2048): 73.887
Elapsed time for attention_prob_times_values (32x2048x2048x3182): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3182): 89.713

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2095.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3183x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3183x2048): 70.586
Elapsed time for attention_prob_times_values (32x2048x2048x3183): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3183): 83.905

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1983.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3184x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3184x2048): 77.315
Elapsed time for attention_prob_times_values (32x2048x2048x3184): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3184): 76.575

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1990.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3185x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3185x2048): 70.656
Elapsed time for attention_prob_times_values (32x2048x2048x3185): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3185): 84.725

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1994.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3186x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3186x2048): 73.683
Elapsed time for attention_prob_times_values (32x2048x2048x3186): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3186): 89.975

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2097.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3187x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3187x2048): 70.929
Elapsed time for attention_prob_times_values (32x2048x2048x3187): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3187): 84.487

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1997.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3188x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3188x2048): 74.124
Elapsed time for attention_prob_times_values (32x2048x2048x3188): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3188): 90.829

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2114.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3189x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3189x2048): 71.008
Elapsed time for attention_prob_times_values (32x2048x2048x3189): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3189): 84.367

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1998.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3190x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3190x2048): 73.600
Elapsed time for attention_prob_times_values (32x2048x2048x3190): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3190): 90.176

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2100.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3191x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3191x2048): 70.785
Elapsed time for attention_prob_times_values (32x2048x2048x3191): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3191): 83.917

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1991.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3192x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3192x2048): 75.339
Elapsed time for attention_prob_times_values (32x2048x2048x3192): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3192): 71.880

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1908.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3193x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3193x2048): 70.815
Elapsed time for attention_prob_times_values (32x2048x2048x3193): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3193): 83.384

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1987.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3194x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3194x2048): 73.495
Elapsed time for attention_prob_times_values (32x2048x2048x3194): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3194): 90.149

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2101.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3195x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3195x2048): 70.907
Elapsed time for attention_prob_times_values (32x2048x2048x3195): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3195): 83.962

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1995.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3196x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3196x2048): 73.221
Elapsed time for attention_prob_times_values (32x2048x2048x3196): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3196): 89.937

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2096.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3197x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3197x2048): 70.678
Elapsed time for attention_prob_times_values (32x2048x2048x3197): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3197): 82.736

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1980.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3198x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3198x2048): 72.799
Elapsed time for attention_prob_times_values (32x2048x2048x3198): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3198): 89.865

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2090.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3199x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3199x2048): 69.999
Elapsed time for attention_prob_times_values (32x2048x2048x3199): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3199): 84.141

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 1986.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3200x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3200x2048): 89.724
Elapsed time for attention_prob_times_values (32x2048x2048x3200): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3200): 91.655

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2357.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3201x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3201x2048): 71.731
Elapsed time for attention_prob_times_values (32x2048x2048x3201): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3201): 83.830

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2010.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3202x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3202x2048): 74.684
Elapsed time for attention_prob_times_values (32x2048x2048x3202): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3202): 88.463

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2107.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3203x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3203x2048): 71.882
Elapsed time for attention_prob_times_values (32x2048x2048x3203): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3203): 83.986

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2015.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3204x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3204x2048): 75.040
Elapsed time for attention_prob_times_values (32x2048x2048x3204): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3204): 90.613

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2137.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3205x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3205x2048): 72.149
Elapsed time for attention_prob_times_values (32x2048x2048x3205): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3205): 83.960

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2020.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3206x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3206x2048): 75.180
Elapsed time for attention_prob_times_values (32x2048x2048x3206): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3206): 90.485

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2139.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3207x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3207x2048): 72.263
Elapsed time for attention_prob_times_values (32x2048x2048x3207): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3207): 84.081

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2025.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3208x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3208x2048): 77.054
Elapsed time for attention_prob_times_values (32x2048x2048x3208): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3208): 67.038

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 1868.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3209x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3209x2048): 71.641
Elapsed time for attention_prob_times_values (32x2048x2048x3209): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3209): 83.906

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2014.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3210x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3210x2048): 74.638
Elapsed time for attention_prob_times_values (32x2048x2048x3210): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3210): 90.630

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2134.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3211x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3211x2048): 71.895
Elapsed time for attention_prob_times_values (32x2048x2048x3211): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3211): 84.297

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2024.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3212x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3212x2048): 75.522
Elapsed time for attention_prob_times_values (32x2048x2048x3212): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3212): 91.286

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 2156.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3213x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3213x2048): 71.672
Elapsed time for attention_prob_times_values (32x2048x2048x3213): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3213): 84.982

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2029.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3214x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3214x2048): 74.915
Elapsed time for attention_prob_times_values (32x2048x2048x3214): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3214): 90.577

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2141.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3215x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3215x2048): 71.486
Elapsed time for attention_prob_times_values (32x2048x2048x3215): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3215): 83.974

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2016.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3216x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3216x2048): 78.198
Elapsed time for attention_prob_times_values (32x2048x2048x3216): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3216): 70.355

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1935.069
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3217x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3217x2048): 71.463
Elapsed time for attention_prob_times_values (32x2048x2048x3217): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3217): 84.184

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2020.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3218x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3218x2048): 74.590
Elapsed time for attention_prob_times_values (32x2048x2048x3218): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3218): 89.562

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2127.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3219x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3219x2048): 71.354
Elapsed time for attention_prob_times_values (32x2048x2048x3219): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3219): 83.688

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2014.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3220x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3220x2048): 75.284
Elapsed time for attention_prob_times_values (32x2048x2048x3220): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3220): 90.889

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2154.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3221x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3221x2048): 71.505
Elapsed time for attention_prob_times_values (32x2048x2048x3221): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3221): 84.708

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2028.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3222x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3222x2048): 74.528
Elapsed time for attention_prob_times_values (32x2048x2048x3222): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3222): 89.776

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2131.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3223x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3223x2048): 71.242
Elapsed time for attention_prob_times_values (32x2048x2048x3223): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3223): 84.652

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2025.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3224x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3224x2048): 76.693
Elapsed time for attention_prob_times_values (32x2048x2048x3224): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3224): 67.515

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 1880.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3225x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3225x2048): 70.776
Elapsed time for attention_prob_times_values (32x2048x2048x3225): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3225): 84.768

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2020.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3226x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3226x2048): 74.367
Elapsed time for attention_prob_times_values (32x2048x2048x3226): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3226): 90.278

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2136.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3227x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3227x2048): 70.960
Elapsed time for attention_prob_times_values (32x2048x2048x3227): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3227): 84.332

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2020.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3228x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3228x2048): 75.001
Elapsed time for attention_prob_times_values (32x2048x2048x3228): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3228): 90.453

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2150.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3229x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3229x2048): 71.098
Elapsed time for attention_prob_times_values (32x2048x2048x3229): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3229): 84.369

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2023.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3230x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3230x2048): 74.310
Elapsed time for attention_prob_times_values (32x2048x2048x3230): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3230): 90.523

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2141.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3231x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3231x2048): 70.789
Elapsed time for attention_prob_times_values (32x2048x2048x3231): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3231): 82.977

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2004.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3232x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3232x2048): 78.038
Elapsed time for attention_prob_times_values (32x2048x2048x3232): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3232): 79.542

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2068.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3233x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3233x2048): 70.686
Elapsed time for attention_prob_times_values (32x2048x2048x3233): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3233): 83.959

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2015.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3234x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3234x2048): 74.105
Elapsed time for attention_prob_times_values (32x2048x2048x3234): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3234): 90.457

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2139.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3235x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3235x2048): 70.822
Elapsed time for attention_prob_times_values (32x2048x2048x3235): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3235): 84.341

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2022.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3236x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3236x2048): 74.599
Elapsed time for attention_prob_times_values (32x2048x2048x3236): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3236): 90.289

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2147.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3237x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3237x2048): 70.720
Elapsed time for attention_prob_times_values (32x2048x2048x3237): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3237): 83.723

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2015.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3238x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3238x2048): 74.018
Elapsed time for attention_prob_times_values (32x2048x2048x3238): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3238): 90.815

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2144.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3239x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3239x2048): 70.944
Elapsed time for attention_prob_times_values (32x2048x2048x3239): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3239): 84.014

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2023.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3240x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3240x2048): 76.090
Elapsed time for attention_prob_times_values (32x2048x2048x3240): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3240): 66.868

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1872.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3241x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3241x2048): 70.527
Elapsed time for attention_prob_times_values (32x2048x2048x3241): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3241): 83.152

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2008.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3242x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3242x2048): 73.818
Elapsed time for attention_prob_times_values (32x2048x2048x3242): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3242): 90.755

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2143.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3243x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3243x2048): 71.112
Elapsed time for attention_prob_times_values (32x2048x2048x3243): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3243): 84.304

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2031.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3244x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3244x2048): 74.641
Elapsed time for attention_prob_times_values (32x2048x2048x3244): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3244): 90.773

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2158.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3245x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3245x2048): 70.874
Elapsed time for attention_prob_times_values (32x2048x2048x3245): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3245): 84.125

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2027.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3246x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3246x2048): 74.104
Elapsed time for attention_prob_times_values (32x2048x2048x3246): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3246): 89.826

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2140.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3247x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3247x2048): 70.825
Elapsed time for attention_prob_times_values (32x2048x2048x3247): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3247): 83.916

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2025.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3248x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3248x2048): 77.898
Elapsed time for attention_prob_times_values (32x2048x2048x3248): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3248): 69.519

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 1937.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 25992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3249x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3249x2048): 70.680
Elapsed time for attention_prob_times_values (32x2048x2048x3249): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3249): 84.138

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2026.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3250x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3250x2048): 74.015
Elapsed time for attention_prob_times_values (32x2048x2048x3250): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3250): 90.726

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2151.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3251x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3251x2048): 70.944
Elapsed time for attention_prob_times_values (32x2048x2048x3251): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3251): 84.199

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2032.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3252x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3252x2048): 74.400
Elapsed time for attention_prob_times_values (32x2048x2048x3252): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3252): 90.981

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2161.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3253x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3253x2048): 71.393
Elapsed time for attention_prob_times_values (32x2048x2048x3253): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3253): 83.842

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2037.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3254x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3254x2048): 73.540
Elapsed time for attention_prob_times_values (32x2048x2048x3254): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3254): 90.915

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2148.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3255x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3255x2048): 70.911
Elapsed time for attention_prob_times_values (32x2048x2048x3255): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3255): 84.281

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2035.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3256x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3256x2048): 75.802
Elapsed time for attention_prob_times_values (32x2048x2048x3256): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3256): 66.049

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 1866.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3257x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3257x2048): 70.600
Elapsed time for attention_prob_times_values (32x2048x2048x3257): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3257): 84.740

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2036.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3258x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3258x2048): 73.534
Elapsed time for attention_prob_times_values (32x2048x2048x3258): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3258): 90.745

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2149.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3259x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3259x2048): 70.367
Elapsed time for attention_prob_times_values (32x2048x2048x3259): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3259): 84.482

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2031.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3260x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3260x2048): 73.723
Elapsed time for attention_prob_times_values (32x2048x2048x3260): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3260): 91.720

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2163.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3261x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3261x2048): 70.683
Elapsed time for attention_prob_times_values (32x2048x2048x3261): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3261): 84.694

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2040.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3262x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3262x2048): 72.817
Elapsed time for attention_prob_times_values (32x2048x2048x3262): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3262): 90.908

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2141.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3263x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3263x2048): 70.436
Elapsed time for attention_prob_times_values (32x2048x2048x3263): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3263): 85.703

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2048.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3264x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3264x2048): 91.007
Elapsed time for attention_prob_times_values (32x2048x2048x3264): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3264): 87.879

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2369.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3265x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3265x2048): 72.489
Elapsed time for attention_prob_times_values (32x2048x2048x3265): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3265): 85.504

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2079.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3266x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3266x2048): 75.258
Elapsed time for attention_prob_times_values (32x2048x2048x3266): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3266): 90.092

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2174.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3267x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3267x2048): 72.647
Elapsed time for attention_prob_times_values (32x2048x2048x3267): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3267): 82.819

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2052.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3268x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3268x2048): 76.818
Elapsed time for attention_prob_times_values (32x2048x2048x3268): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3268): 90.936

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2209.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3269x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3269x2048): 73.162
Elapsed time for attention_prob_times_values (32x2048x2048x3269): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3269): 84.012

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2075.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3270x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3270x2048): 75.054
Elapsed time for attention_prob_times_values (32x2048x2048x3270): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3270): 90.473

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2178.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3271x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3271x2048): 72.472
Elapsed time for attention_prob_times_values (32x2048x2048x3271): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3271): 85.111

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2078.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3272x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3272x2048): 77.778
Elapsed time for attention_prob_times_values (32x2048x2048x3272): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3272): 65.939

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1895.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3273x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3273x2048): 72.033
Elapsed time for attention_prob_times_values (32x2048x2048x3273): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3273): 84.456

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2065.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3274x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3274x2048): 74.868
Elapsed time for attention_prob_times_values (32x2048x2048x3274): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3274): 89.859

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2170.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3275x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3275x2048): 71.808
Elapsed time for attention_prob_times_values (32x2048x2048x3275): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3275): 85.057

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2070.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3276x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3276x2048): 75.606
Elapsed time for attention_prob_times_values (32x2048x2048x3276): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3276): 90.819

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2194.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3277x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3277x2048): 72.176
Elapsed time for attention_prob_times_values (32x2048x2048x3277): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3277): 84.325

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2069.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3278x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3278x2048): 75.185
Elapsed time for attention_prob_times_values (32x2048x2048x3278): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3278): 90.654

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2187.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3279x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3279x2048): 71.894
Elapsed time for attention_prob_times_values (32x2048x2048x3279): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3279): 83.571

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2057.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3280x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3280x2048): 78.382
Elapsed time for attention_prob_times_values (32x2048x2048x3280): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3280): 69.755

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1965.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3281x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3281x2048): 71.462
Elapsed time for attention_prob_times_values (32x2048x2048x3281): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3281): 85.039

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2068.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3282x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3282x2048): 74.955
Elapsed time for attention_prob_times_values (32x2048x2048x3282): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3282): 90.367

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2183.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3283x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3283x2048): 71.949
Elapsed time for attention_prob_times_values (32x2048x2048x3283): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3283): 85.147

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2078.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3284x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3284x2048): 76.029
Elapsed time for attention_prob_times_values (32x2048x2048x3284): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3284): 91.219

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2210.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3285x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3285x2048): 72.371
Elapsed time for attention_prob_times_values (32x2048x2048x3285): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3285): 85.408

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2089.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3286x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3286x2048): 74.978
Elapsed time for attention_prob_times_values (32x2048x2048x3286): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3286): 90.659

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2189.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3287x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3287x2048): 71.765
Elapsed time for attention_prob_times_values (32x2048x2048x3287): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3287): 84.751

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2073.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3288x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3288x2048): 77.322
Elapsed time for attention_prob_times_values (32x2048x2048x3288): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3288): 66.755

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 1912.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3289x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3289x2048): 71.934
Elapsed time for attention_prob_times_values (32x2048x2048x3289): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3289): 85.352

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2084.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3290x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3290x2048): 74.969
Elapsed time for attention_prob_times_values (32x2048x2048x3290): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3290): 91.550

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2201.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3291x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3291x2048): 71.041
Elapsed time for attention_prob_times_values (32x2048x2048x3291): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3291): 84.858

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2065.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3292x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3292x2048): 75.547
Elapsed time for attention_prob_times_values (32x2048x2048x3292): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3292): 91.860

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2215.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3293x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3293x2048): 71.346
Elapsed time for attention_prob_times_values (32x2048x2048x3293): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3293): 85.639

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2080.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3294x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3294x2048): 75.163
Elapsed time for attention_prob_times_values (32x2048x2048x3294): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3294): 90.096

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2191.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3295x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3295x2048): 70.250
Elapsed time for attention_prob_times_values (32x2048x2048x3295): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3295): 85.441

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2061.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3296x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3296x2048): 78.352
Elapsed time for attention_prob_times_values (32x2048x2048x3296): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3296): 77.060

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2078.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3297x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3297x2048): 70.870
Elapsed time for attention_prob_times_values (32x2048x2048x3297): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3297): 85.552

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2074.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3298x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3298x2048): 74.563
Elapsed time for attention_prob_times_values (32x2048x2048x3298): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3298): 91.198

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2196.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3299x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3299x2048): 71.261
Elapsed time for attention_prob_times_values (32x2048x2048x3299): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3299): 84.907

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2074.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3300x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3300x2048): 75.071
Elapsed time for attention_prob_times_values (32x2048x2048x3300): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3300): 90.750

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2200.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3301x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3301x2048): 71.102
Elapsed time for attention_prob_times_values (32x2048x2048x3301): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3301): 83.824

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2061.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3302x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3302x2048): 74.395
Elapsed time for attention_prob_times_values (32x2048x2048x3302): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3302): 90.745

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2190.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3303x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3303x2048): 71.165
Elapsed time for attention_prob_times_values (32x2048x2048x3303): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3303): 85.066

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2077.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3304x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3304x2048): 76.857
Elapsed time for attention_prob_times_values (32x2048x2048x3304): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3304): 67.600

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1928.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3305x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3305x2048): 70.946
Elapsed time for attention_prob_times_values (32x2048x2048x3305): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3305): 83.724

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2059.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3306x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3306x2048): 73.970
Elapsed time for attention_prob_times_values (32x2048x2048x3306): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3306): 89.410

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2172.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3307x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3307x2048): 71.247
Elapsed time for attention_prob_times_values (32x2048x2048x3307): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3307): 84.411

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2073.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3308x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3308x2048): 74.825
Elapsed time for attention_prob_times_values (32x2048x2048x3308): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3308): 90.486

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2198.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3309x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3309x2048): 71.055
Elapsed time for attention_prob_times_values (32x2048x2048x3309): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3309): 84.504

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2072.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3310x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3310x2048): 74.371
Elapsed time for attention_prob_times_values (32x2048x2048x3310): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3310): 89.199

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2178.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3311x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3311x2048): 71.083
Elapsed time for attention_prob_times_values (32x2048x2048x3311): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3311): 83.898

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2067.714
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3312x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3312x2048): 77.551
Elapsed time for attention_prob_times_values (32x2048x2048x3312): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3312): 71.357

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1997.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3313x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3313x2048): 70.928
Elapsed time for attention_prob_times_values (32x2048x2048x3313): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3313): 85.332

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2082.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3314x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3314x2048): 74.035
Elapsed time for attention_prob_times_values (32x2048x2048x3314): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3314): 89.530

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2179.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3315x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3315x2048): 71.154
Elapsed time for attention_prob_times_values (32x2048x2048x3315): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3315): 85.089

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2084.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3316x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3316x2048): 74.553
Elapsed time for attention_prob_times_values (32x2048x2048x3316): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3316): 90.954

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2204.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3317x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3317x2048): 71.283
Elapsed time for attention_prob_times_values (32x2048x2048x3317): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3317): 83.694

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2072.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3318x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3318x2048): 74.053
Elapsed time for attention_prob_times_values (32x2048x2048x3318): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3318): 89.146

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2178.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3319x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3319x2048): 70.939
Elapsed time for attention_prob_times_values (32x2048x2048x3319): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3319): 85.155

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2084.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3320x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3320x2048): 75.866
Elapsed time for attention_prob_times_values (32x2048x2048x3320): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3320): 67.255

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 1920.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3321x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3321x2048): 71.020
Elapsed time for attention_prob_times_values (32x2048x2048x3321): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3321): 84.323

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2077.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3322x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3322x2048): 73.908
Elapsed time for attention_prob_times_values (32x2048x2048x3322): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3322): 89.983

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2187.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3323x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3323x2048): 70.363
Elapsed time for attention_prob_times_values (32x2048x2048x3323): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3323): 84.627

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2071.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3324x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3324x2048): 74.082
Elapsed time for attention_prob_times_values (32x2048x2048x3324): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3324): 89.929

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2190.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3325x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3325x2048): 70.648
Elapsed time for attention_prob_times_values (32x2048x2048x3325): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3325): 85.449

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2086.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3326x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3326x2048): 73.300
Elapsed time for attention_prob_times_values (32x2048x2048x3326): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3326): 90.133

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2181.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3327x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3327x2048): 69.720
Elapsed time for attention_prob_times_values (32x2048x2048x3327): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3327): 84.635

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2063.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3328x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3328x2048): 89.783
Elapsed time for attention_prob_times_values (32x2048x2048x3328): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3328): 91.159

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2442.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3329x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3329x2048): 71.651
Elapsed time for attention_prob_times_values (32x2048x2048x3329): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3329): 83.356

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2081.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3330x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3330x2048): 75.605
Elapsed time for attention_prob_times_values (32x2048x2048x3330): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3330): 89.817

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2217.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3331x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3331x2048): 71.810
Elapsed time for attention_prob_times_values (32x2048x2048x3331): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3331): 83.781

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2089.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3332x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3332x2048): 75.405
Elapsed time for attention_prob_times_values (32x2048x2048x3332): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3332): 90.976

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2229.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3333x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3333x2048): 72.244
Elapsed time for attention_prob_times_values (32x2048x2048x3333): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3333): 83.032

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2089.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3334x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3334x2048): 75.781
Elapsed time for attention_prob_times_values (32x2048x2048x3334): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3334): 90.048

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2225.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3335x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3335x2048): 72.425
Elapsed time for attention_prob_times_values (32x2048x2048x3335): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3335): 84.156

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2106.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3336x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3336x2048): 77.330
Elapsed time for attention_prob_times_values (32x2048x2048x3336): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3336): 65.101

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 1913.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3337x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3337x2048): 71.779
Elapsed time for attention_prob_times_values (32x2048x2048x3337): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3337): 83.758

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2092.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3338x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3338x2048): 75.237
Elapsed time for attention_prob_times_values (32x2048x2048x3338): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3338): 90.722

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2227.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3339x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3339x2048): 72.410
Elapsed time for attention_prob_times_values (32x2048x2048x3339): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3339): 83.616

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2102.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3340x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3340x2048): 75.918
Elapsed time for attention_prob_times_values (32x2048x2048x3340): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3340): 90.809

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2240.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3341x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3341x2048): 72.438
Elapsed time for attention_prob_times_values (32x2048x2048x3341): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3341): 83.240

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2099.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3342x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3342x2048): 75.404
Elapsed time for attention_prob_times_values (32x2048x2048x3342): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3342): 90.612

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2231.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3343x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3343x2048): 71.744
Elapsed time for attention_prob_times_values (32x2048x2048x3343): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3343): 83.841

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2096.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3344x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3344x2048): 78.753
Elapsed time for attention_prob_times_values (32x2048x2048x3344): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3344): 69.064

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 1996.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3345x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3345x2048): 72.076
Elapsed time for attention_prob_times_values (32x2048x2048x3345): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3345): 84.186

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2107.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3346x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3346x2048): 75.064
Elapsed time for attention_prob_times_values (32x2048x2048x3346): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3346): 90.946

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2232.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3347x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3347x2048): 71.824
Elapsed time for attention_prob_times_values (32x2048x2048x3347): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3347): 83.301

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2094.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3348x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3348x2048): 75.588
Elapsed time for attention_prob_times_values (32x2048x2048x3348): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3348): 91.615

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2249.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3349x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3349x2048): 71.948
Elapsed time for attention_prob_times_values (32x2048x2048x3349): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3349): 82.444

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2087.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3350x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3350x2048): 75.024
Elapsed time for attention_prob_times_values (32x2048x2048x3350): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3350): 90.402

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2228.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3351x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3351x2048): 71.885
Elapsed time for attention_prob_times_values (32x2048x2048x3351): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3351): 83.810

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2103.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3352x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3352x2048): 76.992
Elapsed time for attention_prob_times_values (32x2048x2048x3352): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3352): 66.601

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1941.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3353x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3353x2048): 71.346
Elapsed time for attention_prob_times_values (32x2048x2048x3353): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3353): 84.081

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2099.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3354x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3354x2048): 74.954
Elapsed time for attention_prob_times_values (32x2048x2048x3354): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3354): 91.067

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2236.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3355x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3355x2048): 71.358
Elapsed time for attention_prob_times_values (32x2048x2048x3355): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3355): 83.882

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2098.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3356x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3356x2048): 75.515
Elapsed time for attention_prob_times_values (32x2048x2048x3356): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3356): 90.884

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2245.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3357x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3357x2048): 71.325
Elapsed time for attention_prob_times_values (32x2048x2048x3357): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3357): 82.660

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2084.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3358x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3358x2048): 74.853
Elapsed time for attention_prob_times_values (32x2048x2048x3358): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3358): 89.948

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2225.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3359x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3359x2048): 70.975
Elapsed time for attention_prob_times_values (32x2048x2048x3359): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3359): 82.844

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2082.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3360x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3360x2048): 78.457
Elapsed time for attention_prob_times_values (32x2048x2048x3360): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3360): 75.229

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2093.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3361x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3361x2048): 70.989
Elapsed time for attention_prob_times_values (32x2048x2048x3361): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3361): 84.131

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2098.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3362x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3362x2048): 74.525
Elapsed time for attention_prob_times_values (32x2048x2048x3362): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3362): 90.804

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2232.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3363x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3363x2048): 71.062
Elapsed time for attention_prob_times_values (32x2048x2048x3363): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3363): 83.497

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2094.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3364x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3364x2048): 75.381
Elapsed time for attention_prob_times_values (32x2048x2048x3364): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3364): 91.431

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2254.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3365x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3365x2048): 71.084
Elapsed time for attention_prob_times_values (32x2048x2048x3365): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3365): 84.048

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2101.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3366x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3366x2048): 74.358
Elapsed time for attention_prob_times_values (32x2048x2048x3366): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3366): 89.737

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2219.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3367x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3367x2048): 71.123
Elapsed time for attention_prob_times_values (32x2048x2048x3367): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3367): 84.123

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2104.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3368x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3368x2048): 76.655
Elapsed time for attention_prob_times_values (32x2048x2048x3368): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3368): 65.659

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1931.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3369x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3369x2048): 70.973
Elapsed time for attention_prob_times_values (32x2048x2048x3369): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3369): 83.729

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2098.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3370x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3370x2048): 74.386
Elapsed time for attention_prob_times_values (32x2048x2048x3370): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3370): 90.230

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2228.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3371x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3371x2048): 71.559
Elapsed time for attention_prob_times_values (32x2048x2048x3371): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3371): 84.152

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2114.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3372x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3372x2048): 75.029
Elapsed time for attention_prob_times_values (32x2048x2048x3372): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3372): 91.631

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2255.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3373x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3373x2048): 71.264
Elapsed time for attention_prob_times_values (32x2048x2048x3373): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3373): 84.612

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2116.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 26992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3374x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3374x2048): 74.565
Elapsed time for attention_prob_times_values (32x2048x2048x3374): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3374): 91.429

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2247.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3375x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3375x2048): 70.887
Elapsed time for attention_prob_times_values (32x2048x2048x3375): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3375): 84.160

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2106.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3376x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3376x2048): 77.721
Elapsed time for attention_prob_times_values (32x2048x2048x3376): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3376): 93.179

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2320.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3377x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3377x2048): 70.801
Elapsed time for attention_prob_times_values (32x2048x2048x3377): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3377): 84.328

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2107.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3378x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3378x2048): 74.300
Elapsed time for attention_prob_times_values (32x2048x2048x3378): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3378): 90.973

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2240.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3379x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3379x2048): 71.193
Elapsed time for attention_prob_times_values (32x2048x2048x3379): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3379): 84.100

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2112.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3380x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3380x2048): 74.803
Elapsed time for attention_prob_times_values (32x2048x2048x3380): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3380): 91.706

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2258.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3381x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3381x2048): 71.452
Elapsed time for attention_prob_times_values (32x2048x2048x3381): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3381): 84.200

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2119.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3382x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3382x2048): 74.242
Elapsed time for attention_prob_times_values (32x2048x2048x3382): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3382): 90.266

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2234.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3383x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3383x2048): 71.033
Elapsed time for attention_prob_times_values (32x2048x2048x3383): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3383): 84.272

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2114.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3384x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3384x2048): 76.205
Elapsed time for attention_prob_times_values (32x2048x2048x3384): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3384): 93.703

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2306.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3385x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3385x2048): 70.879
Elapsed time for attention_prob_times_values (32x2048x2048x3385): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3385): 84.145

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2111.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3386x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3386x2048): 73.742
Elapsed time for attention_prob_times_values (32x2048x2048x3386): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3386): 90.032

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2225.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3387x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3387x2048): 70.988
Elapsed time for attention_prob_times_values (32x2048x2048x3387): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3387): 83.904

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2111.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3388x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3388x2048): 74.671
Elapsed time for attention_prob_times_values (32x2048x2048x3388): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3388): 91.187

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2255.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3389x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3389x2048): 71.024
Elapsed time for attention_prob_times_values (32x2048x2048x3389): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3389): 84.365

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2119.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3390x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3390x2048): 73.465
Elapsed time for attention_prob_times_values (32x2048x2048x3390): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3390): 90.146

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2225.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3391x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3391x2048): 70.137
Elapsed time for attention_prob_times_values (32x2048x2048x3391): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3391): 84.093

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2102.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3392x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3392x2048): 90.647
Elapsed time for attention_prob_times_values (32x2048x2048x3392): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3392): 93.627

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2533.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3393x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3393x2048): 72.181
Elapsed time for attention_prob_times_values (32x2048x2048x3393): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3393): 83.386

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2128.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3394x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3394x2048): 75.789
Elapsed time for attention_prob_times_values (32x2048x2048x3394): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3394): 89.721

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2260.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3395x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3395x2048): 72.604
Elapsed time for attention_prob_times_values (32x2048x2048x3395): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3395): 83.437

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2137.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3396x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3396x2048): 76.615
Elapsed time for attention_prob_times_values (32x2048x2048x3396): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3396): 91.473

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2295.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3397x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3397x2048): 72.902
Elapsed time for attention_prob_times_values (32x2048x2048x3397): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3397): 83.494

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2143.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3398x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3398x2048): 76.093
Elapsed time for attention_prob_times_values (32x2048x2048x3398): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3398): 89.646

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2267.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3399x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3399x2048): 73.155
Elapsed time for attention_prob_times_values (32x2048x2048x3399): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3399): 84.144

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2156.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3400x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3400x2048): 78.534
Elapsed time for attention_prob_times_values (32x2048x2048x3400): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3400): 94.014

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2358.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3401x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3401x2048): 72.379
Elapsed time for attention_prob_times_values (32x2048x2048x3401): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3401): 84.023

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2144.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3402x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3402x2048): 75.693
Elapsed time for attention_prob_times_values (32x2048x2048x3402): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3402): 90.436

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2272.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3403x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3403x2048): 72.498
Elapsed time for attention_prob_times_values (32x2048x2048x3403): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3403): 83.911

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2145.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3404x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3404x2048): 75.977
Elapsed time for attention_prob_times_values (32x2048x2048x3404): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3404): 91.206

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2287.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3405x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3405x2048): 72.162
Elapsed time for attention_prob_times_values (32x2048x2048x3405): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3405): 84.224

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2145.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3406x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3406x2048): 75.477
Elapsed time for attention_prob_times_values (32x2048x2048x3406): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3406): 90.044

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2267.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3407x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3407x2048): 71.932
Elapsed time for attention_prob_times_values (32x2048x2048x3407): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3407): 83.515

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2134.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3408x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3408x2048): 79.291
Elapsed time for attention_prob_times_values (32x2048x2048x3408): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3408): 94.353

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2380.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3409x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3409x2048): 71.745
Elapsed time for attention_prob_times_values (32x2048x2048x3409): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3409): 83.922

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2137.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3410x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3410x2048): 75.303
Elapsed time for attention_prob_times_values (32x2048x2048x3410): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3410): 90.231

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2269.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3411x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3411x2048): 71.943
Elapsed time for attention_prob_times_values (32x2048x2048x3411): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3411): 82.367

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2123.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3412x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3412x2048): 75.765
Elapsed time for attention_prob_times_values (32x2048x2048x3412): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3412): 90.994

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2286.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3413x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3413x2048): 71.938
Elapsed time for attention_prob_times_values (32x2048x2048x3413): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3413): 84.094

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2145.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3414x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3414x2048): 75.199
Elapsed time for attention_prob_times_values (32x2048x2048x3414): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3414): 90.205

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2269.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3415x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3415x2048): 71.610
Elapsed time for attention_prob_times_values (32x2048x2048x3415): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3415): 84.223

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2142.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3416x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3416x2048): 77.283
Elapsed time for attention_prob_times_values (32x2048x2048x3416): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3416): 94.468

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2353.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3417x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3417x2048): 71.972
Elapsed time for attention_prob_times_values (32x2048x2048x3417): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3417): 84.079

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2147.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3418x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3418x2048): 74.423
Elapsed time for attention_prob_times_values (32x2048x2048x3418): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3418): 90.266

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2260.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3419x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3419x2048): 71.509
Elapsed time for attention_prob_times_values (32x2048x2048x3419): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3419): 83.495

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2134.805
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3420x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3420x2048): 75.742
Elapsed time for attention_prob_times_values (32x2048x2048x3420): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3420): 91.391

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2296.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3421x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3421x2048): 71.492
Elapsed time for attention_prob_times_values (32x2048x2048x3421): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3421): 84.284

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2145.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3422x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3422x2048): 75.050
Elapsed time for attention_prob_times_values (32x2048x2048x3422): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3422): 90.015

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2270.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3423x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3423x2048): 71.107
Elapsed time for attention_prob_times_values (32x2048x2048x3423): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3423): 72.316

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1989.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3424x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3424x2048): 79.213
Elapsed time for attention_prob_times_values (32x2048x2048x3424): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3424): 94.362

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2390.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3425x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3425x2048): 71.204
Elapsed time for attention_prob_times_values (32x2048x2048x3425): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3425): 102.184

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2329.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3426x2048): 0.0760
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3426x2048): 12.107
Elapsed time for attention_prob_times_values (32x2048x2048x3426): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3426): 104.064

Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 602.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0848
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3427x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3427x2048): 71.981
Elapsed time for attention_prob_times_values (32x2048x2048x3427): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3427): 102.022

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2344.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3428x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3428x2048): 75.368
Elapsed time for attention_prob_times_values (32x2048x2048x3428): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3428): 105.048

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2438.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3429x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3429x2048): 71.526
Elapsed time for attention_prob_times_values (32x2048x2048x3429): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3429): 102.124

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2337.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3430x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3430x2048): 74.962
Elapsed time for attention_prob_times_values (32x2048x2048x3430): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3430): 103.978

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 2421.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3431x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3431x2048): 71.372
Elapsed time for attention_prob_times_values (32x2048x2048x3431): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3431): 102.216

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2337.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3432x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3432x2048): 77.031
Elapsed time for attention_prob_times_values (32x2048x2048x3432): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3432): 95.060

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2366.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3433x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3433x2048): 71.347
Elapsed time for attention_prob_times_values (32x2048x2048x3433): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3433): 102.276

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2338.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3434x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3434x2048): 74.528
Elapsed time for attention_prob_times_values (32x2048x2048x3434): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3434): 104.273

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2418.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3435x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3435x2048): 71.822
Elapsed time for attention_prob_times_values (32x2048x2048x3435): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3435): 102.230

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2348.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3436x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3436x2048): 74.862
Elapsed time for attention_prob_times_values (32x2048x2048x3436): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3436): 104.179

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2425.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3437x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3437x2048): 71.373
Elapsed time for attention_prob_times_values (32x2048x2048x3437): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3437): 102.238

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2341.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3438x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3438x2048): 74.936
Elapsed time for attention_prob_times_values (32x2048x2048x3438): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3438): 104.164

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2428.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3439x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3439x2048): 71.264
Elapsed time for attention_prob_times_values (32x2048x2048x3439): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3439): 102.479

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2342.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3440x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3440x2048): 78.000
Elapsed time for attention_prob_times_values (32x2048x2048x3440): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3440): 95.481

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2393.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3441x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3441x2048): 71.299
Elapsed time for attention_prob_times_values (32x2048x2048x3441): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3441): 102.139

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2341.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3442x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3442x2048): 74.535
Elapsed time for attention_prob_times_values (32x2048x2048x3442): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3442): 104.263

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2424.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3443x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3443x2048): 71.247
Elapsed time for attention_prob_times_values (32x2048x2048x3443): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3443): 102.357

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2343.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3444x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3444x2048): 74.924
Elapsed time for attention_prob_times_values (32x2048x2048x3444): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3444): 104.349

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 2434.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3445x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3445x2048): 71.385
Elapsed time for attention_prob_times_values (32x2048x2048x3445): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3445): 102.325

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2347.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3446x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3446x2048): 74.168
Elapsed time for attention_prob_times_values (32x2048x2048x3446): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3446): 104.364

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2421.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3447x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3447x2048): 71.220
Elapsed time for attention_prob_times_values (32x2048x2048x3447): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3447): 102.468

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2347.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3448x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3448x2048): 76.090
Elapsed time for attention_prob_times_values (32x2048x2048x3448): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3448): 95.562

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2366.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3449x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3449x2048): 71.248
Elapsed time for attention_prob_times_values (32x2048x2048x3449): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3449): 102.400

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2348.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3450x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3450x2048): 74.169
Elapsed time for attention_prob_times_values (32x2048x2048x3450): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3450): 104.331

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2423.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3451x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3451x2048): 70.924
Elapsed time for attention_prob_times_values (32x2048x2048x3451): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3451): 102.525

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2344.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3452x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3452x2048): 73.914
Elapsed time for attention_prob_times_values (32x2048x2048x3452): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3452): 104.828

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2424.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3453x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3453x2048): 70.628
Elapsed time for attention_prob_times_values (32x2048x2048x3453): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3453): 102.638

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2340.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3454x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3454x2048): 73.044
Elapsed time for attention_prob_times_values (32x2048x2048x3454): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3454): 104.452

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2405.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3455x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3455x2048): 70.362
Elapsed time for attention_prob_times_values (32x2048x2048x3455): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3455): 102.608

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2336.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3456x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3456x2048): 89.874
Elapsed time for attention_prob_times_values (32x2048x2048x3456): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3456): 96.226

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2602.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3457x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3457x2048): 72.180
Elapsed time for attention_prob_times_values (32x2048x2048x3457): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3457): 100.225

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2350.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3458x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3458x2048): 75.853
Elapsed time for attention_prob_times_values (32x2048x2048x3458): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3458): 102.336

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2440.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3459x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3459x2048): 72.227
Elapsed time for attention_prob_times_values (32x2048x2048x3459): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3459): 100.407

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2354.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3460x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3460x2048): 75.985
Elapsed time for attention_prob_times_values (32x2048x2048x3460): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3460): 102.425

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2445.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3461x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3461x2048): 72.481
Elapsed time for attention_prob_times_values (32x2048x2048x3461): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3461): 100.486

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2361.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3462x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3462x2048): 75.918
Elapsed time for attention_prob_times_values (32x2048x2048x3462): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3462): 102.373

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2445.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3463x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3463x2048): 72.292
Elapsed time for attention_prob_times_values (32x2048x2048x3463): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3463): 100.593

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2360.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3464x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3464x2048): 77.844
Elapsed time for attention_prob_times_values (32x2048x2048x3464): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3464): 92.666

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2374.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3465x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3465x2048): 72.276
Elapsed time for attention_prob_times_values (32x2048x2048x3465): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3465): 100.633

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2361.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3466x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3466x2048): 75.727
Elapsed time for attention_prob_times_values (32x2048x2048x3466): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3466): 102.521

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2445.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3467x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3467x2048): 72.369
Elapsed time for attention_prob_times_values (32x2048x2048x3467): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3467): 100.631

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2364.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3468x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3468x2048): 76.364
Elapsed time for attention_prob_times_values (32x2048x2048x3468): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3468): 102.618

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 2460.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3469x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3469x2048): 72.291
Elapsed time for attention_prob_times_values (32x2048x2048x3469): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3469): 100.731

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2365.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3470x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3470x2048): 75.759
Elapsed time for attention_prob_times_values (32x2048x2048x3470): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3470): 102.500

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2448.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3471x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3471x2048): 71.924
Elapsed time for attention_prob_times_values (32x2048x2048x3471): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3471): 100.841

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2360.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3472x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3472x2048): 78.900
Elapsed time for attention_prob_times_values (32x2048x2048x3472): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3472): 91.826

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2387.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3473x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3473x2048): 71.791
Elapsed time for attention_prob_times_values (32x2048x2048x3473): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3473): 100.918

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2360.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3474x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3474x2048): 75.499
Elapsed time for attention_prob_times_values (32x2048x2048x3474): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3474): 102.680

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2448.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3475x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3475x2048): 72.189
Elapsed time for attention_prob_times_values (32x2048x2048x3475): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3475): 100.825

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2368.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3476x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3476x2048): 76.016
Elapsed time for attention_prob_times_values (32x2048x2048x3476): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3476): 102.766

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2460.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3477x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3477x2048): 72.075
Elapsed time for attention_prob_times_values (32x2048x2048x3477): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3477): 100.930

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2368.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3478x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3478x2048): 75.027
Elapsed time for attention_prob_times_values (32x2048x2048x3478): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3478): 102.725

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2443.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3479x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3479x2048): 71.886
Elapsed time for attention_prob_times_values (32x2048x2048x3479): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3479): 100.977

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2366.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3480x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3480x2048): 77.150
Elapsed time for attention_prob_times_values (32x2048x2048x3480): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3480): 92.548

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2371.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3481x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3481x2048): 71.564
Elapsed time for attention_prob_times_values (32x2048x2048x3481): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3481): 101.098

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2362.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3482x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3482x2048): 75.346
Elapsed time for attention_prob_times_values (32x2048x2048x3482): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3482): 102.805

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2452.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3483x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3483x2048): 71.821
Elapsed time for attention_prob_times_values (32x2048x2048x3483): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3483): 101.172

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2369.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3484x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3484x2048): 75.948
Elapsed time for attention_prob_times_values (32x2048x2048x3484): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3484): 102.883

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2465.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3485x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3485x2048): 71.621
Elapsed time for attention_prob_times_values (32x2048x2048x3485): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3485): 101.170

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2367.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3486x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3486x2048): 75.247
Elapsed time for attention_prob_times_values (32x2048x2048x3486): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3486): 102.867

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2454.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3487x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3487x2048): 71.435
Elapsed time for attention_prob_times_values (32x2048x2048x3487): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3487): 101.316

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2366.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3488x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3488x2048): 78.968
Elapsed time for attention_prob_times_values (32x2048x2048x3488): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3488): 92.521

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2407.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3489x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3489x2048): 71.222
Elapsed time for attention_prob_times_values (32x2048x2048x3489): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3489): 101.361

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2364.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3490x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3490x2048): 75.367
Elapsed time for attention_prob_times_values (32x2048x2048x3490): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3490): 103.044

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2460.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3491x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3491x2048): 71.714
Elapsed time for attention_prob_times_values (32x2048x2048x3491): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3491): 101.357

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2374.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3492x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3492x2048): 75.867
Elapsed time for attention_prob_times_values (32x2048x2048x3492): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3492): 103.149

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 2472.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3493x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3493x2048): 71.912
Elapsed time for attention_prob_times_values (32x2048x2048x3493): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3493): 101.248

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2378.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3494x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3494x2048): 74.988
Elapsed time for attention_prob_times_values (32x2048x2048x3494): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3494): 103.100

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2456.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3495x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3495x2048): 71.852
Elapsed time for attention_prob_times_values (32x2048x2048x3495): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3495): 101.342

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2380.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3496x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3496x2048): 77.505
Elapsed time for attention_prob_times_values (32x2048x2048x3496): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3496): 93.163

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2395.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3497x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3497x2048): 71.800
Elapsed time for attention_prob_times_values (32x2048x2048x3497): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3497): 101.398

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2380.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3498x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3498x2048): 74.778
Elapsed time for attention_prob_times_values (32x2048x2048x3498): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3498): 103.191

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2456.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 27992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3499x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3499x2048): 71.950
Elapsed time for attention_prob_times_values (32x2048x2048x3499): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3499): 101.456

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2385.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3500x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3500x2048): 75.742
Elapsed time for attention_prob_times_values (32x2048x2048x3500): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3500): 103.372

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 2477.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3501x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3501x2048): 71.440
Elapsed time for attention_prob_times_values (32x2048x2048x3501): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3501): 101.399

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2376.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3502x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3502x2048): 75.312
Elapsed time for attention_prob_times_values (32x2048x2048x3502): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3502): 103.266

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2470.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3503x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3503x2048): 71.676
Elapsed time for attention_prob_times_values (32x2048x2048x3503): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3503): 101.574

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2384.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3504x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3504x2048): 78.414
Elapsed time for attention_prob_times_values (32x2048x2048x3504): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3504): 92.814

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2412.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3505x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3505x2048): 71.428
Elapsed time for attention_prob_times_values (32x2048x2048x3505): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3505): 101.541

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2380.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3506x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3506x2048): 74.709
Elapsed time for attention_prob_times_values (32x2048x2048x3506): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3506): 103.360

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2462.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3507x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3507x2048): 71.660
Elapsed time for attention_prob_times_values (32x2048x2048x3507): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3507): 101.614

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2386.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3508x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3508x2048): 75.315
Elapsed time for attention_prob_times_values (32x2048x2048x3508): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3508): 103.505

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 2476.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3509x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3509x2048): 71.496
Elapsed time for attention_prob_times_values (32x2048x2048x3509): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3509): 101.610

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2384.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3510x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3510x2048): 74.647
Elapsed time for attention_prob_times_values (32x2048x2048x3510): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3510): 103.445

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2464.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3511x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3511x2048): 71.639
Elapsed time for attention_prob_times_values (32x2048x2048x3511): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3511): 101.626

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2389.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3512x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3512x2048): 76.701
Elapsed time for attention_prob_times_values (32x2048x2048x3512): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3512): 93.647

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 2398.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3513x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3513x2048): 101.736
Elapsed time for attention_prob_times_values (32x2048x2048x3513): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3513): 101.775

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2894.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3514x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3514x2048): 102.320
Elapsed time for attention_prob_times_values (32x2048x2048x3514): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3514): 103.595

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2929.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3515x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3515x2048): 101.786
Elapsed time for attention_prob_times_values (32x2048x2048x3515): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3515): 101.809

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2897.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3516x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3516x2048): 102.742
Elapsed time for attention_prob_times_values (32x2048x2048x3516): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3516): 103.760

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2939.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3517x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3517x2048): 101.896
Elapsed time for attention_prob_times_values (32x2048x2048x3517): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3517): 101.825

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2900.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3518x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3518x2048): 102.421
Elapsed time for attention_prob_times_values (32x2048x2048x3518): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3518): 103.738

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2936.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3519x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3519x2048): 101.844
Elapsed time for attention_prob_times_values (32x2048x2048x3519): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3519): 101.897

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2902.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3520x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3520x2048): 107.552
Elapsed time for attention_prob_times_values (32x2048x2048x3520): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3520): 93.425

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2849.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3521x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3521x2048): 102.627
Elapsed time for attention_prob_times_values (32x2048x2048x3521): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3521): 102.025

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2917.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3522x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3522x2048): 103.086
Elapsed time for attention_prob_times_values (32x2048x2048x3522): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3522): 103.943

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2951.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3523x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3523x2048): 102.463
Elapsed time for attention_prob_times_values (32x2048x2048x3523): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3523): 102.094

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2917.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3524x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3524x2048): 103.368
Elapsed time for attention_prob_times_values (32x2048x2048x3524): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3524): 104.040

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 2958.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3525x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3525x2048): 102.257
Elapsed time for attention_prob_times_values (32x2048x2048x3525): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3525): 102.141

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2916.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3526x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3526x2048): 102.829
Elapsed time for attention_prob_times_values (32x2048x2048x3526): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3526): 104.080

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2953.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3527x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3527x2048): 102.191
Elapsed time for attention_prob_times_values (32x2048x2048x3527): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3527): 102.246

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2918.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3528x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3528x2048): 103.372
Elapsed time for attention_prob_times_values (32x2048x2048x3528): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3528): 92.914

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2795.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3529x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3529x2048): 101.791
Elapsed time for attention_prob_times_values (32x2048x2048x3529): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3529): 102.357

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2916.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3530x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3530x2048): 102.436
Elapsed time for attention_prob_times_values (32x2048x2048x3530): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3530): 104.171

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2952.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3531x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3531x2048): 101.956
Elapsed time for attention_prob_times_values (32x2048x2048x3531): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3531): 102.413

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2921.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3532x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3532x2048): 102.927
Elapsed time for attention_prob_times_values (32x2048x2048x3532): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3532): 104.222

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2961.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3533x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3533x2048): 101.917
Elapsed time for attention_prob_times_values (32x2048x2048x3533): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3533): 102.434

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2922.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3534x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3534x2048): 102.558
Elapsed time for attention_prob_times_values (32x2048x2048x3534): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3534): 104.141

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2956.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3535x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3535x2048): 101.990
Elapsed time for attention_prob_times_values (32x2048x2048x3535): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3535): 102.521

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2926.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3536x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3536x2048): 103.647
Elapsed time for attention_prob_times_values (32x2048x2048x3536): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3536): 93.424

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2812.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3537x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3537x2048): 101.711
Elapsed time for attention_prob_times_values (32x2048x2048x3537): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3537): 102.613

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2925.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3538x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3538x2048): 102.312
Elapsed time for attention_prob_times_values (32x2048x2048x3538): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3538): 104.308

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2958.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3539x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3539x2048): 101.725
Elapsed time for attention_prob_times_values (32x2048x2048x3539): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3539): 102.623

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2927.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3540x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3540x2048): 102.766
Elapsed time for attention_prob_times_values (32x2048x2048x3540): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3540): 104.452

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2968.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3541x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3541x2048): 101.749
Elapsed time for attention_prob_times_values (32x2048x2048x3541): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3541): 102.726

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2930.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3542x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3542x2048): 102.357
Elapsed time for attention_prob_times_values (32x2048x2048x3542): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3542): 104.443

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2964.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3543x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3543x2048): 101.731
Elapsed time for attention_prob_times_values (32x2048x2048x3543): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3543): 102.726

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2931.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3544x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3544x2048): 103.058
Elapsed time for attention_prob_times_values (32x2048x2048x3544): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3544): 93.783

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2817.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3545x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3545x2048): 101.542
Elapsed time for attention_prob_times_values (32x2048x2048x3545): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3545): 102.877

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2932.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3546x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3546x2048): 102.122
Elapsed time for attention_prob_times_values (32x2048x2048x3546): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3546): 104.577

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2966.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3547x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3547x2048): 101.623
Elapsed time for attention_prob_times_values (32x2048x2048x3547): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3547): 102.938

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2936.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3548x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3548x2048): 102.625
Elapsed time for attention_prob_times_values (32x2048x2048x3548): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3548): 104.666

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2976.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3549x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3549x2048): 101.620
Elapsed time for attention_prob_times_values (32x2048x2048x3549): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3549): 103.014

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2939.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3550x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3550x2048): 102.305
Elapsed time for attention_prob_times_values (32x2048x2048x3550): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3550): 104.348

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2968.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3551x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3551x2048): 101.737
Elapsed time for attention_prob_times_values (32x2048x2048x3551): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3551): 103.125

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2943.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3552x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3552x2048): 107.459
Elapsed time for attention_prob_times_values (32x2048x2048x3552): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3552): 94.145

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2885.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3553x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3553x2048): 102.575
Elapsed time for attention_prob_times_values (32x2048x2048x3553): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3553): 103.136

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2957.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3554x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3554x2048): 103.074
Elapsed time for attention_prob_times_values (32x2048x2048x3554): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3554): 104.772

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2989.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3555x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3555x2048): 102.404
Elapsed time for attention_prob_times_values (32x2048x2048x3555): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3555): 103.133

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2956.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3556x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3556x2048): 103.276
Elapsed time for attention_prob_times_values (32x2048x2048x3556): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3556): 104.911

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 2995.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3557x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3557x2048): 102.229
Elapsed time for attention_prob_times_values (32x2048x2048x3557): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3557): 103.161

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2956.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3558x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3558x2048): 102.783
Elapsed time for attention_prob_times_values (32x2048x2048x3558): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3558): 104.854

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2989.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3559x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3559x2048): 102.091
Elapsed time for attention_prob_times_values (32x2048x2048x3559): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3559): 103.174

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 2956.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3560x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3560x2048): 103.312
Elapsed time for attention_prob_times_values (32x2048x2048x3560): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3560): 94.506

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2844.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3561x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3561x2048): 101.718
Elapsed time for attention_prob_times_values (32x2048x2048x3561): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3561): 103.152

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2952.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3562x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3562x2048): 102.383
Elapsed time for attention_prob_times_values (32x2048x2048x3562): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3562): 104.945

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2987.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3563x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3563x2048): 101.884
Elapsed time for attention_prob_times_values (32x2048x2048x3563): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3563): 103.049

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2954.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3564x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3564x2048): 102.903
Elapsed time for attention_prob_times_values (32x2048x2048x3564): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3564): 105.111

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2999.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3565x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3565x2048): 101.910
Elapsed time for attention_prob_times_values (32x2048x2048x3565): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3565): 103.149

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2958.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3566x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3566x2048): 102.566
Elapsed time for attention_prob_times_values (32x2048x2048x3566): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3566): 105.071

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 2995.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3567x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3567x2048): 101.939
Elapsed time for attention_prob_times_values (32x2048x2048x3567): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3567): 103.356

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2962.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3568x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3568x2048): 103.772
Elapsed time for attention_prob_times_values (32x2048x2048x3568): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3568): 94.376

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2854.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3569x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3569x2048): 101.758
Elapsed time for attention_prob_times_values (32x2048x2048x3569): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3569): 103.432

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2963.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3570x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3570x2048): 102.342
Elapsed time for attention_prob_times_values (32x2048x2048x3570): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3570): 105.194

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2997.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3571x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3571x2048): 101.751
Elapsed time for attention_prob_times_values (32x2048x2048x3571): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3571): 103.211

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2961.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3572x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3572x2048): 102.715
Elapsed time for attention_prob_times_values (32x2048x2048x3572): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3572): 105.347

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 3006.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3573x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3573x2048): 101.787
Elapsed time for attention_prob_times_values (32x2048x2048x3573): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3573): 103.261

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2964.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3574x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3574x2048): 102.433
Elapsed time for attention_prob_times_values (32x2048x2048x3574): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3574): 105.193

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 3001.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3575x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3575x2048): 101.878
Elapsed time for attention_prob_times_values (32x2048x2048x3575): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3575): 103.276

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2967.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3576x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3576x2048): 103.154
Elapsed time for attention_prob_times_values (32x2048x2048x3576): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3576): 94.781

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2858.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3577x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3577x2048): 101.642
Elapsed time for attention_prob_times_values (32x2048x2048x3577): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3577): 103.331

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2966.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3578x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3578x2048): 102.261
Elapsed time for attention_prob_times_values (32x2048x2048x3578): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3578): 105.380

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 3005.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3579x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3579x2048): 101.678
Elapsed time for attention_prob_times_values (32x2048x2048x3579): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3579): 103.509

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2970.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3580x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3580x2048): 102.662
Elapsed time for attention_prob_times_values (32x2048x2048x3580): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3580): 105.478

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 3014.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3581x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3581x2048): 101.712
Elapsed time for attention_prob_times_values (32x2048x2048x3581): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3581): 102.956

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2965.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3582x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3582x2048): 102.227
Elapsed time for attention_prob_times_values (32x2048x2048x3582): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3582): 105.520

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 3009.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3583x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3583x2048): 101.648
Elapsed time for attention_prob_times_values (32x2048x2048x3583): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3583): 103.198

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2969.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3584x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3584x2048): 107.331
Elapsed time for attention_prob_times_values (32x2048x2048x3584): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3584): 96.223

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2942.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3585x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3585x2048): 102.537
Elapsed time for attention_prob_times_values (32x2048x2048x3585): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3585): 100.554

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2945.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3586x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3586x2048): 103.076
Elapsed time for attention_prob_times_values (32x2048x2048x3586): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3586): 102.944

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2988.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3587x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3587x2048): 102.445
Elapsed time for attention_prob_times_values (32x2048x2048x3587): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3587): 100.925

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 2951.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3588x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3588x2048): 103.421
Elapsed time for attention_prob_times_values (32x2048x2048x3588): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3588): 103.062

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2997.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3589x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3589x2048): 102.282
Elapsed time for attention_prob_times_values (32x2048x2048x3589): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3589): 100.800

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2948.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3590x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3590x2048): 102.763
Elapsed time for attention_prob_times_values (32x2048x2048x3590): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3590): 103.036

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 2988.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3591x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3591x2048): 102.168
Elapsed time for attention_prob_times_values (32x2048x2048x3591): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3591): 100.725

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2947.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3592x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3592x2048): 103.396
Elapsed time for attention_prob_times_values (32x2048x2048x3592): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3592): 91.348

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2819.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3593x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3593x2048): 101.857
Elapsed time for attention_prob_times_values (32x2048x2048x3593): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3593): 100.819

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2945.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3594x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3594x2048): 102.474
Elapsed time for attention_prob_times_values (32x2048x2048x3594): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3594): 103.237

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2990.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3595x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3595x2048): 102.006
Elapsed time for attention_prob_times_values (32x2048x2048x3595): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3595): 101.054

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2953.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3596x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3596x2048): 102.901
Elapsed time for attention_prob_times_values (32x2048x2048x3596): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3596): 103.366

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 3000.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3597x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3597x2048): 102.040
Elapsed time for attention_prob_times_values (32x2048x2048x3597): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3597): 101.019

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2954.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3598x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3598x2048): 102.598
Elapsed time for attention_prob_times_values (32x2048x2048x3598): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3598): 103.252

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2996.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3599x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3599x2048): 102.056
Elapsed time for attention_prob_times_values (32x2048x2048x3599): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3599): 101.272

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2960.125
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3600x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3600x2048): 103.743
Elapsed time for attention_prob_times_values (32x2048x2048x3600): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3600): 91.547

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2832.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3601x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3601x2048): 101.856
Elapsed time for attention_prob_times_values (32x2048x2048x3601): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3601): 101.308

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2959.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3602x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3602x2048): 102.391
Elapsed time for attention_prob_times_values (32x2048x2048x3602): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3602): 103.375

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2997.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3603x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3603x2048): 101.869
Elapsed time for attention_prob_times_values (32x2048x2048x3603): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3603): 101.410

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2962.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3604x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3604x2048): 102.743
Elapsed time for attention_prob_times_values (32x2048x2048x3604): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3604): 103.482

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3006.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3605x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3605x2048): 101.900
Elapsed time for attention_prob_times_values (32x2048x2048x3605): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3605): 101.108

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2960.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3606x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3606x2048): 102.481
Elapsed time for attention_prob_times_values (32x2048x2048x3606): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3606): 103.439

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3003.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3607x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3607x2048): 101.915
Elapsed time for attention_prob_times_values (32x2048x2048x3607): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3607): 101.434

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2966.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3608x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3608x2048): 103.121
Elapsed time for attention_prob_times_values (32x2048x2048x3608): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3608): 93.312

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2859.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3609x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3609x2048): 101.778
Elapsed time for attention_prob_times_values (32x2048x2048x3609): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3609): 101.407

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2966.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3610x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3610x2048): 102.300
Elapsed time for attention_prob_times_values (32x2048x2048x3610): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3610): 103.522

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3005.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3611x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3611x2048): 101.753
Elapsed time for attention_prob_times_values (32x2048x2048x3611): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3611): 101.544

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2969.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3612x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3612x2048): 102.711
Elapsed time for attention_prob_times_values (32x2048x2048x3612): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3612): 103.691

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3015.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3613x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3613x2048): 101.768
Elapsed time for attention_prob_times_values (32x2048x2048x3613): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3613): 101.166

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2965.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3614x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3614x2048): 102.373
Elapsed time for attention_prob_times_values (32x2048x2048x3614): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3614): 103.592

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3010.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3615x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3615x2048): 101.870
Elapsed time for attention_prob_times_values (32x2048x2048x3615): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3615): 101.522

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2973.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3616x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3616x2048): 107.611
Elapsed time for attention_prob_times_values (32x2048x2048x3616): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3616): 92.562

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2910.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3617x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3617x2048): 102.677
Elapsed time for attention_prob_times_values (32x2048x2048x3617): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3617): 101.741

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2990.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3618x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3618x2048): 103.187
Elapsed time for attention_prob_times_values (32x2048x2048x3618): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3618): 103.768

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3028.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3619x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3619x2048): 102.473
Elapsed time for attention_prob_times_values (32x2048x2048x3619): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3619): 101.792

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2989.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3620x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3620x2048): 103.448
Elapsed time for attention_prob_times_values (32x2048x2048x3620): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3620): 103.846

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3034.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3621x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3621x2048): 102.365
Elapsed time for attention_prob_times_values (32x2048x2048x3621): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3621): 101.648

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2987.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3622x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3622x2048): 102.918
Elapsed time for attention_prob_times_values (32x2048x2048x3622): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3622): 103.816

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3028.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3623x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3623x2048): 102.321
Elapsed time for attention_prob_times_values (32x2048x2048x3623): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3623): 101.742

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2989.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3624x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3624x2048): 103.486
Elapsed time for attention_prob_times_values (32x2048x2048x3624): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3624): 97.325

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 2940.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3625x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3625x2048): 101.979
Elapsed time for attention_prob_times_values (32x2048x2048x3625): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3625): 101.386

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2981.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3626x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3626x2048): 102.547
Elapsed time for attention_prob_times_values (32x2048x2048x3626): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3626): 103.912

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3027.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3627x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3627x2048): 102.142
Elapsed time for attention_prob_times_values (32x2048x2048x3627): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3627): 101.626

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2988.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3628x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3628x2048): 103.060
Elapsed time for attention_prob_times_values (32x2048x2048x3628): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3628): 104.052

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3038.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3629x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3629x2048): 102.170
Elapsed time for attention_prob_times_values (32x2048x2048x3629): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3629): 101.543

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2989.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3630x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3630x2048): 102.771
Elapsed time for attention_prob_times_values (32x2048x2048x3630): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3630): 103.945

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3034.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3631x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3631x2048): 102.198
Elapsed time for attention_prob_times_values (32x2048x2048x3631): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3631): 101.531

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2991.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3632x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3632x2048): 103.878
Elapsed time for attention_prob_times_values (32x2048x2048x3632): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3632): 98.083

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2963.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3633x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3633x2048): 102.000
Elapsed time for attention_prob_times_values (32x2048x2048x3633): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3633): 101.541

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2990.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3634x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3634x2048): 102.580
Elapsed time for attention_prob_times_values (32x2048x2048x3634): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3634): 104.102

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3037.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3635x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3635x2048): 102.002
Elapsed time for attention_prob_times_values (32x2048x2048x3635): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3635): 101.830

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2996.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3636x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3636x2048): 102.952
Elapsed time for attention_prob_times_values (32x2048x2048x3636): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3636): 104.215

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 3045.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3637x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3637x2048): 102.084
Elapsed time for attention_prob_times_values (32x2048x2048x3637): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3637): 101.605

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2995.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3638x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3638x2048): 102.675
Elapsed time for attention_prob_times_values (32x2048x2048x3638): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3638): 104.108

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3041.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3639x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3639x2048): 102.105
Elapsed time for attention_prob_times_values (32x2048x2048x3639): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3639): 101.586

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2997.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3640x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3640x2048): 103.330
Elapsed time for attention_prob_times_values (32x2048x2048x3640): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3640): 97.666

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2956.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3641x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3641x2048): 101.902
Elapsed time for attention_prob_times_values (32x2048x2048x3641): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3641): 101.674

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 2997.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3642x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3642x2048): 102.444
Elapsed time for attention_prob_times_values (32x2048x2048x3642): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3642): 104.193

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3042.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3643x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3643x2048): 101.940
Elapsed time for attention_prob_times_values (32x2048x2048x3643): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3643): 101.748

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3000.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3644x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3644x2048): 102.850
Elapsed time for attention_prob_times_values (32x2048x2048x3644): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3644): 104.423

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3053.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3645x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3645x2048): 101.948
Elapsed time for attention_prob_times_values (32x2048x2048x3645): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3645): 101.778

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3002.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3646x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3646x2048): 102.616
Elapsed time for attention_prob_times_values (32x2048x2048x3646): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3646): 103.938

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3044.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3647x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3647x2048): 102.016
Elapsed time for attention_prob_times_values (32x2048x2048x3647): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3647): 101.713

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3004.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3648x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3648x2048): 107.704
Elapsed time for attention_prob_times_values (32x2048x2048x3648): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3648): 99.284

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3048.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3649x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3649x2048): 102.831
Elapsed time for attention_prob_times_values (32x2048x2048x3649): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3649): 101.766

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3018.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3650x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3650x2048): 103.330
Elapsed time for attention_prob_times_values (32x2048x2048x3650): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3650): 104.467

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3066.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3651x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3651x2048): 102.710
Elapsed time for attention_prob_times_values (32x2048x2048x3651): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3651): 102.230

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3025.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3652x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3652x2048): 103.571
Elapsed time for attention_prob_times_values (32x2048x2048x3652): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3652): 104.426

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3071.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3653x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3653x2048): 102.508
Elapsed time for attention_prob_times_values (32x2048x2048x3653): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3653): 101.949

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3019.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3654x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3654x2048): 103.008
Elapsed time for attention_prob_times_values (32x2048x2048x3654): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3654): 104.338

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3063.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3655x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3655x2048): 102.411
Elapsed time for attention_prob_times_values (32x2048x2048x3655): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3655): 102.291

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3024.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3656x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3656x2048): 103.578
Elapsed time for attention_prob_times_values (32x2048x2048x3656): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3656): 98.021

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 2977.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3657x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3657x2048): 102.086
Elapsed time for attention_prob_times_values (32x2048x2048x3657): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3657): 102.217

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3020.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3658x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3658x2048): 102.676
Elapsed time for attention_prob_times_values (32x2048x2048x3658): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3658): 104.526

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3064.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3659x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3659x2048): 102.258
Elapsed time for attention_prob_times_values (32x2048x2048x3659): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3659): 102.167

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3024.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3660x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3660x2048): 103.123
Elapsed time for attention_prob_times_values (32x2048x2048x3660): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3660): 104.767

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3075.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3661x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3661x2048): 102.195
Elapsed time for attention_prob_times_values (32x2048x2048x3661): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3661): 102.104

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3023.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3662x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3662x2048): 102.765
Elapsed time for attention_prob_times_values (32x2048x2048x3662): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3662): 104.627

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3070.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3663x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3663x2048): 102.154
Elapsed time for attention_prob_times_values (32x2048x2048x3663): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3663): 102.177

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3025.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3664x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3664x2048): 103.831
Elapsed time for attention_prob_times_values (32x2048x2048x3664): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3664): 99.080

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3003.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3665x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3665x2048): 101.983
Elapsed time for attention_prob_times_values (32x2048x2048x3665): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3665): 102.347

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3027.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3666x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3666x2048): 102.652
Elapsed time for attention_prob_times_values (32x2048x2048x3666): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3666): 104.651

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3072.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3667x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3667x2048): 102.013
Elapsed time for attention_prob_times_values (32x2048x2048x3667): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3667): 102.568

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3032.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3668x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3668x2048): 103.006
Elapsed time for attention_prob_times_values (32x2048x2048x3668): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3668): 104.964

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3083.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3669x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3669x2048): 102.001
Elapsed time for attention_prob_times_values (32x2048x2048x3669): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3669): 102.487

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3032.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3670x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3670x2048): 102.553
Elapsed time for attention_prob_times_values (32x2048x2048x3670): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3670): 104.763

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3075.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3671x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3671x2048): 102.076
Elapsed time for attention_prob_times_values (32x2048x2048x3671): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3671): 102.294

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3032.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3672x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3672x2048): 103.334
Elapsed time for attention_prob_times_values (32x2048x2048x3672): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3672): 98.300

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 2991.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3673x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3673x2048): 101.875
Elapsed time for attention_prob_times_values (32x2048x2048x3673): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3673): 102.695

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3037.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3674x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3674x2048): 102.456
Elapsed time for attention_prob_times_values (32x2048x2048x3674): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3674): 104.771

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3077.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3675x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3675x2048): 101.868
Elapsed time for attention_prob_times_values (32x2048x2048x3675): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3675): 102.692

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3038.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3676x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3676x2048): 102.845
Elapsed time for attention_prob_times_values (32x2048x2048x3676): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3676): 104.870

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3086.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3677x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3677x2048): 101.889
Elapsed time for attention_prob_times_values (32x2048x2048x3677): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3677): 102.690

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3040.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3678x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3678x2048): 102.476
Elapsed time for attention_prob_times_values (32x2048x2048x3678): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3678): 105.020

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3084.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3679x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3679x2048): 101.954
Elapsed time for attention_prob_times_values (32x2048x2048x3679): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3679): 102.646

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3042.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3680x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3680x2048): 107.627
Elapsed time for attention_prob_times_values (32x2048x2048x3680): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3680): 99.482

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3075.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3681x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3681x2048): 102.771
Elapsed time for attention_prob_times_values (32x2048x2048x3681): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3681): 102.896

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3060.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3682x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3682x2048): 103.272
Elapsed time for attention_prob_times_values (32x2048x2048x3682): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3682): 105.031

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3099.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3683x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3683x2048): 102.610
Elapsed time for attention_prob_times_values (32x2048x2048x3683): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3683): 102.646

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3055.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3684x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3684x2048): 103.506
Elapsed time for attention_prob_times_values (32x2048x2048x3684): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3684): 105.403

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 3110.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3685x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3685x2048): 102.403
Elapsed time for attention_prob_times_values (32x2048x2048x3685): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3685): 103.015

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3059.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3686x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3686x2048): 102.933
Elapsed time for attention_prob_times_values (32x2048x2048x3686): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3686): 104.943

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3096.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3687x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3687x2048): 102.334
Elapsed time for attention_prob_times_values (32x2048x2048x3687): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3687): 102.992

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3059.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3688x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3688x2048): 103.531
Elapsed time for attention_prob_times_values (32x2048x2048x3688): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3688): 98.959

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3016.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3689x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3689x2048): 102.065
Elapsed time for attention_prob_times_values (32x2048x2048x3689): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3689): 102.907

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3056.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3690x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3690x2048): 102.643
Elapsed time for attention_prob_times_values (32x2048x2048x3690): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3690): 104.668

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3091.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3691x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3691x2048): 102.136
Elapsed time for attention_prob_times_values (32x2048x2048x3691): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3691): 102.968

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3059.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3692x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3692x2048): 103.055
Elapsed time for attention_prob_times_values (32x2048x2048x3692): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3692): 105.268

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 3108.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3693x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3693x2048): 102.183
Elapsed time for attention_prob_times_values (32x2048x2048x3693): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3693): 102.996

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3062.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3694x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3694x2048): 102.805
Elapsed time for attention_prob_times_values (32x2048x2048x3694): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3694): 105.163

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3104.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3695x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3695x2048): 102.219
Elapsed time for attention_prob_times_values (32x2048x2048x3695): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3695): 103.008

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3064.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3696x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3696x2048): 103.947
Elapsed time for attention_prob_times_values (32x2048x2048x3696): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3696): 99.744

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3041.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3697x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3697x2048): 102.068
Elapsed time for attention_prob_times_values (32x2048x2048x3697): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3697): 102.906

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3062.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3698x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3698x2048): 102.569
Elapsed time for attention_prob_times_values (32x2048x2048x3698): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3698): 105.043

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3102.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3699x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3699x2048): 102.054
Elapsed time for attention_prob_times_values (32x2048x2048x3699): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3699): 102.964

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3064.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3700x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3700x2048): 102.944
Elapsed time for attention_prob_times_values (32x2048x2048x3700): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3700): 105.368

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3114.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3701x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3701x2048): 102.091
Elapsed time for attention_prob_times_values (32x2048x2048x3701): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3701): 102.596

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3061.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3702x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3702x2048): 102.710
Elapsed time for attention_prob_times_values (32x2048x2048x3702): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3702): 104.947

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3106.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3703x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3703x2048): 102.111
Elapsed time for attention_prob_times_values (32x2048x2048x3703): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3703): 101.805

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3051.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3704x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3704x2048): 103.341
Elapsed time for attention_prob_times_values (32x2048x2048x3704): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3704): 98.950

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3026.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3705x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3705x2048): 101.909
Elapsed time for attention_prob_times_values (32x2048x2048x3705): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3705): 102.778

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3064.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3706x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3706x2048): 102.566
Elapsed time for attention_prob_times_values (32x2048x2048x3706): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3706): 105.470

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3115.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3707x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3707x2048): 101.967
Elapsed time for attention_prob_times_values (32x2048x2048x3707): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3707): 103.019

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3070.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3708x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3708x2048): 102.953
Elapsed time for attention_prob_times_values (32x2048x2048x3708): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3708): 105.657

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3125.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3709x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3709x2048): 102.009
Elapsed time for attention_prob_times_values (32x2048x2048x3709): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3709): 102.988

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3072.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3710x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3710x2048): 102.621
Elapsed time for attention_prob_times_values (32x2048x2048x3710): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3710): 105.312

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3116.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3711x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3711x2048): 101.928
Elapsed time for attention_prob_times_values (32x2048x2048x3711): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3711): 101.952

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3057.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3712x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3712x2048): 107.598
Elapsed time for attention_prob_times_values (32x2048x2048x3712): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3712): 101.445

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 3132.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3713x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3713x2048): 102.914
Elapsed time for attention_prob_times_values (32x2048x2048x3713): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3713): 101.300

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3063.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3714x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3714x2048): 103.479
Elapsed time for attention_prob_times_values (32x2048x2048x3714): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3714): 103.525

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3106.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3715x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3715x2048): 102.855
Elapsed time for attention_prob_times_values (32x2048x2048x3715): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3715): 101.631

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3069.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3716x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3716x2048): 103.722
Elapsed time for attention_prob_times_values (32x2048x2048x3716): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3716): 103.637

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3113.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3717x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3717x2048): 102.711
Elapsed time for attention_prob_times_values (32x2048x2048x3717): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3717): 101.538

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3067.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3718x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3718x2048): 103.218
Elapsed time for attention_prob_times_values (32x2048x2048x3718): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3718): 103.593

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3106.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3719x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3719x2048): 102.492
Elapsed time for attention_prob_times_values (32x2048x2048x3719): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3719): 101.780

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3069.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3720x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3720x2048): 103.746
Elapsed time for attention_prob_times_values (32x2048x2048x3720): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3720): 99.632

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3055.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3721x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3721x2048): 102.080
Elapsed time for attention_prob_times_values (32x2048x2048x3721): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3721): 101.713

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3064.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3722x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3722x2048): 102.905
Elapsed time for attention_prob_times_values (32x2048x2048x3722): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3722): 103.779

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3108.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3723x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3723x2048): 102.217
Elapsed time for attention_prob_times_values (32x2048x2048x3723): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3723): 101.867

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3070.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3724x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3724x2048): 103.265
Elapsed time for attention_prob_times_values (32x2048x2048x3724): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3724): 103.853

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3116.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3725x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3725x2048): 102.133
Elapsed time for attention_prob_times_values (32x2048x2048x3725): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3725): 101.897

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3070.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3726x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3726x2048): 103.011
Elapsed time for attention_prob_times_values (32x2048x2048x3726): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3726): 103.801

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3113.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3727x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3727x2048): 102.230
Elapsed time for attention_prob_times_values (32x2048x2048x3727): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3727): 102.016

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3075.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3728x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3728x2048): 104.158
Elapsed time for attention_prob_times_values (32x2048x2048x3728): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3728): 100.639

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3083.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3729x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3729x2048): 102.011
Elapsed time for attention_prob_times_values (32x2048x2048x3729): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3729): 102.004

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3073.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3730x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3730x2048): 102.763
Elapsed time for attention_prob_times_values (32x2048x2048x3730): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3730): 103.830

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3113.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3731x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3731x2048): 101.955
Elapsed time for attention_prob_times_values (32x2048x2048x3731): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3731): 101.992

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3074.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3732x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3732x2048): 103.100
Elapsed time for attention_prob_times_values (32x2048x2048x3732): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3732): 103.919

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3121.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3733x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3733x2048): 102.019
Elapsed time for attention_prob_times_values (32x2048x2048x3733): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3733): 101.982

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3076.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3734x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3734x2048): 102.747
Elapsed time for attention_prob_times_values (32x2048x2048x3734): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3734): 103.821

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3116.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3735x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3735x2048): 102.022
Elapsed time for attention_prob_times_values (32x2048x2048x3735): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3735): 102.101

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3080.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3736x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3736x2048): 103.508
Elapsed time for attention_prob_times_values (32x2048x2048x3736): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3736): 100.056

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3071.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3737x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3737x2048): 101.842
Elapsed time for attention_prob_times_values (32x2048x2048x3737): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3737): 102.190

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3080.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3738x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3738x2048): 102.432
Elapsed time for attention_prob_times_values (32x2048x2048x3738): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3738): 103.947

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3116.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3739x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3739x2048): 101.869
Elapsed time for attention_prob_times_values (32x2048x2048x3739): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3739): 102.176

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3082.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3740x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3740x2048): 102.958
Elapsed time for attention_prob_times_values (32x2048x2048x3740): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3740): 104.082

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3128.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3741x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3741x2048): 101.914
Elapsed time for attention_prob_times_values (32x2048x2048x3741): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3741): 102.265

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3085.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3742x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3742x2048): 102.501
Elapsed time for attention_prob_times_values (32x2048x2048x3742): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3742): 104.028

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3121.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3743x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3743x2048): 101.929
Elapsed time for attention_prob_times_values (32x2048x2048x3743): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3743): 102.364

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3089.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3744x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3744x2048): 107.744
Elapsed time for attention_prob_times_values (32x2048x2048x3744): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3744): 101.324

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 3159.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3745x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3745x2048): 102.709
Elapsed time for attention_prob_times_values (32x2048x2048x3745): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3745): 102.307

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3101.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3746x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3746x2048): 103.287
Elapsed time for attention_prob_times_values (32x2048x2048x3746): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3746): 104.044

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3137.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3747x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3747x2048): 102.529
Elapsed time for attention_prob_times_values (32x2048x2048x3747): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3747): 102.302

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3100.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3748x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3748x2048): 103.535
Elapsed time for attention_prob_times_values (32x2048x2048x3748): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3748): 104.143

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3144.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 29992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3749x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3749x2048): 102.382
Elapsed time for attention_prob_times_values (32x2048x2048x3749): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3749): 102.226

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3098.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3750x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3750x2048): 102.923
Elapsed time for attention_prob_times_values (32x2048x2048x3750): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3750): 104.119

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3136.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3751x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3751x2048): 102.300
Elapsed time for attention_prob_times_values (32x2048x2048x3751): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3751): 102.228

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3099.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3752x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3752x2048): 103.546
Elapsed time for attention_prob_times_values (32x2048x2048x3752): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3752): 100.494

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3091.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3753x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3753x2048): 102.001
Elapsed time for attention_prob_times_values (32x2048x2048x3753): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3753): 102.312

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3097.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3754x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3754x2048): 102.568
Elapsed time for attention_prob_times_values (32x2048x2048x3754): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3754): 104.153

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3134.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3755x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3755x2048): 102.147
Elapsed time for attention_prob_times_values (32x2048x2048x3755): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3755): 102.389

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3102.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3756x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3756x2048): 103.045
Elapsed time for attention_prob_times_values (32x2048x2048x3756): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3756): 104.306

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3145.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3757x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3757x2048): 102.135
Elapsed time for attention_prob_times_values (32x2048x2048x3757): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3757): 102.397

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3103.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3758x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3758x2048): 102.712
Elapsed time for attention_prob_times_values (32x2048x2048x3758): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3758): 104.274

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3141.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3759x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3759x2048): 102.216
Elapsed time for attention_prob_times_values (32x2048x2048x3759): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3759): 102.464

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3107.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3760x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3760x2048): 104.002
Elapsed time for attention_prob_times_values (32x2048x2048x3760): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3760): 101.282

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3117.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3761x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3761x2048): 102.002
Elapsed time for attention_prob_times_values (32x2048x2048x3761): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3761): 102.496

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3106.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3762x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3762x2048): 102.491
Elapsed time for attention_prob_times_values (32x2048x2048x3762): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3762): 104.401

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3143.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3763x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3763x2048): 101.987
Elapsed time for attention_prob_times_values (32x2048x2048x3763): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3763): 102.503

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3108.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3764x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3764x2048): 102.919
Elapsed time for attention_prob_times_values (32x2048x2048x3764): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3764): 104.405

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3151.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3765x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3765x2048): 102.039
Elapsed time for attention_prob_times_values (32x2048x2048x3765): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3765): 102.510

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3110.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3766x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3766x2048): 102.587
Elapsed time for attention_prob_times_values (32x2048x2048x3766): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3766): 104.434

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3148.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3767x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3767x2048): 102.088
Elapsed time for attention_prob_times_values (32x2048x2048x3767): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3767): 102.553

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3113.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3768x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3768x2048): 103.287
Elapsed time for attention_prob_times_values (32x2048x2048x3768): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3768): 100.964

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3108.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3769x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3769x2048): 101.885
Elapsed time for attention_prob_times_values (32x2048x2048x3769): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3769): 102.565

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3112.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3770x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3770x2048): 102.441
Elapsed time for attention_prob_times_values (32x2048x2048x3770): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3770): 104.507

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3150.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3771x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3771x2048): 101.959
Elapsed time for attention_prob_times_values (32x2048x2048x3771): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3771): 102.624

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3115.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3772x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3772x2048): 102.858
Elapsed time for attention_prob_times_values (32x2048x2048x3772): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3772): 104.627

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3160.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3773x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3773x2048): 101.983
Elapsed time for attention_prob_times_values (32x2048x2048x3773): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3773): 102.619

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3117.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3774x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3774x2048): 102.584
Elapsed time for attention_prob_times_values (32x2048x2048x3774): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3774): 104.646

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3158.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3775x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3775x2048): 102.131
Elapsed time for attention_prob_times_values (32x2048x2048x3775): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3775): 102.688

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3122.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3776x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3776x2048): 107.780
Elapsed time for attention_prob_times_values (32x2048x2048x3776): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3776): 102.778

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 3209.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3777x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3777x2048): 102.840
Elapsed time for attention_prob_times_values (32x2048x2048x3777): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3777): 102.758

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3136.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3778x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3778x2048): 103.374
Elapsed time for attention_prob_times_values (32x2048x2048x3778): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3778): 104.814

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3176.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3779x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3779x2048): 102.725
Elapsed time for attention_prob_times_values (32x2048x2048x3779): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3779): 102.934

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3138.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3780x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3780x2048): 103.605
Elapsed time for attention_prob_times_values (32x2048x2048x3780): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3780): 104.663

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3179.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3781x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3781x2048): 102.547
Elapsed time for attention_prob_times_values (32x2048x2048x3781): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3781): 102.931

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3137.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3782x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3782x2048): 103.108
Elapsed time for attention_prob_times_values (32x2048x2048x3782): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3782): 104.860

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3176.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3783x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3783x2048): 102.488
Elapsed time for attention_prob_times_values (32x2048x2048x3783): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3783): 102.991

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3139.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3784x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3784x2048): 103.576
Elapsed time for attention_prob_times_values (32x2048x2048x3784): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3784): 101.171

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3128.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3785x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3785x2048): 102.168
Elapsed time for attention_prob_times_values (32x2048x2048x3785): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3785): 103.140

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3138.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3786x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3786x2048): 102.843
Elapsed time for attention_prob_times_values (32x2048x2048x3786): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3786): 105.036

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3177.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3787x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3787x2048): 102.362
Elapsed time for attention_prob_times_values (32x2048x2048x3787): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3787): 103.202

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3143.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3788x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3788x2048): 103.249
Elapsed time for attention_prob_times_values (32x2048x2048x3788): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3788): 105.187

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3188.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3789x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3789x2048): 102.374
Elapsed time for attention_prob_times_values (32x2048x2048x3789): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3789): 103.314

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3147.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3790x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3790x2048): 102.977
Elapsed time for attention_prob_times_values (32x2048x2048x3790): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3790): 105.134

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3184.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3791x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3791x2048): 102.423
Elapsed time for attention_prob_times_values (32x2048x2048x3791): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3791): 103.401

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3150.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3792x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3792x2048): 104.051
Elapsed time for attention_prob_times_values (32x2048x2048x3792): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3792): 102.181

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3157.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3793x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3793x2048): 102.217
Elapsed time for attention_prob_times_values (32x2048x2048x3793): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3793): 103.581

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3151.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3794x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3794x2048): 102.748
Elapsed time for attention_prob_times_values (32x2048x2048x3794): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3794): 105.257

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3186.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3795x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3795x2048): 102.222
Elapsed time for attention_prob_times_values (32x2048x2048x3795): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3795): 103.540

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3153.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3796x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3796x2048): 103.175
Elapsed time for attention_prob_times_values (32x2048x2048x3796): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3796): 105.430

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3197.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3797x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3797x2048): 102.279
Elapsed time for attention_prob_times_values (32x2048x2048x3797): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3797): 102.416

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3138.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3798x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3798x2048): 102.879
Elapsed time for attention_prob_times_values (32x2048x2048x3798): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3798): 105.428

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3194.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3799x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3799x2048): 102.298
Elapsed time for attention_prob_times_values (32x2048x2048x3799): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3799): 103.702

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3159.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3800x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3800x2048): 103.446
Elapsed time for attention_prob_times_values (32x2048x2048x3800): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3800): 101.802

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3149.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3801x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3801x2048): 102.144
Elapsed time for attention_prob_times_values (32x2048x2048x3801): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3801): 103.749

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3159.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3802x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3802x2048): 102.740
Elapsed time for attention_prob_times_values (32x2048x2048x3802): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3802): 105.542

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3196.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3803x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3803x2048): 102.110
Elapsed time for attention_prob_times_values (32x2048x2048x3803): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3803): 103.801

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3161.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3804x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3804x2048): 103.027
Elapsed time for attention_prob_times_values (32x2048x2048x3804): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3804): 105.577

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3203.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3805x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3805x2048): 102.191
Elapsed time for attention_prob_times_values (32x2048x2048x3805): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3805): 103.944

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3166.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3806x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3806x2048): 102.784
Elapsed time for attention_prob_times_values (32x2048x2048x3806): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3806): 105.365

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3198.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3807x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3807x2048): 102.237
Elapsed time for attention_prob_times_values (32x2048x2048x3807): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3807): 103.953

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3169.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3808x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3808x2048): 107.925
Elapsed time for attention_prob_times_values (32x2048x2048x3808): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3808): 103.252

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 3245.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3809x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3809x2048): 102.949
Elapsed time for attention_prob_times_values (32x2048x2048x3809): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3809): 103.167

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3169.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3810x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3810x2048): 103.468
Elapsed time for attention_prob_times_values (32x2048x2048x3810): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3810): 105.514

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3214.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3811x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3811x2048): 102.830
Elapsed time for attention_prob_times_values (32x2048x2048x3811): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3811): 103.962

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3181.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3812x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3812x2048): 103.672
Elapsed time for attention_prob_times_values (32x2048x2048x3812): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3812): 104.940

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 3210.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3813x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3813x2048): 102.589
Elapsed time for attention_prob_times_values (32x2048x2048x3813): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3813): 103.951

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3179.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3814x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3814x2048): 103.146
Elapsed time for attention_prob_times_values (32x2048x2048x3814): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3814): 104.276

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3193.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3815x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3815x2048): 102.489
Elapsed time for attention_prob_times_values (32x2048x2048x3815): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3815): 102.759

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3161.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3816x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3816x2048): 103.631
Elapsed time for attention_prob_times_values (32x2048x2048x3816): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3816): 102.063

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3168.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3817x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3817x2048): 102.165
Elapsed time for attention_prob_times_values (32x2048x2048x3817): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3817): 102.516

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3154.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3818x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3818x2048): 102.780
Elapsed time for attention_prob_times_values (32x2048x2048x3818): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3818): 104.611

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3196.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3819x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3819x2048): 102.289
Elapsed time for attention_prob_times_values (32x2048x2048x3819): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3819): 102.950

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3164.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3820x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3820x2048): 103.189
Elapsed time for attention_prob_times_values (32x2048x2048x3820): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3820): 104.554

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 3203.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3821x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3821x2048): 102.284
Elapsed time for attention_prob_times_values (32x2048x2048x3821): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3821): 102.355

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3156.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3822x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3822x2048): 102.915
Elapsed time for attention_prob_times_values (32x2048x2048x3822): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3822): 104.375

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3198.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3823x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3823x2048): 102.277
Elapsed time for attention_prob_times_values (32x2048x2048x3823): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3823): 102.525

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3160.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3824x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3824x2048): 103.976
Elapsed time for attention_prob_times_values (32x2048x2048x3824): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3824): 103.092

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3196.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3825x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3825x2048): 102.088
Elapsed time for attention_prob_times_values (32x2048x2048x3825): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3825): 102.301

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3156.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3826x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3826x2048): 102.674
Elapsed time for attention_prob_times_values (32x2048x2048x3826): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3826): 104.305

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3196.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3827x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3827x2048): 102.066
Elapsed time for attention_prob_times_values (32x2048x2048x3827): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3827): 102.194

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3155.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3828x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3828x2048): 102.963
Elapsed time for attention_prob_times_values (32x2048x2048x3828): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3828): 104.319

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3203.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3829x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3829x2048): 102.061
Elapsed time for attention_prob_times_values (32x2048x2048x3829): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3829): 102.403

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3160.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3830x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3830x2048): 102.679
Elapsed time for attention_prob_times_values (32x2048x2048x3830): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3830): 103.968

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3194.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3831x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3831x2048): 102.115
Elapsed time for attention_prob_times_values (32x2048x2048x3831): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3831): 102.301

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3161.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3832x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3832x2048): 103.325
Elapsed time for attention_prob_times_values (32x2048x2048x3832): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3832): 102.508

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3183.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3833x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3833x2048): 101.936
Elapsed time for attention_prob_times_values (32x2048x2048x3833): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3833): 101.658

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3150.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3834x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3834x2048): 102.532
Elapsed time for attention_prob_times_values (32x2048x2048x3834): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3834): 104.384

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3202.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3835x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3835x2048): 101.970
Elapsed time for attention_prob_times_values (32x2048x2048x3835): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3835): 102.372

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3163.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3836x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3836x2048): 102.972
Elapsed time for attention_prob_times_values (32x2048x2048x3836): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3836): 104.621

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 3214.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3837x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3837x2048): 102.016
Elapsed time for attention_prob_times_values (32x2048x2048x3837): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3837): 102.096

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 3161.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3838x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3838x2048): 102.624
Elapsed time for attention_prob_times_values (32x2048x2048x3838): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3838): 104.289

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 3205.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3839x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3839x2048): 102.086
Elapsed time for attention_prob_times_values (32x2048x2048x3839): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3839): 102.593

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3171.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3840x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3840x2048): 107.522
Elapsed time for attention_prob_times_values (32x2048x2048x3840): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3840): 104.298

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 3282.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3841x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3841x2048): 102.827
Elapsed time for attention_prob_times_values (32x2048x2048x3841): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3841): 100.647

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3154.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3842x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3842x2048): 103.397
Elapsed time for attention_prob_times_values (32x2048x2048x3842): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3842): 102.841

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3198.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3843x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3843x2048): 102.743
Elapsed time for attention_prob_times_values (32x2048x2048x3843): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3843): 100.913

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3158.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3844x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3844x2048): 103.644
Elapsed time for attention_prob_times_values (32x2048x2048x3844): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3844): 102.995

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3206.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3845x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3845x2048): 102.549
Elapsed time for attention_prob_times_values (32x2048x2048x3845): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3845): 100.822

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3155.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3846x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3846x2048): 103.117
Elapsed time for attention_prob_times_values (32x2048x2048x3846): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3846): 102.804

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3196.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3847x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3847x2048): 102.500
Elapsed time for attention_prob_times_values (32x2048x2048x3847): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3847): 101.008

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3159.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3848x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3848x2048): 103.647
Elapsed time for attention_prob_times_values (32x2048x2048x3848): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3848): 97.291

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 3117.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3849x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3849x2048): 102.210
Elapsed time for attention_prob_times_values (32x2048x2048x3849): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3849): 101.049

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3157.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3850x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3850x2048): 102.816
Elapsed time for attention_prob_times_values (32x2048x2048x3850): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3850): 102.972

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3197.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3851x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3851x2048): 102.326
Elapsed time for attention_prob_times_values (32x2048x2048x3851): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3851): 101.007

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3160.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3852x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3852x2048): 103.229
Elapsed time for attention_prob_times_values (32x2048x2048x3852): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3852): 103.116

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 3208.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3853x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3853x2048): 102.371
Elapsed time for attention_prob_times_values (32x2048x2048x3853): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3853): 101.086

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3163.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3854x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3854x2048): 103.025
Elapsed time for attention_prob_times_values (32x2048x2048x3854): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3854): 103.074

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3205.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3855x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3855x2048): 102.422
Elapsed time for attention_prob_times_values (32x2048x2048x3855): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3855): 101.327

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3169.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3856x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3856x2048): 104.107
Elapsed time for attention_prob_times_values (32x2048x2048x3856): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3856): 98.008

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 3142.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3857x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3857x2048): 102.229
Elapsed time for attention_prob_times_values (32x2048x2048x3857): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3857): 101.391

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 3169.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3858x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3858x2048): 102.817
Elapsed time for attention_prob_times_values (32x2048x2048x3858): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (32x2048x2048x3858): 103.140

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 3206.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 8, hidden_size: 30872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (32x2048x3859x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (32x2048x3859x2048): 102.212
slurmstepd: error: *** JOB 1507285 ON frontier09088 CANCELLED AT 2023-11-22T21:55:39 DUE TO TIME LIMIT ***
