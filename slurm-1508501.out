
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-25 00:30:33,296] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 16, hidden_size: 16, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1x2048): 0.988
Elapsed time for attention_prob_times_values (64x2048x2048x1): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1): 0.205

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 0.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 32, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x2x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x2x2048): 1.809
Elapsed time for attention_prob_times_values (64x2048x2048x2): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x2): 1.733

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 1.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 48, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x3x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x3x2048): 2.548
Elapsed time for attention_prob_times_values (64x2048x2048x3): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x3): 2.372

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 2.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x4x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x4x2048): 3.283
Elapsed time for attention_prob_times_values (64x2048x2048x4): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x4): 3.009

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 3.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 80, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x5x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x5x2048): 4.063
Elapsed time for attention_prob_times_values (64x2048x2048x5): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x5): 3.014

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 3.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x6x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x6x2048): 4.839
Elapsed time for attention_prob_times_values (64x2048x2048x6): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x6): 3.553

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 4.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x7x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x7x2048): 5.642
Elapsed time for attention_prob_times_values (64x2048x2048x7): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x7): 4.038

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 5.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x8x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x8x2048): 6.471
Elapsed time for attention_prob_times_values (64x2048x2048x8): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x8): 6.844

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 7.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x9x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x9x2048): 6.945
Elapsed time for attention_prob_times_values (64x2048x2048x9): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x9): 7.367

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 8.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x10x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x10x2048): 7.683
Elapsed time for attention_prob_times_values (64x2048x2048x10): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x10): 8.427

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 9.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x11x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x11x2048): 8.422
Elapsed time for attention_prob_times_values (64x2048x2048x11): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x11): 8.968

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 10.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x12x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x12x2048): 9.163
Elapsed time for attention_prob_times_values (64x2048x2048x12): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x12): 10.038

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 11.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x13x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x13x2048): 9.886
Elapsed time for attention_prob_times_values (64x2048x2048x13): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x13): 10.511

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 12.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x14x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x14x2048): 10.647
Elapsed time for attention_prob_times_values (64x2048x2048x14): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x14): 11.642

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 13.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x15x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x15x2048): 11.407
Elapsed time for attention_prob_times_values (64x2048x2048x15): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x15): 12.055

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 14.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x16x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x16x2048): 12.217
Elapsed time for attention_prob_times_values (64x2048x2048x16): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x16): 13.414

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 15.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x17x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x17x2048): 12.522
Elapsed time for attention_prob_times_values (64x2048x2048x17): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x17): 13.503

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 16.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x18x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x18x2048): 13.242
Elapsed time for attention_prob_times_values (64x2048x2048x18): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x18): 14.648

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 17.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x19x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x19x2048): 13.895
Elapsed time for attention_prob_times_values (64x2048x2048x19): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x19): 15.102

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 18.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x20x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x20x2048): 14.627
Elapsed time for attention_prob_times_values (64x2048x2048x20): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x20): 16.212

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 20.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x21x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x21x2048): 15.328
Elapsed time for attention_prob_times_values (64x2048x2048x21): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x21): 16.604

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 21.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x22x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x22x2048): 16.063
Elapsed time for attention_prob_times_values (64x2048x2048x22): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x22): 17.712

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 22.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x23x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x23x2048): 16.772
Elapsed time for attention_prob_times_values (64x2048x2048x23): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x23): 18.027

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 23.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x24x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x24x2048): 17.473
Elapsed time for attention_prob_times_values (64x2048x2048x24): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x24): 19.645

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 25.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x25x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x25x2048): 17.759
Elapsed time for attention_prob_times_values (64x2048x2048x25): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x25): 19.533

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 25.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x26x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x26x2048): 18.452
Elapsed time for attention_prob_times_values (64x2048x2048x26): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x26): 20.630

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 27.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x27x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x27x2048): 19.101
Elapsed time for attention_prob_times_values (64x2048x2048x27): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x27): 20.943

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 28.409
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x28x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x28x2048): 19.716
Elapsed time for attention_prob_times_values (64x2048x2048x28): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x28): 22.185

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 30.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x29x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x29x2048): 20.375
Elapsed time for attention_prob_times_values (64x2048x2048x29): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x29): 22.333

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 30.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x30x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x30x2048): 21.130
Elapsed time for attention_prob_times_values (64x2048x2048x30): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x30): 23.596

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 32.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x31x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x31x2048): 21.767
Elapsed time for attention_prob_times_values (64x2048x2048x31): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x31): 23.833

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 33.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x32x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x32x2048): 38.390
Elapsed time for attention_prob_times_values (64x2048x2048x32): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x32): 25.643

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 46.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x33x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x33x2048): 25.414
Elapsed time for attention_prob_times_values (64x2048x2048x33): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x33): 25.293

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 38.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x34x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x34x2048): 25.657
Elapsed time for attention_prob_times_values (64x2048x2048x34): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x34): 26.364

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 39.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x35x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x35x2048): 24.745
Elapsed time for attention_prob_times_values (64x2048x2048x35): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x35): 26.871

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 39.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x36x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x36x2048): 25.254
Elapsed time for attention_prob_times_values (64x2048x2048x36): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x36): 27.876

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 41.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x37x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x37x2048): 25.286
Elapsed time for attention_prob_times_values (64x2048x2048x37): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x37): 28.205

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 42.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x38x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x38x2048): 26.080
Elapsed time for attention_prob_times_values (64x2048x2048x38): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x38): 29.294

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 43.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x39x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x39x2048): 26.333
Elapsed time for attention_prob_times_values (64x2048x2048x39): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x39): 29.599

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 44.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x40x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x40x2048): 27.245
Elapsed time for attention_prob_times_values (64x2048x2048x40): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x40): 31.516

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 47.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x41x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x41x2048): 26.909
Elapsed time for attention_prob_times_values (64x2048x2048x41): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x41): 30.530

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 46.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x42x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x42x2048): 27.702
Elapsed time for attention_prob_times_values (64x2048x2048x42): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x42): 31.801

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 49.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x43x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x43x2048): 28.147
Elapsed time for attention_prob_times_values (64x2048x2048x43): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x43): 32.009

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 50.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x44x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x44x2048): 29.047
Elapsed time for attention_prob_times_values (64x2048x2048x44): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x44): 33.360

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 52.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x45x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x45x2048): 29.457
Elapsed time for attention_prob_times_values (64x2048x2048x45): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x45): 33.136

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 53.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x46x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x46x2048): 30.358
Elapsed time for attention_prob_times_values (64x2048x2048x46): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x46): 34.724

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 55.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x47x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x47x2048): 30.793
Elapsed time for attention_prob_times_values (64x2048x2048x47): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x47): 34.474

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 56.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x48x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x48x2048): 31.837
Elapsed time for attention_prob_times_values (64x2048x2048x48): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x48): 37.297

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 60.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x49x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x49x2048): 32.112
Elapsed time for attention_prob_times_values (64x2048x2048x49): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x49): 35.847

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 59.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x50x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x50x2048): 32.844
Elapsed time for attention_prob_times_values (64x2048x2048x50): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x50): 36.975

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 61.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x51x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x51x2048): 33.384
Elapsed time for attention_prob_times_values (64x2048x2048x51): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x51): 37.142

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 63.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x52x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x52x2048): 34.088
Elapsed time for attention_prob_times_values (64x2048x2048x52): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x52): 38.734

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 65.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x53x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x53x2048): 34.724
Elapsed time for attention_prob_times_values (64x2048x2048x53): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x53): 38.548

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 66.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x54x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x54x2048): 35.441
Elapsed time for attention_prob_times_values (64x2048x2048x54): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x54): 40.171

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 69.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x55x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x55x2048): 36.055
Elapsed time for attention_prob_times_values (64x2048x2048x55): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x55): 39.594

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 70.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x56x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x56x2048): 36.818
Elapsed time for attention_prob_times_values (64x2048x2048x56): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x56): 42.964

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 74.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x57x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x57x2048): 36.268
Elapsed time for attention_prob_times_values (64x2048x2048x57): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x57): 41.218

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 72.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x58x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x58x2048): 36.915
Elapsed time for attention_prob_times_values (64x2048x2048x58): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x58): 42.584

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 75.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x59x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x59x2048): 37.330
Elapsed time for attention_prob_times_values (64x2048x2048x59): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x59): 42.460

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 76.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x60x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x60x2048): 38.069
Elapsed time for attention_prob_times_values (64x2048x2048x60): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x60): 44.153

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 79.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x61x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x61x2048): 38.487
Elapsed time for attention_prob_times_values (64x2048x2048x61): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x61): 43.857

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 80.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x62x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x62x2048): 39.107
Elapsed time for attention_prob_times_values (64x2048x2048x62): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x62): 45.240

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 82.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x63x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x63x2048): 39.516
Elapsed time for attention_prob_times_values (64x2048x2048x63): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x63): 44.710

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 83.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x64x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x64x2048): 54.322
Elapsed time for attention_prob_times_values (64x2048x2048x64): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x64): 48.886

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 102.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x65x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x65x2048): 41.546
Elapsed time for attention_prob_times_values (64x2048x2048x65): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x65): 32.218

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 73.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x66x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x66x2048): 42.390
Elapsed time for attention_prob_times_values (64x2048x2048x66): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x66): 33.473

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 75.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x67x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x67x2048): 42.143
Elapsed time for attention_prob_times_values (64x2048x2048x67): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x67): 33.086

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 75.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x68x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x68x2048): 43.157
Elapsed time for attention_prob_times_values (64x2048x2048x68): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x68): 34.425

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 78.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x69x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x69x2048): 42.732
Elapsed time for attention_prob_times_values (64x2048x2048x69): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x69): 33.976

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 78.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x70x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x70x2048): 43.712
Elapsed time for attention_prob_times_values (64x2048x2048x70): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x70): 35.422

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 81.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x71x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x71x2048): 43.599
Elapsed time for attention_prob_times_values (64x2048x2048x71): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x71): 35.007

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 81.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x72x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x72x2048): 44.738
Elapsed time for attention_prob_times_values (64x2048x2048x72): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x72): 34.964

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 83.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x73x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x73x2048): 43.384
Elapsed time for attention_prob_times_values (64x2048x2048x73): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x73): 35.743

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 83.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x74x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x74x2048): 44.048
Elapsed time for attention_prob_times_values (64x2048x2048x74): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x74): 37.187

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 86.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x75x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x75x2048): 44.280
Elapsed time for attention_prob_times_values (64x2048x2048x75): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x75): 36.288

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 86.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x76x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x76x2048): 45.310
Elapsed time for attention_prob_times_values (64x2048x2048x76): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x76): 38.107

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 90.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x77x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x77x2048): 45.083
Elapsed time for attention_prob_times_values (64x2048x2048x77): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x77): 37.248

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 89.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x78x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x78x2048): 46.233
Elapsed time for attention_prob_times_values (64x2048x2048x78): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x78): 38.927

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 93.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x79x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x79x2048): 46.074
Elapsed time for attention_prob_times_values (64x2048x2048x79): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x79): 38.109

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 93.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x80x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x80x2048): 47.771
Elapsed time for attention_prob_times_values (64x2048x2048x80): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x80): 38.394

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 95.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x81x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x81x2048): 45.159
Elapsed time for attention_prob_times_values (64x2048x2048x81): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x81): 38.915

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 94.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x82x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x82x2048): 46.093
Elapsed time for attention_prob_times_values (64x2048x2048x82): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x82): 40.600

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 98.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x83x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x83x2048): 45.630
Elapsed time for attention_prob_times_values (64x2048x2048x83): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x83): 39.631

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 97.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x84x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x84x2048): 46.843
Elapsed time for attention_prob_times_values (64x2048x2048x84): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x84): 41.526

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 101.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x85x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x85x2048): 46.718
Elapsed time for attention_prob_times_values (64x2048x2048x85): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x85): 40.483

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 100.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x86x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x86x2048): 47.628
Elapsed time for attention_prob_times_values (64x2048x2048x86): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x86): 42.099

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 104.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x87x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x87x2048): 47.829
Elapsed time for attention_prob_times_values (64x2048x2048x87): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x87): 41.339

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 104.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x88x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x88x2048): 48.832
Elapsed time for attention_prob_times_values (64x2048x2048x88): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x88): 41.430

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 106.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x89x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x89x2048): 47.345
Elapsed time for attention_prob_times_values (64x2048x2048x89): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x89): 42.183

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 106.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x90x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x90x2048): 48.093
Elapsed time for attention_prob_times_values (64x2048x2048x90): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x90): 43.971

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 110.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x91x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x91x2048): 47.989
Elapsed time for attention_prob_times_values (64x2048x2048x91): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x91): 43.049

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 109.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x92x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x92x2048): 48.894
Elapsed time for attention_prob_times_values (64x2048x2048x92): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x92): 44.831

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 114.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x93x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x93x2048): 48.320
Elapsed time for attention_prob_times_values (64x2048x2048x93): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x93): 43.747

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 112.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x94x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x94x2048): 49.548
Elapsed time for attention_prob_times_values (64x2048x2048x94): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x94): 45.452

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 117.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x95x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x95x2048): 49.519
Elapsed time for attention_prob_times_values (64x2048x2048x95): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x95): 44.400

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 116.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x96x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x96x2048): 63.357
Elapsed time for attention_prob_times_values (64x2048x2048x96): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x96): 46.063

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 133.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x97x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x97x2048): 49.994
Elapsed time for attention_prob_times_values (64x2048x2048x97): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x97): 45.335

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 119.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x98x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x98x2048): 51.012
Elapsed time for attention_prob_times_values (64x2048x2048x98): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x98): 47.163

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 124.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x99x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x99x2048): 50.619
Elapsed time for attention_prob_times_values (64x2048x2048x99): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x99): 46.251

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 123.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x100x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x100x2048): 51.766
Elapsed time for attention_prob_times_values (64x2048x2048x100): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x100): 48.033

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 127.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x101x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x101x2048): 51.235
Elapsed time for attention_prob_times_values (64x2048x2048x101): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x101): 47.078

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 126.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x102x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x102x2048): 52.284
Elapsed time for attention_prob_times_values (64x2048x2048x102): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x102): 48.713

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 130.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x103x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x103x2048): 51.794
Elapsed time for attention_prob_times_values (64x2048x2048x103): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x103): 47.921

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 129.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x104x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x104x2048): 53.321
Elapsed time for attention_prob_times_values (64x2048x2048x104): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x104): 48.336

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 133.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x105x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x105x2048): 51.478
Elapsed time for attention_prob_times_values (64x2048x2048x105): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x105): 48.470

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 131.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x106x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x106x2048): 52.738
Elapsed time for attention_prob_times_values (64x2048x2048x106): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x106): 50.346

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 136.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x107x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x107x2048): 52.141
Elapsed time for attention_prob_times_values (64x2048x2048x107): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x107): 48.754

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 134.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x108x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x108x2048): 53.438
Elapsed time for attention_prob_times_values (64x2048x2048x108): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x108): 51.113

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 140.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x109x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x109x2048): 52.773
Elapsed time for attention_prob_times_values (64x2048x2048x109): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x109): 49.645

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 138.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x110x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x110x2048): 54.012
Elapsed time for attention_prob_times_values (64x2048x2048x110): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x110): 51.636

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 143.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x111x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x111x2048): 53.614
Elapsed time for attention_prob_times_values (64x2048x2048x111): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x111): 50.312

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 141.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x112x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x112x2048): 55.527
Elapsed time for attention_prob_times_values (64x2048x2048x112): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x112): 52.216

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 148.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x113x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x113x2048): 53.457
Elapsed time for attention_prob_times_values (64x2048x2048x113): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x113): 51.198

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 144.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x114x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x114x2048): 54.467
Elapsed time for attention_prob_times_values (64x2048x2048x114): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x114): 52.907

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 149.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x115x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x115x2048): 53.994
Elapsed time for attention_prob_times_values (64x2048x2048x115): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x115): 51.688

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 147.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x116x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x116x2048): 55.166
Elapsed time for attention_prob_times_values (64x2048x2048x116): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x116): 53.835

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 153.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x117x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x117x2048): 54.671
Elapsed time for attention_prob_times_values (64x2048x2048x117): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x117): 52.472

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 151.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x118x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x118x2048): 55.670
Elapsed time for attention_prob_times_values (64x2048x2048x118): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x118): 54.483

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 156.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x119x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x119x2048): 55.433
Elapsed time for attention_prob_times_values (64x2048x2048x119): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x119): 52.916

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 154.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x120x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x120x2048): 57.080
Elapsed time for attention_prob_times_values (64x2048x2048x120): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x120): 54.811

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 160.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x121x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x121x2048): 54.856
Elapsed time for attention_prob_times_values (64x2048x2048x121): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x121): 42.969

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 139.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x122x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x122x2048): 55.747
Elapsed time for attention_prob_times_values (64x2048x2048x122): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x122): 55.883

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 162.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x123x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x123x2048): 55.141
Elapsed time for attention_prob_times_values (64x2048x2048x123): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x123): 43.873

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 142.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x124x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x124x2048): 56.615
Elapsed time for attention_prob_times_values (64x2048x2048x124): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x124): 56.443

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 166.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x125x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x125x2048): 55.948
Elapsed time for attention_prob_times_values (64x2048x2048x125): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x125): 43.505

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 144.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x126x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x126x2048): 56.362
Elapsed time for attention_prob_times_values (64x2048x2048x126): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x126): 57.376

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 168.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x127x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x127x2048): 56.173
Elapsed time for attention_prob_times_values (64x2048x2048x127): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x127): 43.160

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 145.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x128x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x128x2048): 66.658
Elapsed time for attention_prob_times_values (64x2048x2048x128): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x128): 60.680

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 190.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x129x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x129x2048): 55.563
Elapsed time for attention_prob_times_values (64x2048x2048x129): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x129): 43.511

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 147.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x130x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x130x2048): 56.589
Elapsed time for attention_prob_times_values (64x2048x2048x130): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x130): 46.006

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 153.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x131x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x131x2048): 56.478
Elapsed time for attention_prob_times_values (64x2048x2048x131): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x131): 44.345

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 151.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x132x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x132x2048): 57.671
Elapsed time for attention_prob_times_values (64x2048x2048x132): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x132): 46.920

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 158.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x133x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x133x2048): 56.571
Elapsed time for attention_prob_times_values (64x2048x2048x133): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x133): 45.199

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 154.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x134x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x134x2048): 57.960
Elapsed time for attention_prob_times_values (64x2048x2048x134): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x134): 47.317

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 161.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x135x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x135x2048): 57.553
Elapsed time for attention_prob_times_values (64x2048x2048x135): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x135): 45.796

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 158.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x136x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x136x2048): 59.086
Elapsed time for attention_prob_times_values (64x2048x2048x136): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x136): 44.957

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 159.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x137x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x137x2048): 57.384
Elapsed time for attention_prob_times_values (64x2048x2048x137): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x137): 46.024

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 160.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x138x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x138x2048): 58.247
Elapsed time for attention_prob_times_values (64x2048x2048x138): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x138): 48.479

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 167.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x139x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x139x2048): 57.770
Elapsed time for attention_prob_times_values (64x2048x2048x139): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x139): 46.689

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 163.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x140x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x140x2048): 59.025
Elapsed time for attention_prob_times_values (64x2048x2048x140): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x140): 49.397

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 171.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x141x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x141x2048): 58.375
Elapsed time for attention_prob_times_values (64x2048x2048x141): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x141): 47.360

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 167.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x142x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x142x2048): 59.556
Elapsed time for attention_prob_times_values (64x2048x2048x142): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x142): 49.775

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 174.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x143x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x143x2048): 58.965
Elapsed time for attention_prob_times_values (64x2048x2048x143): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x143): 47.889

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 170.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x144x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x144x2048): 78.532
Elapsed time for attention_prob_times_values (64x2048x2048x144): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x144): 48.505

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 194.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x145x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x145x2048): 58.620
Elapsed time for attention_prob_times_values (64x2048x2048x145): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x145): 48.487

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 173.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x146x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x146x2048): 59.808
Elapsed time for attention_prob_times_values (64x2048x2048x146): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x146): 50.979

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 180.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x147x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x147x2048): 59.439
Elapsed time for attention_prob_times_values (64x2048x2048x147): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x147): 48.988

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 177.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x148x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x148x2048): 60.583
Elapsed time for attention_prob_times_values (64x2048x2048x148): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x148): 51.701

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 184.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x149x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x149x2048): 59.946
Elapsed time for attention_prob_times_values (64x2048x2048x149): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x149): 49.662

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 180.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x150x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x150x2048): 60.980
Elapsed time for attention_prob_times_values (64x2048x2048x150): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x150): 52.069

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 187.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x151x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x151x2048): 60.578
Elapsed time for attention_prob_times_values (64x2048x2048x151): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x151): 50.292

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 184.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x152x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x152x2048): 62.062
Elapsed time for attention_prob_times_values (64x2048x2048x152): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x152): 50.312

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 187.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x153x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x153x2048): 60.221
Elapsed time for attention_prob_times_values (64x2048x2048x153): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x153): 50.913

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 187.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x154x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x154x2048): 61.149
Elapsed time for attention_prob_times_values (64x2048x2048x154): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x154): 52.984

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 193.387
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x155x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x155x2048): 60.838
Elapsed time for attention_prob_times_values (64x2048x2048x155): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x155): 51.237

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 190.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x156x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x156x2048): 62.031
Elapsed time for attention_prob_times_values (64x2048x2048x156): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x156): 54.002

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 198.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x157x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x157x2048): 61.281
Elapsed time for attention_prob_times_values (64x2048x2048x157): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x157): 51.975

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 194.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x158x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x158x2048): 62.506
Elapsed time for attention_prob_times_values (64x2048x2048x158): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x158): 54.335

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 201.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x159x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x159x2048): 62.019
Elapsed time for attention_prob_times_values (64x2048x2048x159): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x159): 52.306

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 197.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x160x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x160x2048): 74.594
Elapsed time for attention_prob_times_values (64x2048x2048x160): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x160): 54.713

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 220.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x161x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x161x2048): 61.085
Elapsed time for attention_prob_times_values (64x2048x2048x161): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x161): 53.591

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 200.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x162x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x162x2048): 62.219
Elapsed time for attention_prob_times_values (64x2048x2048x162): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x162): 55.389

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 206.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x163x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x163x2048): 61.564
Elapsed time for attention_prob_times_values (64x2048x2048x163): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x163): 53.969

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 204.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x164x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x164x2048): 62.977
Elapsed time for attention_prob_times_values (64x2048x2048x164): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x164): 56.304

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 211.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x165x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x165x2048): 62.038
Elapsed time for attention_prob_times_values (64x2048x2048x165): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x165): 54.519

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 207.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x166x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x166x2048): 63.174
Elapsed time for attention_prob_times_values (64x2048x2048x166): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x166): 56.671

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 214.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x167x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x167x2048): 62.467
Elapsed time for attention_prob_times_values (64x2048x2048x167): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x167): 55.188

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 211.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x168x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x168x2048): 64.392
Elapsed time for attention_prob_times_values (64x2048x2048x168): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x168): 55.186

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 215.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x169x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x169x2048): 62.121
Elapsed time for attention_prob_times_values (64x2048x2048x169): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x169): 55.402

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 213.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x170x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x170x2048): 63.389
Elapsed time for attention_prob_times_values (64x2048x2048x170): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x170): 57.773

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 221.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x171x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x171x2048): 62.662
Elapsed time for attention_prob_times_values (64x2048x2048x171): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x171): 55.854

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 216.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x172x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x172x2048): 64.135
Elapsed time for attention_prob_times_values (64x2048x2048x172): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x172): 58.277

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 225.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x173x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x173x2048): 63.210
Elapsed time for attention_prob_times_values (64x2048x2048x173): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x173): 56.450

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 220.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x174x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x174x2048): 64.401
Elapsed time for attention_prob_times_values (64x2048x2048x174): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x174): 58.857

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 228.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x175x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x175x2048): 63.723
Elapsed time for attention_prob_times_values (64x2048x2048x175): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x175): 57.003

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 224.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x176x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x176x2048): 66.530
Elapsed time for attention_prob_times_values (64x2048x2048x176): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x176): 58.248

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 232.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x177x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x177x2048): 63.416
Elapsed time for attention_prob_times_values (64x2048x2048x177): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x177): 57.613

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 227.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x178x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x178x2048): 64.742
Elapsed time for attention_prob_times_values (64x2048x2048x178): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x178): 60.021

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 235.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x179x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x179x2048): 63.868
Elapsed time for attention_prob_times_values (64x2048x2048x179): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x179): 58.340

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 231.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x180x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x180x2048): 65.406
Elapsed time for attention_prob_times_values (64x2048x2048x180): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x180): 60.777

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 240.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x181x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x181x2048): 64.368
Elapsed time for attention_prob_times_values (64x2048x2048x181): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x181): 58.847

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 235.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x182x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x182x2048): 65.677
Elapsed time for attention_prob_times_values (64x2048x2048x182): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x182): 60.959

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 243.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x183x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x183x2048): 64.762
Elapsed time for attention_prob_times_values (64x2048x2048x183): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x183): 59.369

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 239.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x184x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x184x2048): 66.860
Elapsed time for attention_prob_times_values (64x2048x2048x184): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x184): 59.935

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 244.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x185x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x185x2048): 64.397
Elapsed time for attention_prob_times_values (64x2048x2048x185): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x185): 60.018

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 241.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x186x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x186x2048): 65.700
Elapsed time for attention_prob_times_values (64x2048x2048x186): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x186): 62.188

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 249.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 2992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x187x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x187x2048): 64.962
Elapsed time for attention_prob_times_values (64x2048x2048x187): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x187): 60.492

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 245.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x188x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x188x2048): 66.479
Elapsed time for attention_prob_times_values (64x2048x2048x188): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x188): 62.504

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 253.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x189x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x189x2048): 65.365
Elapsed time for attention_prob_times_values (64x2048x2048x189): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x189): 61.034

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 249.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x190x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x190x2048): 66.642
Elapsed time for attention_prob_times_values (64x2048x2048x190): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x190): 63.215

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 257.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x191x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x191x2048): 65.965
Elapsed time for attention_prob_times_values (64x2048x2048x191): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x191): 61.486

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 253.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x192x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x192x2048): 75.890
Elapsed time for attention_prob_times_values (64x2048x2048x192): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x192): 65.112

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 280.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x193x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x193x2048): 65.021
Elapsed time for attention_prob_times_values (64x2048x2048x193): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x193): 49.697

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 226.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x194x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x194x2048): 66.290
Elapsed time for attention_prob_times_values (64x2048x2048x194): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x194): 51.753

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 234.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x195x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x195x2048): 65.245
Elapsed time for attention_prob_times_values (64x2048x2048x195): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x195): 50.315

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 229.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x196x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x196x2048): 66.841
Elapsed time for attention_prob_times_values (64x2048x2048x196): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x196): 51.967

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 237.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x197x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x197x2048): 65.641
Elapsed time for attention_prob_times_values (64x2048x2048x197): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x197): 50.646

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 233.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x198x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x198x2048): 67.036
Elapsed time for attention_prob_times_values (64x2048x2048x198): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x198): 52.485

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 241.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x199x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x199x2048): 65.937
Elapsed time for attention_prob_times_values (64x2048x2048x199): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x199): 50.974

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 236.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x200x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x200x2048): 68.185
Elapsed time for attention_prob_times_values (64x2048x2048x200): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x200): 50.608

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 239.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x201x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x201x2048): 65.834
Elapsed time for attention_prob_times_values (64x2048x2048x201): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x201): 50.747

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 237.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x202x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x202x2048): 67.049
Elapsed time for attention_prob_times_values (64x2048x2048x202): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x202): 53.381

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 247.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x203x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x203x2048): 66.143
Elapsed time for attention_prob_times_values (64x2048x2048x203): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x203): 51.350

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 241.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x204x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x204x2048): 67.723
Elapsed time for attention_prob_times_values (64x2048x2048x204): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x204): 54.113

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 251.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x205x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x205x2048): 66.506
Elapsed time for attention_prob_times_values (64x2048x2048x205): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x205): 51.670

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 244.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x206x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x206x2048): 67.907
Elapsed time for attention_prob_times_values (64x2048x2048x206): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x206): 54.222

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 254.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x207x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x207x2048): 67.107
Elapsed time for attention_prob_times_values (64x2048x2048x207): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x207): 52.127

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 248.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x208x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x208x2048): 69.699
Elapsed time for attention_prob_times_values (64x2048x2048x208): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x208): 53.382

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 256.950
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x209x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x209x2048): 66.651
Elapsed time for attention_prob_times_values (64x2048x2048x209): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x209): 52.572

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 250.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x210x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x210x2048): 68.011
Elapsed time for attention_prob_times_values (64x2048x2048x210): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x210): 55.275

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 261.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x211x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x211x2048): 67.061
Elapsed time for attention_prob_times_values (64x2048x2048x211): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x211): 52.957

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 254.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x212x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x212x2048): 68.540
Elapsed time for attention_prob_times_values (64x2048x2048x212): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x212): 55.736

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 265.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x213x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x213x2048): 67.516
Elapsed time for attention_prob_times_values (64x2048x2048x213): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x213): 53.364

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 258.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x214x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x214x2048): 68.576
Elapsed time for attention_prob_times_values (64x2048x2048x214): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x214): 56.001

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 267.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x215x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x215x2048): 67.768
Elapsed time for attention_prob_times_values (64x2048x2048x215): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x215): 53.769

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 261.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x216x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x216x2048): 69.944
Elapsed time for attention_prob_times_values (64x2048x2048x216): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x216): 54.626

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 268.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x217x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x217x2048): 67.520
Elapsed time for attention_prob_times_values (64x2048x2048x217): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x217): 54.095

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 263.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x218x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x218x2048): 68.637
Elapsed time for attention_prob_times_values (64x2048x2048x218): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x218): 56.783

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 273.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x219x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x219x2048): 67.861
Elapsed time for attention_prob_times_values (64x2048x2048x219): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x219): 54.606

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 267.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x220x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x220x2048): 69.170
Elapsed time for attention_prob_times_values (64x2048x2048x220): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x220): 57.644

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 279.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x221x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x221x2048): 68.405
Elapsed time for attention_prob_times_values (64x2048x2048x221): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x221): 55.037

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 271.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x222x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x222x2048): 69.459
Elapsed time for attention_prob_times_values (64x2048x2048x222): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x222): 57.785

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 281.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x223x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x223x2048): 68.750
Elapsed time for attention_prob_times_values (64x2048x2048x223): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x223): 55.437

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 275.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x224x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x224x2048): 78.727
Elapsed time for attention_prob_times_values (64x2048x2048x224): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x224): 58.638

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 302.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x225x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x225x2048): 67.437
Elapsed time for attention_prob_times_values (64x2048x2048x225): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x225): 56.395

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 277.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x226x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x226x2048): 68.790
Elapsed time for attention_prob_times_values (64x2048x2048x226): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x226): 58.556

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 286.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x227x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x227x2048): 67.471
Elapsed time for attention_prob_times_values (64x2048x2048x227): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x227): 56.688

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 280.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x228x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x228x2048): 69.314
Elapsed time for attention_prob_times_values (64x2048x2048x228): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x228): 59.006

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 290.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x229x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x229x2048): 68.101
Elapsed time for attention_prob_times_values (64x2048x2048x229): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x229): 56.973

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 284.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x230x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x230x2048): 69.491
Elapsed time for attention_prob_times_values (64x2048x2048x230): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x230): 59.244

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 293.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x231x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x231x2048): 68.380
Elapsed time for attention_prob_times_values (64x2048x2048x231): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x231): 57.325

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 287.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x232x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x232x2048): 70.807
Elapsed time for attention_prob_times_values (64x2048x2048x232): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x232): 57.976

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 294.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x233x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x233x2048): 67.945
Elapsed time for attention_prob_times_values (64x2048x2048x233): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x233): 57.416

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 288.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x234x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x234x2048): 69.323
Elapsed time for attention_prob_times_values (64x2048x2048x234): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x234): 60.103

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 299.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x235x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x235x2048): 68.316
Elapsed time for attention_prob_times_values (64x2048x2048x235): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x235): 57.947

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 292.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x236x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x236x2048): 69.961
Elapsed time for attention_prob_times_values (64x2048x2048x236): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x236): 60.644

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 304.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x237x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x237x2048): 68.499
Elapsed time for attention_prob_times_values (64x2048x2048x237): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x237): 58.160

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 295.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x238x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x238x2048): 70.139
Elapsed time for attention_prob_times_values (64x2048x2048x238): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x238): 60.866

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 307.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x239x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x239x2048): 69.024
Elapsed time for attention_prob_times_values (64x2048x2048x239): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x239): 58.740

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 300.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x240x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x240x2048): 55.118
Elapsed time for attention_prob_times_values (64x2048x2048x240): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x240): 60.962

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 274.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x241x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x241x2048): 52.878
Elapsed time for attention_prob_times_values (64x2048x2048x241): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x241): 59.277

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 266.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x242x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x242x2048): 53.773
Elapsed time for attention_prob_times_values (64x2048x2048x242): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x242): 61.700

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 274.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x243x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x243x2048): 53.374
Elapsed time for attention_prob_times_values (64x2048x2048x243): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x243): 59.451

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 269.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x244x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x244x2048): 54.266
Elapsed time for attention_prob_times_values (64x2048x2048x244): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x244): 62.323

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 279.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x245x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x245x2048): 53.698
Elapsed time for attention_prob_times_values (64x2048x2048x245): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x245): 59.927

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 273.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x246x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x246x2048): 54.266
Elapsed time for attention_prob_times_values (64x2048x2048x246): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x246): 62.395

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 281.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x247x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x247x2048): 53.921
Elapsed time for attention_prob_times_values (64x2048x2048x247): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x247): 60.227

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 276.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x248x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x248x2048): 55.101
Elapsed time for attention_prob_times_values (64x2048x2048x248): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x248): 61.496

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 283.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 3984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x249x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x249x2048): 53.293
Elapsed time for attention_prob_times_values (64x2048x2048x249): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x249): 61.608

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 279.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x250x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x250x2048): 53.747
Elapsed time for attention_prob_times_values (64x2048x2048x250): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x250): 63.395

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 285.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x251x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x251x2048): 53.533
Elapsed time for attention_prob_times_values (64x2048x2048x251): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x251): 61.402

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 281.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x252x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x252x2048): 54.542
Elapsed time for attention_prob_times_values (64x2048x2048x252): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x252): 63.573

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 289.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x253x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x253x2048): 53.358
Elapsed time for attention_prob_times_values (64x2048x2048x253): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x253): 62.833

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 285.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x254x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x254x2048): 54.152
Elapsed time for attention_prob_times_values (64x2048x2048x254): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x254): 64.243

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 292.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x255x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x255x2048): 53.158
Elapsed time for attention_prob_times_values (64x2048x2048x255): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x255): 59.739

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 280.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x256x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x256x2048): 75.443
Elapsed time for attention_prob_times_values (64x2048x2048x256): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x256): 68.228

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 358.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x257x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x257x2048): 55.958
Elapsed time for attention_prob_times_values (64x2048x2048x257): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x257): 52.625

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 272.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x258x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x258x2048): 56.459
Elapsed time for attention_prob_times_values (64x2048x2048x258): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x258): 55.346

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 281.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x259x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x259x2048): 56.062
Elapsed time for attention_prob_times_values (64x2048x2048x259): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x259): 53.369

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 275.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x260x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x260x2048): 57.035
Elapsed time for attention_prob_times_values (64x2048x2048x260): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x260): 55.951

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 285.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x261x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x261x2048): 56.005
Elapsed time for attention_prob_times_values (64x2048x2048x261): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x261): 53.924

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 279.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x262x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x262x2048): 56.549
Elapsed time for attention_prob_times_values (64x2048x2048x262): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x262): 56.241

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 287.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x263x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x263x2048): 55.807
Elapsed time for attention_prob_times_values (64x2048x2048x263): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x263): 54.303

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 281.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x264x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x264x2048): 57.068
Elapsed time for attention_prob_times_values (64x2048x2048x264): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x264): 53.600

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 283.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x265x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x265x2048): 54.942
Elapsed time for attention_prob_times_values (64x2048x2048x265): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x265): 54.302

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 280.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x266x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x266x2048): 55.461
Elapsed time for attention_prob_times_values (64x2048x2048x266): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x266): 57.035

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 289.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x267x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x267x2048): 55.306
Elapsed time for attention_prob_times_values (64x2048x2048x267): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x267): 54.702

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 284.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x268x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x268x2048): 56.174
Elapsed time for attention_prob_times_values (64x2048x2048x268): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x268): 57.667

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 295.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x269x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x269x2048): 55.623
Elapsed time for attention_prob_times_values (64x2048x2048x269): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x269): 55.087

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 288.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x270x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x270x2048): 56.181
Elapsed time for attention_prob_times_values (64x2048x2048x270): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x270): 57.744

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 297.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x271x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x271x2048): 55.888
Elapsed time for attention_prob_times_values (64x2048x2048x271): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x271): 55.481

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 291.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x272x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x272x2048): 57.684
Elapsed time for attention_prob_times_values (64x2048x2048x272): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x272): 71.813

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 335.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x273x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x273x2048): 55.342
Elapsed time for attention_prob_times_values (64x2048x2048x273): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x273): 55.708

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 292.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x274x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x274x2048): 56.157
Elapsed time for attention_prob_times_values (64x2048x2048x274): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x274): 58.405

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 302.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x275x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x275x2048): 55.503
Elapsed time for attention_prob_times_values (64x2048x2048x275): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x275): 56.132

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 295.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x276x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x276x2048): 56.520
Elapsed time for attention_prob_times_values (64x2048x2048x276): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x276): 59.022

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 306.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x277x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x277x2048): 55.886
Elapsed time for attention_prob_times_values (64x2048x2048x277): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x277): 56.596

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 299.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x278x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x278x2048): 56.386
Elapsed time for attention_prob_times_values (64x2048x2048x278): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x278): 59.207

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 308.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x279x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x279x2048): 56.235
Elapsed time for attention_prob_times_values (64x2048x2048x279): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x279): 56.922

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 303.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x280x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x280x2048): 57.507
Elapsed time for attention_prob_times_values (64x2048x2048x280): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x280): 73.451

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 346.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x281x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x281x2048): 55.539
Elapsed time for attention_prob_times_values (64x2048x2048x281): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x281): 57.268

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 303.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x282x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x282x2048): 55.849
Elapsed time for attention_prob_times_values (64x2048x2048x282): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x282): 59.839

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 312.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x283x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x283x2048): 55.826
Elapsed time for attention_prob_times_values (64x2048x2048x283): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x283): 57.629

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 307.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x284x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x284x2048): 56.911
Elapsed time for attention_prob_times_values (64x2048x2048x284): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x284): 60.715

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 319.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x285x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x285x2048): 56.113
Elapsed time for attention_prob_times_values (64x2048x2048x285): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x285): 58.005

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 311.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x286x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x286x2048): 56.627
Elapsed time for attention_prob_times_values (64x2048x2048x286): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x286): 60.687

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 320.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x287x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x287x2048): 56.310
Elapsed time for attention_prob_times_values (64x2048x2048x287): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x287): 58.401

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 314.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x288x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x288x2048): 76.377
Elapsed time for attention_prob_times_values (64x2048x2048x288): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x288): 73.595

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 412.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x289x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x289x2048): 59.907
Elapsed time for attention_prob_times_values (64x2048x2048x289): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x289): 58.915

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 327.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x290x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x290x2048): 60.594
Elapsed time for attention_prob_times_values (64x2048x2048x290): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x290): 61.242

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 336.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x291x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x291x2048): 59.831
Elapsed time for attention_prob_times_values (64x2048x2048x291): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x291): 59.301

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 330.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x292x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x292x2048): 61.630
Elapsed time for attention_prob_times_values (64x2048x2048x292): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x292): 62.007

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 343.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x293x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x293x2048): 59.838
Elapsed time for attention_prob_times_values (64x2048x2048x293): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x293): 59.833

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 333.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x294x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x294x2048): 60.388
Elapsed time for attention_prob_times_values (64x2048x2048x294): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x294): 62.110

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 342.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x295x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x295x2048): 59.577
Elapsed time for attention_prob_times_values (64x2048x2048x295): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x295): 60.105

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 335.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x296x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x296x2048): 61.403
Elapsed time for attention_prob_times_values (64x2048x2048x296): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x296): 77.270

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 384.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x297x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x297x2048): 58.645
Elapsed time for attention_prob_times_values (64x2048x2048x297): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x297): 60.289

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 335.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x298x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x298x2048): 59.164
Elapsed time for attention_prob_times_values (64x2048x2048x298): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x298): 63.002

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 345.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x299x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x299x2048): 58.880
Elapsed time for attention_prob_times_values (64x2048x2048x299): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x299): 60.575

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 338.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x300x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x300x2048): 60.473
Elapsed time for attention_prob_times_values (64x2048x2048x300): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x300): 63.610

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 352.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x301x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x301x2048): 59.336
Elapsed time for attention_prob_times_values (64x2048x2048x301): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x301): 61.350

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 344.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x302x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x302x2048): 60.021
Elapsed time for attention_prob_times_values (64x2048x2048x302): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x302): 63.914

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 354.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x303x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x303x2048): 59.475
Elapsed time for attention_prob_times_values (64x2048x2048x303): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x303): 61.948

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 347.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x304x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x304x2048): 60.757
Elapsed time for attention_prob_times_values (64x2048x2048x304): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x304): 79.020

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 394.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x305x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x305x2048): 58.731
Elapsed time for attention_prob_times_values (64x2048x2048x305): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x305): 62.148

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 348.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x306x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x306x2048): 59.498
Elapsed time for attention_prob_times_values (64x2048x2048x306): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x306): 64.675

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 358.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x307x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x307x2048): 59.141
Elapsed time for attention_prob_times_values (64x2048x2048x307): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x307): 62.291

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 351.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x308x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x308x2048): 60.636
Elapsed time for attention_prob_times_values (64x2048x2048x308): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x308): 65.214

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 365.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x309x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x309x2048): 59.506
Elapsed time for attention_prob_times_values (64x2048x2048x309): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x309): 62.952

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 356.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x310x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x310x2048): 60.002
Elapsed time for attention_prob_times_values (64x2048x2048x310): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x310): 65.441

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 365.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x311x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x311x2048): 59.723
Elapsed time for attention_prob_times_values (64x2048x2048x311): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x311): 63.497

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 360.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x312x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x312x2048): 61.397
Elapsed time for attention_prob_times_values (64x2048x2048x312): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x312): 80.773

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 409.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x313x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x313x2048): 58.936
Elapsed time for attention_prob_times_values (64x2048x2048x313): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x313): 63.565

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 360.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x314x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x314x2048): 59.437
Elapsed time for attention_prob_times_values (64x2048x2048x314): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x314): 66.143

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 369.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x315x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x315x2048): 59.352
Elapsed time for attention_prob_times_values (64x2048x2048x315): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x315): 63.799

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 364.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x316x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x316x2048): 60.756
Elapsed time for attention_prob_times_values (64x2048x2048x316): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x316): 66.242

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 376.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x317x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x317x2048): 59.445
Elapsed time for attention_prob_times_values (64x2048x2048x317): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x317): 64.216

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 367.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x318x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x318x2048): 60.058
Elapsed time for attention_prob_times_values (64x2048x2048x318): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x318): 66.798

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 377.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x319x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x319x2048): 59.553
Elapsed time for attention_prob_times_values (64x2048x2048x319): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x319): 64.872

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 371.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x320x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x320x2048): 77.265
Elapsed time for attention_prob_times_values (64x2048x2048x320): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x320): 80.520

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 473.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x321x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x321x2048): 62.093
Elapsed time for attention_prob_times_values (64x2048x2048x321): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x321): 57.399

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 358.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x322x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x322x2048): 63.094
Elapsed time for attention_prob_times_values (64x2048x2048x322): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x322): 59.894

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 370.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x323x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x323x2048): 62.170
Elapsed time for attention_prob_times_values (64x2048x2048x323): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x323): 57.597

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 361.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x324x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x324x2048): 63.870
Elapsed time for attention_prob_times_values (64x2048x2048x324): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x324): 60.132

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 375.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x325x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x325x2048): 62.057
Elapsed time for attention_prob_times_values (64x2048x2048x325): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x325): 57.949

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 364.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x326x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x326x2048): 62.657
Elapsed time for attention_prob_times_values (64x2048x2048x326): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x326): 60.453

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 374.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x327x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x327x2048): 61.696
Elapsed time for attention_prob_times_values (64x2048x2048x327): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x327): 58.296

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 366.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x328x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x328x2048): 63.804
Elapsed time for attention_prob_times_values (64x2048x2048x328): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x328): 71.286

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 412.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x329x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x329x2048): 60.814
Elapsed time for attention_prob_times_values (64x2048x2048x329): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x329): 56.691

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 360.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x330x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x330x2048): 61.504
Elapsed time for attention_prob_times_values (64x2048x2048x330): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x330): 60.969

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 376.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x331x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x331x2048): 61.250
Elapsed time for attention_prob_times_values (64x2048x2048x331): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x331): 57.062

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 364.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x332x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x332x2048): 62.786
Elapsed time for attention_prob_times_values (64x2048x2048x332): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x332): 61.440

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 384.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x333x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x333x2048): 61.428
Elapsed time for attention_prob_times_values (64x2048x2048x333): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x333): 57.184

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 367.410
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x334x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x334x2048): 62.152
Elapsed time for attention_prob_times_values (64x2048x2048x334): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x334): 61.617

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 384.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x335x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x335x2048): 61.409
Elapsed time for attention_prob_times_values (64x2048x2048x335): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x335): 57.510

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 370.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x336x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x336x2048): 63.124
Elapsed time for attention_prob_times_values (64x2048x2048x336): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x336): 73.130

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 423.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x337x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x337x2048): 60.713
Elapsed time for attention_prob_times_values (64x2048x2048x337): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x337): 57.848

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 371.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x338x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x338x2048): 61.534
Elapsed time for attention_prob_times_values (64x2048x2048x338): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x338): 60.574

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 383.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x339x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x339x2048): 61.201
Elapsed time for attention_prob_times_values (64x2048x2048x339): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x339): 58.223

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 375.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x340x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x340x2048): 62.316
Elapsed time for attention_prob_times_values (64x2048x2048x340): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x340): 62.652

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 394.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x341x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x341x2048): 61.537
Elapsed time for attention_prob_times_values (64x2048x2048x341): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x341): 58.571

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 379.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x342x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x342x2048): 61.961
Elapsed time for attention_prob_times_values (64x2048x2048x342): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x342): 61.540

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 391.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x343x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x343x2048): 61.681
Elapsed time for attention_prob_times_values (64x2048x2048x343): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x343): 58.771

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 382.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x344x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x344x2048): 63.452
Elapsed time for attention_prob_times_values (64x2048x2048x344): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x344): 74.785

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 437.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x345x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x345x2048): 60.831
Elapsed time for attention_prob_times_values (64x2048x2048x345): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x345): 58.885

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 382.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x346x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x346x2048): 61.363
Elapsed time for attention_prob_times_values (64x2048x2048x346): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x346): 61.674

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 394.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x347x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x347x2048): 61.016
Elapsed time for attention_prob_times_values (64x2048x2048x347): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x347): 59.411

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 386.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x348x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x348x2048): 62.445
Elapsed time for attention_prob_times_values (64x2048x2048x348): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x348): 62.311

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 401.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x349x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x349x2048): 61.220
Elapsed time for attention_prob_times_values (64x2048x2048x349): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x349): 59.658

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 389.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x350x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x350x2048): 61.916
Elapsed time for attention_prob_times_values (64x2048x2048x350): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x350): 62.357

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 401.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x351x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x351x2048): 61.417
Elapsed time for attention_prob_times_values (64x2048x2048x351): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x351): 59.973

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 393.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x352x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x352x2048): 79.566
Elapsed time for attention_prob_times_values (64x2048x2048x352): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x352): 74.741

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 501.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x353x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x353x2048): 63.739
Elapsed time for attention_prob_times_values (64x2048x2048x353): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x353): 60.818

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 405.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x354x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x354x2048): 64.703
Elapsed time for attention_prob_times_values (64x2048x2048x354): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x354): 62.965

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 416.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x355x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x355x2048): 63.635
Elapsed time for attention_prob_times_values (64x2048x2048x355): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x355): 60.818

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 407.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x356x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x356x2048): 65.301
Elapsed time for attention_prob_times_values (64x2048x2048x356): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x356): 63.411

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 422.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x357x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x357x2048): 63.494
Elapsed time for attention_prob_times_values (64x2048x2048x357): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x357): 61.095

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 409.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x358x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x358x2048): 64.291
Elapsed time for attention_prob_times_values (64x2048x2048x358): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x358): 63.486

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 421.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x359x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x359x2048): 63.121
Elapsed time for attention_prob_times_values (64x2048x2048x359): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x359): 61.424

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 411.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x360x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x360x2048): 65.144
Elapsed time for attention_prob_times_values (64x2048x2048x360): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x360): 77.522

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 469.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x361x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x361x2048): 62.358
Elapsed time for attention_prob_times_values (64x2048x2048x361): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x361): 61.314

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 410.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x362x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x362x2048): 63.146
Elapsed time for attention_prob_times_values (64x2048x2048x362): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x362): 63.994

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 423.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x363x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x363x2048): 62.646
Elapsed time for attention_prob_times_values (64x2048x2048x363): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x363): 61.616

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 414.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x364x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x364x2048): 63.916
Elapsed time for attention_prob_times_values (64x2048x2048x364): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x364): 64.703

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 430.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x365x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x365x2048): 62.933
Elapsed time for attention_prob_times_values (64x2048x2048x365): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x365): 62.045

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 418.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x366x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x366x2048): 63.696
Elapsed time for attention_prob_times_values (64x2048x2048x366): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x366): 64.670

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 431.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x367x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x367x2048): 63.027
Elapsed time for attention_prob_times_values (64x2048x2048x367): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x367): 62.309

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 422.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x368x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x368x2048): 64.973
Elapsed time for attention_prob_times_values (64x2048x2048x368): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x368): 77.641

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 477.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x369x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x369x2048): 62.320
Elapsed time for attention_prob_times_values (64x2048x2048x369): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x369): 62.715

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 422.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x370x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x370x2048): 63.200
Elapsed time for attention_prob_times_values (64x2048x2048x370): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x370): 65.310

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 435.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x371x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x371x2048): 62.484
Elapsed time for attention_prob_times_values (64x2048x2048x371): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x371): 62.946

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 426.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x372x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x372x2048): 63.687
Elapsed time for attention_prob_times_values (64x2048x2048x372): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x372): 65.839

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 441.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x373x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x373x2048): 62.438
Elapsed time for attention_prob_times_values (64x2048x2048x373): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x373): 63.342

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 429.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 5984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x374x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x374x2048): 63.421
Elapsed time for attention_prob_times_values (64x2048x2048x374): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x374): 65.882

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 442.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x375x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x375x2048): 62.974
Elapsed time for attention_prob_times_values (64x2048x2048x375): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x375): 63.634

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 434.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x376x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x376x2048): 64.691
Elapsed time for attention_prob_times_values (64x2048x2048x376): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x376): 81.396

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 495.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x377x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x377x2048): 62.385
Elapsed time for attention_prob_times_values (64x2048x2048x377): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x377): 63.911

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 435.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x378x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x378x2048): 63.041
Elapsed time for attention_prob_times_values (64x2048x2048x378): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x378): 66.405

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 446.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x379x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x379x2048): 62.422
Elapsed time for attention_prob_times_values (64x2048x2048x379): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x379): 64.211

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 438.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x380x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x380x2048): 63.710
Elapsed time for attention_prob_times_values (64x2048x2048x380): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x380): 66.569

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 451.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x381x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x381x2048): 62.228
Elapsed time for attention_prob_times_values (64x2048x2048x381): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x381): 64.410

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 440.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x382x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x382x2048): 63.230
Elapsed time for attention_prob_times_values (64x2048x2048x382): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x382): 67.076

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 453.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x383x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x383x2048): 62.328
Elapsed time for attention_prob_times_values (64x2048x2048x383): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x383): 64.352

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 442.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x384x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x384x2048): 76.773
Elapsed time for attention_prob_times_values (64x2048x2048x384): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x384): 81.314

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 552.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x385x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x385x2048): 64.538
Elapsed time for attention_prob_times_values (64x2048x2048x385): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x385): 57.266

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 425.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x386x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x386x2048): 65.451
Elapsed time for attention_prob_times_values (64x2048x2048x386): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x386): 60.575

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 442.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x387x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x387x2048): 64.763
Elapsed time for attention_prob_times_values (64x2048x2048x387): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x387): 57.877

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 430.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x388x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x388x2048): 65.994
Elapsed time for attention_prob_times_values (64x2048x2048x388): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x388): 61.042

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 447.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x389x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x389x2048): 64.914
Elapsed time for attention_prob_times_values (64x2048x2048x389): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x389): 58.284

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 434.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x390x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x390x2048): 65.577
Elapsed time for attention_prob_times_values (64x2048x2048x390): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x390): 61.203

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 449.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x391x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x391x2048): 64.782
Elapsed time for attention_prob_times_values (64x2048x2048x391): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x391): 58.560

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 437.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x392x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x392x2048): 66.007
Elapsed time for attention_prob_times_values (64x2048x2048x392): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x392): 72.567

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 492.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x393x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x393x2048): 63.827
Elapsed time for attention_prob_times_values (64x2048x2048x393): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x393): 58.537

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 436.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x394x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x394x2048): 64.482
Elapsed time for attention_prob_times_values (64x2048x2048x394): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x394): 61.680

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 451.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x395x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x395x2048): 64.119
Elapsed time for attention_prob_times_values (64x2048x2048x395): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x395): 58.803

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 439.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x396x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x396x2048): 65.131
Elapsed time for attention_prob_times_values (64x2048x2048x396): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x396): 62.297

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 457.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x397x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x397x2048): 64.365
Elapsed time for attention_prob_times_values (64x2048x2048x397): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x397): 59.129

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 443.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x398x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x398x2048): 65.094
Elapsed time for attention_prob_times_values (64x2048x2048x398): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x398): 62.233

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 459.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x399x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x399x2048): 64.443
Elapsed time for attention_prob_times_values (64x2048x2048x399): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x399): 59.336

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 446.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x400x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x400x2048): 66.229
Elapsed time for attention_prob_times_values (64x2048x2048x400): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x400): 73.654

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 505.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x401x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x401x2048): 63.774
Elapsed time for attention_prob_times_values (64x2048x2048x401): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x401): 59.594

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 447.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x402x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x402x2048): 64.525
Elapsed time for attention_prob_times_values (64x2048x2048x402): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x402): 62.671

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 462.976
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x403x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x403x2048): 64.155
Elapsed time for attention_prob_times_values (64x2048x2048x403): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x403): 59.828

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 451.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x404x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x404x2048): 65.120
Elapsed time for attention_prob_times_values (64x2048x2048x404): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x404): 63.246

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 469.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x405x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x405x2048): 64.342
Elapsed time for attention_prob_times_values (64x2048x2048x405): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x405): 60.090

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 455.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x406x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x406x2048): 65.130
Elapsed time for attention_prob_times_values (64x2048x2048x406): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x406): 63.234

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 471.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x407x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x407x2048): 64.632
Elapsed time for attention_prob_times_values (64x2048x2048x407): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x407): 60.335

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 459.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x408x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x408x2048): 66.177
Elapsed time for attention_prob_times_values (64x2048x2048x408): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x408): 75.504

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 520.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x409x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x409x2048): 64.100
Elapsed time for attention_prob_times_values (64x2048x2048x409): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x409): 60.571

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 460.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x410x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x410x2048): 64.802
Elapsed time for attention_prob_times_values (64x2048x2048x410): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x410): 63.654

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 475.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x411x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x411x2048): 64.301
Elapsed time for attention_prob_times_values (64x2048x2048x411): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x411): 60.863

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 464.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x412x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x412x2048): 65.431
Elapsed time for attention_prob_times_values (64x2048x2048x412): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x412): 64.221

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 482.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x413x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x413x2048): 64.400
Elapsed time for attention_prob_times_values (64x2048x2048x413): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x413): 61.083

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 467.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x414x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x414x2048): 64.933
Elapsed time for attention_prob_times_values (64x2048x2048x414): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x414): 64.240

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 482.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x415x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x415x2048): 64.493
Elapsed time for attention_prob_times_values (64x2048x2048x415): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x415): 61.339

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 470.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x416x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x416x2048): 82.041
Elapsed time for attention_prob_times_values (64x2048x2048x416): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x416): 75.423

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 589.447
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x417x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x417x2048): 67.462
Elapsed time for attention_prob_times_values (64x2048x2048x417): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x417): 61.939

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 485.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x418x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x418x2048): 68.440
Elapsed time for attention_prob_times_values (64x2048x2048x418): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x418): 64.758

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 501.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x419x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x419x2048): 67.465
Elapsed time for attention_prob_times_values (64x2048x2048x419): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x419): 62.150

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 488.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x420x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x420x2048): 69.084
Elapsed time for attention_prob_times_values (64x2048x2048x420): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x420): 65.301

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 507.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x421x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x421x2048): 67.349
Elapsed time for attention_prob_times_values (64x2048x2048x421): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x421): 62.373

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 490.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x422x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x422x2048): 68.041
Elapsed time for attention_prob_times_values (64x2048x2048x422): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x422): 65.252

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 505.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x423x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x423x2048): 67.007
Elapsed time for attention_prob_times_values (64x2048x2048x423): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x423): 62.760

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 493.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x424x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x424x2048): 68.956
Elapsed time for attention_prob_times_values (64x2048x2048x424): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x424): 78.042

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 558.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x425x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x425x2048): 66.156
Elapsed time for attention_prob_times_values (64x2048x2048x425): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x425): 62.751

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 492.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x426x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x426x2048): 66.811
Elapsed time for attention_prob_times_values (64x2048x2048x426): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x426): 65.838

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 507.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x427x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x427x2048): 66.367
Elapsed time for attention_prob_times_values (64x2048x2048x427): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x427): 63.030

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 496.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x428x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x428x2048): 67.991
Elapsed time for attention_prob_times_values (64x2048x2048x428): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x428): 66.366

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 516.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x429x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x429x2048): 66.747
Elapsed time for attention_prob_times_values (64x2048x2048x429): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x429): 63.375

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 500.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x430x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x430x2048): 67.508
Elapsed time for attention_prob_times_values (64x2048x2048x430): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x430): 66.402

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 516.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x431x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x431x2048): 66.781
Elapsed time for attention_prob_times_values (64x2048x2048x431): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x431): 63.666

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 504.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x432x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x432x2048): 68.414
Elapsed time for attention_prob_times_values (64x2048x2048x432): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x432): 78.787

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 567.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x433x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x433x2048): 66.023
Elapsed time for attention_prob_times_values (64x2048x2048x433): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x433): 64.016

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 504.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x434x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x434x2048): 66.832
Elapsed time for attention_prob_times_values (64x2048x2048x434): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x434): 67.020

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 520.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x435x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x435x2048): 66.387
Elapsed time for attention_prob_times_values (64x2048x2048x435): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x435): 64.202

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 508.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x436x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x436x2048): 67.892
Elapsed time for attention_prob_times_values (64x2048x2048x436): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x436): 67.458

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 528.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 6992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x437x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x437x2048): 66.674
Elapsed time for attention_prob_times_values (64x2048x2048x437): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x437): 64.603

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 513.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x438x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x438x2048): 67.218
Elapsed time for attention_prob_times_values (64x2048x2048x438): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x438): 67.471

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 528.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x439x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x439x2048): 66.757
Elapsed time for attention_prob_times_values (64x2048x2048x439): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x439): 64.784

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 516.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x440x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x440x2048): 68.572
Elapsed time for attention_prob_times_values (64x2048x2048x440): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x440): 80.868

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 584.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x441x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x441x2048): 66.040
Elapsed time for attention_prob_times_values (64x2048x2048x441): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x441): 65.054

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 517.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x442x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x442x2048): 66.678
Elapsed time for attention_prob_times_values (64x2048x2048x442): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x442): 68.036

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 532.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x443x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x443x2048): 66.317
Elapsed time for attention_prob_times_values (64x2048x2048x443): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x443): 65.413

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 521.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x444x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x444x2048): 67.728
Elapsed time for attention_prob_times_values (64x2048x2048x444): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x444): 68.255

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 539.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x445x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x445x2048): 66.511
Elapsed time for attention_prob_times_values (64x2048x2048x445): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x445): 65.670

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 525.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x446x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x446x2048): 67.204
Elapsed time for attention_prob_times_values (64x2048x2048x446): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x446): 68.656

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 541.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x447x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x447x2048): 66.549
Elapsed time for attention_prob_times_values (64x2048x2048x447): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x447): 65.865

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 528.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x448x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x448x2048): 82.527
Elapsed time for attention_prob_times_values (64x2048x2048x448): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x448): 80.685

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 652.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x449x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x449x2048): 68.897
Elapsed time for attention_prob_times_values (64x2048x2048x449): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x449): 60.193

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 515.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x450x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x450x2048): 69.980
Elapsed time for attention_prob_times_values (64x2048x2048x450): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x450): 62.732

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 531.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x451x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x451x2048): 68.870
Elapsed time for attention_prob_times_values (64x2048x2048x451): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x451): 60.196

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 516.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x452x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x452x2048): 70.677
Elapsed time for attention_prob_times_values (64x2048x2048x452): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x452): 63.030

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 537.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x453x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x453x2048): 68.765
Elapsed time for attention_prob_times_values (64x2048x2048x453): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x453): 60.341

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 519.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x454x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x454x2048): 69.517
Elapsed time for attention_prob_times_values (64x2048x2048x454): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x454): 63.106

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 535.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x455x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x455x2048): 68.538
Elapsed time for attention_prob_times_values (64x2048x2048x455): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x455): 60.500

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 521.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x456x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x456x2048): 70.713
Elapsed time for attention_prob_times_values (64x2048x2048x456): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x456): 80.415

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 611.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x457x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x457x2048): 67.339
Elapsed time for attention_prob_times_values (64x2048x2048x457): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x457): 60.498

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 518.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x458x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x458x2048): 68.318
Elapsed time for attention_prob_times_values (64x2048x2048x458): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x458): 63.505

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 536.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x459x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x459x2048): 67.816
Elapsed time for attention_prob_times_values (64x2048x2048x459): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x459): 60.770

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 523.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x460x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x460x2048): 69.425
Elapsed time for attention_prob_times_values (64x2048x2048x460): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x460): 64.051

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 545.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x461x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x461x2048): 67.942
Elapsed time for attention_prob_times_values (64x2048x2048x461): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x461): 61.182

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 528.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x462x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x462x2048): 68.741
Elapsed time for attention_prob_times_values (64x2048x2048x462): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x462): 64.147

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 545.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x463x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x463x2048): 68.032
Elapsed time for attention_prob_times_values (64x2048x2048x463): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x463): 61.292

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 531.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x464x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x464x2048): 69.702
Elapsed time for attention_prob_times_values (64x2048x2048x464): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x464): 82.154

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 622.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x465x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x465x2048): 67.250
Elapsed time for attention_prob_times_values (64x2048x2048x465): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x465): 61.525

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 531.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x466x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x466x2048): 68.126
Elapsed time for attention_prob_times_values (64x2048x2048x466): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x466): 64.591

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 549.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x467x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x467x2048): 67.574
Elapsed time for attention_prob_times_values (64x2048x2048x467): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x467): 61.764

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 535.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x468x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x468x2048): 68.969
Elapsed time for attention_prob_times_values (64x2048x2048x468): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x468): 65.116

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 556.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x469x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x469x2048): 67.651
Elapsed time for attention_prob_times_values (64x2048x2048x469): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x469): 61.912

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 538.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x470x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x470x2048): 68.355
Elapsed time for attention_prob_times_values (64x2048x2048x470): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x470): 65.117

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 556.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x471x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x471x2048): 67.965
Elapsed time for attention_prob_times_values (64x2048x2048x471): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x471): 62.282

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 543.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x472x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x472x2048): 69.768
Elapsed time for attention_prob_times_values (64x2048x2048x472): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x472): 83.211

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 635.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x473x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x473x2048): 67.258
Elapsed time for attention_prob_times_values (64x2048x2048x473): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x473): 62.422

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 543.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x474x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x474x2048): 67.829
Elapsed time for attention_prob_times_values (64x2048x2048x474): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x474): 65.353

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 559.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x475x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x475x2048): 67.334
Elapsed time for attention_prob_times_values (64x2048x2048x475): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x475): 62.642

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 546.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x476x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x476x2048): 68.797
Elapsed time for attention_prob_times_values (64x2048x2048x476): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x476): 65.969

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 568.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x477x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x477x2048): 67.520
Elapsed time for attention_prob_times_values (64x2048x2048x477): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x477): 62.835

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 550.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x478x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x478x2048): 68.159
Elapsed time for attention_prob_times_values (64x2048x2048x478): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x478): 65.804

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 567.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x479x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x479x2048): 67.642
Elapsed time for attention_prob_times_values (64x2048x2048x479): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x479): 62.970

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 553.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x480x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x480x2048): 84.160
Elapsed time for attention_prob_times_values (64x2048x2048x480): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x480): 85.566

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 721.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x481x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x481x2048): 69.721
Elapsed time for attention_prob_times_values (64x2048x2048x481): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x481): 63.407

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 565.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x482x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x482x2048): 70.565
Elapsed time for attention_prob_times_values (64x2048x2048x482): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x482): 66.331

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 583.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x483x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x483x2048): 69.483
Elapsed time for attention_prob_times_values (64x2048x2048x483): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x483): 63.607

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 567.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x484x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x484x2048): 71.085
Elapsed time for attention_prob_times_values (64x2048x2048x484): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x484): 64.842

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 580.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x485x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x485x2048): 69.218
Elapsed time for attention_prob_times_values (64x2048x2048x485): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x485): 63.699

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 569.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x486x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x486x2048): 70.157
Elapsed time for attention_prob_times_values (64x2048x2048x486): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x486): 66.354

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 586.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x487x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x487x2048): 68.983
Elapsed time for attention_prob_times_values (64x2048x2048x487): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x487): 64.187

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 572.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x488x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x488x2048): 70.932
Elapsed time for attention_prob_times_values (64x2048x2048x488): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x488): 85.930

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 670.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x489x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x489x2048): 68.122
Elapsed time for attention_prob_times_values (64x2048x2048x489): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x489): 64.056

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 570.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x490x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x490x2048): 68.927
Elapsed time for attention_prob_times_values (64x2048x2048x490): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x490): 66.682

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 586.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x491x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x491x2048): 68.494
Elapsed time for attention_prob_times_values (64x2048x2048x491): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x491): 64.132

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 574.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x492x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x492x2048): 69.683
Elapsed time for attention_prob_times_values (64x2048x2048x492): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x492): 65.377

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 586.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x493x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x493x2048): 68.531
Elapsed time for attention_prob_times_values (64x2048x2048x493): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x493): 64.475

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 578.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x494x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x494x2048): 69.390
Elapsed time for attention_prob_times_values (64x2048x2048x494): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x494): 67.076

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 594.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x495x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x495x2048): 68.547
Elapsed time for attention_prob_times_values (64x2048x2048x495): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x495): 64.482

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 580.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x496x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x496x2048): 70.842
Elapsed time for attention_prob_times_values (64x2048x2048x496): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x496): 87.648

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 685.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x497x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x497x2048): 67.917
Elapsed time for attention_prob_times_values (64x2048x2048x497): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x497): 64.713

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 580.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x498x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x498x2048): 68.988
Elapsed time for attention_prob_times_values (64x2048x2048x498): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x498): 65.671

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 590.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 7984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x499x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x499x2048): 68.137
Elapsed time for attention_prob_times_values (64x2048x2048x499): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x499): 64.988

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 585.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x500x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x500x2048): 69.431
Elapsed time for attention_prob_times_values (64x2048x2048x500): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x500): 66.148

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 597.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x501x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x501x2048): 68.331
Elapsed time for attention_prob_times_values (64x2048x2048x501): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x501): 65.109

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 588.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x502x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x502x2048): 69.210
Elapsed time for attention_prob_times_values (64x2048x2048x502): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x502): 66.121

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 598.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x503x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x503x2048): 68.364
Elapsed time for attention_prob_times_values (64x2048x2048x503): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x503): 63.330

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 582.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x504x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x504x2048): 70.082
Elapsed time for attention_prob_times_values (64x2048x2048x504): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x504): 88.651

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 694.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x505x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x505x2048): 67.801
Elapsed time for attention_prob_times_values (64x2048x2048x505): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x505): 60.532

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 568.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x506x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x506x2048): 68.552
Elapsed time for attention_prob_times_values (64x2048x2048x506): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x506): 66.295

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 600.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x507x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x507x2048): 67.972
Elapsed time for attention_prob_times_values (64x2048x2048x507): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x507): 62.054

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 578.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x508x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x508x2048): 69.282
Elapsed time for attention_prob_times_values (64x2048x2048x508): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x508): 66.345

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 605.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x509x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x509x2048): 67.973
Elapsed time for attention_prob_times_values (64x2048x2048x509): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x509): 62.234

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 581.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x510x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x510x2048): 68.497
Elapsed time for attention_prob_times_values (64x2048x2048x510): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x510): 67.713

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 610.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x511x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x511x2048): 67.982
Elapsed time for attention_prob_times_values (64x2048x2048x511): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x511): 63.615

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 590.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x512x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x512x2048): 81.192
Elapsed time for attention_prob_times_values (64x2048x2048x512): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x512): 92.530

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 778.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x513x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x513x2048): 70.104
Elapsed time for attention_prob_times_values (64x2048x2048x513): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x513): 59.288

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 579.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x514x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x514x2048): 74.265
Elapsed time for attention_prob_times_values (64x2048x2048x514): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x514): 63.661

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 619.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x515x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x515x2048): 69.936
Elapsed time for attention_prob_times_values (64x2048x2048x515): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x515): 59.523

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 581.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x516x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x516x2048): 71.429
Elapsed time for attention_prob_times_values (64x2048x2048x516): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x516): 63.674

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 610.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x517x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x517x2048): 70.136
Elapsed time for attention_prob_times_values (64x2048x2048x517): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x517): 60.402

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 589.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x518x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x518x2048): 70.921
Elapsed time for attention_prob_times_values (64x2048x2048x518): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x518): 63.771

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 610.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x519x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x519x2048): 69.960
Elapsed time for attention_prob_times_values (64x2048x2048x519): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x519): 60.932

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 593.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x520x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x520x2048): 71.264
Elapsed time for attention_prob_times_values (64x2048x2048x520): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x520): 74.185

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 663.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x521x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x521x2048): 69.202
Elapsed time for attention_prob_times_values (64x2048x2048x521): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x521): 60.016

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 587.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x522x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x522x2048): 69.892
Elapsed time for attention_prob_times_values (64x2048x2048x522): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x522): 64.298

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 613.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x523x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x523x2048): 69.489
Elapsed time for attention_prob_times_values (64x2048x2048x523): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x523): 60.225

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 591.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x524x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x524x2048): 70.411
Elapsed time for attention_prob_times_values (64x2048x2048x524): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x524): 64.754

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 619.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x525x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x525x2048): 69.643
Elapsed time for attention_prob_times_values (64x2048x2048x525): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x525): 60.375

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 595.245
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x526x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x526x2048): 70.281
Elapsed time for attention_prob_times_values (64x2048x2048x526): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x526): 64.759

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 621.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x527x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x527x2048): 69.707
Elapsed time for attention_prob_times_values (64x2048x2048x527): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x527): 60.791

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 599.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x528x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x528x2048): 71.430
Elapsed time for attention_prob_times_values (64x2048x2048x528): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x528): 75.599

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 679.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x529x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x529x2048): 69.127
Elapsed time for attention_prob_times_values (64x2048x2048x529): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x529): 60.670

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 598.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x530x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x530x2048): 69.809
Elapsed time for attention_prob_times_values (64x2048x2048x530): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x530): 64.742

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 623.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x531x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x531x2048): 69.324
Elapsed time for attention_prob_times_values (64x2048x2048x531): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x531): 60.847

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 602.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x532x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x532x2048): 70.307
Elapsed time for attention_prob_times_values (64x2048x2048x532): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x532): 65.018

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 629.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x533x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x533x2048): 69.585
Elapsed time for attention_prob_times_values (64x2048x2048x533): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x533): 61.002

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 606.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x534x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x534x2048): 70.204
Elapsed time for attention_prob_times_values (64x2048x2048x534): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x534): 64.980

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 630.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x535x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x535x2048): 69.919
Elapsed time for attention_prob_times_values (64x2048x2048x535): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x535): 61.277

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 611.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x536x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x536x2048): 71.111
Elapsed time for attention_prob_times_values (64x2048x2048x536): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x536): 76.376

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 690.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x537x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x537x2048): 68.972
Elapsed time for attention_prob_times_values (64x2048x2048x537): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x537): 61.400

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 610.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x538x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x538x2048): 69.661
Elapsed time for attention_prob_times_values (64x2048x2048x538): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x538): 64.423

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 629.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x539x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x539x2048): 69.284
Elapsed time for attention_prob_times_values (64x2048x2048x539): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x539): 61.616

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 614.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x540x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x540x2048): 70.434
Elapsed time for attention_prob_times_values (64x2048x2048x540): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x540): 65.470

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 640.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x541x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x541x2048): 69.316
Elapsed time for attention_prob_times_values (64x2048x2048x541): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x541): 61.789

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 617.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x542x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x542x2048): 70.005
Elapsed time for attention_prob_times_values (64x2048x2048x542): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x542): 64.778

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 637.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x543x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x543x2048): 69.588
Elapsed time for attention_prob_times_values (64x2048x2048x543): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x543): 61.910

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 621.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x544x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x544x2048): 85.525
Elapsed time for attention_prob_times_values (64x2048x2048x544): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x544): 78.318

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 776.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x545x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x545x2048): 72.029
Elapsed time for attention_prob_times_values (64x2048x2048x545): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x545): 62.264

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 635.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x546x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x546x2048): 72.880
Elapsed time for attention_prob_times_values (64x2048x2048x546): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x546): 65.038

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 655.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x547x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x547x2048): 71.888
Elapsed time for attention_prob_times_values (64x2048x2048x547): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x547): 62.445

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 638.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x548x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x548x2048): 73.504
Elapsed time for attention_prob_times_values (64x2048x2048x548): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x548): 65.578

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 662.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x549x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x549x2048): 71.820
Elapsed time for attention_prob_times_values (64x2048x2048x549): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x549): 62.839

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 642.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x550x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x550x2048): 72.452
Elapsed time for attention_prob_times_values (64x2048x2048x550): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x550): 65.490

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 660.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x551x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x551x2048): 71.484
Elapsed time for attention_prob_times_values (64x2048x2048x551): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x551): 62.979

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 643.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x552x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x552x2048): 73.441
Elapsed time for attention_prob_times_values (64x2048x2048x552): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x552): 78.507

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 730.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x553x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x553x2048): 70.753
Elapsed time for attention_prob_times_values (64x2048x2048x553): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x553): 63.079

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 642.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x554x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x554x2048): 71.494
Elapsed time for attention_prob_times_values (64x2048x2048x554): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x554): 66.044

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 663.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x555x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x555x2048): 70.779
Elapsed time for attention_prob_times_values (64x2048x2048x555): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x555): 63.243

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 646.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x556x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x556x2048): 72.521
Elapsed time for attention_prob_times_values (64x2048x2048x556): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x556): 67.377

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 676.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x557x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x557x2048): 71.111
Elapsed time for attention_prob_times_values (64x2048x2048x557): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x557): 63.865

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 652.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x558x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x558x2048): 72.171
Elapsed time for attention_prob_times_values (64x2048x2048x558): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x558): 67.663

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 678.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x559x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x559x2048): 71.104
Elapsed time for attention_prob_times_values (64x2048x2048x559): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x559): 64.248

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 657.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x560x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x560x2048): 73.132
Elapsed time for attention_prob_times_values (64x2048x2048x560): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x560): 80.008

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 745.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x561x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x561x2048): 70.330
Elapsed time for attention_prob_times_values (64x2048x2048x561): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x561): 64.406

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 656.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 8992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x562x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x562x2048): 71.280
Elapsed time for attention_prob_times_values (64x2048x2048x562): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x562): 68.170

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 681.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x563x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x563x2048): 70.702
Elapsed time for attention_prob_times_values (64x2048x2048x563): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x563): 64.419

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 660.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x564x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x564x2048): 72.271
Elapsed time for attention_prob_times_values (64x2048x2048x564): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x564): 68.828

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 691.854
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x565x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x565x2048): 70.878
Elapsed time for attention_prob_times_values (64x2048x2048x565): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x565): 64.793

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 665.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x566x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x566x2048): 71.602
Elapsed time for attention_prob_times_values (64x2048x2048x566): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x566): 68.618

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 689.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x567x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x567x2048): 71.030
Elapsed time for attention_prob_times_values (64x2048x2048x567): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x567): 64.837

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 668.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x568x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x568x2048): 72.954
Elapsed time for attention_prob_times_values (64x2048x2048x568): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x568): 80.797

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 757.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x569x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x569x2048): 70.369
Elapsed time for attention_prob_times_values (64x2048x2048x569): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x569): 65.089

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 668.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x570x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x570x2048): 70.956
Elapsed time for attention_prob_times_values (64x2048x2048x570): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x570): 67.890

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 687.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x571x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x571x2048): 70.635
Elapsed time for attention_prob_times_values (64x2048x2048x571): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x571): 65.261

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 673.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x572x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x572x2048): 72.179
Elapsed time for attention_prob_times_values (64x2048x2048x572): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x572): 68.074

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 696.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x573x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x573x2048): 70.739
Elapsed time for attention_prob_times_values (64x2048x2048x573): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x573): 65.526

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 677.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x574x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x574x2048): 71.367
Elapsed time for attention_prob_times_values (64x2048x2048x574): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x574): 68.471

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 696.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x575x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x575x2048): 70.842
Elapsed time for attention_prob_times_values (64x2048x2048x575): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x575): 65.638

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 680.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x576x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x576x2048): 85.737
Elapsed time for attention_prob_times_values (64x2048x2048x576): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x576): 83.534

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 846.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x577x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x577x2048): 72.871
Elapsed time for attention_prob_times_values (64x2048x2048x577): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x577): 61.371

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 667.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x578x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x578x2048): 73.867
Elapsed time for attention_prob_times_values (64x2048x2048x578): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x578): 64.014

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 688.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x579x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x579x2048): 72.817
Elapsed time for attention_prob_times_values (64x2048x2048x579): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x579): 61.491

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 669.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x580x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x580x2048): 74.542
Elapsed time for attention_prob_times_values (64x2048x2048x580): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x580): 63.675

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 691.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x581x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x581x2048): 72.550
Elapsed time for attention_prob_times_values (64x2048x2048x581): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x581): 61.616

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 671.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x582x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x582x2048): 73.507
Elapsed time for attention_prob_times_values (64x2048x2048x582): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x582): 64.117

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 691.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x583x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x583x2048): 74.919
Elapsed time for attention_prob_times_values (64x2048x2048x583): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x583): 62.021

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 686.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x584x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x584x2048): 74.676
Elapsed time for attention_prob_times_values (64x2048x2048x584): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x584): 83.168

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 796.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x585x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x585x2048): 71.659
Elapsed time for attention_prob_times_values (64x2048x2048x585): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x585): 61.516

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 671.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x586x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x586x2048): 72.443
Elapsed time for attention_prob_times_values (64x2048x2048x586): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x586): 64.523

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 693.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x587x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x587x2048): 71.970
Elapsed time for attention_prob_times_values (64x2048x2048x587): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x587): 61.908

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 677.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x588x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x588x2048): 74.549
Elapsed time for attention_prob_times_values (64x2048x2048x588): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x588): 64.953

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 707.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x589x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x589x2048): 71.988
Elapsed time for attention_prob_times_values (64x2048x2048x589): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x589): 62.175

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 680.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x590x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x590x2048): 72.786
Elapsed time for attention_prob_times_values (64x2048x2048x590): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x590): 64.754

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 700.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x591x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x591x2048): 71.870
Elapsed time for attention_prob_times_values (64x2048x2048x591): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x591): 62.284

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 682.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x592x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x592x2048): 73.699
Elapsed time for attention_prob_times_values (64x2048x2048x592): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x592): 84.623

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 807.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x593x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x593x2048): 71.259
Elapsed time for attention_prob_times_values (64x2048x2048x593): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x593): 62.421

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 683.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x594x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x594x2048): 72.165
Elapsed time for attention_prob_times_values (64x2048x2048x594): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x594): 65.025

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 703.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x595x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x595x2048): 71.457
Elapsed time for attention_prob_times_values (64x2048x2048x595): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x595): 62.873

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 688.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x596x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x596x2048): 72.915
Elapsed time for attention_prob_times_values (64x2048x2048x596): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x596): 65.598

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 712.215
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x597x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x597x2048): 71.560
Elapsed time for attention_prob_times_values (64x2048x2048x597): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x597): 63.069

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 692.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x598x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x598x2048): 72.292
Elapsed time for attention_prob_times_values (64x2048x2048x598): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x598): 64.933

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 707.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x599x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x599x2048): 71.914
Elapsed time for attention_prob_times_values (64x2048x2048x599): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x599): 63.227

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 697.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x600x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x600x2048): 73.648
Elapsed time for attention_prob_times_values (64x2048x2048x600): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x600): 85.484

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 820.934
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x601x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x601x2048): 71.082
Elapsed time for attention_prob_times_values (64x2048x2048x601): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x601): 63.495

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 696.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x602x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x602x2048): 71.895
Elapsed time for attention_prob_times_values (64x2048x2048x602): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x602): 65.655

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 714.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x603x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x603x2048): 71.367
Elapsed time for attention_prob_times_values (64x2048x2048x603): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x603): 63.626

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 701.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x604x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x604x2048): 72.849
Elapsed time for attention_prob_times_values (64x2048x2048x604): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x604): 66.294

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 724.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x605x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x605x2048): 71.428
Elapsed time for attention_prob_times_values (64x2048x2048x605): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x605): 63.828

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 704.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x606x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x606x2048): 72.227
Elapsed time for attention_prob_times_values (64x2048x2048x606): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x606): 66.088

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 722.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x607x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x607x2048): 71.503
Elapsed time for attention_prob_times_values (64x2048x2048x607): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x607): 63.901

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 707.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x608x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x608x2048): 86.943
Elapsed time for attention_prob_times_values (64x2048x2048x608): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x608): 87.611

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 916.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x609x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x609x2048): 73.267
Elapsed time for attention_prob_times_values (64x2048x2048x609): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x609): 64.325

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 720.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x610x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x610x2048): 74.168
Elapsed time for attention_prob_times_values (64x2048x2048x610): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x610): 66.045

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 735.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x611x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x611x2048): 73.075
Elapsed time for attention_prob_times_values (64x2048x2048x611): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x611): 64.421

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 722.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x612x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x612x2048): 74.492
Elapsed time for attention_prob_times_values (64x2048x2048x612): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x612): 66.185

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 740.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x613x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x613x2048): 72.800
Elapsed time for attention_prob_times_values (64x2048x2048x613): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x613): 64.705

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 724.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x614x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x614x2048): 73.653
Elapsed time for attention_prob_times_values (64x2048x2048x614): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x614): 65.936

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 737.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x615x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x615x2048): 72.650
Elapsed time for attention_prob_times_values (64x2048x2048x615): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x615): 64.962

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 727.710
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x616x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x616x2048): 74.444
Elapsed time for attention_prob_times_values (64x2048x2048x616): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x616): 87.647

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 855.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x617x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x617x2048): 71.842
Elapsed time for attention_prob_times_values (64x2048x2048x617): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x617): 64.667

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 724.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x618x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x618x2048): 72.608
Elapsed time for attention_prob_times_values (64x2048x2048x618): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x618): 66.591

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 740.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x619x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x619x2048): 72.083
Elapsed time for attention_prob_times_values (64x2048x2048x619): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x619): 64.937

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 729.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x620x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x620x2048): 73.302
Elapsed time for attention_prob_times_values (64x2048x2048x620): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x620): 67.638

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 751.933
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x621x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x621x2048): 72.260
Elapsed time for attention_prob_times_values (64x2048x2048x621): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x621): 65.357

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 734.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x622x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x622x2048): 73.071
Elapsed time for attention_prob_times_values (64x2048x2048x622): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x622): 66.906

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 748.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x623x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x623x2048): 72.268
Elapsed time for attention_prob_times_values (64x2048x2048x623): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x623): 65.533

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 737.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x624x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x624x2048): 74.381
Elapsed time for attention_prob_times_values (64x2048x2048x624): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x624): 89.163

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 871.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x625x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x625x2048): 71.644
Elapsed time for attention_prob_times_values (64x2048x2048x625): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x625): 65.805

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 738.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x626x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x626x2048): 72.601
Elapsed time for attention_prob_times_values (64x2048x2048x626): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x626): 67.260

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 752.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x627x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x627x2048): 71.829
Elapsed time for attention_prob_times_values (64x2048x2048x627): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x627): 65.745

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 741.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x628x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x628x2048): 73.244
Elapsed time for attention_prob_times_values (64x2048x2048x628): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x628): 67.594

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 760.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x629x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x629x2048): 71.996
Elapsed time for attention_prob_times_values (64x2048x2048x629): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x629): 65.419

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 742.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x630x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x630x2048): 72.885
Elapsed time for attention_prob_times_values (64x2048x2048x630): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x630): 67.806

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 761.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x631x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x631x2048): 72.244
Elapsed time for attention_prob_times_values (64x2048x2048x631): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x631): 65.548

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 746.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x632x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x632x2048): 73.849
Elapsed time for attention_prob_times_values (64x2048x2048x632): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x632): 90.038

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 882.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x633x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x633x2048): 71.566
Elapsed time for attention_prob_times_values (64x2048x2048x633): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x633): 65.589

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 745.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x634x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x634x2048): 72.260
Elapsed time for attention_prob_times_values (64x2048x2048x634): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x634): 67.786

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 762.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x635x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x635x2048): 71.502
Elapsed time for attention_prob_times_values (64x2048x2048x635): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x635): 66.575

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 753.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x636x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x636x2048): 72.738
Elapsed time for attention_prob_times_values (64x2048x2048x636): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x636): 67.787

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 767.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x637x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x637x2048): 71.423
Elapsed time for attention_prob_times_values (64x2048x2048x637): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x637): 66.447

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 754.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x638x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x638x2048): 72.335
Elapsed time for attention_prob_times_values (64x2048x2048x638): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x638): 68.282

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 770.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x639x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x639x2048): 71.427
Elapsed time for attention_prob_times_values (64x2048x2048x639): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x639): 65.521

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 750.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x640x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x640x2048): 83.308
Elapsed time for attention_prob_times_values (64x2048x2048x640): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x640): 93.014

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 966.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x641x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x641x2048): 73.333
Elapsed time for attention_prob_times_values (64x2048x2048x641): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x641): 60.469

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 730.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x642x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x642x2048): 74.357
Elapsed time for attention_prob_times_values (64x2048x2048x642): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x642): 63.922

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 758.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x643x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x643x2048): 73.123
Elapsed time for attention_prob_times_values (64x2048x2048x643): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x643): 61.186

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 735.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x644x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x644x2048): 74.886
Elapsed time for attention_prob_times_values (64x2048x2048x644): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x644): 63.802

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 762.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x645x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x645x2048): 73.626
Elapsed time for attention_prob_times_values (64x2048x2048x645): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x645): 61.650

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 743.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x646x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x646x2048): 74.297
Elapsed time for attention_prob_times_values (64x2048x2048x646): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x646): 63.995

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 762.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x647x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x647x2048): 73.474
Elapsed time for attention_prob_times_values (64x2048x2048x647): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x647): 61.476

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 743.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x648x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x648x2048): 74.856
Elapsed time for attention_prob_times_values (64x2048x2048x648): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x648): 77.107

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 845.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x649x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x649x2048): 72.664
Elapsed time for attention_prob_times_values (64x2048x2048x649): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x649): 61.794

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 744.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x650x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x650x2048): 73.293
Elapsed time for attention_prob_times_values (64x2048x2048x650): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x650): 64.320

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 764.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x651x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x651x2048): 72.844
Elapsed time for attention_prob_times_values (64x2048x2048x651): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x651): 61.812

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 747.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x652x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x652x2048): 73.908
Elapsed time for attention_prob_times_values (64x2048x2048x652): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x652): 64.972

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 773.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x653x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x653x2048): 73.061
Elapsed time for attention_prob_times_values (64x2048x2048x653): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x653): 62.256

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 753.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x654x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x654x2048): 73.696
Elapsed time for attention_prob_times_values (64x2048x2048x654): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x654): 65.220

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 776.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x655x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x655x2048): 73.024
Elapsed time for attention_prob_times_values (64x2048x2048x655): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x655): 62.373

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 755.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x656x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x656x2048): 74.900
Elapsed time for attention_prob_times_values (64x2048x2048x656): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x656): 78.307

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 861.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x657x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x657x2048): 72.496
Elapsed time for attention_prob_times_values (64x2048x2048x657): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x657): 62.323

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 755.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x658x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x658x2048): 73.314
Elapsed time for attention_prob_times_values (64x2048x2048x658): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x658): 64.870

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 776.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x659x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x659x2048): 72.607
Elapsed time for attention_prob_times_values (64x2048x2048x659): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x659): 62.597

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 759.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x660x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x660x2048): 73.822
Elapsed time for attention_prob_times_values (64x2048x2048x660): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x660): 65.178

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 783.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x661x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x661x2048): 72.884
Elapsed time for attention_prob_times_values (64x2048x2048x661): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x661): 62.825

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 764.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x662x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x662x2048): 73.505
Elapsed time for attention_prob_times_values (64x2048x2048x662): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x662): 65.287

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 784.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x663x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x663x2048): 73.012
Elapsed time for attention_prob_times_values (64x2048x2048x663): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x663): 62.914

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 767.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x664x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x664x2048): 74.536
Elapsed time for attention_prob_times_values (64x2048x2048x664): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x664): 78.910

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 872.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x665x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x665x2048): 72.445
Elapsed time for attention_prob_times_values (64x2048x2048x665): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x665): 63.032

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 767.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x666x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x666x2048): 73.170
Elapsed time for attention_prob_times_values (64x2048x2048x666): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x666): 65.329

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 787.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x667x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x667x2048): 72.565
Elapsed time for attention_prob_times_values (64x2048x2048x667): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x667): 62.867

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 769.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x668x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x668x2048): 73.790
Elapsed time for attention_prob_times_values (64x2048x2048x668): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x668): 65.740

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 795.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x669x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x669x2048): 72.726
Elapsed time for attention_prob_times_values (64x2048x2048x669): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x669): 63.280

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 775.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x670x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x670x2048): 73.388
Elapsed time for attention_prob_times_values (64x2048x2048x670): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x670): 65.819

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 795.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x671x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x671x2048): 72.863
Elapsed time for attention_prob_times_values (64x2048x2048x671): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x671): 63.480

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 779.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x672x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x672x2048): 87.901
Elapsed time for attention_prob_times_values (64x2048x2048x672): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x672): 80.735

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 967.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x673x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x673x2048): 74.760
Elapsed time for attention_prob_times_values (64x2048x2048x673): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x673): 63.822

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 792.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x674x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x674x2048): 75.757
Elapsed time for attention_prob_times_values (64x2048x2048x674): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x674): 65.762

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 811.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x675x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x675x2048): 74.776
Elapsed time for attention_prob_times_values (64x2048x2048x675): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x675): 63.907

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 795.758
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x676x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x676x2048): 76.413
Elapsed time for attention_prob_times_values (64x2048x2048x676): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x676): 66.206

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 820.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x677x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x677x2048): 74.740
Elapsed time for attention_prob_times_values (64x2048x2048x677): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x677): 63.801

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 797.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x678x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x678x2048): 75.524
Elapsed time for attention_prob_times_values (64x2048x2048x678): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x678): 66.009

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 816.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x679x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x679x2048): 74.712
Elapsed time for attention_prob_times_values (64x2048x2048x679): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x679): 63.832

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 799.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x680x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x680x2048): 80.265
Elapsed time for attention_prob_times_values (64x2048x2048x680): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x680): 80.671

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 935.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x681x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x681x2048): 74.006
Elapsed time for attention_prob_times_values (64x2048x2048x681): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x681): 63.722

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 797.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x682x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x682x2048): 74.655
Elapsed time for attention_prob_times_values (64x2048x2048x682): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x682): 66.244

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 818.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x683x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x683x2048): 74.134
Elapsed time for attention_prob_times_values (64x2048x2048x683): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x683): 63.545

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 798.732
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x684x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x684x2048): 75.743
Elapsed time for attention_prob_times_values (64x2048x2048x684): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x684): 66.659

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 828.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x685x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x685x2048): 74.401
Elapsed time for attention_prob_times_values (64x2048x2048x685): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x685): 64.145

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 806.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x686x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x686x2048): 75.210
Elapsed time for attention_prob_times_values (64x2048x2048x686): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x686): 66.773

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 828.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 10992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x687x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x687x2048): 75.210
Elapsed time for attention_prob_times_values (64x2048x2048x687): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x687): 64.439

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 814.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x688x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x688x2048): 76.152
Elapsed time for attention_prob_times_values (64x2048x2048x688): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x688): 81.957

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 927.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x689x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x689x2048): 73.639
Elapsed time for attention_prob_times_values (64x2048x2048x689): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x689): 64.425

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 808.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x690x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x690x2048): 74.474
Elapsed time for attention_prob_times_values (64x2048x2048x690): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x690): 67.183

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 832.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x691x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x691x2048): 73.800
Elapsed time for attention_prob_times_values (64x2048x2048x691): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x691): 64.591

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 812.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x692x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x692x2048): 75.299
Elapsed time for attention_prob_times_values (64x2048x2048x692): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x692): 67.474

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 840.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x693x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x693x2048): 74.048
Elapsed time for attention_prob_times_values (64x2048x2048x693): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x693): 64.880

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 818.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x694x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x694x2048): 74.662
Elapsed time for attention_prob_times_values (64x2048x2048x694): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x694): 67.447

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 839.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x695x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x695x2048): 74.016
Elapsed time for attention_prob_times_values (64x2048x2048x695): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x695): 64.927

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 820.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x696x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x696x2048): 75.903
Elapsed time for attention_prob_times_values (64x2048x2048x696): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x696): 82.578

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 939.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x697x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x697x2048): 73.376
Elapsed time for attention_prob_times_values (64x2048x2048x697): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x697): 65.271

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 821.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x698x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x698x2048): 74.153
Elapsed time for attention_prob_times_values (64x2048x2048x698): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x698): 67.777

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 843.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x699x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x699x2048): 73.569
Elapsed time for attention_prob_times_values (64x2048x2048x699): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x699): 65.455

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 825.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x700x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x700x2048): 75.279
Elapsed time for attention_prob_times_values (64x2048x2048x700): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x700): 67.878

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 852.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x701x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x701x2048): 73.673
Elapsed time for attention_prob_times_values (64x2048x2048x701): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x701): 65.695

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 830.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x702x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x702x2048): 74.610
Elapsed time for attention_prob_times_values (64x2048x2048x702): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x702): 68.145

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 852.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x703x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x703x2048): 73.735
Elapsed time for attention_prob_times_values (64x2048x2048x703): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x703): 65.814

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 833.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x704x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x704x2048): 87.449
Elapsed time for attention_prob_times_values (64x2048x2048x704): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x704): 85.188

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1035.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x705x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x705x2048): 75.629
Elapsed time for attention_prob_times_values (64x2048x2048x705): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x705): 62.577

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 822.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x706x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x706x2048): 76.634
Elapsed time for attention_prob_times_values (64x2048x2048x706): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x706): 65.227

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 847.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x707x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x707x2048): 75.620
Elapsed time for attention_prob_times_values (64x2048x2048x707): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x707): 62.656

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 825.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x708x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x708x2048): 77.279
Elapsed time for attention_prob_times_values (64x2048x2048x708): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x708): 64.272

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 846.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x709x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x709x2048): 75.454
Elapsed time for attention_prob_times_values (64x2048x2048x709): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x709): 62.707

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 827.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x710x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x710x2048): 76.230
Elapsed time for attention_prob_times_values (64x2048x2048x710): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x710): 65.518

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 852.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x711x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x711x2048): 75.109
Elapsed time for attention_prob_times_values (64x2048x2048x711): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x711): 62.808

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 828.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x712x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x712x2048): 77.144
Elapsed time for attention_prob_times_values (64x2048x2048x712): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x712): 84.348

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 977.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x713x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x713x2048): 74.330
Elapsed time for attention_prob_times_values (64x2048x2048x713): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x713): 61.369

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 816.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x714x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x714x2048): 75.225
Elapsed time for attention_prob_times_values (64x2048x2048x714): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x714): 65.833

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 853.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x715x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x715x2048): 74.625
Elapsed time for attention_prob_times_values (64x2048x2048x715): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x715): 61.614

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 821.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x716x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x716x2048): 76.109
Elapsed time for attention_prob_times_values (64x2048x2048x716): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x716): 66.164

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 862.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x717x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x717x2048): 74.675
Elapsed time for attention_prob_times_values (64x2048x2048x717): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x717): 61.570

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 823.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x718x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x718x2048): 75.565
Elapsed time for attention_prob_times_values (64x2048x2048x718): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x718): 66.083

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 861.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x719x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x719x2048): 74.700
Elapsed time for attention_prob_times_values (64x2048x2048x719): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x719): 62.834

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 835.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x720x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x720x2048): 76.784
Elapsed time for attention_prob_times_values (64x2048x2048x720): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x720): 85.566

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 991.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x721x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x721x2048): 74.026
Elapsed time for attention_prob_times_values (64x2048x2048x721): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x721): 62.164

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 828.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x722x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x722x2048): 74.822
Elapsed time for attention_prob_times_values (64x2048x2048x722): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x722): 64.957

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 854.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x723x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x723x2048): 74.401
Elapsed time for attention_prob_times_values (64x2048x2048x723): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x723): 63.351

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 841.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x724x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x724x2048): 75.863
Elapsed time for attention_prob_times_values (64x2048x2048x724): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x724): 65.033

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 862.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x725x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x725x2048): 74.451
Elapsed time for attention_prob_times_values (64x2048x2048x725): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x725): 62.004

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 834.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x726x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x726x2048): 75.188
Elapsed time for attention_prob_times_values (64x2048x2048x726): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x726): 64.938

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 860.209
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x727x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x727x2048): 74.513
Elapsed time for attention_prob_times_values (64x2048x2048x727): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x727): 62.490

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 840.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x728x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x728x2048): 76.396
Elapsed time for attention_prob_times_values (64x2048x2048x728): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x728): 86.266

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 1002.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x729x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x729x2048): 74.001
Elapsed time for attention_prob_times_values (64x2048x2048x729): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x729): 63.644

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 847.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x730x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x730x2048): 74.692
Elapsed time for attention_prob_times_values (64x2048x2048x730): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x730): 65.368

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 864.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x731x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x731x2048): 74.187
Elapsed time for attention_prob_times_values (64x2048x2048x731): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x731): 63.734

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 851.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x732x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x732x2048): 75.507
Elapsed time for attention_prob_times_values (64x2048x2048x732): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x732): 65.730

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 874.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x733x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x733x2048): 74.226
Elapsed time for attention_prob_times_values (64x2048x2048x733): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x733): 63.931

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 855.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x734x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x734x2048): 74.988
Elapsed time for attention_prob_times_values (64x2048x2048x734): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x734): 65.505

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 871.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x735x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x735x2048): 74.365
Elapsed time for attention_prob_times_values (64x2048x2048x735): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x735): 64.031

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 859.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x736x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x736x2048): 88.570
Elapsed time for attention_prob_times_values (64x2048x2048x736): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x736): 88.245

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1105.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x737x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x737x2048): 75.950
Elapsed time for attention_prob_times_values (64x2048x2048x737): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x737): 64.412

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 872.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x738x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x738x2048): 76.730
Elapsed time for attention_prob_times_values (64x2048x2048x738): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x738): 65.854

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 888.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x739x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x739x2048): 75.885
Elapsed time for attention_prob_times_values (64x2048x2048x739): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x739): 64.477

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 874.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x740x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x740x2048): 77.231
Elapsed time for attention_prob_times_values (64x2048x2048x740): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x740): 66.127

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 895.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x741x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x741x2048): 75.755
Elapsed time for attention_prob_times_values (64x2048x2048x741): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x741): 64.664

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 877.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x742x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x742x2048): 76.468
Elapsed time for attention_prob_times_values (64x2048x2048x742): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x742): 66.140

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 893.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x743x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x743x2048): 75.620
Elapsed time for attention_prob_times_values (64x2048x2048x743): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x743): 64.665

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 879.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x744x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x744x2048): 77.397
Elapsed time for attention_prob_times_values (64x2048x2048x744): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x744): 88.125

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1040.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x745x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x745x2048): 74.746
Elapsed time for attention_prob_times_values (64x2048x2048x745): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x745): 64.655

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 876.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x746x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x746x2048): 75.703
Elapsed time for attention_prob_times_values (64x2048x2048x746): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x746): 66.381

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 895.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x747x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x747x2048): 75.063
Elapsed time for attention_prob_times_values (64x2048x2048x747): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x747): 64.783

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 881.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x748x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x748x2048): 76.339
Elapsed time for attention_prob_times_values (64x2048x2048x748): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x748): 66.773

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 903.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 11984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x749x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x749x2048): 75.060
Elapsed time for attention_prob_times_values (64x2048x2048x749): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x749): 64.492

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 881.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x750x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x750x2048): 75.828
Elapsed time for attention_prob_times_values (64x2048x2048x750): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x750): 66.703

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 902.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x751x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x751x2048): 75.023
Elapsed time for attention_prob_times_values (64x2048x2048x751): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x751): 65.226

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 888.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x752x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x752x2048): 77.128
Elapsed time for attention_prob_times_values (64x2048x2048x752): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x752): 89.435

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1056.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x753x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x753x2048): 74.414
Elapsed time for attention_prob_times_values (64x2048x2048x753): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x753): 65.399

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 888.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x754x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x754x2048): 75.313
Elapsed time for attention_prob_times_values (64x2048x2048x754): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x754): 67.029

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 906.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x755x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x755x2048): 74.533
Elapsed time for attention_prob_times_values (64x2048x2048x755): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x755): 64.668

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 886.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x756x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x756x2048): 75.842
Elapsed time for attention_prob_times_values (64x2048x2048x756): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x756): 67.407

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 914.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x757x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x757x2048): 74.675
Elapsed time for attention_prob_times_values (64x2048x2048x757): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x757): 64.893

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 890.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x758x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x758x2048): 75.388
Elapsed time for attention_prob_times_values (64x2048x2048x758): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x758): 67.271

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 913.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x759x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x759x2048): 74.889
Elapsed time for attention_prob_times_values (64x2048x2048x759): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x759): 64.820

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 893.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x760x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x760x2048): 76.356
Elapsed time for attention_prob_times_values (64x2048x2048x760): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x760): 90.119

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1064.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x761x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x761x2048): 74.128
Elapsed time for attention_prob_times_values (64x2048x2048x761): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x761): 64.861

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 891.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x762x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x762x2048): 74.909
Elapsed time for attention_prob_times_values (64x2048x2048x762): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x762): 67.347

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 915.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x763x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x763x2048): 74.310
Elapsed time for attention_prob_times_values (64x2048x2048x763): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x763): 64.903

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 895.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x764x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x764x2048): 75.591
Elapsed time for attention_prob_times_values (64x2048x2048x764): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x764): 67.625

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 923.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x765x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x765x2048): 74.363
Elapsed time for attention_prob_times_values (64x2048x2048x765): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x765): 65.115

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 899.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x766x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x766x2048): 75.031
Elapsed time for attention_prob_times_values (64x2048x2048x766): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x766): 67.778

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 923.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x767x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x767x2048): 74.285
Elapsed time for attention_prob_times_values (64x2048x2048x767): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x767): 64.788

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 898.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x768x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x768x2048): 85.141
Elapsed time for attention_prob_times_values (64x2048x2048x768): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x768): 94.096

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 1162.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x769x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x769x2048): 76.129
Elapsed time for attention_prob_times_values (64x2048x2048x769): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x769): 60.706

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 879.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x770x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x770x2048): 77.079
Elapsed time for attention_prob_times_values (64x2048x2048x770): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x770): 63.948

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 910.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x771x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x771x2048): 75.887
Elapsed time for attention_prob_times_values (64x2048x2048x771): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x771): 61.194

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 883.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x772x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x772x2048): 77.638
Elapsed time for attention_prob_times_values (64x2048x2048x772): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x772): 64.185

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 917.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x773x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x773x2048): 76.170
Elapsed time for attention_prob_times_values (64x2048x2048x773): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x773): 61.325

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 888.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x774x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x774x2048): 76.990
Elapsed time for attention_prob_times_values (64x2048x2048x774): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x774): 64.202

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 916.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x775x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x775x2048): 76.018
Elapsed time for attention_prob_times_values (64x2048x2048x775): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x775): 61.509

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 891.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x776x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x776x2048): 77.418
Elapsed time for attention_prob_times_values (64x2048x2048x776): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x776): 81.487

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1042.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x777x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x777x2048): 75.426
Elapsed time for attention_prob_times_values (64x2048x2048x777): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x777): 61.348

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 889.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x778x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x778x2048): 76.021
Elapsed time for attention_prob_times_values (64x2048x2048x778): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x778): 64.470

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 917.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x779x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x779x2048): 75.532
Elapsed time for attention_prob_times_values (64x2048x2048x779): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x779): 61.584

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 893.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x780x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x780x2048): 76.706
Elapsed time for attention_prob_times_values (64x2048x2048x780): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x780): 64.927

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 927.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x781x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x781x2048): 75.648
Elapsed time for attention_prob_times_values (64x2048x2048x781): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x781): 61.712

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 897.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x782x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x782x2048): 76.455
Elapsed time for attention_prob_times_values (64x2048x2048x782): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x782): 64.806

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 927.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x783x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x783x2048): 75.685
Elapsed time for attention_prob_times_values (64x2048x2048x783): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x783): 61.809

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 900.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x784x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x784x2048): 77.597
Elapsed time for attention_prob_times_values (64x2048x2048x784): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x784): 82.550

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1059.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x785x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x785x2048): 75.044
Elapsed time for attention_prob_times_values (64x2048x2048x785): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x785): 61.947

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 900.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x786x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x786x2048): 75.926
Elapsed time for attention_prob_times_values (64x2048x2048x786): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x786): 64.941

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 929.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x787x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x787x2048): 75.299
Elapsed time for attention_prob_times_values (64x2048x2048x787): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x787): 62.033

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 904.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x788x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x788x2048): 76.619
Elapsed time for attention_prob_times_values (64x2048x2048x788): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x788): 65.301

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 938.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x789x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x789x2048): 75.439
Elapsed time for attention_prob_times_values (64x2048x2048x789): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x789): 62.178

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 908.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x790x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x790x2048): 76.136
Elapsed time for attention_prob_times_values (64x2048x2048x790): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x790): 65.222

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 937.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x791x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x791x2048): 75.493
Elapsed time for attention_prob_times_values (64x2048x2048x791): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x791): 62.244

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 911.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x792x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x792x2048): 77.205
Elapsed time for attention_prob_times_values (64x2048x2048x792): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x792): 83.085

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1070.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x793x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x793x2048): 75.010
Elapsed time for attention_prob_times_values (64x2048x2048x793): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x793): 62.401

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 912.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x794x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x794x2048): 75.673
Elapsed time for attention_prob_times_values (64x2048x2048x794): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x794): 65.387

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 940.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x795x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x795x2048): 75.107
Elapsed time for attention_prob_times_values (64x2048x2048x795): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x795): 62.511

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 915.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x796x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x796x2048): 76.619
Elapsed time for attention_prob_times_values (64x2048x2048x796): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x796): 65.838

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 951.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x797x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x797x2048): 75.296
Elapsed time for attention_prob_times_values (64x2048x2048x797): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x797): 62.620

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 919.863
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x798x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x798x2048): 75.984
Elapsed time for attention_prob_times_values (64x2048x2048x798): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x798): 65.668

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 948.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x799x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x799x2048): 75.400
Elapsed time for attention_prob_times_values (64x2048x2048x799): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x799): 62.700

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 923.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x800x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x800x2048): 89.504
Elapsed time for attention_prob_times_values (64x2048x2048x800): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x800): 84.575

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1174.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x801x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x801x2048): 77.385
Elapsed time for attention_prob_times_values (64x2048x2048x801): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x801): 63.243

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 940.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x802x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x802x2048): 78.203
Elapsed time for attention_prob_times_values (64x2048x2048x802): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x802): 65.963

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 968.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x803x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x803x2048): 77.138
Elapsed time for attention_prob_times_values (64x2048x2048x803): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x803): 63.151

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 940.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x804x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x804x2048): 78.866
Elapsed time for attention_prob_times_values (64x2048x2048x804): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x804): 66.341

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 977.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x805x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x805x2048): 76.936
Elapsed time for attention_prob_times_values (64x2048x2048x805): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x805): 63.346

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 943.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x806x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x806x2048): 77.915
Elapsed time for attention_prob_times_values (64x2048x2048x806): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x806): 66.140

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 972.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x807x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x807x2048): 76.805
Elapsed time for attention_prob_times_values (64x2048x2048x807): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x807): 63.487

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 946.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x808x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x808x2048): 78.843
Elapsed time for attention_prob_times_values (64x2048x2048x808): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x808): 84.716

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1112.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x809x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x809x2048): 76.130
Elapsed time for attention_prob_times_values (64x2048x2048x809): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x809): 63.448

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 944.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x810x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x810x2048): 76.957
Elapsed time for attention_prob_times_values (64x2048x2048x810): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x810): 66.374

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 973.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x811x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x811x2048): 76.369
Elapsed time for attention_prob_times_values (64x2048x2048x811): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x811): 63.545

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 948.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x812x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x812x2048): 78.106
Elapsed time for attention_prob_times_values (64x2048x2048x812): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x812): 66.847

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 986.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x813x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x813x2048): 76.570
Elapsed time for attention_prob_times_values (64x2048x2048x813): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x813): 63.826

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 954.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x814x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x814x2048): 77.491
Elapsed time for attention_prob_times_values (64x2048x2048x814): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x814): 78.908

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1072.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x815x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x815x2048): 76.897
Elapsed time for attention_prob_times_values (64x2048x2048x815): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x815): 76.713

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1054.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x816x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x816x2048): 81.800
Elapsed time for attention_prob_times_values (64x2048x2048x816): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x816): 85.791

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1151.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x817x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x817x2048): 76.404
Elapsed time for attention_prob_times_values (64x2048x2048x817): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x817): 76.825

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1054.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x818x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x818x2048): 77.167
Elapsed time for attention_prob_times_values (64x2048x2048x818): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x818): 79.187

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1077.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x819x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x819x2048): 76.312
Elapsed time for attention_prob_times_values (64x2048x2048x819): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x819): 77.109

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1058.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x820x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x820x2048): 78.227
Elapsed time for attention_prob_times_values (64x2048x2048x820): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x820): 79.353

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1088.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x821x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x821x2048): 78.928
Elapsed time for attention_prob_times_values (64x2048x2048x821): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x821): 77.095

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1078.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x822x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x822x2048): 77.476
Elapsed time for attention_prob_times_values (64x2048x2048x822): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x822): 79.421

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1085.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x823x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x823x2048): 76.742
Elapsed time for attention_prob_times_values (64x2048x2048x823): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x823): 77.311

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1067.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x824x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x824x2048): 78.522
Elapsed time for attention_prob_times_values (64x2048x2048x824): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x824): 86.193

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1140.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x825x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x825x2048): 75.770
Elapsed time for attention_prob_times_values (64x2048x2048x825): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x825): 77.483

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1064.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x826x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x826x2048): 76.678
Elapsed time for attention_prob_times_values (64x2048x2048x826): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x826): 79.811

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1087.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x827x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x827x2048): 76.004
Elapsed time for attention_prob_times_values (64x2048x2048x827): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x827): 77.713

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1069.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x828x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x828x2048): 77.606
Elapsed time for attention_prob_times_values (64x2048x2048x828): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x828): 80.110

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1098.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x829x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x829x2048): 76.054
Elapsed time for attention_prob_times_values (64x2048x2048x829): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x829): 77.768

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1073.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x830x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x830x2048): 76.837
Elapsed time for attention_prob_times_values (64x2048x2048x830): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x830): 80.194

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1096.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x831x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x831x2048): 76.051
Elapsed time for attention_prob_times_values (64x2048x2048x831): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x831): 78.073

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1077.479
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x832x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x832x2048): 89.453
Elapsed time for attention_prob_times_values (64x2048x2048x832): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x832): 88.148

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1243.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x833x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x833x2048): 77.661
Elapsed time for attention_prob_times_values (64x2048x2048x833): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x833): 78.228

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1092.427
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x834x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x834x2048): 78.633
Elapsed time for attention_prob_times_values (64x2048x2048x834): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x834): 80.535

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1116.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x835x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x835x2048): 77.562
Elapsed time for attention_prob_times_values (64x2048x2048x835): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x835): 78.396

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1095.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x836x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x836x2048): 79.273
Elapsed time for attention_prob_times_values (64x2048x2048x836): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x836): 80.864

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1125.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x837x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x837x2048): 77.415
Elapsed time for attention_prob_times_values (64x2048x2048x837): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x837): 78.509

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1097.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x838x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x838x2048): 78.257
Elapsed time for attention_prob_times_values (64x2048x2048x838): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x838): 80.849

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1120.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x839x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x839x2048): 77.072
Elapsed time for attention_prob_times_values (64x2048x2048x839): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x839): 78.703

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1098.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x840x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x840x2048): 79.246
Elapsed time for attention_prob_times_values (64x2048x2048x840): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x840): 87.826

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1176.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x841x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x841x2048): 76.448
Elapsed time for attention_prob_times_values (64x2048x2048x841): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x841): 79.052

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1099.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x842x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x842x2048): 77.313
Elapsed time for attention_prob_times_values (64x2048x2048x842): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x842): 81.304

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1121.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x843x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x843x2048): 76.713
Elapsed time for attention_prob_times_values (64x2048x2048x843): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x843): 79.154

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1104.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x844x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x844x2048): 78.437
Elapsed time for attention_prob_times_values (64x2048x2048x844): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x844): 81.438

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1133.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x845x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x845x2048): 76.722
Elapsed time for attention_prob_times_values (64x2048x2048x845): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x845): 79.352

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1108.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x846x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x846x2048): 77.598
Elapsed time for attention_prob_times_values (64x2048x2048x846): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x846): 81.551

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1130.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x847x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x847x2048): 76.826
Elapsed time for attention_prob_times_values (64x2048x2048x847): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x847): 79.563

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1112.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x848x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x848x2048): 78.939
Elapsed time for attention_prob_times_values (64x2048x2048x848): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x848): 88.939

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1191.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x849x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x849x2048): 76.306
Elapsed time for attention_prob_times_values (64x2048x2048x849): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x849): 79.827

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1113.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x850x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x850x2048): 77.138
Elapsed time for attention_prob_times_values (64x2048x2048x850): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x850): 81.894

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1134.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x851x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x851x2048): 76.483
Elapsed time for attention_prob_times_values (64x2048x2048x851): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x851): 79.898

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1117.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x852x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x852x2048): 77.954
Elapsed time for attention_prob_times_values (64x2048x2048x852): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x852): 82.175

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1145.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x853x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x853x2048): 76.513
Elapsed time for attention_prob_times_values (64x2048x2048x853): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x853): 80.039

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1120.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x854x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x854x2048): 77.339
Elapsed time for attention_prob_times_values (64x2048x2048x854): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x854): 82.097

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1142.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x855x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x855x2048): 76.619
Elapsed time for attention_prob_times_values (64x2048x2048x855): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x855): 80.177

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1125.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x856x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x856x2048): 78.589
Elapsed time for attention_prob_times_values (64x2048x2048x856): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x856): 89.399

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1202.413
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x857x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x857x2048): 75.991
Elapsed time for attention_prob_times_values (64x2048x2048x857): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x857): 80.381

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1124.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x858x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x858x2048): 76.839
Elapsed time for attention_prob_times_values (64x2048x2048x858): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x858): 82.474

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1146.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x859x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x859x2048): 76.195
Elapsed time for attention_prob_times_values (64x2048x2048x859): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x859): 80.555

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1129.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x860x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x860x2048): 77.904
Elapsed time for attention_prob_times_values (64x2048x2048x860): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x860): 82.688

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1158.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x861x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x861x2048): 76.181
Elapsed time for attention_prob_times_values (64x2048x2048x861): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x861): 80.693

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1132.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x862x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x862x2048): 77.102
Elapsed time for attention_prob_times_values (64x2048x2048x862): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x862): 82.838

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1155.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x863x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x863x2048): 76.283
Elapsed time for attention_prob_times_values (64x2048x2048x863): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x863): 80.904

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1137.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x864x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x864x2048): 90.101
Elapsed time for attention_prob_times_values (64x2048x2048x864): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x864): 90.833

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1311.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x865x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x865x2048): 77.906
Elapsed time for attention_prob_times_values (64x2048x2048x865): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x865): 81.013

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1152.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x866x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x866x2048): 78.774
Elapsed time for attention_prob_times_values (64x2048x2048x866): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x866): 83.144

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1175.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x867x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x867x2048): 77.732
Elapsed time for attention_prob_times_values (64x2048x2048x867): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x867): 81.086

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1154.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x868x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x868x2048): 79.193
Elapsed time for attention_prob_times_values (64x2048x2048x868): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x868): 83.304

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1182.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x869x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x869x2048): 77.534
Elapsed time for attention_prob_times_values (64x2048x2048x869): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x869): 81.061

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1155.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x870x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x870x2048): 78.325
Elapsed time for attention_prob_times_values (64x2048x2048x870): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x870): 83.361

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1178.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x871x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x871x2048): 77.354
Elapsed time for attention_prob_times_values (64x2048x2048x871): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x871): 81.281

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1158.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x872x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x872x2048): 79.052
Elapsed time for attention_prob_times_values (64x2048x2048x872): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x872): 90.885

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1236.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x873x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x873x2048): 76.706
Elapsed time for attention_prob_times_values (64x2048x2048x873): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x873): 81.505

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1157.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 13984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x874x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x874x2048): 77.572
Elapsed time for attention_prob_times_values (64x2048x2048x874): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x874): 83.705

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1180.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x875x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x875x2048): 76.924
Elapsed time for attention_prob_times_values (64x2048x2048x875): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x875): 81.516

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1161.328
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x876x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x876x2048): 78.159
Elapsed time for attention_prob_times_values (64x2048x2048x876): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x876): 84.018

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1189.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x877x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x877x2048): 77.021
Elapsed time for attention_prob_times_values (64x2048x2048x877): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x877): 81.675

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1165.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x878x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x878x2048): 77.700
Elapsed time for attention_prob_times_values (64x2048x2048x878): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x878): 84.102

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1188.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x879x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x879x2048): 77.113
Elapsed time for attention_prob_times_values (64x2048x2048x879): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x879): 81.811

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1169.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x880x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x880x2048): 79.254
Elapsed time for attention_prob_times_values (64x2048x2048x880): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x880): 91.812

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1254.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x881x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x881x2048): 76.777
Elapsed time for attention_prob_times_values (64x2048x2048x881): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x881): 82.024

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1171.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x882x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x882x2048): 77.506
Elapsed time for attention_prob_times_values (64x2048x2048x882): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x882): 84.389

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1194.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x883x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x883x2048): 76.838
Elapsed time for attention_prob_times_values (64x2048x2048x883): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x883): 82.159

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1175.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x884x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x884x2048): 77.876
Elapsed time for attention_prob_times_values (64x2048x2048x884): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x884): 84.623

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1201.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x885x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x885x2048): 76.769
Elapsed time for attention_prob_times_values (64x2048x2048x885): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x885): 82.277

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1177.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x886x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x886x2048): 77.374
Elapsed time for attention_prob_times_values (64x2048x2048x886): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x886): 84.728

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1200.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x887x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x887x2048): 76.963
Elapsed time for attention_prob_times_values (64x2048x2048x887): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x887): 82.409

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1182.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x888x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x888x2048): 78.449
Elapsed time for attention_prob_times_values (64x2048x2048x888): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x888): 92.433

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1262.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x889x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x889x2048): 76.638
Elapsed time for attention_prob_times_values (64x2048x2048x889): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x889): 82.649

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1184.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x890x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x890x2048): 77.250
Elapsed time for attention_prob_times_values (64x2048x2048x890): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x890): 84.916

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1205.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x891x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x891x2048): 76.584
Elapsed time for attention_prob_times_values (64x2048x2048x891): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x891): 82.699

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1186.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x892x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x892x2048): 77.775
Elapsed time for attention_prob_times_values (64x2048x2048x892): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x892): 85.266

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1215.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x893x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x893x2048): 76.282
Elapsed time for attention_prob_times_values (64x2048x2048x893): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x893): 82.724

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1186.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x894x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x894x2048): 77.341
Elapsed time for attention_prob_times_values (64x2048x2048x894): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x894): 85.258

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1214.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x895x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x895x2048): 76.487
Elapsed time for attention_prob_times_values (64x2048x2048x895): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x895): 83.097

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1193.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x896x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x896x2048): 86.637
Elapsed time for attention_prob_times_values (64x2048x2048x896): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x896): 94.728

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1357.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x897x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x897x2048): 77.954
Elapsed time for attention_prob_times_values (64x2048x2048x897): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x897): 74.421

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1143.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x898x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x898x2048): 78.789
Elapsed time for attention_prob_times_values (64x2048x2048x898): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x898): 77.027

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1170.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x899x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x899x2048): 77.613
Elapsed time for attention_prob_times_values (64x2048x2048x899): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x899): 74.996

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1147.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x900x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x900x2048): 79.510
Elapsed time for attention_prob_times_values (64x2048x2048x900): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x900): 77.322

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1180.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x901x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x901x2048): 78.202
Elapsed time for attention_prob_times_values (64x2048x2048x901): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x901): 75.127

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1155.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x902x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x902x2048): 78.882
Elapsed time for attention_prob_times_values (64x2048x2048x902): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x902): 77.335

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1178.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x903x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x903x2048): 78.015
Elapsed time for attention_prob_times_values (64x2048x2048x903): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x903): 75.293

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1157.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x904x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x904x2048): 79.448
Elapsed time for attention_prob_times_values (64x2048x2048x904): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x904): 82.813

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1226.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x905x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x905x2048): 77.492
Elapsed time for attention_prob_times_values (64x2048x2048x905): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x905): 75.469

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1157.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x906x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x906x2048): 78.379
Elapsed time for attention_prob_times_values (64x2048x2048x906): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x906): 77.595

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1181.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x907x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x907x2048): 77.829
Elapsed time for attention_prob_times_values (64x2048x2048x907): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x907): 75.579

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1163.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x908x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x908x2048): 79.023
Elapsed time for attention_prob_times_values (64x2048x2048x908): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x908): 77.855

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1191.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x909x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x909x2048): 77.740
Elapsed time for attention_prob_times_values (64x2048x2048x909): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x909): 75.710

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1166.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x910x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x910x2048): 78.547
Elapsed time for attention_prob_times_values (64x2048x2048x910): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x910): 77.648

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1188.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x911x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x911x2048): 77.616
Elapsed time for attention_prob_times_values (64x2048x2048x911): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x911): 75.879

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1169.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x912x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x912x2048): 79.670
Elapsed time for attention_prob_times_values (64x2048x2048x912): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x912): 83.819

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1245.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x913x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x913x2048): 77.845
Elapsed time for attention_prob_times_values (64x2048x2048x913): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x913): 76.130

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1175.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x914x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x914x2048): 78.265
Elapsed time for attention_prob_times_values (64x2048x2048x914): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x914): 78.075

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1194.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x915x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x915x2048): 77.232
Elapsed time for attention_prob_times_values (64x2048x2048x915): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x915): 76.246

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1173.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x916x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x916x2048): 78.703
Elapsed time for attention_prob_times_values (64x2048x2048x916): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x916): 78.374

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1202.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x917x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x917x2048): 77.465
Elapsed time for attention_prob_times_values (64x2048x2048x917): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x917): 76.339

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1178.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x918x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x918x2048): 78.206
Elapsed time for attention_prob_times_values (64x2048x2048x918): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x918): 78.414

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1201.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x919x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x919x2048): 77.544
Elapsed time for attention_prob_times_values (64x2048x2048x919): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x919): 76.543

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1183.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x920x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x920x2048): 79.374
Elapsed time for attention_prob_times_values (64x2048x2048x920): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x920): 84.224

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1256.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x921x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x921x2048): 77.106
Elapsed time for attention_prob_times_values (64x2048x2048x921): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x921): 76.701

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1183.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x922x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x922x2048): 77.724
Elapsed time for attention_prob_times_values (64x2048x2048x922): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x922): 78.807

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1205.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x923x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x923x2048): 77.244
Elapsed time for attention_prob_times_values (64x2048x2048x923): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x923): 76.887

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1188.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x924x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x924x2048): 78.658
Elapsed time for attention_prob_times_values (64x2048x2048x924): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x924): 79.029

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1217.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x925x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x925x2048): 77.311
Elapsed time for attention_prob_times_values (64x2048x2048x925): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x925): 76.957

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1191.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x926x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x926x2048): 78.251
Elapsed time for attention_prob_times_values (64x2048x2048x926): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x926): 79.100

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1216.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x927x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x927x2048): 77.565
Elapsed time for attention_prob_times_values (64x2048x2048x927): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x927): 77.140

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1197.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x928x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x928x2048): 90.611
Elapsed time for attention_prob_times_values (64x2048x2048x928): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x928): 85.485

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1363.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x929x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x929x2048): 79.135
Elapsed time for attention_prob_times_values (64x2048x2048x929): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x929): 77.287

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1213.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x930x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x930x2048): 80.092
Elapsed time for attention_prob_times_values (64x2048x2048x930): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x930): 79.322

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1237.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x931x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x931x2048): 78.890
Elapsed time for attention_prob_times_values (64x2048x2048x931): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x931): 77.416

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1214.928
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x932x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x932x2048): 80.787
Elapsed time for attention_prob_times_values (64x2048x2048x932): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x932): 79.513

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1247.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x933x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x933x2048): 78.884
Elapsed time for attention_prob_times_values (64x2048x2048x933): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x933): 77.414

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1217.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x934x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x934x2048): 79.560
Elapsed time for attention_prob_times_values (64x2048x2048x934): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x934): 79.624

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1241.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x935x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x935x2048): 78.690
Elapsed time for attention_prob_times_values (64x2048x2048x935): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x935): 77.637

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1220.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x936x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x936x2048): 80.612
Elapsed time for attention_prob_times_values (64x2048x2048x936): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x936): 85.595

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1297.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 14992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x937x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x937x2048): 78.157
Elapsed time for attention_prob_times_values (64x2048x2048x937): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x937): 77.754

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1219.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x938x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x938x2048): 78.878
Elapsed time for attention_prob_times_values (64x2048x2048x938): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x938): 79.925

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1243.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x939x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x939x2048): 78.397
Elapsed time for attention_prob_times_values (64x2048x2048x939): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x939): 77.803

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1223.951
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x940x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x940x2048): 80.017
Elapsed time for attention_prob_times_values (64x2048x2048x940): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x940): 80.153

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1256.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x941x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x941x2048): 78.555
Elapsed time for attention_prob_times_values (64x2048x2048x941): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x941): 77.983

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1229.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x942x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x942x2048): 79.347
Elapsed time for attention_prob_times_values (64x2048x2048x942): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x942): 80.196

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1253.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x943x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x943x2048): 78.527
Elapsed time for attention_prob_times_values (64x2048x2048x943): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x943): 78.140

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1232.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x944x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x944x2048): 80.552
Elapsed time for attention_prob_times_values (64x2048x2048x944): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x944): 86.548

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1314.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x945x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x945x2048): 78.078
Elapsed time for attention_prob_times_values (64x2048x2048x945): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x945): 78.280

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1232.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x946x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x946x2048): 79.072
Elapsed time for attention_prob_times_values (64x2048x2048x946): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x946): 80.577

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1259.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x947x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x947x2048): 78.175
Elapsed time for attention_prob_times_values (64x2048x2048x947): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x947): 78.408

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1236.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x948x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x948x2048): 79.826
Elapsed time for attention_prob_times_values (64x2048x2048x948): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x948): 80.671

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1268.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x949x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x949x2048): 78.426
Elapsed time for attention_prob_times_values (64x2048x2048x949): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x949): 78.579

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1242.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x950x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x950x2048): 79.224
Elapsed time for attention_prob_times_values (64x2048x2048x950): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x950): 80.719

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1266.937
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x951x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x951x2048): 78.356
Elapsed time for attention_prob_times_values (64x2048x2048x951): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x951): 78.724

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1245.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x952x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x952x2048): 80.160
Elapsed time for attention_prob_times_values (64x2048x2048x952): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x952): 87.054

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1325.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x953x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x953x2048): 77.905
Elapsed time for attention_prob_times_values (64x2048x2048x953): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x953): 78.893

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1245.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x954x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x954x2048): 78.730
Elapsed time for attention_prob_times_values (64x2048x2048x954): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x954): 81.132

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1271.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x955x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x955x2048): 78.026
Elapsed time for attention_prob_times_values (64x2048x2048x955): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x955): 79.038

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1250.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x956x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x956x2048): 79.586
Elapsed time for attention_prob_times_values (64x2048x2048x956): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x956): 81.173

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1280.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x957x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x957x2048): 78.039
Elapsed time for attention_prob_times_values (64x2048x2048x957): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x957): 79.183

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1254.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x958x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x958x2048): 78.987
Elapsed time for attention_prob_times_values (64x2048x2048x958): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x958): 81.445

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1280.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x959x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x959x2048): 78.205
Elapsed time for attention_prob_times_values (64x2048x2048x959): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x959): 79.323

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1258.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x960x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x960x2048): 90.768
Elapsed time for attention_prob_times_values (64x2048x2048x960): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x960): 88.839

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1436.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x961x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x961x2048): 79.699
Elapsed time for attention_prob_times_values (64x2048x2048x961): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x961): 79.389

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1273.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x962x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x962x2048): 80.517
Elapsed time for attention_prob_times_values (64x2048x2048x962): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x962): 81.806

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1301.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x963x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x963x2048): 79.446
Elapsed time for attention_prob_times_values (64x2048x2048x963): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x963): 79.656

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1276.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x964x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x964x2048): 81.185
Elapsed time for attention_prob_times_values (64x2048x2048x964): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x964): 81.973

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1310.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x965x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x965x2048): 79.329
Elapsed time for attention_prob_times_values (64x2048x2048x965): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x965): 79.810

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1279.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x966x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x966x2048): 80.164
Elapsed time for attention_prob_times_values (64x2048x2048x966): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x966): 81.976

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1304.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x967x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x967x2048): 79.340
Elapsed time for attention_prob_times_values (64x2048x2048x967): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x967): 79.967

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1283.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x968x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x968x2048): 81.092
Elapsed time for attention_prob_times_values (64x2048x2048x968): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x968): 88.510

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1364.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x969x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x969x2048): 78.407
Elapsed time for attention_prob_times_values (64x2048x2048x969): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x969): 80.138

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1279.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x970x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x970x2048): 79.303
Elapsed time for attention_prob_times_values (64x2048x2048x970): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x970): 82.102

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1303.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x971x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x971x2048): 78.648
Elapsed time for attention_prob_times_values (64x2048x2048x971): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x971): 80.312

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1285.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x972x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x972x2048): 80.256
Elapsed time for attention_prob_times_values (64x2048x2048x972): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x972): 82.569

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1317.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x973x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x973x2048): 78.703
Elapsed time for attention_prob_times_values (64x2048x2048x973): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x973): 80.513

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1289.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x974x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x974x2048): 79.775
Elapsed time for attention_prob_times_values (64x2048x2048x974): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x974): 82.605

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1316.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x975x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x975x2048): 78.862
Elapsed time for attention_prob_times_values (64x2048x2048x975): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x975): 80.572

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1294.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x976x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x976x2048): 80.955
Elapsed time for attention_prob_times_values (64x2048x2048x976): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x976): 89.485

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1381.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x977x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x977x2048): 78.223
Elapsed time for attention_prob_times_values (64x2048x2048x977): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x977): 80.889

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1293.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x978x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x978x2048): 79.161
Elapsed time for attention_prob_times_values (64x2048x2048x978): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x978): 82.751

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1317.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x979x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x979x2048): 78.391
Elapsed time for attention_prob_times_values (64x2048x2048x979): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x979): 80.980

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1298.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x980x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x980x2048): 79.900
Elapsed time for attention_prob_times_values (64x2048x2048x980): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x980): 82.961

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1327.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x981x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x981x2048): 78.479
Elapsed time for attention_prob_times_values (64x2048x2048x981): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x981): 81.169

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1303.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x982x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x982x2048): 79.339
Elapsed time for attention_prob_times_values (64x2048x2048x982): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x982): 83.046

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1326.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x983x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x983x2048): 78.591
Elapsed time for attention_prob_times_values (64x2048x2048x983): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x983): 81.272

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1307.261
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x984x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x984x2048): 80.500
Elapsed time for attention_prob_times_values (64x2048x2048x984): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x984): 89.867

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1390.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x985x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x985x2048): 77.970
Elapsed time for attention_prob_times_values (64x2048x2048x985): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x985): 81.452

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1305.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x986x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x986x2048): 78.811
Elapsed time for attention_prob_times_values (64x2048x2048x986): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x986): 83.225

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1328.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x987x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x987x2048): 78.087
Elapsed time for attention_prob_times_values (64x2048x2048x987): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x987): 81.537

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1310.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x988x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x988x2048): 79.721
Elapsed time for attention_prob_times_values (64x2048x2048x988): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x988): 83.607

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1341.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x989x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x989x2048): 78.257
Elapsed time for attention_prob_times_values (64x2048x2048x989): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x989): 81.657

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1314.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x990x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x990x2048): 79.039
Elapsed time for attention_prob_times_values (64x2048x2048x990): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x990): 83.697

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1338.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x991x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x991x2048): 78.231
Elapsed time for attention_prob_times_values (64x2048x2048x991): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x991): 81.765

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1318.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x992x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x992x2048): 91.212
Elapsed time for attention_prob_times_values (64x2048x2048x992): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x992): 91.216

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1505.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x993x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x993x2048): 79.896
Elapsed time for attention_prob_times_values (64x2048x2048x993): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x993): 81.963

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1336.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x994x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x994x2048): 80.592
Elapsed time for attention_prob_times_values (64x2048x2048x994): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x994): 83.775

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1358.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x995x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x995x2048): 79.607
Elapsed time for attention_prob_times_values (64x2048x2048x995): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x995): 81.789

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1335.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x996x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x996x2048): 81.084
Elapsed time for attention_prob_times_values (64x2048x2048x996): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x996): 83.981

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1366.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x997x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x997x2048): 79.510
Elapsed time for attention_prob_times_values (64x2048x2048x997): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x997): 81.805

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1336.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x998x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x998x2048): 80.180
Elapsed time for attention_prob_times_values (64x2048x2048x998): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x998): 84.058

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1361.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 15984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x999x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x999x2048): 79.190
Elapsed time for attention_prob_times_values (64x2048x2048x999): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x999): 81.948

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1337.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1000x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1000x2048): 80.916
Elapsed time for attention_prob_times_values (64x2048x2048x1000): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1000): 91.155

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1425.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1001x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1001x2048): 78.533
Elapsed time for attention_prob_times_values (64x2048x2048x1001): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1001): 82.055

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1335.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1002x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1002x2048): 79.344
Elapsed time for attention_prob_times_values (64x2048x2048x1002): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1002): 84.297

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1361.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1003x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1003x2048): 78.731
Elapsed time for attention_prob_times_values (64x2048x2048x1003): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1003): 82.141

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1340.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1004x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1004x2048): 80.390
Elapsed time for attention_prob_times_values (64x2048x2048x1004): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1004): 84.393

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1374.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1005x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1005x2048): 78.847
Elapsed time for attention_prob_times_values (64x2048x2048x1005): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1005): 82.114

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1343.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1006x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1006x2048): 79.448
Elapsed time for attention_prob_times_values (64x2048x2048x1006): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1006): 84.454

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1368.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1007x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1007x2048): 78.768
Elapsed time for attention_prob_times_values (64x2048x2048x1007): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1007): 82.224

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1346.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1008x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1008x2048): 81.100
Elapsed time for attention_prob_times_values (64x2048x2048x1008): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1008): 92.153

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1445.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1009x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1009x2048): 78.263
Elapsed time for attention_prob_times_values (64x2048x2048x1009): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1009): 82.408

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1345.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1010x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1010x2048): 79.101
Elapsed time for attention_prob_times_values (64x2048x2048x1010): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1010): 84.701

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1372.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1011x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1011x2048): 78.408
Elapsed time for attention_prob_times_values (64x2048x2048x1011): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1011): 82.532

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1350.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1012x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1012x2048): 80.373
Elapsed time for attention_prob_times_values (64x2048x2048x1012): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1012): 84.888

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1388.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1013x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1013x2048): 78.614
Elapsed time for attention_prob_times_values (64x2048x2048x1013): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1013): 82.537

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1355.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1014x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1014x2048): 79.268
Elapsed time for attention_prob_times_values (64x2048x2048x1014): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1014): 84.951

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1381.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1015x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1015x2048): 78.472
Elapsed time for attention_prob_times_values (64x2048x2048x1015): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1015): 82.751

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1358.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1016x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1016x2048): 80.280
Elapsed time for attention_prob_times_values (64x2048x2048x1016): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1016): 92.434

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1450.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1017x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1017x2048): 78.259
Elapsed time for attention_prob_times_values (64x2048x2048x1017): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1017): 82.695

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1358.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1018x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1018x2048): 79.038
Elapsed time for attention_prob_times_values (64x2048x2048x1018): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1018): 85.138

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1385.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1019x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1019x2048): 78.936
Elapsed time for attention_prob_times_values (64x2048x2048x1019): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1019): 82.975

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1369.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1020x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1020x2048): 84.133
Elapsed time for attention_prob_times_values (64x2048x2048x1020): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1020): 85.276

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1434.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1021x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1021x2048): 78.481
Elapsed time for attention_prob_times_values (64x2048x2048x1021): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1021): 82.749

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1365.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1022x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1022x2048): 79.242
Elapsed time for attention_prob_times_values (64x2048x2048x1022): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1022): 85.396

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1394.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1023x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1023x2048): 78.380
Elapsed time for attention_prob_times_values (64x2048x2048x1023): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1023): 82.747

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1367.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1024x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1024x2048): 89.204
Elapsed time for attention_prob_times_values (64x2048x2048x1024): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1024): 94.707

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1561.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1025x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1025x2048): 79.837
Elapsed time for attention_prob_times_values (64x2048x2048x1025): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1025): 75.504

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1320.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1026x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1026x2048): 80.917
Elapsed time for attention_prob_times_values (64x2048x2048x1026): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1026): 77.986

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1352.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1027x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1027x2048): 79.539
Elapsed time for attention_prob_times_values (64x2048x2048x1027): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1027): 75.806

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1323.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1028x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1028x2048): 86.359
Elapsed time for attention_prob_times_values (64x2048x2048x1028): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1028): 77.902

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1397.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1029x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1029x2048): 80.377
Elapsed time for attention_prob_times_values (64x2048x2048x1029): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1029): 75.834

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1332.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1030x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1030x2048): 80.564
Elapsed time for attention_prob_times_values (64x2048x2048x1030): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1030): 78.156

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1356.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1031x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1031x2048): 79.624
Elapsed time for attention_prob_times_values (64x2048x2048x1031): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1031): 76.111

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1331.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1032x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1032x2048): 81.245
Elapsed time for attention_prob_times_values (64x2048x2048x1032): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1032): 83.280

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1408.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1033x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1033x2048): 78.875
Elapsed time for attention_prob_times_values (64x2048x2048x1033): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1033): 76.323

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1329.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1034x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1034x2048): 79.673
Elapsed time for attention_prob_times_values (64x2048x2048x1034): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1034): 78.515

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1356.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1035x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1035x2048): 79.095
Elapsed time for attention_prob_times_values (64x2048x2048x1035): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1035): 76.481

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1335.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1036x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1036x2048): 80.907
Elapsed time for attention_prob_times_values (64x2048x2048x1036): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1036): 78.741

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1371.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1037x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1037x2048): 79.224
Elapsed time for attention_prob_times_values (64x2048x2048x1037): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1037): 76.642

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1340.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1038x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1038x2048): 79.905
Elapsed time for attention_prob_times_values (64x2048x2048x1038): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1038): 78.763

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1365.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1039x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1039x2048): 79.156
Elapsed time for attention_prob_times_values (64x2048x2048x1039): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1039): 76.787

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1343.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1040x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1040x2048): 81.301
Elapsed time for attention_prob_times_values (64x2048x2048x1040): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1040): 84.270

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1427.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1041x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1041x2048): 78.599
Elapsed time for attention_prob_times_values (64x2048x2048x1041): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1041): 77.021

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1343.293
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1042x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1042x2048): 79.382
Elapsed time for attention_prob_times_values (64x2048x2048x1042): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1042): 78.971

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1368.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1043x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1043x2048): 78.586
Elapsed time for attention_prob_times_values (64x2048x2048x1043): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1043): 77.118

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1346.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1044x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1044x2048): 80.446
Elapsed time for attention_prob_times_values (64x2048x2048x1044): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1044): 79.277

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1382.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1045x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1045x2048): 79.126
Elapsed time for attention_prob_times_values (64x2048x2048x1045): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1045): 77.215

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1354.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1046x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1046x2048): 79.708
Elapsed time for attention_prob_times_values (64x2048x2048x1046): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1046): 79.191

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1377.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1047x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1047x2048): 78.991
Elapsed time for attention_prob_times_values (64x2048x2048x1047): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1047): 77.292

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1356.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1048x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1048x2048): 80.793
Elapsed time for attention_prob_times_values (64x2048x2048x1048): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1048): 84.363

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1434.123
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1049x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1049x2048): 78.599
Elapsed time for attention_prob_times_values (64x2048x2048x1049): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1049): 77.483

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1357.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1050x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1050x2048): 79.377
Elapsed time for attention_prob_times_values (64x2048x2048x1050): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1050): 79.499

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1382.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1051x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1051x2048): 78.720
Elapsed time for attention_prob_times_values (64x2048x2048x1051): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1051): 77.521

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1360.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1052x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1052x2048): 80.198
Elapsed time for attention_prob_times_values (64x2048x2048x1052): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1052): 79.731

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1394.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1053x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1053x2048): 78.894
Elapsed time for attention_prob_times_values (64x2048x2048x1053): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1053): 77.681

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1366.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1054x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1054x2048): 79.577
Elapsed time for attention_prob_times_values (64x2048x2048x1054): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1054): 79.797

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1392.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1055x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1055x2048): 78.988
Elapsed time for attention_prob_times_values (64x2048x2048x1055): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1055): 77.849

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1371.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1056x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1056x2048): 91.751
Elapsed time for attention_prob_times_values (64x2048x2048x1056): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1056): 85.913

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1552.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1057x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1057x2048): 80.400
Elapsed time for attention_prob_times_values (64x2048x2048x1057): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1057): 77.982

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1386.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1058x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1058x2048): 81.175
Elapsed time for attention_prob_times_values (64x2048x2048x1058): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1058): 80.059

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1413.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1059x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1059x2048): 80.194
Elapsed time for attention_prob_times_values (64x2048x2048x1059): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1059): 78.124

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1388.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1060x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1060x2048): 81.926
Elapsed time for attention_prob_times_values (64x2048x2048x1060): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1060): 80.275

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1424.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1061x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1061x2048): 80.227
Elapsed time for attention_prob_times_values (64x2048x2048x1061): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1061): 78.154

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1391.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1062x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1062x2048): 80.936
Elapsed time for attention_prob_times_values (64x2048x2048x1062): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1062): 80.343

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1418.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1063x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1063x2048): 79.965
Elapsed time for attention_prob_times_values (64x2048x2048x1063): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1063): 78.296

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1393.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1064x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1064x2048): 81.931
Elapsed time for attention_prob_times_values (64x2048x2048x1064): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1064): 85.605

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1475.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1065x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1065x2048): 79.444
Elapsed time for attention_prob_times_values (64x2048x2048x1065): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1065): 78.443

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1392.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1066x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1066x2048): 80.212
Elapsed time for attention_prob_times_values (64x2048x2048x1066): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1066): 80.564

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1419.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1067x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1067x2048): 79.572
Elapsed time for attention_prob_times_values (64x2048x2048x1067): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1067): 78.493

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1396.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1068x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1068x2048): 81.256
Elapsed time for attention_prob_times_values (64x2048x2048x1068): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1068): 80.745

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1432.683
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1069x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1069x2048): 79.761
Elapsed time for attention_prob_times_values (64x2048x2048x1069): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1069): 78.573

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1401.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1070x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1070x2048): 80.775
Elapsed time for attention_prob_times_values (64x2048x2048x1070): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1070): 80.861

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1431.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1071x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1071x2048): 79.836
Elapsed time for attention_prob_times_values (64x2048x2048x1071): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1071): 78.620

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1404.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1072x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1072x2048): 81.870
Elapsed time for attention_prob_times_values (64x2048x2048x1072): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1072): 86.645

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1494.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1073x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1073x2048): 79.409
Elapsed time for attention_prob_times_values (64x2048x2048x1073): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1073): 78.958

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1406.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1074x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1074x2048): 80.320
Elapsed time for attention_prob_times_values (64x2048x2048x1074): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1074): 81.207

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1436.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1075x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1075x2048): 79.555
Elapsed time for attention_prob_times_values (64x2048x2048x1075): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1075): 79.094

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1411.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1076x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1076x2048): 81.146
Elapsed time for attention_prob_times_values (64x2048x2048x1076): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1076): 81.337

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1447.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1077x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1077x2048): 79.668
Elapsed time for attention_prob_times_values (64x2048x2048x1077): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1077): 79.182

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1415.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1078x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1078x2048): 80.472
Elapsed time for attention_prob_times_values (64x2048x2048x1078): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1078): 81.411

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1444.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1079x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1079x2048): 79.769
Elapsed time for attention_prob_times_values (64x2048x2048x1079): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1079): 79.270

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1420.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1080x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1080x2048): 81.636
Elapsed time for attention_prob_times_values (64x2048x2048x1080): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1080): 86.955

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1505.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1081x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1081x2048): 79.207
Elapsed time for attention_prob_times_values (64x2048x2048x1081): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1081): 79.461

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1419.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1082x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1082x2048): 80.102
Elapsed time for attention_prob_times_values (64x2048x2048x1082): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1082): 81.694

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1448.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1083x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1083x2048): 79.377
Elapsed time for attention_prob_times_values (64x2048x2048x1083): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1083): 79.592

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1424.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1084x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1084x2048): 80.966
Elapsed time for attention_prob_times_values (64x2048x2048x1084): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1084): 81.974

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1461.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1085x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1085x2048): 79.513
Elapsed time for attention_prob_times_values (64x2048x2048x1085): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1085): 79.739

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1429.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1086x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1086x2048): 80.300
Elapsed time for attention_prob_times_values (64x2048x2048x1086): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1086): 81.922

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1457.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1087x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1087x2048): 79.686
Elapsed time for attention_prob_times_values (64x2048x2048x1087): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1087): 79.912

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1435.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1088x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1088x2048): 91.748
Elapsed time for attention_prob_times_values (64x2048x2048x1088): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1088): 89.193

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1628.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1089x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1089x2048): 80.950
Elapsed time for attention_prob_times_values (64x2048x2048x1089): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1089): 79.936

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1449.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1090x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1090x2048): 81.731
Elapsed time for attention_prob_times_values (64x2048x2048x1090): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1090): 82.276

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1478.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1091x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1091x2048): 80.768
Elapsed time for attention_prob_times_values (64x2048x2048x1091): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1091): 80.156

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1452.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1092x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1092x2048): 82.411
Elapsed time for attention_prob_times_values (64x2048x2048x1092): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1092): 82.491

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1489.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1093x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1093x2048): 80.611
Elapsed time for attention_prob_times_values (64x2048x2048x1093): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1093): 80.196

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1453.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1094x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1094x2048): 81.338
Elapsed time for attention_prob_times_values (64x2048x2048x1094): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1094): 82.514

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1482.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1095x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1095x2048): 80.495
Elapsed time for attention_prob_times_values (64x2048x2048x1095): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1095): 80.479

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1457.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1096x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1096x2048): 82.577
Elapsed time for attention_prob_times_values (64x2048x2048x1096): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1096): 88.296

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1546.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1097x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1097x2048): 80.018
Elapsed time for attention_prob_times_values (64x2048x2048x1097): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1097): 80.692

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1457.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1098x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1098x2048): 80.726
Elapsed time for attention_prob_times_values (64x2048x2048x1098): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1098): 82.816

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1484.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1099x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1099x2048): 80.088
Elapsed time for attention_prob_times_values (64x2048x2048x1099): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1099): 80.794

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1461.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1100x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1100x2048): 81.695
Elapsed time for attention_prob_times_values (64x2048x2048x1100): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1100): 83.042

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1497.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1101x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1101x2048): 80.006
Elapsed time for attention_prob_times_values (64x2048x2048x1101): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1101): 80.902

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1464.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1102x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1102x2048): 80.973
Elapsed time for attention_prob_times_values (64x2048x2048x1102): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1102): 83.107

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1494.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1103x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1103x2048): 80.115
Elapsed time for attention_prob_times_values (64x2048x2048x1103): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1103): 81.078

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1469.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1104x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1104x2048): 82.369
Elapsed time for attention_prob_times_values (64x2048x2048x1104): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1104): 89.193

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1563.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1105x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1105x2048): 79.690
Elapsed time for attention_prob_times_values (64x2048x2048x1105): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1105): 81.230

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1469.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1106x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1106x2048): 80.630
Elapsed time for attention_prob_times_values (64x2048x2048x1106): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1106): 83.375

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1498.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1107x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1107x2048): 79.801
Elapsed time for attention_prob_times_values (64x2048x2048x1107): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1107): 81.390

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1474.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1108x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1108x2048): 81.496
Elapsed time for attention_prob_times_values (64x2048x2048x1108): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1108): 83.579

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1511.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1109x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1109x2048): 80.040
Elapsed time for attention_prob_times_values (64x2048x2048x1109): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1109): 81.497

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1480.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1110x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1110x2048): 80.686
Elapsed time for attention_prob_times_values (64x2048x2048x1110): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1110): 83.604

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1506.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1111x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1111x2048): 80.074
Elapsed time for attention_prob_times_values (64x2048x2048x1111): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1111): 81.586

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1483.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1112x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1112x2048): 82.301
Elapsed time for attention_prob_times_values (64x2048x2048x1112): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1112): 89.585

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1576.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1113x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1113x2048): 79.665
Elapsed time for attention_prob_times_values (64x2048x2048x1113): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1113): 81.778

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1484.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1114x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1114x2048): 80.401
Elapsed time for attention_prob_times_values (64x2048x2048x1114): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1114): 83.846

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1510.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1115x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1115x2048): 79.800
Elapsed time for attention_prob_times_values (64x2048x2048x1115): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1115): 81.848

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1488.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1116x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1116x2048): 81.615
Elapsed time for attention_prob_times_values (64x2048x2048x1116): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1116): 84.070

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1527.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1117x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1117x2048): 79.642
Elapsed time for attention_prob_times_values (64x2048x2048x1117): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1117): 81.949

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1490.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1118x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1118x2048): 80.555
Elapsed time for attention_prob_times_values (64x2048x2048x1118): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1118): 84.094

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1519.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1119x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1119x2048): 80.026
Elapsed time for attention_prob_times_values (64x2048x2048x1119): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1119): 82.056

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1497.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1120x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1120x2048): 91.964
Elapsed time for attention_prob_times_values (64x2048x2048x1120): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1120): 91.117

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1693.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1121x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1121x2048): 81.205
Elapsed time for attention_prob_times_values (64x2048x2048x1121): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1121): 82.237

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1513.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1122x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1122x2048): 81.990
Elapsed time for attention_prob_times_values (64x2048x2048x1122): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1122): 84.318

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1540.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1123x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1123x2048): 80.991
Elapsed time for attention_prob_times_values (64x2048x2048x1123): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1123): 82.242

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1513.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1124x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1124x2048): 82.479
Elapsed time for attention_prob_times_values (64x2048x2048x1124): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1124): 84.531

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1549.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1125x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1125x2048): 80.809
Elapsed time for attention_prob_times_values (64x2048x2048x1125): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1125): 82.300

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1515.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1126x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1126x2048): 81.539
Elapsed time for attention_prob_times_values (64x2048x2048x1126): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1126): 84.540

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1543.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1127x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1127x2048): 80.523
Elapsed time for attention_prob_times_values (64x2048x2048x1127): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1127): 82.369

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1515.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1128x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1128x2048): 82.236
Elapsed time for attention_prob_times_values (64x2048x2048x1128): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1128): 90.518

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1605.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1129x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1129x2048): 79.979
Elapsed time for attention_prob_times_values (64x2048x2048x1129): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1129): 82.480

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1513.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1130x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1130x2048): 80.669
Elapsed time for attention_prob_times_values (64x2048x2048x1130): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1130): 84.694

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1541.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1131x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1131x2048): 80.274
Elapsed time for attention_prob_times_values (64x2048x2048x1131): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1131): 82.524

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1519.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1132x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1132x2048): 81.522
Elapsed time for attention_prob_times_values (64x2048x2048x1132): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1132): 84.894

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1554.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1133x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1133x2048): 80.247
Elapsed time for attention_prob_times_values (64x2048x2048x1133): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1133): 82.633

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1522.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1134x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1134x2048): 81.085
Elapsed time for attention_prob_times_values (64x2048x2048x1134): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1134): 84.937

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1553.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1135x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1135x2048): 80.328
Elapsed time for attention_prob_times_values (64x2048x2048x1135): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1135): 82.648

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1526.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1136x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1136x2048): 82.374
Elapsed time for attention_prob_times_values (64x2048x2048x1136): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1136): 91.535

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1625.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1137x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1137x2048): 79.800
Elapsed time for attention_prob_times_values (64x2048x2048x1137): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1137): 82.879

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1525.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1138x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1138x2048): 80.642
Elapsed time for attention_prob_times_values (64x2048x2048x1138): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1138): 85.101

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1555.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1139x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1139x2048): 79.868
Elapsed time for attention_prob_times_values (64x2048x2048x1139): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1139): 82.810

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1528.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1140x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1140x2048): 81.197
Elapsed time for attention_prob_times_values (64x2048x2048x1140): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1140): 85.304

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1565.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1141x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1141x2048): 80.001
Elapsed time for attention_prob_times_values (64x2048x2048x1141): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1141): 83.009

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1534.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1142x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1142x2048): 80.732
Elapsed time for attention_prob_times_values (64x2048x2048x1142): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1142): 85.238

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1562.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1143x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1143x2048): 80.011
Elapsed time for attention_prob_times_values (64x2048x2048x1143): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1143): 83.072

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1537.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1144x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1144x2048): 81.811
Elapsed time for attention_prob_times_values (64x2048x2048x1144): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1144): 91.535

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1630.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1145x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1145x2048): 79.622
Elapsed time for attention_prob_times_values (64x2048x2048x1145): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1145): 83.217

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1537.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1146x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1146x2048): 80.241
Elapsed time for attention_prob_times_values (64x2048x2048x1146): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1146): 85.432

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1564.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1147x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1147x2048): 79.527
Elapsed time for attention_prob_times_values (64x2048x2048x1147): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1147): 83.360

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1540.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1148x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1148x2048): 80.910
Elapsed time for attention_prob_times_values (64x2048x2048x1148): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1148): 85.646

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1575.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1149x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1149x2048): 79.191
Elapsed time for attention_prob_times_values (64x2048x2048x1149): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1149): 83.333

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1539.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1150x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1150x2048): 80.207
Elapsed time for attention_prob_times_values (64x2048x2048x1150): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1150): 85.654

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1571.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1151x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1151x2048): 79.460
Elapsed time for attention_prob_times_values (64x2048x2048x1151): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1151): 83.489

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1545.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1152x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1152x2048): 88.902
Elapsed time for attention_prob_times_values (64x2048x2048x1152): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1152): 94.176

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1737.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1153x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1153x2048): 80.920
Elapsed time for attention_prob_times_values (64x2048x2048x1153): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1153): 76.912

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1499.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1154x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1154x2048): 81.930
Elapsed time for attention_prob_times_values (64x2048x2048x1154): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1154): 79.189

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1532.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1155x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1155x2048): 80.449
Elapsed time for attention_prob_times_values (64x2048x2048x1155): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1155): 77.176

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1500.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1156x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1156x2048): 82.554
Elapsed time for attention_prob_times_values (64x2048x2048x1156): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1156): 79.340

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1542.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1157x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1157x2048): 80.936
Elapsed time for attention_prob_times_values (64x2048x2048x1157): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1157): 77.236

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1507.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1158x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1158x2048): 81.714
Elapsed time for attention_prob_times_values (64x2048x2048x1158): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1158): 79.468

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1538.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1159x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1159x2048): 80.810
Elapsed time for attention_prob_times_values (64x2048x2048x1159): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1159): 77.397

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1510.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1160x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1160x2048): 82.455
Elapsed time for attention_prob_times_values (64x2048x2048x1160): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1160): 84.288

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1594.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1161x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1161x2048): 80.313
Elapsed time for attention_prob_times_values (64x2048x2048x1161): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1161): 77.519

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1510.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1162x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1162x2048): 81.172
Elapsed time for attention_prob_times_values (64x2048x2048x1162): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1162): 79.616

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1539.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1163x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1163x2048): 80.677
Elapsed time for attention_prob_times_values (64x2048x2048x1163): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1163): 77.683

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1517.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1164x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1164x2048): 82.143
Elapsed time for attention_prob_times_values (64x2048x2048x1164): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1164): 79.812

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1553.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1165x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1165x2048): 80.823
Elapsed time for attention_prob_times_values (64x2048x2048x1165): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1165): 77.761

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1522.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1166x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1166x2048): 82.367
Elapsed time for attention_prob_times_values (64x2048x2048x1166): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1166): 79.866

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1558.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1167x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1167x2048): 80.994
Elapsed time for attention_prob_times_values (64x2048x2048x1167): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1167): 77.899

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1527.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1168x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1168x2048): 82.937
Elapsed time for attention_prob_times_values (64x2048x2048x1168): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1168): 85.473

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1620.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1169x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1169x2048): 80.201
Elapsed time for attention_prob_times_values (64x2048x2048x1169): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1169): 78.130

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1524.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1170x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1170x2048): 81.150
Elapsed time for attention_prob_times_values (64x2048x2048x1170): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1170): 80.118

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1554.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1171x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1171x2048): 80.436
Elapsed time for attention_prob_times_values (64x2048x2048x1171): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1171): 78.250

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1530.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1172x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1172x2048): 81.940
Elapsed time for attention_prob_times_values (64x2048x2048x1172): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1172): 80.355

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1567.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1173x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1173x2048): 80.524
Elapsed time for attention_prob_times_values (64x2048x2048x1173): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1173): 78.332

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1534.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1174x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1174x2048): 81.417
Elapsed time for attention_prob_times_values (64x2048x2048x1174): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1174): 80.336

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1564.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1175x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1175x2048): 82.162
Elapsed time for attention_prob_times_values (64x2048x2048x1175): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1175): 78.445

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1553.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1176x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1176x2048): 83.820
Elapsed time for attention_prob_times_values (64x2048x2048x1176): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1176): 85.688

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1641.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1177x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1177x2048): 80.183
Elapsed time for attention_prob_times_values (64x2048x2048x1177): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1177): 78.581

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1539.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1178x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1178x2048): 80.853
Elapsed time for attention_prob_times_values (64x2048x2048x1178): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1178): 80.592

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1566.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1179x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1179x2048): 80.123
Elapsed time for attention_prob_times_values (64x2048x2048x1179): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1179): 78.770

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1542.889
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1180x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1180x2048): 81.672
Elapsed time for attention_prob_times_values (64x2048x2048x1180): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1180): 80.879

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1579.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1181x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1181x2048): 80.140
Elapsed time for attention_prob_times_values (64x2048x2048x1181): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1181): 78.894

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1546.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1182x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1182x2048): 81.063
Elapsed time for attention_prob_times_values (64x2048x2048x1182): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1182): 80.913

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1576.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1183x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1183x2048): 80.383
Elapsed time for attention_prob_times_values (64x2048x2048x1183): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1183): 79.040

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1553.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1184x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1184x2048): 92.215
Elapsed time for attention_prob_times_values (64x2048x2048x1184): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1184): 87.072

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1746.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1185x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1185x2048): 81.807
Elapsed time for attention_prob_times_values (64x2048x2048x1185): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1185): 79.197

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1570.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1186x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1186x2048): 82.695
Elapsed time for attention_prob_times_values (64x2048x2048x1186): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1186): 81.234

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1600.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 18992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1187x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1187x2048): 81.597
Elapsed time for attention_prob_times_values (64x2048x2048x1187): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1187): 79.262

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1571.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1188x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1188x2048): 83.366
Elapsed time for attention_prob_times_values (64x2048x2048x1188): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1188): 81.445

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1611.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1189x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1189x2048): 81.313
Elapsed time for attention_prob_times_values (64x2048x2048x1189): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1189): 79.342

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1572.422
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1190x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1190x2048): 82.324
Elapsed time for attention_prob_times_values (64x2048x2048x1190): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1190): 81.433

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1604.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1191x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1191x2048): 81.220
Elapsed time for attention_prob_times_values (64x2048x2048x1191): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1191): 79.461

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1575.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1192x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1192x2048): 83.366
Elapsed time for attention_prob_times_values (64x2048x2048x1192): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1192): 86.507

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1666.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1193x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1193x2048): 80.813
Elapsed time for attention_prob_times_values (64x2048x2048x1193): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1193): 79.604

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1575.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1194x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1194x2048): 81.615
Elapsed time for attention_prob_times_values (64x2048x2048x1194): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1194): 81.744

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1605.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1195x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1195x2048): 80.956
Elapsed time for attention_prob_times_values (64x2048x2048x1195): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1195): 79.664

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1579.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1196x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1196x2048): 82.790
Elapsed time for attention_prob_times_values (64x2048x2048x1196): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1196): 81.953

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1621.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1197x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1197x2048): 81.050
Elapsed time for attention_prob_times_values (64x2048x2048x1197): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1197): 79.673

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1583.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1198x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1198x2048): 82.050
Elapsed time for attention_prob_times_values (64x2048x2048x1198): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1198): 81.948

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1616.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1199x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1199x2048): 81.169
Elapsed time for attention_prob_times_values (64x2048x2048x1199): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1199): 79.827

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1588.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1200x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1200x2048): 83.436
Elapsed time for attention_prob_times_values (64x2048x2048x1200): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1200): 87.761

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1689.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1201x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1201x2048): 80.777
Elapsed time for attention_prob_times_values (64x2048x2048x1201): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1201): 80.029

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1589.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1202x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1202x2048): 81.644
Elapsed time for attention_prob_times_values (64x2048x2048x1202): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1202): 82.237

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1620.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1203x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1203x2048): 80.803
Elapsed time for attention_prob_times_values (64x2048x2048x1203): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1203): 80.143

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1593.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1204x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1204x2048): 82.577
Elapsed time for attention_prob_times_values (64x2048x2048x1204): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1204): 82.339

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1633.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1205x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1205x2048): 81.114
Elapsed time for attention_prob_times_values (64x2048x2048x1205): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1205): 80.241

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1599.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1206x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1206x2048): 81.731
Elapsed time for attention_prob_times_values (64x2048x2048x1206): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1206): 82.415

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1628.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1207x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1207x2048): 81.059
Elapsed time for attention_prob_times_values (64x2048x2048x1207): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1207): 80.329

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1602.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1208x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1208x2048): 83.104
Elapsed time for attention_prob_times_values (64x2048x2048x1208): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1208): 87.685

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1696.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1209x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1209x2048): 80.880
Elapsed time for attention_prob_times_values (64x2048x2048x1209): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1209): 80.467

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1604.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1210x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1210x2048): 81.419
Elapsed time for attention_prob_times_values (64x2048x2048x1210): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1210): 82.734

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1633.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1211x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1211x2048): 80.779
Elapsed time for attention_prob_times_values (64x2048x2048x1211): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1211): 80.545

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1606.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1212x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1212x2048): 82.333
Elapsed time for attention_prob_times_values (64x2048x2048x1212): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1212): 82.908

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1647.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1213x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1213x2048): 80.790
Elapsed time for attention_prob_times_values (64x2048x2048x1213): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1213): 80.687

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1610.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1214x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1214x2048): 81.624
Elapsed time for attention_prob_times_values (64x2048x2048x1214): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1214): 82.934

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1642.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1215x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1215x2048): 80.952
Elapsed time for attention_prob_times_values (64x2048x2048x1215): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1215): 80.785

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1616.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1216x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1216x2048): 92.422
Elapsed time for attention_prob_times_values (64x2048x2048x1216): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1216): 90.134

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1825.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1217x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1217x2048): 82.091
Elapsed time for attention_prob_times_values (64x2048x2048x1217): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1217): 80.846

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1630.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1218x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1218x2048): 83.092
Elapsed time for attention_prob_times_values (64x2048x2048x1218): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1218): 83.152

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1665.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1219x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1219x2048): 82.032
Elapsed time for attention_prob_times_values (64x2048x2048x1219): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1219): 80.979

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1633.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1220x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1220x2048): 83.917
Elapsed time for attention_prob_times_values (64x2048x2048x1220): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1220): 83.324

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1677.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1221x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1221x2048): 81.885
Elapsed time for attention_prob_times_values (64x2048x2048x1221): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1221): 81.045

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1635.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1222x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1222x2048): 82.718
Elapsed time for attention_prob_times_values (64x2048x2048x1222): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1222): 83.264

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1667.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1223x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1223x2048): 81.737
Elapsed time for attention_prob_times_values (64x2048x2048x1223): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1223): 81.223

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1638.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1224x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1224x2048): 84.419
Elapsed time for attention_prob_times_values (64x2048x2048x1224): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1224): 88.898

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1742.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1225x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1225x2048): 81.243
Elapsed time for attention_prob_times_values (64x2048x2048x1225): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1225): 81.298

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1636.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1226x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1226x2048): 82.431
Elapsed time for attention_prob_times_values (64x2048x2048x1226): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1226): 83.471

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1671.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1227x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1227x2048): 81.513
Elapsed time for attention_prob_times_values (64x2048x2048x1227): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1227): 81.410

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1643.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1228x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1228x2048): 83.104
Elapsed time for attention_prob_times_values (64x2048x2048x1228): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1228): 83.738

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1684.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1229x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1229x2048): 81.394
Elapsed time for attention_prob_times_values (64x2048x2048x1229): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1229): 81.525

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1645.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1230x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1230x2048): 82.400
Elapsed time for attention_prob_times_values (64x2048x2048x1230): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1230): 83.677

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1678.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1231x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1231x2048): 81.572
Elapsed time for attention_prob_times_values (64x2048x2048x1231): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1231): 81.733

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1652.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1232x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1232x2048): 83.849
Elapsed time for attention_prob_times_values (64x2048x2048x1232): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1232): 89.911

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1757.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1233x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1233x2048): 81.091
Elapsed time for attention_prob_times_values (64x2048x2048x1233): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1233): 81.892

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1651.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1234x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1234x2048): 82.123
Elapsed time for attention_prob_times_values (64x2048x2048x1234): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1234): 83.967

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1684.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1235x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1235x2048): 81.149
Elapsed time for attention_prob_times_values (64x2048x2048x1235): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1235): 81.902

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1654.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1236x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1236x2048): 82.711
Elapsed time for attention_prob_times_values (64x2048x2048x1236): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1236): 84.136

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1694.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1237x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1237x2048): 81.067
Elapsed time for attention_prob_times_values (64x2048x2048x1237): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1237): 71.991

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1550.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1238x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1238x2048): 81.929
Elapsed time for attention_prob_times_values (64x2048x2048x1238): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1238): 77.405

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1619.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1239x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1239x2048): 81.172
Elapsed time for attention_prob_times_values (64x2048x2048x1239): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1239): 72.249

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1556.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1240x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1240x2048): 82.792
Elapsed time for attention_prob_times_values (64x2048x2048x1240): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1240): 89.959

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1756.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1241x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1241x2048): 80.642
Elapsed time for attention_prob_times_values (64x2048x2048x1241): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1241): 72.511

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1557.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1242x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1242x2048): 81.303
Elapsed time for attention_prob_times_values (64x2048x2048x1242): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1242): 77.655

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1621.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1243x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1243x2048): 80.651
Elapsed time for attention_prob_times_values (64x2048x2048x1243): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1243): 72.820

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1563.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1244x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1244x2048): 81.922
Elapsed time for attention_prob_times_values (64x2048x2048x1244): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1244): 78.009

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1633.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1245x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1245x2048): 80.667
Elapsed time for attention_prob_times_values (64x2048x2048x1245): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1245): 73.091

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1568.599
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1246x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1246x2048): 81.612
Elapsed time for attention_prob_times_values (64x2048x2048x1246): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1246): 77.918

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1631.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1247x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1247x2048): 80.787
Elapsed time for attention_prob_times_values (64x2048x2048x1247): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1247): 73.506

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1576.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1248x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1248x2048): 93.144
Elapsed time for attention_prob_times_values (64x2048x2048x1248): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1248): 91.224

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1889.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 19984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1249x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1249x2048): 81.545
Elapsed time for attention_prob_times_values (64x2048x2048x1249): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1249): 73.349

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1584.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1250x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1250x2048): 82.399
Elapsed time for attention_prob_times_values (64x2048x2048x1250): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1250): 78.209

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1647.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1251x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1251x2048): 81.496
Elapsed time for attention_prob_times_values (64x2048x2048x1251): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1251): 73.304

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1585.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1252x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1252x2048): 82.797
Elapsed time for attention_prob_times_values (64x2048x2048x1252): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1252): 78.446

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1656.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1253x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1253x2048): 81.309
Elapsed time for attention_prob_times_values (64x2048x2048x1253): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1253): 73.261

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1586.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1254x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1254x2048): 82.093
Elapsed time for attention_prob_times_values (64x2048x2048x1254): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1254): 78.368

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1651.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1255x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1255x2048): 81.193
Elapsed time for attention_prob_times_values (64x2048x2048x1255): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1255): 73.144

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1586.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1256x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1256x2048): 82.952
Elapsed time for attention_prob_times_values (64x2048x2048x1256): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1256): 90.967

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1789.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1257x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1257x2048): 80.851
Elapsed time for attention_prob_times_values (64x2048x2048x1257): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1257): 73.329

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1587.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1258x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1258x2048): 81.841
Elapsed time for attention_prob_times_values (64x2048x2048x1258): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1258): 78.557

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1655.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1259x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1259x2048): 81.028
Elapsed time for attention_prob_times_values (64x2048x2048x1259): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1259): 73.503

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1593.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1260x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1260x2048): 82.316
Elapsed time for attention_prob_times_values (64x2048x2048x1260): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1260): 78.800

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1665.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1261x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1261x2048): 81.131
Elapsed time for attention_prob_times_values (64x2048x2048x1261): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1261): 73.719

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1599.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1262x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1262x2048): 82.194
Elapsed time for attention_prob_times_values (64x2048x2048x1262): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1262): 78.809

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1667.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1263x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1263x2048): 81.091
Elapsed time for attention_prob_times_values (64x2048x2048x1263): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1263): 73.712

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1601.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1264x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1264x2048): 83.376
Elapsed time for attention_prob_times_values (64x2048x2048x1264): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1264): 92.178

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1816.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1265x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1265x2048): 80.805
Elapsed time for attention_prob_times_values (64x2048x2048x1265): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1265): 73.478

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1598.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1266x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1266x2048): 81.806
Elapsed time for attention_prob_times_values (64x2048x2048x1266): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1266): 78.991

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1670.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1267x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1267x2048): 80.756
Elapsed time for attention_prob_times_values (64x2048x2048x1267): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1267): 73.279

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1597.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1268x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1268x2048): 82.027
Elapsed time for attention_prob_times_values (64x2048x2048x1268): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1268): 79.236

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1677.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1269x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1269x2048): 80.943
Elapsed time for attention_prob_times_values (64x2048x2048x1269): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1269): 73.214

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1601.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1270x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1270x2048): 81.989
Elapsed time for attention_prob_times_values (64x2048x2048x1270): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1270): 79.146

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1678.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1271x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1271x2048): 80.970
Elapsed time for attention_prob_times_values (64x2048x2048x1271): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1271): 73.198

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1603.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1272x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1272x2048): 82.747
Elapsed time for attention_prob_times_values (64x2048x2048x1272): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1272): 92.034

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1819.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1273x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1273x2048): 80.568
Elapsed time for attention_prob_times_values (64x2048x2048x1273): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1273): 73.152

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1601.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1274x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1274x2048): 81.618
Elapsed time for attention_prob_times_values (64x2048x2048x1274): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1274): 79.292

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1681.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1275x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1275x2048): 79.996
Elapsed time for attention_prob_times_values (64x2048x2048x1275): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1275): 73.116

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1598.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1276x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1276x2048): 82.029
Elapsed time for attention_prob_times_values (64x2048x2048x1276): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1276): 79.191

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1687.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1277x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1277x2048): 80.211
Elapsed time for attention_prob_times_values (64x2048x2048x1277): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1277): 73.125

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1602.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1278x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1278x2048): 81.314
Elapsed time for attention_prob_times_values (64x2048x2048x1278): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1278): 79.039

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1680.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1279x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1279x2048): 79.709
Elapsed time for attention_prob_times_values (64x2048x2048x1279): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1279): 73.226

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1601.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1280x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1280x2048): 90.562
Elapsed time for attention_prob_times_values (64x2048x2048x1280): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1280): 94.878

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1946.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1281x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1281x2048): 80.617
Elapsed time for attention_prob_times_values (64x2048x2048x1281): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1281): 68.470

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1556.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1282x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1282x2048): 82.323
Elapsed time for attention_prob_times_values (64x2048x2048x1282): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1282): 73.763

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1636.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1283x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1283x2048): 81.124
Elapsed time for attention_prob_times_values (64x2048x2048x1283): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1283): 68.320

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1561.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1284x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1284x2048): 82.967
Elapsed time for attention_prob_times_values (64x2048x2048x1284): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1284): 74.019

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1647.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1285x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1285x2048): 81.034
Elapsed time for attention_prob_times_values (64x2048x2048x1285): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1285): 68.537

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1565.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1286x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1286x2048): 82.584
Elapsed time for attention_prob_times_values (64x2048x2048x1286): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1286): 73.903

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1645.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1287x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1287x2048): 81.352
Elapsed time for attention_prob_times_values (64x2048x2048x1287): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1287): 68.818

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1573.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1288x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1288x2048): 83.426
Elapsed time for attention_prob_times_values (64x2048x2048x1288): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1288): 84.728

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1776.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1289x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1289x2048): 80.885
Elapsed time for attention_prob_times_values (64x2048x2048x1289): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1289): 68.873

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1572.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1290x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1290x2048): 82.085
Elapsed time for attention_prob_times_values (64x2048x2048x1290): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1290): 74.120

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1648.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1291x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1291x2048): 81.067
Elapsed time for attention_prob_times_values (64x2048x2048x1291): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1291): 69.223

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1581.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1292x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1292x2048): 82.677
Elapsed time for attention_prob_times_values (64x2048x2048x1292): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1292): 74.390

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 1659.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1293x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1293x2048): 81.039
Elapsed time for attention_prob_times_values (64x2048x2048x1293): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1293): 69.403

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1585.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1294x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1294x2048): 82.526
Elapsed time for attention_prob_times_values (64x2048x2048x1294): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1294): 74.234

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1658.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1295x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1295x2048): 81.334
Elapsed time for attention_prob_times_values (64x2048x2048x1295): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1295): 69.703

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1594.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1296x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1296x2048): 84.041
Elapsed time for attention_prob_times_values (64x2048x2048x1296): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1296): 85.629

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1802.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1297x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1297x2048): 80.637
Elapsed time for attention_prob_times_values (64x2048x2048x1297): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1297): 69.817

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1591.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1298x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1298x2048): 81.962
Elapsed time for attention_prob_times_values (64x2048x2048x1298): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1298): 74.448

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1660.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1299x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1299x2048): 81.018
Elapsed time for attention_prob_times_values (64x2048x2048x1299): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1299): 69.810

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1597.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1300x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1300x2048): 82.273
Elapsed time for attention_prob_times_values (64x2048x2048x1300): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1300): 74.696

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1668.800
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1301x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1301x2048): 81.062
Elapsed time for attention_prob_times_values (64x2048x2048x1301): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1301): 69.780

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1599.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1302x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1302x2048): 82.311
Elapsed time for attention_prob_times_values (64x2048x2048x1302): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1302): 74.550

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1669.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1303x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1303x2048): 81.068
Elapsed time for attention_prob_times_values (64x2048x2048x1303): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1303): 70.103

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1605.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1304x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1304x2048): 83.360
Elapsed time for attention_prob_times_values (64x2048x2048x1304): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1304): 85.685

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1806.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1305x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1305x2048): 80.766
Elapsed time for attention_prob_times_values (64x2048x2048x1305): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1305): 70.214

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1606.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1306x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1306x2048): 81.464
Elapsed time for attention_prob_times_values (64x2048x2048x1306): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1306): 74.741

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1668.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1307x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1307x2048): 80.772
Elapsed time for attention_prob_times_values (64x2048x2048x1307): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1307): 70.487

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1612.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1308x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1308x2048): 82.464
Elapsed time for attention_prob_times_values (64x2048x2048x1308): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1308): 75.033

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1684.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1309x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1309x2048): 80.856
Elapsed time for attention_prob_times_values (64x2048x2048x1309): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1309): 70.603

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1617.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1310x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1310x2048): 82.267
Elapsed time for attention_prob_times_values (64x2048x2048x1310): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1310): 74.987

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1684.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1311x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1311x2048): 81.171
Elapsed time for attention_prob_times_values (64x2048x2048x1311): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1311): 70.898

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1626.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1312x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1312x2048): 93.057
Elapsed time for attention_prob_times_values (64x2048x2048x1312): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1312): 87.332

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1937.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1313x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1313x2048): 82.071
Elapsed time for attention_prob_times_values (64x2048x2048x1313): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1313): 70.948

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1637.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1314x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1314x2048): 83.358
Elapsed time for attention_prob_times_values (64x2048x2048x1314): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1314): 75.106

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1701.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1315x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1315x2048): 82.169
Elapsed time for attention_prob_times_values (64x2048x2048x1315): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1315): 70.837

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1639.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1316x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1316x2048): 84.034
Elapsed time for attention_prob_times_values (64x2048x2048x1316): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1316): 75.419

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1714.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1317x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1317x2048): 82.043
Elapsed time for attention_prob_times_values (64x2048x2048x1317): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1317): 70.713

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1639.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1318x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1318x2048): 83.218
Elapsed time for attention_prob_times_values (64x2048x2048x1318): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1318): 75.367

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1708.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1319x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1319x2048): 81.912
Elapsed time for attention_prob_times_values (64x2048x2048x1319): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1319): 70.673

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1639.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1320x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1320x2048): 84.273
Elapsed time for attention_prob_times_values (64x2048x2048x1320): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1320): 86.840

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1849.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1321x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1321x2048): 81.436
Elapsed time for attention_prob_times_values (64x2048x2048x1321): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1321): 70.653

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1637.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1322x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1322x2048): 82.368
Elapsed time for attention_prob_times_values (64x2048x2048x1322): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1322): 75.484

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1706.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1323x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1323x2048): 81.419
Elapsed time for attention_prob_times_values (64x2048x2048x1323): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1323): 70.846

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1641.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1324x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1324x2048): 83.184
Elapsed time for attention_prob_times_values (64x2048x2048x1324): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1324): 75.839

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1720.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1325x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1325x2048): 81.692
Elapsed time for attention_prob_times_values (64x2048x2048x1325): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1325): 71.002

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1648.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1326x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1326x2048): 82.900
Elapsed time for attention_prob_times_values (64x2048x2048x1326): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1326): 75.782

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1719.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1327x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1327x2048): 81.878
Elapsed time for attention_prob_times_values (64x2048x2048x1327): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1327): 71.024

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1653.247
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1328x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1328x2048): 84.248
Elapsed time for attention_prob_times_values (64x2048x2048x1328): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1328): 87.805

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1870.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1329x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1329x2048): 81.275
Elapsed time for attention_prob_times_values (64x2048x2048x1329): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1329): 70.949

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1649.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1330x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1330x2048): 82.359
Elapsed time for attention_prob_times_values (64x2048x2048x1330): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1330): 75.978

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1721.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1331x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1331x2048): 81.438
Elapsed time for attention_prob_times_values (64x2048x2048x1331): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1331): 70.796

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1651.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1332x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1332x2048): 82.980
Elapsed time for attention_prob_times_values (64x2048x2048x1332): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1332): 76.226

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1733.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1333x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1333x2048): 81.575
Elapsed time for attention_prob_times_values (64x2048x2048x1333): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1333): 70.907

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1656.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1334x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1334x2048): 82.634
Elapsed time for attention_prob_times_values (64x2048x2048x1334): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1334): 76.156

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1731.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1335x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1335x2048): 81.556
Elapsed time for attention_prob_times_values (64x2048x2048x1335): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1335): 70.878

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1657.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1336x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1336x2048): 83.751
Elapsed time for attention_prob_times_values (64x2048x2048x1336): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1336): 87.975

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1877.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1337x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1337x2048): 81.108
Elapsed time for attention_prob_times_values (64x2048x2048x1337): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1337): 70.959

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1657.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1338x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1338x2048): 82.203
Elapsed time for attention_prob_times_values (64x2048x2048x1338): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1338): 76.321

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1733.939
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1339x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1339x2048): 81.387
Elapsed time for attention_prob_times_values (64x2048x2048x1339): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1339): 70.935

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1661.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1340x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1340x2048): 82.832
Elapsed time for attention_prob_times_values (64x2048x2048x1340): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1340): 76.670

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1746.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1341x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1341x2048): 81.417
Elapsed time for attention_prob_times_values (64x2048x2048x1341): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1341): 71.001

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1665.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1342x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1342x2048): 82.305
Elapsed time for attention_prob_times_values (64x2048x2048x1342): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1342): 76.572

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1742.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1343x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1343x2048): 81.352
Elapsed time for attention_prob_times_values (64x2048x2048x1343): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1343): 71.052

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1667.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1344x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1344x2048): 92.458
Elapsed time for attention_prob_times_values (64x2048x2048x1344): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1344): 90.239

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 2009.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1345x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1345x2048): 82.417
Elapsed time for attention_prob_times_values (64x2048x2048x1345): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1345): 70.966

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1678.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1346x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1346x2048): 83.709
Elapsed time for attention_prob_times_values (64x2048x2048x1346): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1346): 76.748

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1764.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1347x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1347x2048): 82.473
Elapsed time for attention_prob_times_values (64x2048x2048x1347): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1347): 71.023

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1682.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1348x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1348x2048): 83.963
Elapsed time for attention_prob_times_values (64x2048x2048x1348): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1348): 77.039

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1772.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1349x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1349x2048): 82.093
Elapsed time for attention_prob_times_values (64x2048x2048x1349): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1349): 71.152

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1683.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1350x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1350x2048): 83.133
Elapsed time for attention_prob_times_values (64x2048x2048x1350): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1350): 76.914

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1765.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1351x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1351x2048): 82.011
Elapsed time for attention_prob_times_values (64x2048x2048x1351): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1351): 71.235

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1685.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1352x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1352x2048): 84.382
Elapsed time for attention_prob_times_values (64x2048x2048x1352): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1352): 89.091

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1917.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1353x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1353x2048): 81.770
Elapsed time for attention_prob_times_values (64x2048x2048x1353): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1353): 71.399

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1687.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1354x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1354x2048): 82.562
Elapsed time for attention_prob_times_values (64x2048x2048x1354): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1354): 77.021

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1765.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1355x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1355x2048): 81.817
Elapsed time for attention_prob_times_values (64x2048x2048x1355): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1355): 71.291

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1689.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1356x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1356x2048): 84.846
Elapsed time for attention_prob_times_values (64x2048x2048x1356): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1356): 77.253

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1794.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1357x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1357x2048): 84.928
Elapsed time for attention_prob_times_values (64x2048x2048x1357): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1357): 71.555

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1724.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1358x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1358x2048): 83.255
Elapsed time for attention_prob_times_values (64x2048x2048x1358): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1358): 77.145

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1779.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1359x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1359x2048): 83.232
Elapsed time for attention_prob_times_values (64x2048x2048x1359): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1359): 71.426

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1709.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1360x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1360x2048): 88.615
Elapsed time for attention_prob_times_values (64x2048x2048x1360): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1360): 90.024

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1987.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1361x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1361x2048): 86.779
Elapsed time for attention_prob_times_values (64x2048x2048x1361): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1361): 71.820

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1749.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1362x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1362x2048): 83.355
Elapsed time for attention_prob_times_values (64x2048x2048x1362): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1362): 77.317

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1787.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1363x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1363x2048): 82.207
Elapsed time for attention_prob_times_values (64x2048x2048x1363): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1363): 71.292

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1702.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1364x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1364x2048): 83.666
Elapsed time for attention_prob_times_values (64x2048x2048x1364): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1364): 77.629

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1796.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1365x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1365x2048): 81.494
Elapsed time for attention_prob_times_values (64x2048x2048x1365): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1365): 71.932

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1706.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1366x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1366x2048): 82.392
Elapsed time for attention_prob_times_values (64x2048x2048x1366): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1366): 77.490

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1784.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1367x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1367x2048): 83.145
Elapsed time for attention_prob_times_values (64x2048x2048x1367): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1367): 71.753

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1722.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1368x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1368x2048): 83.802
Elapsed time for attention_prob_times_values (64x2048x2048x1368): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1368): 90.259

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1944.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1369x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1369x2048): 82.642
Elapsed time for attention_prob_times_values (64x2048x2048x1369): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1369): 72.149

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1724.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1370x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1370x2048): 86.307
Elapsed time for attention_prob_times_values (64x2048x2048x1370): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1370): 77.746

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1832.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1371x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1371x2048): 85.318
Elapsed time for attention_prob_times_values (64x2048x2048x1371): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1371): 72.825

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1761.869
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1372x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1372x2048): 83.728
Elapsed time for attention_prob_times_values (64x2048x2048x1372): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1372): 78.110

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1813.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1373x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1373x2048): 81.948
Elapsed time for attention_prob_times_values (64x2048x2048x1373): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1373): 72.916

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1732.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1374x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1374x2048): 85.841
Elapsed time for attention_prob_times_values (64x2048x2048x1374): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1374): 78.093

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1837.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1375x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1375x2048): 84.361
Elapsed time for attention_prob_times_values (64x2048x2048x1375): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1375): 73.529

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 1766.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1376x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1376x2048): 92.354
Elapsed time for attention_prob_times_values (64x2048x2048x1376): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1376): 91.725

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 2070.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1377x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1377x2048): 82.596
Elapsed time for attention_prob_times_values (64x2048x2048x1377): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1377): 73.590

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1752.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1378x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1378x2048): 83.559
Elapsed time for attention_prob_times_values (64x2048x2048x1378): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1378): 78.362

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1822.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1379x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1379x2048): 82.382
Elapsed time for attention_prob_times_values (64x2048x2048x1379): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1379): 73.491

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1751.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1380x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1380x2048): 83.775
Elapsed time for attention_prob_times_values (64x2048x2048x1380): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1380): 78.573

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1829.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1381x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1381x2048): 82.149
Elapsed time for attention_prob_times_values (64x2048x2048x1381): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1381): 73.513

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1751.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1382x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1382x2048): 82.975
Elapsed time for attention_prob_times_values (64x2048x2048x1382): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1382): 78.494

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1822.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1383x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1383x2048): 82.018
Elapsed time for attention_prob_times_values (64x2048x2048x1383): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1383): 73.382

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1751.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1384x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1384x2048): 84.182
Elapsed time for attention_prob_times_values (64x2048x2048x1384): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1384): 91.019

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1978.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1385x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1385x2048): 81.647
Elapsed time for attention_prob_times_values (64x2048x2048x1385): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1385): 73.410

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1750.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1386x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1386x2048): 82.686
Elapsed time for attention_prob_times_values (64x2048x2048x1386): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1386): 78.670

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1826.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1387x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1387x2048): 81.726
Elapsed time for attention_prob_times_values (64x2048x2048x1387): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1387): 73.507

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1754.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1388x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1388x2048): 83.118
Elapsed time for attention_prob_times_values (64x2048x2048x1388): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1388): 78.971

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1837.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1389x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1389x2048): 81.776
Elapsed time for attention_prob_times_values (64x2048x2048x1389): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1389): 73.757

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1760.848
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1390x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1390x2048): 82.925
Elapsed time for attention_prob_times_values (64x2048x2048x1390): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1390): 78.875

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1836.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1391x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1391x2048): 81.835
Elapsed time for attention_prob_times_values (64x2048x2048x1391): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1391): 73.782

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1764.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1392x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1392x2048): 84.657
Elapsed time for attention_prob_times_values (64x2048x2048x1392): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1392): 91.829

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2004.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1393x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1393x2048): 81.616
Elapsed time for attention_prob_times_values (64x2048x2048x1393): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1393): 73.695

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1763.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1394x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1394x2048): 82.558
Elapsed time for attention_prob_times_values (64x2048x2048x1394): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1394): 79.096

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1840.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1395x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1395x2048): 81.536
Elapsed time for attention_prob_times_values (64x2048x2048x1395): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1395): 73.631

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1764.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1396x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1396x2048): 83.131
Elapsed time for attention_prob_times_values (64x2048x2048x1396): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1396): 79.348

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1852.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1397x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1397x2048): 81.528
Elapsed time for attention_prob_times_values (64x2048x2048x1397): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1397): 73.583

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1765.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1398x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1398x2048): 82.677
Elapsed time for attention_prob_times_values (64x2048x2048x1398): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1398): 79.257

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1848.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1399x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1399x2048): 81.687
Elapsed time for attention_prob_times_values (64x2048x2048x1399): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1399): 73.524

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1769.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1400x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1400x2048): 83.407
Elapsed time for attention_prob_times_values (64x2048x2048x1400): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1400): 91.752

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1998.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1401x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1401x2048): 81.281
Elapsed time for attention_prob_times_values (64x2048x2048x1401): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1401): 73.704

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1769.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1402x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1402x2048): 82.394
Elapsed time for attention_prob_times_values (64x2048x2048x1402): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1402): 79.426

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1852.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1403x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1403x2048): 81.249
Elapsed time for attention_prob_times_values (64x2048x2048x1403): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1403): 73.755

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1772.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1404x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1404x2048): 82.659
Elapsed time for attention_prob_times_values (64x2048x2048x1404): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1404): 79.784

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1862.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1405x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1405x2048): 81.421
Elapsed time for attention_prob_times_values (64x2048x2048x1405): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1405): 73.796

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1777.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1406x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1406x2048): 81.909
Elapsed time for attention_prob_times_values (64x2048x2048x1406): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1406): 79.723

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1855.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1407x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1407x2048): 80.719
Elapsed time for attention_prob_times_values (64x2048x2048x1407): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1407): 73.934

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1773.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1408x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1408x2048): 90.732
Elapsed time for attention_prob_times_values (64x2048x2048x1408): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1408): 94.440

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 2128.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1409x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1409x2048): 81.853
Elapsed time for attention_prob_times_values (64x2048x2048x1409): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1409): 67.613

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1704.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1410x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1410x2048): 83.114
Elapsed time for attention_prob_times_values (64x2048x2048x1410): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1410): 73.304

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1794.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1411x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1411x2048): 82.362
Elapsed time for attention_prob_times_values (64x2048x2048x1411): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1411): 68.118

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1718.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1412x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1412x2048): 83.955
Elapsed time for attention_prob_times_values (64x2048x2048x1412): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1412): 73.553

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1808.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1413x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1413x2048): 82.323
Elapsed time for attention_prob_times_values (64x2048x2048x1413): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1413): 68.085

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1720.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1414x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1414x2048): 83.308
Elapsed time for attention_prob_times_values (64x2048x2048x1414): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1414): 73.311

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1801.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1415x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1415x2048): 82.433
Elapsed time for attention_prob_times_values (64x2048x2048x1415): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1415): 68.280

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1726.091
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1416x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1416x2048): 84.217
Elapsed time for attention_prob_times_values (64x2048x2048x1416): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1416): 85.730

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1964.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1417x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1417x2048): 82.021
Elapsed time for attention_prob_times_values (64x2048x2048x1417): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1417): 68.454

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1726.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1418x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1418x2048): 82.865
Elapsed time for attention_prob_times_values (64x2048x2048x1418): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1418): 73.508

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1804.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1419x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1419x2048): 82.091
Elapsed time for attention_prob_times_values (64x2048x2048x1419): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1419): 68.613

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1732.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1420x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1420x2048): 83.407
Elapsed time for attention_prob_times_values (64x2048x2048x1420): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1420): 73.766

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1815.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1421x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1421x2048): 82.190
Elapsed time for attention_prob_times_values (64x2048x2048x1421): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1421): 68.769

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1737.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1422x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1422x2048): 83.207
Elapsed time for attention_prob_times_values (64x2048x2048x1422): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1422): 73.555

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1813.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1423x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1423x2048): 82.269
Elapsed time for attention_prob_times_values (64x2048x2048x1423): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1423): 69.090

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1745.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1424x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1424x2048): 84.886
Elapsed time for attention_prob_times_values (64x2048x2048x1424): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1424): 86.465

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1991.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1425x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1425x2048): 82.004
Elapsed time for attention_prob_times_values (64x2048x2048x1425): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1425): 69.115

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1745.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1426x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1426x2048): 83.197
Elapsed time for attention_prob_times_values (64x2048x2048x1426): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1426): 73.792

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1820.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1427x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1427x2048): 81.929
Elapsed time for attention_prob_times_values (64x2048x2048x1427): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1427): 69.208

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1748.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1428x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1428x2048): 83.150
Elapsed time for attention_prob_times_values (64x2048x2048x1428): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1428): 74.168

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1827.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1429x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1429x2048): 82.054
Elapsed time for attention_prob_times_values (64x2048x2048x1429): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1429): 69.202

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1751.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1430x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1430x2048): 82.827
Elapsed time for attention_prob_times_values (64x2048x2048x1430): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1430): 74.018

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1824.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1431x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1431x2048): 82.081
Elapsed time for attention_prob_times_values (64x2048x2048x1431): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1431): 69.396

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1756.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1432x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1432x2048): 84.155
Elapsed time for attention_prob_times_values (64x2048x2048x1432): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1432): 86.699

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1996.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1433x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1433x2048): 81.856
Elapsed time for attention_prob_times_values (64x2048x2048x1433): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1433): 69.592

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1759.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1434x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1434x2048): 82.721
Elapsed time for attention_prob_times_values (64x2048x2048x1434): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1434): 74.182

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1830.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1435x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1435x2048): 81.816
Elapsed time for attention_prob_times_values (64x2048x2048x1435): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1435): 69.847

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1765.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1436x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1436x2048): 83.374
Elapsed time for attention_prob_times_values (64x2048x2048x1436): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1436): 74.429

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1843.319
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 22992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1437x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1437x2048): 81.993
Elapsed time for attention_prob_times_values (64x2048x2048x1437): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1437): 69.861

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1769.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1438x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1438x2048): 83.032
Elapsed time for attention_prob_times_values (64x2048x2048x1438): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1438): 74.358

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1841.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1439x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1439x2048): 82.146
Elapsed time for attention_prob_times_values (64x2048x2048x1439): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1439): 70.134

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1776.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1440x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1440x2048): 94.382
Elapsed time for attention_prob_times_values (64x2048x2048x1440): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1440): 88.323

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2144.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1441x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1441x2048): 82.916
Elapsed time for attention_prob_times_values (64x2048x2048x1441): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1441): 70.151

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1787.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1442x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1442x2048): 84.209
Elapsed time for attention_prob_times_values (64x2048x2048x1442): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1442): 74.483

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1860.099
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1443x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1443x2048): 83.097
Elapsed time for attention_prob_times_values (64x2048x2048x1443): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1443): 70.145

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1791.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1444x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1444x2048): 84.668
Elapsed time for attention_prob_times_values (64x2048x2048x1444): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1444): 74.877

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1872.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1445x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1445x2048): 82.915
Elapsed time for attention_prob_times_values (64x2048x2048x1445): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1445): 70.017

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1790.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1446x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1446x2048): 84.170
Elapsed time for attention_prob_times_values (64x2048x2048x1446): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1446): 74.746

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1868.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1447x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1447x2048): 83.047
Elapsed time for attention_prob_times_values (64x2048x2048x1447): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1447): 69.943

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1792.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1448x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1448x2048): 85.142
Elapsed time for attention_prob_times_values (64x2048x2048x1448): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1448): 87.698

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2041.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1449x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1449x2048): 82.604
Elapsed time for attention_prob_times_values (64x2048x2048x1449): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1449): 70.054

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1792.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1450x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1450x2048): 83.258
Elapsed time for attention_prob_times_values (64x2048x2048x1450): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1450): 74.845

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1864.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1451x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1451x2048): 82.212
Elapsed time for attention_prob_times_values (64x2048x2048x1451): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1451): 70.067

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1790.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1452x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1452x2048): 84.076
Elapsed time for attention_prob_times_values (64x2048x2048x1452): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1452): 75.196

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1880.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1453x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1453x2048): 82.780
Elapsed time for attention_prob_times_values (64x2048x2048x1453): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1453): 70.206

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1800.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1454x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1454x2048): 83.769
Elapsed time for attention_prob_times_values (64x2048x2048x1454): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1454): 75.084

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1878.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1455x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1455x2048): 82.843
Elapsed time for attention_prob_times_values (64x2048x2048x1455): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1455): 70.332

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 1805.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1456x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1456x2048): 85.205
Elapsed time for attention_prob_times_values (64x2048x2048x1456): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1456): 88.678

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2064.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1457x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1457x2048): 82.407
Elapsed time for attention_prob_times_values (64x2048x2048x1457): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1457): 70.220

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1802.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1458x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1458x2048): 83.529
Elapsed time for attention_prob_times_values (64x2048x2048x1458): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1458): 75.208

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1882.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1459x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1459x2048): 82.528
Elapsed time for attention_prob_times_values (64x2048x2048x1459): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1459): 70.269

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1806.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1460x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1460x2048): 84.134
Elapsed time for attention_prob_times_values (64x2048x2048x1460): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1460): 75.612

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1896.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1461x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1461x2048): 82.737
Elapsed time for attention_prob_times_values (64x2048x2048x1461): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1461): 70.304

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1811.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1462x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1462x2048): 83.559
Elapsed time for attention_prob_times_values (64x2048x2048x1462): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1462): 75.369

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1889.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1463x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1463x2048): 82.732
Elapsed time for attention_prob_times_values (64x2048x2048x1463): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1463): 70.357

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1814.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1464x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1464x2048): 84.568
Elapsed time for attention_prob_times_values (64x2048x2048x1464): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1464): 88.777

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 2068.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1465x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1465x2048): 82.509
Elapsed time for attention_prob_times_values (64x2048x2048x1465): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1465): 70.303

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1813.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1466x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1466x2048): 83.352
Elapsed time for attention_prob_times_values (64x2048x2048x1466): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1466): 75.595

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1895.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1467x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1467x2048): 82.453
Elapsed time for attention_prob_times_values (64x2048x2048x1467): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1467): 70.393

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1816.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1468x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1468x2048): 83.891
Elapsed time for attention_prob_times_values (64x2048x2048x1468): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1468): 75.931

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1908.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1469x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1469x2048): 82.345
Elapsed time for attention_prob_times_values (64x2048x2048x1469): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1469): 70.421

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1818.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1470x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1470x2048): 83.628
Elapsed time for attention_prob_times_values (64x2048x2048x1470): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1470): 75.799

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1906.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1471x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1471x2048): 82.831
Elapsed time for attention_prob_times_values (64x2048x2048x1471): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1471): 70.594

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1828.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1472x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1472x2048): 91.880
Elapsed time for attention_prob_times_values (64x2048x2048x1472): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1472): 85.818

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 2129.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1473x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1473x2048): 83.906
Elapsed time for attention_prob_times_values (64x2048x2048x1473): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1473): 70.397

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1838.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1474x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1474x2048): 84.993
Elapsed time for attention_prob_times_values (64x2048x2048x1474): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1474): 76.052

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1929.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1475x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1475x2048): 83.791
Elapsed time for attention_prob_times_values (64x2048x2048x1475): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1475): 70.643

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1843.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1476x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1476x2048): 85.316
Elapsed time for attention_prob_times_values (64x2048x2048x1476): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1476): 76.485

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1940.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1477x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1477x2048): 83.723
Elapsed time for attention_prob_times_values (64x2048x2048x1477): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1477): 70.695

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1845.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1478x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1478x2048): 84.589
Elapsed time for attention_prob_times_values (64x2048x2048x1478): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1478): 76.215

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1931.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1479x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1479x2048): 83.448
Elapsed time for attention_prob_times_values (64x2048x2048x1479): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1479): 70.888

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1848.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1480x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1480x2048): 85.607
Elapsed time for attention_prob_times_values (64x2048x2048x1480): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1480): 83.198

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2035.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1481x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1481x2048): 82.975
Elapsed time for attention_prob_times_values (64x2048x2048x1481): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1481): 71.099

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1848.673
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1482x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1482x2048): 83.914
Elapsed time for attention_prob_times_values (64x2048x2048x1482): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1482): 76.418

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1932.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1483x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1483x2048): 82.874
Elapsed time for attention_prob_times_values (64x2048x2048x1483): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1483): 71.264

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1852.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1484x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1484x2048): 84.527
Elapsed time for attention_prob_times_values (64x2048x2048x1484): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1484): 76.882

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1947.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1485x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1485x2048): 82.914
Elapsed time for attention_prob_times_values (64x2048x2048x1485): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1485): 71.215

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1854.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1486x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1486x2048): 84.045
Elapsed time for attention_prob_times_values (64x2048x2048x1486): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1486): 76.749

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1943.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1487x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1487x2048): 82.987
Elapsed time for attention_prob_times_values (64x2048x2048x1487): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1487): 71.604

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1863.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1488x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1488x2048): 85.618
Elapsed time for attention_prob_times_values (64x2048x2048x1488): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1488): 84.108

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2057.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1489x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1489x2048): 82.895
Elapsed time for attention_prob_times_values (64x2048x2048x1489): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1489): 71.916

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1868.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1490x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1490x2048): 84.018
Elapsed time for attention_prob_times_values (64x2048x2048x1490): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1490): 76.862

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1949.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1491x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1491x2048): 82.793
Elapsed time for attention_prob_times_values (64x2048x2048x1491): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1491): 71.829

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1868.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1492x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1492x2048): 84.357
Elapsed time for attention_prob_times_values (64x2048x2048x1492): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1492): 77.808

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1968.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1493x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1493x2048): 82.749
Elapsed time for attention_prob_times_values (64x2048x2048x1493): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1493): 72.006

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1873.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1494x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1494x2048): 83.807
Elapsed time for attention_prob_times_values (64x2048x2048x1494): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1494): 77.742

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1963.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1495x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1495x2048): 83.007
Elapsed time for attention_prob_times_values (64x2048x2048x1495): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1495): 72.117

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1880.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1496x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1496x2048): 85.141
Elapsed time for attention_prob_times_values (64x2048x2048x1496): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1496): 84.007

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2061.403
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1497x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1497x2048): 82.601
Elapsed time for attention_prob_times_values (64x2048x2048x1497): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1497): 72.405

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1882.162
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1498x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1498x2048): 83.644
Elapsed time for attention_prob_times_values (64x2048x2048x1498): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1498): 77.988

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1969.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 23984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1499x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1499x2048): 82.687
Elapsed time for attention_prob_times_values (64x2048x2048x1499): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1499): 72.773

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1890.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1500x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1500x2048): 84.234
Elapsed time for attention_prob_times_values (64x2048x2048x1500): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1500): 78.323

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1983.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1501x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1501x2048): 82.747
Elapsed time for attention_prob_times_values (64x2048x2048x1501): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1501): 73.105

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1898.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1502x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1502x2048): 83.936
Elapsed time for attention_prob_times_values (64x2048x2048x1502): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1502): 78.230

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1981.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1503x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1503x2048): 82.952
Elapsed time for attention_prob_times_values (64x2048x2048x1503): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1503): 73.795

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1912.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1504x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1504x2048): 93.298
Elapsed time for attention_prob_times_values (64x2048x2048x1504): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1504): 86.088

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 2193.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1505x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1505x2048): 83.821
Elapsed time for attention_prob_times_values (64x2048x2048x1505): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1505): 73.194

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1915.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1506x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1506x2048): 85.006
Elapsed time for attention_prob_times_values (64x2048x2048x1506): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1506): 78.415

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2001.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1507x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1507x2048): 83.769
Elapsed time for attention_prob_times_values (64x2048x2048x1507): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1507): 73.078

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1916.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1508x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1508x2048): 85.296
Elapsed time for attention_prob_times_values (64x2048x2048x1508): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1508): 78.654

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 2010.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1509x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1509x2048): 83.603
Elapsed time for attention_prob_times_values (64x2048x2048x1509): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1509): 73.086

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1916.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1510x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1510x2048): 84.593
Elapsed time for attention_prob_times_values (64x2048x2048x1510): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1510): 78.512

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2002.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1511x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1511x2048): 83.363
Elapsed time for attention_prob_times_values (64x2048x2048x1511): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1511): 73.038

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1916.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1512x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1512x2048): 85.526
Elapsed time for attention_prob_times_values (64x2048x2048x1512): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1512): 84.673

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 2095.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1513x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1513x2048): 83.014
Elapsed time for attention_prob_times_values (64x2048x2048x1513): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1513): 73.436

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1920.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1514x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1514x2048): 84.102
Elapsed time for attention_prob_times_values (64x2048x2048x1514): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1514): 78.593

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2003.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1515x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1515x2048): 83.053
Elapsed time for attention_prob_times_values (64x2048x2048x1515): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1515): 73.544

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1924.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1516x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1516x2048): 84.632
Elapsed time for attention_prob_times_values (64x2048x2048x1516): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1516): 78.961

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 2016.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1517x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1517x2048): 83.021
Elapsed time for attention_prob_times_values (64x2048x2048x1517): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1517): 73.757

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1929.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1518x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1518x2048): 84.322
Elapsed time for attention_prob_times_values (64x2048x2048x1518): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1518): 78.790

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2013.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1519x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1519x2048): 83.230
Elapsed time for attention_prob_times_values (64x2048x2048x1519): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1519): 73.770

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1934.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1520x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1520x2048): 85.886
Elapsed time for attention_prob_times_values (64x2048x2048x1520): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1520): 85.837

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 2125.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1521x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1521x2048): 82.909
Elapsed time for attention_prob_times_values (64x2048x2048x1521): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1521): 73.201

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1925.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1522x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1522x2048): 83.886
Elapsed time for attention_prob_times_values (64x2048x2048x1522): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1522): 78.945

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 2015.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1523x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1523x2048): 82.862
Elapsed time for attention_prob_times_values (64x2048x2048x1523): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1523): 73.047

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1925.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1524x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1524x2048): 84.277
Elapsed time for attention_prob_times_values (64x2048x2048x1524): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1524): 79.286

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 2027.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (64x2048x1525x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (64x2048x1525x2048): 82.975
Elapsed time for attention_prob_times_values (64x2048x2048x1525): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (64x2048x2048x1525): 72.800

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1925.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 16, hidden_size: 24416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
slurmstepd: error: *** JOB 1508501 ON frontier08698 CANCELLED AT 2023-11-25T02:30:44 DUE TO TIME LIMIT ***
