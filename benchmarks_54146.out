1.13.1 

[2023-11-24 22:34:51,455] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-24 22:34:51,942] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.144.140, master_port=6000
[2023-11-24 22:34:51,943] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-24 22:34:53,227] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Traceback (most recent call last):
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 490, in <module>
    benchmark_transformer(configuration, seq_length, train_batch_size)
  File "/fsx/home-jacob/TransformerSizing/torch_transformer_flops.py", line 433, in benchmark_transformer
    out = layer(inp, attention_mask)
  File "/fsx/home-jacob/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 725, in forward
    context_layer = self.flash_attention(query_layer, key_layer, value_layer)
  File "/fsx/home-jacob/TransformerSizing/megatron/model/transformer.py", line 592, in flash_attention
    output = self.flash_qkv_fn(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 642, in flash_attn_varlen_qkvpacked_func
    return FlashAttnVarlenQKVPackedFunc.apply(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 557, in forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = _flash_attn_varlen_forward(
  File "/fsx/home-jacob/TransformerSizing/megatron/model/flash_attention.py", line 483, in _flash_attn_varlen_forward
    out, q, k, v, out_padded, softmax_lse, S_dmask, rng_state = flash_attn_cuda.varlen_fwd(
TypeError: varlen_fwd(): incompatible function arguments. The following argument types are supported:
    1. (arg0: at::Tensor, arg1: at::Tensor, arg2: at::Tensor, arg3: Optional[at::Tensor], arg4: at::Tensor, arg5: at::Tensor, arg6: int, arg7: int, arg8: float, arg9: float, arg10: bool, arg11: bool, arg12: int, arg13: int, arg14: bool, arg15: Optional[at::Generator]) -> List[at::Tensor]

Invoked with: tensor([[[ 0.7305, -1.1934,  1.2031,  ..., -0.9224, -0.6294, -0.5181],
         [ 0.9824, -0.6187,  0.6123,  ...,  0.4561,  1.1309,  0.6030],
         [ 0.5093,  1.8711, -2.0273,  ...,  1.6963, -0.3892, -0.3447],
         ...,
         [-0.3040, -0.0175,  0.1103,  ...,  0.4756,  0.6733,  1.1279],
         [ 0.7793,  0.7549,  0.9717,  ...,  0.7480,  0.9512,  1.1367],
         [-0.0352,  0.4382, -0.3989,  ..., -0.9463, -0.1896,  0.4128]],

        [[ 0.4004, -0.2472,  0.4385,  ...,  0.7661, -0.7480,  0.0164],
         [ 0.6934, -1.0762, -0.2291,  ...,  0.0154, -0.8169,  0.0105],
         [ 0.6753, -0.4260, -0.2678,  ..., -1.1387, -0.0934, -0.3081],
         ...,
         [-0.0854,  0.2629, -0.0222,  ..., -0.1954, -1.0127, -0.0872],
         [-0.6245,  0.6367,  0.4895,  ...,  0.0534,  0.2411, -0.5796],
         [ 0.6040,  0.1267,  0.0872,  ...,  0.4875,  0.1636, -1.0264]],

        [[-0.6348,  0.5132,  0.4426,  ..., -0.6558, -0.1218, -0.5620],
         [-0.5010, -0.9229, -0.8979,  ...,  0.3843, -1.0049, -0.0331],
         [ 0.7671, -0.7886, -0.7764,  ...,  0.2717,  0.2656, -2.2109],
         ...,
         [ 0.4214, -0.5234, -0.2135,  ..., -0.3926,  1.2695, -0.4338],
         [-0.1017,  0.0046, -0.8452,  ..., -0.1061,  0.3093,  1.6006],
         [ 0.0471, -0.4248, -0.8667,  ...,  0.2793, -0.6807, -0.4434]],

        ...,

        [[ 1.3066,  0.2325, -1.2588,  ..., -0.2812,  0.1276,  0.1436],
         [-1.1455,  0.4392,  0.3557,  ..., -0.3447, -0.1783, -0.0911],
         [-0.4624, -0.3442, -0.3843,  ..., -0.6138, -0.5356,  0.2778],
         ...,
         [-0.1164,  0.1857,  0.2181,  ...,  0.2512, -0.3379,  1.0586],
         [-0.6060,  0.0789, -0.8198,  ..., -0.2188, -0.2578, -0.4060],
         [ 0.5342,  0.2155,  0.4314,  ...,  0.2065,  0.3066, -0.1675]],

        [[ 0.1902, -0.3599,  0.5063,  ..., -0.1155, -1.4121, -0.0044],
         [ 0.1451, -0.9976, -0.0108,  ...,  0.2891,  0.2703, -0.3345],
         [-0.1145,  0.0901,  0.3120,  ..., -0.0044, -0.1304, -0.1829],
         ...,
         [-0.1744, -1.5430, -0.8970,  ..., -0.6709,  0.3372, -0.5640],
         [-0.5151, -0.0753,  0.9629,  ..., -0.7500,  0.7642, -0.4868],
         [-0.2020, -0.6562,  0.6470,  ...,  0.1942,  0.8091,  0.1893]],

        [[ 0.2054,  0.3948,  0.7490,  ...,  0.4365,  0.4724,  0.3152],
         [ 0.4250,  0.2908,  0.2396,  ...,  0.4937, -0.5337, -0.3301],
         [ 0.8857, -1.8682,  1.2236,  ...,  1.1309,  0.0163,  0.3701],
         ...,
         [-0.0914,  0.1342,  0.8970,  ...,  0.0316, -0.6406, -1.1475],
         [ 0.2368, -0.2834,  0.2302,  ...,  0.4341, -0.6006, -0.0182],
         [-0.6836, -0.7529, -1.0020,  ...,  0.8579, -0.2074, -0.0351]]],
       device='cuda:0', dtype=torch.float16), tensor([[[-0.6636,  0.6260,  0.6035,  ..., -0.3142, -0.0068,  0.4060],
         [-0.7588,  0.4275,  0.1440,  ..., -1.3672,  0.0388, -0.1322],
         [-0.9912, -0.3191, -0.4548,  ..., -1.1094, -0.3765,  0.0436],
         ...,
         [ 0.3513,  0.3599, -1.4482,  ..., -0.1929, -0.2915,  0.1796],
         [-0.2649, -0.5200, -0.6650,  ...,  0.7119, -1.0752,  0.9658],
         [-1.3457, -0.0311, -0.1431,  ..., -0.2832, -0.8936, -0.4375]],

        [[-0.0089,  0.7861, -0.5591,  ..., -0.3022, -0.5557,  0.0151],
         [-0.8179, -1.3701,  0.0837,  ...,  1.5342, -0.0174,  0.4858],
         [ 0.7393, -0.3965,  0.1698,  ..., -0.9263,  0.6748,  0.1365],
         ...,
         [ 0.4048, -1.7617,  0.2189,  ..., -0.0221,  0.6006,  0.2469],
         [ 0.0522,  0.7197,  0.9751,  ...,  0.9170,  0.4814, -0.2129],
         [-0.2629, -0.4524, -0.3528,  ..., -0.0359, -1.0732, -0.2974]],

        [[-0.0062,  0.4919, -0.5439,  ..., -1.0156,  1.2129,  1.8359],
         [ 0.5259, -0.6392,  0.9932,  ..., -0.5488,  0.2791, -0.1650],
         [-0.3206, -0.4541,  0.1660,  ..., -0.5933,  0.5903,  0.4124],
         ...,
         [ 0.4915, -0.1257, -0.6895,  ...,  0.5161, -0.6416, -0.3091],
         [-0.1676, -1.1455,  0.1295,  ..., -0.4187,  0.4617,  0.6421],
         [ 0.7358, -0.9595,  0.1354,  ...,  0.5571,  0.7559,  0.8501]],

        ...,

        [[ 0.3845,  0.2708,  0.2372,  ..., -1.1504,  0.1885,  0.5464],
         [-0.0688, -0.4268,  0.8574,  ..., -0.1002,  0.8188, -0.9448],
         [ 0.0607, -0.7471, -0.3591,  ..., -0.9111, -0.3555,  1.6396],
         ...,
         [ 0.3506,  0.1348,  1.8896,  ..., -1.2383,  1.1230, -0.6235],
         [ 0.4338, -0.8882, -0.7251,  ...,  0.8003,  1.0625,  0.3433],
         [ 0.4778, -0.9209,  1.5723,  ..., -0.3135,  0.4856,  0.9165]],

        [[ 0.1462,  0.2615, -1.1191,  ...,  0.1479,  1.3203,  0.3193],
         [-1.5020, -0.3523,  0.6411,  ..., -0.1251,  0.6934,  0.2805],
         [-0.3198, -0.1597, -0.4160,  ...,  0.4795,  0.8237,  0.7837],
         ...,
         [-0.2959, -0.1755, -1.7676,  ..., -0.0269, -0.6074, -0.8413],
         [ 0.9292,  0.7402, -0.9517,  ..., -0.2098,  1.1338, -0.1273],
         [ 0.3301, -0.3252,  0.2568,  ...,  1.8223, -0.0735,  0.3953]],

        [[-0.2184,  0.1354,  0.0966,  ...,  0.4043, -0.5303,  0.7373],
         [ 0.7593,  0.3225, -0.5679,  ...,  0.2317, -0.0618,  0.4924],
         [-0.1860,  0.6704, -0.4658,  ...,  0.2681, -0.7114, -0.0897],
         ...,
         [-0.9126, -0.8257,  0.1797,  ..., -0.0086, -1.8662, -1.0713],
         [ 0.6885, -0.7861,  1.1816,  ...,  0.6689,  0.5322,  0.5142],
         [ 0.0804,  0.0790,  0.5952,  ..., -0.7588,  0.5854, -0.1859]]],
       device='cuda:0', dtype=torch.float16), tensor([[[ 0.7422,  0.1486, -0.5620,  ..., -1.3027,  0.1202,  0.9858],
         [ 0.5625,  0.3950,  0.3745,  ...,  0.6938,  0.4790,  0.0875],
         [-0.7329, -0.2649,  1.0488,  ..., -0.2188, -0.7373, -0.2244],
         ...,
         [ 0.5898, -0.8164,  0.0952,  ..., -0.2260,  1.2656, -0.4968],
         [-0.5864,  0.0411, -0.0886,  ..., -0.1644, -0.7144, -0.2878],
         [-0.3640,  0.5034,  0.5229,  ...,  0.2651,  0.1997,  0.3560]],

        [[-0.2406,  0.4236, -1.1641,  ..., -0.9365, -0.5527,  0.7847],
         [-0.5938,  0.6240,  0.0503,  ...,  0.0058,  0.6748,  0.2007],
         [-0.1560, -0.6665,  0.6147,  ..., -0.9268,  0.3276, -0.0228],
         ...,
         [-0.0726, -0.1439,  0.3950,  ..., -0.7861, -0.2419,  0.3286],
         [ 0.6548,  0.4900,  0.1520,  ..., -0.0792,  0.3079,  0.3618],
         [ 1.0879,  0.0546,  0.4482,  ..., -1.3262, -1.5078, -1.1260]],

        [[-0.4683,  0.7607, -0.4810,  ...,  0.1105, -0.1324, -0.4368],
         [-0.9429,  0.2064, -0.0053,  ...,  0.9941, -0.4521,  0.6128],
         [ 0.8999, -0.0580, -0.1470,  ..., -0.2888,  0.2620,  0.9048],
         ...,
         [ 0.2354, -0.6558, -1.1279,  ..., -1.0215,  0.3801, -0.4651],
         [ 0.2307, -0.2394, -0.0571,  ...,  1.6797,  0.3901,  0.4438],
         [ 0.1279, -0.6494,  0.5210,  ...,  0.0600, -0.0667,  0.0809]],

        ...,

        [[-0.4863,  0.4788, -0.0846,  ..., -1.1201,  0.2172, -0.0207],
         [ 0.3813,  0.6226, -0.6094,  ..., -0.6123,  0.1680,  0.2056],
         [-0.1140,  0.3352, -0.5322,  ..., -0.2294, -0.4006,  0.0558],
         ...,
         [ 0.6440,  0.1243,  1.3379,  ..., -0.6924, -0.3799, -0.3225],
         [-0.8979, -0.6758, -1.0010,  ..., -0.3120, -0.6855,  0.6240],
         [-0.3030, -0.0563,  1.1367,  ..., -0.4373,  0.5762,  0.0717]],

        [[-0.3955,  0.2391, -0.3613,  ..., -0.0732,  0.6587,  0.1691],
         [ 1.1260,  0.1858, -0.2356,  ..., -1.3848, -0.5991, -0.0818],
         [-0.5024, -0.1841,  0.2240,  ...,  0.5552, -0.5635,  0.9209],
         ...,
         [ 0.5859,  0.3511, -0.7563,  ..., -0.9761,  0.0817, -0.4285],
         [ 0.5674,  0.4673, -0.0295,  ...,  0.4719, -0.3701,  0.1526],
         [ 0.3245,  0.9839, -0.5469,  ..., -0.2673,  1.6270,  0.0750]],

        [[ 0.0370, -1.1943,  0.0828,  ..., -0.7544,  0.2133,  0.7920],
         [-0.3796,  0.2686, -0.7695,  ..., -1.4365,  0.6431, -0.2455],
         [-0.3799,  0.7129, -0.9868,  ..., -1.4814, -0.4714, -0.2279],
         ...,
         [-0.0336, -0.0222,  0.6689,  ...,  0.0933, -0.1265, -0.2559],
         [ 0.3840,  0.2666,  0.1643,  ..., -0.1385, -0.1231, -0.6416],
         [ 0.2502,  0.4314, -0.2020,  ..., -0.8979, -0.6367, -0.8726]]],
       device='cuda:0', dtype=torch.float16), None, tensor([   0, 2048, 4096, 6144, 8192], device='cuda:0', dtype=torch.int32), tensor([   0, 2048, 4096, 6144, 8192], device='cuda:0', dtype=torch.int32), 2048, 2048, 0.0, 0.3535533905932738, False, True, False, None
