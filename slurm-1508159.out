
Lmod is automatically replacing "PrgEnv-cray/8.3.3" with "PrgEnv-gnu/8.3.3".


Lmod is automatically replacing "cce/15.0.0" with "gcc/12.2.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-mpich/8.1.23     2) darshan-runtime/3.4.0

0
2
4
6
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
/opt/rocm-5.6.0/llvm/bin/clang++: /lustre/orion/csc439/scratch/jahatef/transformerSizing/miniconda3/lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /opt/rocm-5.6.0/llvm/bin/clang++)
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
Use of uninitialized value $HIP_CLANG_VERSION in concatenation (.) or string at /opt/rocm-5.6.0/bin//hipconfig.pl line 79.
[2023-11-24 14:33:52,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-24 14:33:52,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-24 14:33:52,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-11-24 14:33:52,001] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 12, hidden_size: 12, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1x2048): 0.0004
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1x2048): 0.933
Elapsed time for attention_prob_times_values (48x2048x2048x1): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1): 0.203

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 0.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 24, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x2x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x2x2048): 1.774
Elapsed time for attention_prob_times_values (48x2048x2048x2): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x2): 1.732

Attention duration (in seconds): 0.0009
Attention throughput (in TFLOP/s): 1.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0009
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 36, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x3x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x3x2048): 2.473
Elapsed time for attention_prob_times_values (48x2048x2048x3): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x3): 2.331

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 2.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 48, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x4x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x4x2048): 3.223
Elapsed time for attention_prob_times_values (48x2048x2048x4): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x4): 2.942

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 3.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 60, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x5x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x5x2048): 3.985
Elapsed time for attention_prob_times_values (48x2048x2048x5): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x5): 2.970

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 3.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 72, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x6x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x6x2048): 4.769
Elapsed time for attention_prob_times_values (48x2048x2048x6): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x6): 2.859

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 3.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 84, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x7x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x7x2048): 5.542
Elapsed time for attention_prob_times_values (48x2048x2048x7): 0.0007
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x7): 3.955

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 4.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x8x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x8x2048): 6.358
Elapsed time for attention_prob_times_values (48x2048x2048x8): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x8): 6.739

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 7.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x9x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x9x2048): 6.850
Elapsed time for attention_prob_times_values (48x2048x2048x9): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x9): 3.564

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 5.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x10x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x10x2048): 7.580
Elapsed time for attention_prob_times_values (48x2048x2048x10): 0.0010
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x10): 3.928

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 5.781
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x11x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x11x2048): 8.301
Elapsed time for attention_prob_times_values (48x2048x2048x11): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x11): 8.834

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 9.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x12x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x12x2048): 9.035
Elapsed time for attention_prob_times_values (48x2048x2048x12): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x12): 9.867

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 10.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x13x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x13x2048): 9.771
Elapsed time for attention_prob_times_values (48x2048x2048x13): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x13): 10.390

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 11.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x14x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x14x2048): 10.513
Elapsed time for attention_prob_times_values (48x2048x2048x14): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x14): 11.439

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 12.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x15x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x15x2048): 11.264
Elapsed time for attention_prob_times_values (48x2048x2048x15): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x15): 11.910

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 13.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x16x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x16x2048): 12.020
Elapsed time for attention_prob_times_values (48x2048x2048x16): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x16): 13.188

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 14.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x17x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x17x2048): 12.338
Elapsed time for attention_prob_times_values (48x2048x2048x17): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x17): 13.122

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 15.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x18x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x18x2048): 13.036
Elapsed time for attention_prob_times_values (48x2048x2048x18): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x18): 14.476

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 16.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x19x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x19x2048): 13.701
Elapsed time for attention_prob_times_values (48x2048x2048x19): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x19): 14.890

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 17.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x20x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x20x2048): 14.416
Elapsed time for attention_prob_times_values (48x2048x2048x20): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x20): 15.993

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 18.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x21x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x21x2048): 15.111
Elapsed time for attention_prob_times_values (48x2048x2048x21): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x21): 16.351

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 19.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x22x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x22x2048): 15.831
Elapsed time for attention_prob_times_values (48x2048x2048x22): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x22): 17.493

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 20.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x23x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x23x2048): 16.543
Elapsed time for attention_prob_times_values (48x2048x2048x23): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x23): 17.851

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 21.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x24x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x24x2048): 17.219
Elapsed time for attention_prob_times_values (48x2048x2048x24): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x24): 19.403

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 23.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x25x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x25x2048): 17.541
Elapsed time for attention_prob_times_values (48x2048x2048x25): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x25): 19.262

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 23.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x26x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x26x2048): 17.642
Elapsed time for attention_prob_times_values (48x2048x2048x26): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x26): 20.329

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 24.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x27x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x27x2048): 18.850
Elapsed time for attention_prob_times_values (48x2048x2048x27): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x27): 20.717

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 25.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x28x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x28x2048): 19.460
Elapsed time for attention_prob_times_values (48x2048x2048x28): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x28): 21.903

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 27.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x29x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x29x2048): 20.163
Elapsed time for attention_prob_times_values (48x2048x2048x29): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x29): 22.121

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 28.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x30x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x30x2048): 20.884
Elapsed time for attention_prob_times_values (48x2048x2048x30): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x30): 23.380

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 29.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x31x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x31x2048): 21.492
Elapsed time for attention_prob_times_values (48x2048x2048x31): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x31): 23.541

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 30.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x32x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x32x2048): 26.350
Elapsed time for attention_prob_times_values (48x2048x2048x32): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x32): 25.101

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 35.352
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x33x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x33x2048): 24.781
Elapsed time for attention_prob_times_values (48x2048x2048x33): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x33): 25.093

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 34.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x34x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x34x2048): 19.166
Elapsed time for attention_prob_times_values (48x2048x2048x34): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x34): 26.065

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 30.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x35x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x35x2048): 24.355
Elapsed time for attention_prob_times_values (48x2048x2048x35): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x35): 26.554

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 35.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x36x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x36x2048): 24.887
Elapsed time for attention_prob_times_values (48x2048x2048x36): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x36): 27.661

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 37.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x37x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x37x2048): 24.876
Elapsed time for attention_prob_times_values (48x2048x2048x37): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x37): 27.859

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 37.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x38x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x38x2048): 25.701
Elapsed time for attention_prob_times_values (48x2048x2048x38): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x38): 28.701

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 39.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x39x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x39x2048): 25.993
Elapsed time for attention_prob_times_values (48x2048x2048x39): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x39): 28.649

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 39.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x40x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x40x2048): 26.882
Elapsed time for attention_prob_times_values (48x2048x2048x40): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x40): 30.903

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 42.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x41x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x41x2048): 26.520
Elapsed time for attention_prob_times_values (48x2048x2048x41): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x41): 30.343

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 41.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x42x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x42x2048): 27.397
Elapsed time for attention_prob_times_values (48x2048x2048x42): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x42): 31.110

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 43.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x43x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x43x2048): 27.750
Elapsed time for attention_prob_times_values (48x2048x2048x43): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x43): 31.643

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 44.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x44x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x44x2048): 28.658
Elapsed time for attention_prob_times_values (48x2048x2048x44): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x44): 32.895

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 46.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x45x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x45x2048): 29.174
Elapsed time for attention_prob_times_values (48x2048x2048x45): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x45): 32.843

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 47.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x46x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x46x2048): 29.868
Elapsed time for attention_prob_times_values (48x2048x2048x46): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x46): 34.375

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 49.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x47x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x47x2048): 30.319
Elapsed time for attention_prob_times_values (48x2048x2048x47): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x47): 34.258

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 49.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x48x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x48x2048): 31.287
Elapsed time for attention_prob_times_values (48x2048x2048x48): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x48): 36.831

Attention duration (in seconds): 0.0011
Attention throughput (in TFLOP/s): 52.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x49x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x49x2048): 31.682
Elapsed time for attention_prob_times_values (48x2048x2048x49): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x49): 35.335

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 52.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x50x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x50x2048): 32.403
Elapsed time for attention_prob_times_values (48x2048x2048x50): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x50): 36.762

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 54.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x51x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x51x2048): 32.988
Elapsed time for attention_prob_times_values (48x2048x2048x51): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x51): 32.712

Attention duration (in seconds): 0.0013
Attention throughput (in TFLOP/s): 52.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0013
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x52x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x52x2048): 33.686
Elapsed time for attention_prob_times_values (48x2048x2048x52): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x52): 37.707

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 57.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x53x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x53x2048): 34.334
Elapsed time for attention_prob_times_values (48x2048x2048x53): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x53): 38.089

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 58.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x54x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x54x2048): 35.022
Elapsed time for attention_prob_times_values (48x2048x2048x54): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x54): 39.738

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 60.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x55x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x55x2048): 35.562
Elapsed time for attention_prob_times_values (48x2048x2048x55): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x55): 35.698

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 58.594
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x56x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x56x2048): 36.291
Elapsed time for attention_prob_times_values (48x2048x2048x56): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x56): 42.525

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 64.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x57x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x57x2048): 33.682
Elapsed time for attention_prob_times_values (48x2048x2048x57): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x57): 40.773

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 61.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
2.1.1+rocm5.6 

num_attention_heads: 24, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1160x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1160x2048): 83.985
Elapsed time for attention_prob_times_values (96x2048x2048x1160): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1160): 88.481

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2429.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1161x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1161x2048): 84.057
Elapsed time for attention_prob_times_values (96x2048x2048x1161): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1161): 81.560

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2335.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1162x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1162x2048): 84.923
Elapsed time for attention_prob_times_values (96x2048x2048x1162): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1162): 83.668

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2379.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1163x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1163x2048): 84.477
Elapsed time for attention_prob_times_values (96x2048x2048x1163): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1163): 81.665

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2346.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1164x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1164x2048): 85.816
Elapsed time for attention_prob_times_values (96x2048x2048x1164): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1164): 83.412

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2392.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1165x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1165x2048): 84.040
Elapsed time for attention_prob_times_values (96x2048x2048x1165): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1165): 81.692

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2345.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 27984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1166x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1166x2048): 79.659
Elapsed time for attention_prob_times_values (96x2048x2048x1166): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1166): 84.165

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2318.646
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1167x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1167x2048): 83.978
Elapsed time for attention_prob_times_values (96x2048x2048x1167): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1167): 80.834

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2335.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1168x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1168x2048): 83.476
Elapsed time for attention_prob_times_values (96x2048x2048x1168): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1168): 89.421

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2450.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1169x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1169x2048): 83.959
num_attention_heads: 12, hidden_size: 696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x58x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x58x2048): 36.468
Elapsed time for attention_prob_times_values (48x2048x2048x58): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x58): 41.825

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 65.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x59x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x59x2048): 36.877
Elapsed time for attention_prob_times_values (48x2048x2048x59): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x59): 41.919

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 66.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x60x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x60x2048): 37.544
Elapsed time for attention_prob_times_values (48x2048x2048x60): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x60): 43.734

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 68.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x61x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x61x2048): 37.973
Elapsed time for attention_prob_times_values (48x2048x2048x61): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x61): 43.159

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 69.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x62x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x62x2048): 38.666
Elapsed time for attention_prob_times_values (48x2048x2048x62): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x62): 44.882

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 71.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x63x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x63x2048): 39.016
Elapsed time for attention_prob_times_values (48x2048x2048x63): 0.0006
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x63): 44.388

Attention duration (in seconds): 0.0012
Attention throughput (in TFLOP/s): 72.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x64x2048): 0.0005
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x64x2048): 53.192
Elapsed time for attention_prob_times_values (48x2048x2048x64): 0.0005
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x64): 48.296

Attention duration (in seconds): 0.0010
Attention throughput (in TFLOP/s): 88.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0010
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x65x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x65x2048): 40.915
Elapsed time for attention_prob_times_values (48x2048x2048x65): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x65): 32.183

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 63.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x66x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x66x2048): 41.669
Elapsed time for attention_prob_times_values (48x2048x2048x66): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x66): 33.443

Attention duration (in seconds): 0.0014
Attention throughput (in TFLOP/s): 65.804
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x67x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x67x2048): 41.463
Elapsed time for attention_prob_times_values (48x2048x2048x67): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x67): 32.932

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 65.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x68x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x68x2048): 42.330
Elapsed time for attention_prob_times_values (48x2048x2048x68): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x68): 33.987

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 67.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x69x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x69x2048): 41.978
Elapsed time for attention_prob_times_values (48x2048x2048x69): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x69): 33.826

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 67.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x70x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x70x2048): 43.067
Elapsed time for attention_prob_times_values (48x2048x2048x70): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x70): 35.258

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 70.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x71x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x71x2048): 42.326
Elapsed time for attention_prob_times_values (48x2048x2048x71): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x71): 34.152

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 69.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x72x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x72x2048): 43.945
Elapsed time for attention_prob_times_values (48x2048x2048x72): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x72): 34.982

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 71.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x73x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x73x2048): 42.719
Elapsed time for attention_prob_times_values (48x2048x2048x73): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x73): 35.529

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 71.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x74x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x74x2048): 43.698
Elapsed time for attention_prob_times_values (48x2048x2048x74): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x74): 37.085

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 74.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x75x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x75x2048): 43.617
Elapsed time for attention_prob_times_values (48x2048x2048x75): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x75): 36.273

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 74.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x76x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x76x2048): 44.490
Elapsed time for attention_prob_times_values (48x2048x2048x76): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x76): 37.940

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 77.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
2.1.1+rocm5.6 

num_attention_heads: 80, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x148x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x148x2048): 61.695
Elapsed time for attention_prob_times_values (320x2048x2048x148): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x148): 52.104

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 709.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 11920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x149x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x149x2048): 61.123
Elapsed time for attention_prob_times_values (320x2048x2048x149): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x149): 49.229

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 689.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x150x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x150x2048): 62.301
Elapsed time for attention_prob_times_values (320x2048x2048x150): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x150): 51.723

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 718.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x151x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x151x2048): 59.960
Elapsed time for attention_prob_times_values (320x2048x2048x151): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x151): 50.678

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 702.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x152x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x152x2048): 63.433
Elapsed time for attention_prob_times_values (320x2048x2048x152): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x152): 50.745

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 725.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x153x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x153x2048): 59.295
Elapsed time for attention_prob_times_values (320x2048x2048x153): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x153): 51.038

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 710.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x154x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x154x2048): 62.444
Elapsed time for attention_prob_times_values (320x2048x2048x154): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x154): 53.583

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 751.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x155x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x155x2048): 59.517
Elapsed time for attention_prob_times_values (320x2048x2048x155): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x155): 51.825

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 726.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x156x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x156x2048): 63.401
Elapsed time for attention_prob_times_values (320x2048x2048x156): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x156): 53.521

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 765.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x157x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x157x2048): 61.369
num_attention_heads: 12, hidden_size: 924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x77x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x77x2048): 44.413
Elapsed time for attention_prob_times_values (48x2048x2048x77): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x77): 37.091

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 76.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x78x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x78x2048): 45.377
Elapsed time for attention_prob_times_values (48x2048x2048x78): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x78): 38.642

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 79.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x79x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x79x2048): 45.273
Elapsed time for attention_prob_times_values (48x2048x2048x79): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x79): 37.268

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 78.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x80x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x80x2048): 47.027
Elapsed time for attention_prob_times_values (48x2048x2048x80): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x80): 38.318

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 81.816
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x81x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x81x2048): 44.443
Elapsed time for attention_prob_times_values (48x2048x2048x81): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x81): 38.720

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 80.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x82x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x82x2048): 43.978
Elapsed time for attention_prob_times_values (48x2048x2048x82): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x82): 40.457

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 82.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x83x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x83x2048): 45.423
Elapsed time for attention_prob_times_values (48x2048x2048x83): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x83): 39.586

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 83.451
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x84x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x84x2048): 46.240
Elapsed time for attention_prob_times_values (48x2048x2048x84): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x84): 41.420

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 86.712
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x85x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x85x2048): 46.069
Elapsed time for attention_prob_times_values (48x2048x2048x85): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x85): 40.540

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 86.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x86x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x86x2048): 46.973
Elapsed time for attention_prob_times_values (48x2048x2048x86): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x86): 42.123

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 89.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x87x2048): 0.0007
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x87x2048): 47.138
Elapsed time for attention_prob_times_values (48x2048x2048x87): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x87): 41.168

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 88.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x88x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x88x2048): 44.981
Elapsed time for attention_prob_times_values (48x2048x2048x88): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x88): 41.386

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 87.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x89x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x89x2048): 46.796
Elapsed time for attention_prob_times_values (48x2048x2048x89): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x89): 42.174

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 90.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x90x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x90x2048): 47.514
Elapsed time for attention_prob_times_values (48x2048x2048x90): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x90): 43.715

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 93.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x91x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x91x2048): 47.507
Elapsed time for attention_prob_times_values (48x2048x2048x91): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x91): 42.881

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 93.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x92x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x92x2048): 48.223
Elapsed time for attention_prob_times_values (48x2048x2048x92): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x92): 44.815

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 96.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x93x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x93x2048): 48.090
Elapsed time for attention_prob_times_values (48x2048x2048x93): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x93): 43.629

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 95.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x94x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x94x2048): 48.667
Elapsed time for attention_prob_times_values (48x2048x2048x94): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x94): 45.553

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 98.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x95x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x95x2048): 48.398
Elapsed time for attention_prob_times_values (48x2048x2048x95): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x95): 44.308

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 97.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x96x2048): 0.0006
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x96x2048): 62.502
Elapsed time for attention_prob_times_values (48x2048x2048x96): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x96): 45.275

Attention duration (in seconds): 0.0015
Attention throughput (in TFLOP/s): 111.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x97x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x97x2048): 49.165
Elapsed time for attention_prob_times_values (48x2048x2048x97): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x97): 45.341

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 100.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x98x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x98x2048): 50.291
Elapsed time for attention_prob_times_values (48x2048x2048x98): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x98): 47.099

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 104.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x99x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x99x2048): 49.894
Elapsed time for attention_prob_times_values (48x2048x2048x99): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x99): 46.187

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 103.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x100x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x100x2048): 51.023
Elapsed time for attention_prob_times_values (48x2048x2048x100): 0.0008
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x100): 47.748

Attention duration (in seconds): 0.0016
Attention throughput (in TFLOP/s): 107.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0016
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x101x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x101x2048): 50.376
Elapsed time for attention_prob_times_values (48x2048x2048x101): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x101): 47.068

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 106.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x102x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x102x2048): 51.360
Elapsed time for attention_prob_times_values (48x2048x2048x102): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x102): 48.185

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 109.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x103x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x103x2048): 50.878
Elapsed time for attention_prob_times_values (48x2048x2048x103): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x103): 47.593

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 108.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x104x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x104x2048): 52.508
Elapsed time for attention_prob_times_values (48x2048x2048x104): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x104): 48.121

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 111.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x105x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x105x2048): 50.913
Elapsed time for attention_prob_times_values (48x2048x2048x105): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x105): 48.319

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 110.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x106x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x106x2048): 51.859
Elapsed time for attention_prob_times_values (48x2048x2048x106): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x106): 50.201

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 114.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x107x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x107x2048): 51.352
Elapsed time for attention_prob_times_values (48x2048x2048x107): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x107): 48.564

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 112.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x108x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x108x2048): 52.685
Elapsed time for attention_prob_times_values (48x2048x2048x108): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x108): 50.792

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 117.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x109x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x109x2048): 52.208
Elapsed time for attention_prob_times_values (48x2048x2048x109): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x109): 49.512

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 115.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x110x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x110x2048): 53.169
Elapsed time for attention_prob_times_values (48x2048x2048x110): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x110): 51.504

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 119.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x111x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x111x2048): 52.970
Elapsed time for attention_prob_times_values (48x2048x2048x111): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x111): 50.325

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 118.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x112x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x112x2048): 54.811
Elapsed time for attention_prob_times_values (48x2048x2048x112): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x112): 52.093

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 123.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x113x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x113x2048): 52.587
Elapsed time for attention_prob_times_values (48x2048x2048x113): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x113): 51.177

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 120.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x114x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x114x2048): 53.480
Elapsed time for attention_prob_times_values (48x2048x2048x114): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x114): 52.936

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 124.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (96x2048x2048x1169): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1169): 80.620

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2335.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1170x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1170x2048): 81.318
Elapsed time for attention_prob_times_values (96x2048x2048x1170): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1170): 81.478

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2313.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1171x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1171x2048): 84.073
Elapsed time for attention_prob_times_values (96x2048x2048x1171): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1171): 80.033

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2332.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1172x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1172x2048): 84.403
Elapsed time for attention_prob_times_values (96x2048x2048x1172): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1172): 83.339

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2387.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1173x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1173x2048): 81.673
Elapsed time for attention_prob_times_values (96x2048x2048x1173): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1173): 82.173

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2334.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1174x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1174x2048): 82.229
Elapsed time for attention_prob_times_values (96x2048x2048x1174): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1174): 84.256

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2373.361
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1175x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1175x2048): 84.657
Elapsed time for attention_prob_times_values (96x2048x2048x1175): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1175): 81.866

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2375.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1176x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1176x2048): 87.466
Elapsed time for attention_prob_times_values (96x2048x2048x1176): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1176): 87.249

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2495.140
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1177x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1177x2048): 82.487
Elapsed time for attention_prob_times_values (96x2048x2048x1177): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1177): 82.434

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2357.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1178x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1178x2048): 82.540
Elapsed time for attention_prob_times_values (96x2048x2048x1178): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1178): 84.752

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2392.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
========================================================================================================================
num_attention_heads: 12, hidden_size: 1380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x115x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x115x2048): 53.401
Elapsed time for attention_prob_times_values (48x2048x2048x115): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x115): 51.681

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 123.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x116x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x116x2048): 54.464
Elapsed time for attention_prob_times_values (48x2048x2048x116): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x116): 53.207

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 127.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x117x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x117x2048): 53.826
Elapsed time for attention_prob_times_values (48x2048x2048x117): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x117): 52.122

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 125.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x118x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x118x2048): 44.345
Elapsed time for attention_prob_times_values (48x2048x2048x118): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x118): 54.257

Attention duration (in seconds): 0.0019
Attention throughput (in TFLOP/s): 116.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x119x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x119x2048): 53.794
Elapsed time for attention_prob_times_values (48x2048x2048x119): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x119): 52.777

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 127.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x120x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x120x2048): 56.108
Elapsed time for attention_prob_times_values (48x2048x2048x120): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x120): 54.907

Attention duration (in seconds): 0.0017
Attention throughput (in TFLOP/s): 133.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0017
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x121x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x121x2048): 53.876
Elapsed time for attention_prob_times_values (48x2048x2048x121): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x121): 43.572

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 116.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x122x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x122x2048): 54.929
Elapsed time for attention_prob_times_values (48x2048x2048x122): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x122): 55.928

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 134.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x123x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x123x2048): 54.436
Elapsed time for attention_prob_times_values (48x2048x2048x123): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x123): 44.625

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 119.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x124x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x124x2048): 55.800
Elapsed time for attention_prob_times_values (48x2048x2048x124): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x124): 56.295

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 137.489
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x125x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x125x2048): 55.321
Elapsed time for attention_prob_times_values (48x2048x2048x125): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x125): 40.488

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 115.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x126x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x126x2048): 55.779
Elapsed time for attention_prob_times_values (48x2048x2048x126): 0.0009
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x126): 57.373

Attention duration (in seconds): 0.0018
Attention throughput (in TFLOP/s): 140.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0018
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x127x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x127x2048): 54.715
Elapsed time for attention_prob_times_values (48x2048x2048x127): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x127): 44.352

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 121.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x128x2048): 0.0008
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x128x2048): 65.866
Elapsed time for attention_prob_times_values (48x2048x2048x128): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x128): 41.468

Attention duration (in seconds): 0.0020
Attention throughput (in TFLOP/s): 127.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0020
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x129x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x129x2048): 54.657
Elapsed time for attention_prob_times_values (48x2048x2048x129): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x129): 42.869

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 120.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x130x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x130x2048): 55.780
Elapsed time for attention_prob_times_values (48x2048x2048x130): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x130): 45.427

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 126.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x131x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x131x2048): 55.532
Elapsed time for attention_prob_times_values (48x2048x2048x131): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x131): 43.026

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 122.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x132x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x132x2048): 56.696
Elapsed time for attention_prob_times_values (48x2048x2048x132): 0.0011
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x132): 46.231

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 129.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x133x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x133x2048): 55.175
Elapsed time for attention_prob_times_values (48x2048x2048x133): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x133): 44.664

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 126.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x134x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x134x2048): 57.148
Elapsed time for attention_prob_times_values (48x2048x2048x134): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x134): 46.642

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 132.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x135x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x135x2048): 56.574
Elapsed time for attention_prob_times_values (48x2048x2048x135): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x135): 45.255

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 129.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x136x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x136x2048): 57.869
Elapsed time for attention_prob_times_values (48x2048x2048x136): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x136): 44.504

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 130.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x137x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x137x2048): 56.543
Elapsed time for attention_prob_times_values (48x2048x2048x137): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x137): 45.349

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 131.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x138x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x138x2048): 57.489
Elapsed time for attention_prob_times_values (48x2048x2048x138): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x138): 47.985

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 136.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x139x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x139x2048): 56.471
Elapsed time for attention_prob_times_values (48x2048x2048x139): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x139): 46.057

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 133.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x140x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x140x2048): 58.193
Elapsed time for attention_prob_times_values (48x2048x2048x140): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x140): 48.831

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 140.224
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x141x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x141x2048): 57.756
Elapsed time for attention_prob_times_values (48x2048x2048x141): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x141): 46.683

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 136.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x142x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x142x2048): 58.851
Elapsed time for attention_prob_times_values (48x2048x2048x142): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x142): 49.062

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 142.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_prob_times_values (320x2048x2048x157): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x157): 51.989

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 746.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x158x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x158x2048): 61.020
Elapsed time for attention_prob_times_values (320x2048x2048x158): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x158): 52.654

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 754.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x159x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x159x2048): 63.224
Elapsed time for attention_prob_times_values (320x2048x2048x159): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x159): 53.118

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 774.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x160x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x160x2048): 74.036
Elapsed time for attention_prob_times_values (320x2048x2048x160): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x160): 53.798

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 841.256
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x161x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x161x2048): 62.254
Elapsed time for attention_prob_times_values (320x2048x2048x161): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x161): 52.710

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 775.117
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x162x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x162x2048): 62.372
Elapsed time for attention_prob_times_values (320x2048x2048x162): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x162): 54.659

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 795.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x163x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x163x2048): 59.394
Elapsed time for attention_prob_times_values (320x2048x2048x163): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x163): 54.524

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 780.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x164x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x164x2048): 62.324
Elapsed time for attention_prob_times_values (320x2048x2048x164): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x164): 55.345

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 809.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x165x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x165x2048): 63.047
Elapsed time for attention_prob_times_values (320x2048x2048x165): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x165): 55.149

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 817.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x166x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x166x2048): 63.712
Elapsed time for attention_prob_times_values (320x2048x2048x166): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x166): 55.396

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 827.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Elapsed time for attention_key_query_prob (48x2048x143x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x143x2048): 58.307
Elapsed time for attention_prob_times_values (48x2048x2048x143): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x143): 47.512

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 140.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x144x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x144x2048): 60.541
Elapsed time for attention_prob_times_values (48x2048x2048x144): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x144): 48.062

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 144.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x145x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x145x2048): 57.728
Elapsed time for attention_prob_times_values (48x2048x2048x145): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x145): 47.922

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 141.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x146x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x146x2048): 52.373
Elapsed time for attention_prob_times_values (48x2048x2048x146): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x146): 44.308

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 130.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x147x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x147x2048): 58.538
Elapsed time for attention_prob_times_values (48x2048x2048x147): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x147): 48.413

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 144.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x148x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x148x2048): 59.754
Elapsed time for attention_prob_times_values (48x2048x2048x148): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x148): 50.700

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 149.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x149x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x149x2048): 59.070
Elapsed time for attention_prob_times_values (48x2048x2048x149): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x149): 48.616

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 146.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x150x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x150x2048): 59.931
Elapsed time for attention_prob_times_values (48x2048x2048x150): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x150): 51.531

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 152.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x151x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x151x2048): 59.709
Elapsed time for attention_prob_times_values (48x2048x2048x151): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x151): 48.695

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 148.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x152x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x152x2048): 61.442
Elapsed time for attention_prob_times_values (48x2048x2048x152): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x152): 49.739

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 152.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x153x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x153x2048): 59.374
Elapsed time for attention_prob_times_values (48x2048x2048x153): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x153): 50.154

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 151.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x154x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x154x2048): 60.260
Elapsed time for attention_prob_times_values (48x2048x2048x154): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x154): 52.648

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 157.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x155x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x155x2048): 59.588
Elapsed time for attention_prob_times_values (48x2048x2048x155): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x155): 50.849

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 154.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x156x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x156x2048): 61.185
Elapsed time for attention_prob_times_values (48x2048x2048x156): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x156): 48.368

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 152.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x157x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x157x2048): 60.633
Elapsed time for attention_prob_times_values (48x2048x2048x157): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x157): 51.465

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 158.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x158x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x158x2048): 61.611
Elapsed time for attention_prob_times_values (48x2048x2048x158): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x158): 48.192

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 154.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x159x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x159x2048): 61.071
Elapsed time for attention_prob_times_values (48x2048x2048x159): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x159): 51.829

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 160.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x160x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x160x2048): 73.309
Elapsed time for attention_prob_times_values (48x2048x2048x160): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x160): 54.184

Attention duration (in seconds): 0.0021
Attention throughput (in TFLOP/s): 179.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0021
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x161x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x161x2048): 60.356
Elapsed time for attention_prob_times_values (48x2048x2048x161): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x161): 52.787

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 162.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x162x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x162x2048): 61.111
Elapsed time for attention_prob_times_values (48x2048x2048x162): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x162): 54.861

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 167.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x163x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x163x2048): 60.118
Elapsed time for attention_prob_times_values (48x2048x2048x163): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x163): 53.401

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 164.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x164x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x164x2048): 62.409
Elapsed time for attention_prob_times_values (48x2048x2048x164): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x164): 52.837

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 167.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x165x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x165x2048): 61.177
Elapsed time for attention_prob_times_values (48x2048x2048x165): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x165): 53.931

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 168.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 1992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x166x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x166x2048): 61.118
Elapsed time for attention_prob_times_values (48x2048x2048x166): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x166): 55.947

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 172.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x167x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x167x2048): 61.580
Elapsed time for attention_prob_times_values (48x2048x2048x167): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x167): 54.680

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 171.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x168x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x168x2048): 63.530
Elapsed time for attention_prob_times_values (48x2048x2048x168): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x168): 54.521

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 174.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x169x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x169x2048): 61.261
Elapsed time for attention_prob_times_values (48x2048x2048x169): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x169): 54.698

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 172.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x170x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x170x2048): 62.400
Elapsed time for attention_prob_times_values (48x2048x2048x170): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x170): 56.954

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 178.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x171x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x171x2048): 61.734
Elapsed time for attention_prob_times_values (48x2048x2048x171): 0.0012
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1179x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1179x2048): 79.915
Elapsed time for attention_prob_times_values (96x2048x2048x1179): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1179): 79.130

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2276.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1180x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1180x2048): 82.818
Elapsed time for attention_prob_times_values (96x2048x2048x1180): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1180): 82.875

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2374.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1181x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1181x2048): 83.639
Elapsed time for attention_prob_times_values (96x2048x2048x1181): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1181): 81.409

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2366.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1182x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1182x2048): 84.302
Elapsed time for attention_prob_times_values (96x2048x2048x1182): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1182): 84.195

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2418.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1183x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1183x2048): 80.774
Elapsed time for attention_prob_times_values (96x2048x2048x1183): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1183): 81.087

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2324.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1184x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1184x2048): 94.777
Elapsed time for attention_prob_times_values (96x2048x2048x1184): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1184): 91.067

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2670.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1185x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1185x2048): 82.346
Elapsed time for attention_prob_times_values (96x2048x2048x1185): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1185): 82.919

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2377.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1186x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1186x2048): 86.358
Elapsed time for attention_prob_times_values (96x2048x2048x1186): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1186): 83.607

Attention duration (in seconds): 0.0225
Attention throughput (in TFLOP/s): 2446.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1187x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1187x2048): 83.309
Elapsed time for attention_prob_times_values (96x2048x2048x1187): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1187): 82.987

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2396.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x171): 55.208

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 175.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x172x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x172x2048): 63.341
Elapsed time for attention_prob_times_values (48x2048x2048x172): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x172): 57.865

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 182.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x173x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x173x2048): 62.151
Elapsed time for attention_prob_times_values (48x2048x2048x173): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x173): 55.897

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 178.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x174x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x174x2048): 63.703
Elapsed time for attention_prob_times_values (48x2048x2048x174): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x174): 58.317

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 185.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x175x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x175x2048): 62.843
Elapsed time for attention_prob_times_values (48x2048x2048x175): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x175): 48.403

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 166.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x176x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x176x2048): 65.557
Elapsed time for attention_prob_times_values (48x2048x2048x176): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x176): 57.504

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 187.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x177x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x177x2048): 62.629
Elapsed time for attention_prob_times_values (48x2048x2048x177): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x177): 49.020

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 169.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x178x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x178x2048): 63.636
Elapsed time for attention_prob_times_values (48x2048x2048x178): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x178): 59.422

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 189.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x179x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x179x2048): 62.980
Elapsed time for attention_prob_times_values (48x2048x2048x179): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x179): 57.538

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 186.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x180x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x180x2048): 64.364
Elapsed time for attention_prob_times_values (48x2048x2048x180): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x180): 60.101

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 193.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x181x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x181x2048): 63.499
Elapsed time for attention_prob_times_values (48x2048x2048x181): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x181): 56.264

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 186.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x182x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x182x2048): 64.832
Elapsed time for attention_prob_times_values (48x2048x2048x182): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x182): 60.340

Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 195.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0023
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x183x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x183x2048): 63.961
Elapsed time for attention_prob_times_values (48x2048x2048x183): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x183): 58.612

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 192.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x184x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x184x2048): 65.920
Elapsed time for attention_prob_times_values (48x2048x2048x184): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x184): 58.898

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 196.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x185x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x185x2048): 63.698
Elapsed time for attention_prob_times_values (48x2048x2048x185): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x185): 59.241

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 194.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x186x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x186x2048): 64.728
Elapsed time for attention_prob_times_values (48x2048x2048x186): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x186): 60.339

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 198.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x187x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x187x2048): 63.493
Elapsed time for attention_prob_times_values (48x2048x2048x187): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x187): 59.208

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 195.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x188x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x188x2048): 65.478
Elapsed time for attention_prob_times_values (48x2048x2048x188): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x188): 61.892

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 203.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x189x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x189x2048): 64.496
Elapsed time for attention_prob_times_values (48x2048x2048x189): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x189): 60.316

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 200.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x190x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x190x2048): 65.647
Elapsed time for attention_prob_times_values (48x2048x2048x190): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x190): 62.648

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 206.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x191x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x191x2048): 63.648
Elapsed time for attention_prob_times_values (48x2048x2048x191): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x191): 60.805

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 201.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x192x2048): 0.0010
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x192x2048): 74.731
Elapsed time for attention_prob_times_values (48x2048x2048x192): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x192): 64.248

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 224.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x193x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x193x2048): 63.488
Elapsed time for attention_prob_times_values (48x2048x2048x193): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x193): 49.634

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 181.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x194x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x194x2048): 65.370
Elapsed time for attention_prob_times_values (48x2048x2048x194): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x194): 51.451

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 188.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x195x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x195x2048): 64.397
Elapsed time for attention_prob_times_values (48x2048x2048x195): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x195): 50.027

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 184.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x196x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x196x2048): 65.900
Elapsed time for attention_prob_times_values (48x2048x2048x196): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x196): 51.713

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 191.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x197x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x197x2048): 64.779
Elapsed time for attention_prob_times_values (48x2048x2048x197): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x197): 50.609

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 188.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x198x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x198x2048): 65.968
Elapsed time for attention_prob_times_values (48x2048x2048x198): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x198): 52.429

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 193.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x199x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x199x2048): 65.297
Elapsed time for attention_prob_times_values (48x2048x2048x199): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x199): 50.584

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 189.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
2.1.1+rocm5.6 

num_attention_heads: 256, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x84x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x84x2048): 48.972
Elapsed time for attention_prob_times_values (1024x2048x2048x84): 0.0237
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x84): 30.478

Attention duration (in seconds): 0.0384
Attention throughput (in TFLOP/s): 826.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0384
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x85x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x85x2048): 49.130
Elapsed time for attention_prob_times_values (1024x2048x2048x85): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x85): 41.399

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 999.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x86x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x86x2048): 49.043
Elapsed time for attention_prob_times_values (1024x2048x2048x86): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x86): 41.484

Attention duration (in seconds): 0.0329
Attention throughput (in TFLOP/s): 1011.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0329
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x87x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x87x2048): 48.745
Elapsed time for attention_prob_times_values (1024x2048x2048x87): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x87): 42.391

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 1031.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x88x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x88x2048): 50.130
Elapsed time for attention_prob_times_values (1024x2048x2048x88): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x88): 43.085

Attention duration (in seconds): 0.0326
Attention throughput (in TFLOP/s): 1065.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0326
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x89x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x89x2048): 47.246
Elapsed time for attention_prob_times_values (1024x2048x2048x89): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x89): 43.184

Attention duration (in seconds): 0.0339
Attention throughput (in TFLOP/s): 1049.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x90x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x90x2048): 49.727
Elapsed time for attention_prob_times_values (1024x2048x2048x90): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x90): 44.766

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 1107.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0328
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x91x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x91x2048): 49.798
Elapsed time for attention_prob_times_values (1024x2048x2048x91): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x91): 43.632

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 1104.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0336
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x92x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x92x2048): 48.812
Elapsed time for attention_prob_times_values (1024x2048x2048x92): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x92): 46.091

Attention duration (in seconds): 0.0333
Attention throughput (in TFLOP/s): 1137.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0333
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x93x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x93x2048): 50.589
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x167x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x167x2048): 61.771
Elapsed time for attention_prob_times_values (320x2048x2048x167): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x167): 55.677

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 822.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x168x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x168x2048): 65.550
Elapsed time for attention_prob_times_values (320x2048x2048x168): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x168): 55.540

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 849.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x169x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x169x2048): 63.257
Elapsed time for attention_prob_times_values (320x2048x2048x169): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x169): 55.641

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 840.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x170x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x170x2048): 64.545
Elapsed time for attention_prob_times_values (320x2048x2048x170): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x170): 58.305

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 874.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x171x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x171x2048): 63.690
Elapsed time for attention_prob_times_values (320x2048x2048x171): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x171): 56.409

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 859.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x172x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x172x2048): 65.395
Elapsed time for attention_prob_times_values (320x2048x2048x172): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x172): 59.243

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 897.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x173x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x173x2048): 62.885
Elapsed time for attention_prob_times_values (320x2048x2048x173): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x173): 56.004

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 859.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x174x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x174x2048): 63.846
Elapsed time for attention_prob_times_values (320x2048x2048x174): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x174): 59.479

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 898.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x175x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x175x2048): 64.811
Elapsed time for attention_prob_times_values (320x2048x2048x175): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x175): 56.456

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 885.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x200x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x200x2048): 67.325
Elapsed time for attention_prob_times_values (48x2048x2048x200): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x200): 50.413

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 192.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x201x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x201x2048): 64.869
Elapsed time for attention_prob_times_values (48x2048x2048x201): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x201): 50.447

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 190.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x202x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x202x2048): 66.127
Elapsed time for attention_prob_times_values (48x2048x2048x202): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x202): 53.188

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 198.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x203x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x203x2048): 65.265
Elapsed time for attention_prob_times_values (48x2048x2048x203): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x203): 51.010

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 193.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x204x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x204x2048): 60.390
Elapsed time for attention_prob_times_values (48x2048x2048x204): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x204): 53.681

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 192.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x205x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x205x2048): 58.730
Elapsed time for attention_prob_times_values (48x2048x2048x205): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x205): 51.459

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 186.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x206x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x206x2048): 66.892
Elapsed time for attention_prob_times_values (48x2048x2048x206): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x206): 53.888

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 203.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x207x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x207x2048): 65.911
Elapsed time for attention_prob_times_values (48x2048x2048x207): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x207): 51.745

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 198.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x208x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x208x2048): 68.946
Elapsed time for attention_prob_times_values (48x2048x2048x208): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x208): 49.990

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 199.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x209x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x209x2048): 65.828
Elapsed time for attention_prob_times_values (48x2048x2048x209): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x209): 45.698

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 186.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x210x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x210x2048): 66.904
Elapsed time for attention_prob_times_values (48x2048x2048x210): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x210): 54.943

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 208.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x211x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x211x2048): 66.015
Elapsed time for attention_prob_times_values (48x2048x2048x211): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x211): 52.590

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 203.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x212x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x212x2048): 67.490
Elapsed time for attention_prob_times_values (48x2048x2048x212): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x212): 55.441

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 212.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x213x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x213x2048): 66.370
Elapsed time for attention_prob_times_values (48x2048x2048x213): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x213): 52.839

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 205.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x214x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x214x2048): 67.617
Elapsed time for attention_prob_times_values (48x2048x2048x214): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x214): 55.466

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 213.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x215x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x215x2048): 66.808
Elapsed time for attention_prob_times_values (48x2048x2048x215): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x215): 53.397

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 208.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x216x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x216x2048): 68.829
Elapsed time for attention_prob_times_values (48x2048x2048x216): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x216): 54.035

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 213.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x217x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x217x2048): 66.657
Elapsed time for attention_prob_times_values (48x2048x2048x217): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x217): 45.261

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 191.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x218x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x218x2048): 67.716
Elapsed time for attention_prob_times_values (48x2048x2048x218): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x218): 56.580

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 219.144
--------
Elapsed time for attention_key_query_prob (96x2048x1188x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1188x2048): 86.981
Elapsed time for attention_prob_times_values (96x2048x2048x1188): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1188): 84.332

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 2470.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1189x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1189x2048): 84.514
Elapsed time for attention_prob_times_values (96x2048x2048x1189): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1189): 82.943

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2416.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1190x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1190x2048): 84.292
Elapsed time for attention_prob_times_values (96x2048x2048x1190): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1190): 84.052

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2431.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1191x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1191x2048): 84.818
Elapsed time for attention_prob_times_values (96x2048x2048x1191): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1191): 82.995

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2425.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1192x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1192x2048): 85.668
Elapsed time for attention_prob_times_values (96x2048x2048x1192): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1192): 87.847

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2510.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1193x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1193x2048): 79.992
Elapsed time for attention_prob_times_values (96x2048x2048x1193): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1193): 83.206

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2362.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1194x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1194x2048): 84.124
Elapsed time for attention_prob_times_values (96x2048x2048x1194): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1194): 85.695

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2460.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1195x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1195x2048): 84.557
Elapsed time for attention_prob_times_values (96x2048x2048x1195): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1195): 80.121

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2386.752
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1196x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1196x2048): 86.212
Elapsed time for attention_prob_times_values (96x2048x2048x1196): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1196): 82.734

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2451.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1197x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1197x2048): 83.981
Elapsed time for attention_prob_times_values (96x2048x2048x1197): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1197): 78.490

Attention duration (in seconds): 0.0238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x219x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x219x2048): 66.931
Elapsed time for attention_prob_times_values (48x2048x2048x219): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x219): 54.319

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 213.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x220x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x220x2048): 68.362
Elapsed time for attention_prob_times_values (48x2048x2048x220): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x220): 57.152

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 222.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x221x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x221x2048): 67.238
Elapsed time for attention_prob_times_values (48x2048x2048x221): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x221): 54.791

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 216.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x222x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x222x2048): 67.946
Elapsed time for attention_prob_times_values (48x2048x2048x222): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x222): 57.381

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 224.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x223x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x223x2048): 67.871
Elapsed time for attention_prob_times_values (48x2048x2048x223): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x223): 54.740

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 218.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x224x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x224x2048): 77.473
Elapsed time for attention_prob_times_values (48x2048x2048x224): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x224): 58.138

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 240.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x225x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x225x2048): 66.409
Elapsed time for attention_prob_times_values (48x2048x2048x225): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x225): 56.029

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 221.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x226x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x226x2048): 67.746
Elapsed time for attention_prob_times_values (48x2048x2048x226): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x226): 58.183

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 228.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x227x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x227x2048): 66.719
Elapsed time for attention_prob_times_values (48x2048x2048x227): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x227): 56.262

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 223.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x228x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x228x2048): 67.224
Elapsed time for attention_prob_times_values (48x2048x2048x228): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x228): 58.743

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 230.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x229x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x229x2048): 64.641
Elapsed time for attention_prob_times_values (48x2048x2048x229): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x229): 56.758

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 222.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x230x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x230x2048): 68.471
Elapsed time for attention_prob_times_values (48x2048x2048x230): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x230): 58.925

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 234.063
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x231x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x231x2048): 67.274
Elapsed time for attention_prob_times_values (48x2048x2048x231): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x231): 55.258

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 224.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x232x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x232x2048): 69.842
Elapsed time for attention_prob_times_values (48x2048x2048x232): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x232): 57.696

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 234.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x233x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x233x2048): 66.911
Elapsed time for attention_prob_times_values (48x2048x2048x233): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x233): 57.087

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 229.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x234x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x234x2048): 67.809
Elapsed time for attention_prob_times_values (48x2048x2048x234): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x234): 59.642

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 237.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x235x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x235x2048): 67.199
Elapsed time for attention_prob_times_values (48x2048x2048x235): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x235): 57.369

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 232.353
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x236x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x236x2048): 68.400
Elapsed time for attention_prob_times_values (48x2048x2048x236): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x236): 60.298

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 241.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x237x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x237x2048): 67.588
Elapsed time for attention_prob_times_values (48x2048x2048x237): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x237): 57.866

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 235.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x238x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x238x2048): 68.034
Elapsed time for attention_prob_times_values (48x2048x2048x238): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x238): 60.398

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 242.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x239x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x239x2048): 67.963
Elapsed time for attention_prob_times_values (48x2048x2048x239): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x239): 58.278

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 238.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x240x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x240x2048): 54.915
Elapsed time for attention_prob_times_values (48x2048x2048x240): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x240): 60.523

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 219.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x241x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x241x2048): 52.715
Elapsed time for attention_prob_times_values (48x2048x2048x241): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x241): 58.203

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 211.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x242x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x242x2048): 53.586
Elapsed time for attention_prob_times_values (48x2048x2048x242): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x242): 61.330

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 219.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x243x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x243x2048): 52.974
Elapsed time for attention_prob_times_values (48x2048x2048x243): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x243): 59.185

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 215.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x244x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x244x2048): 54.157
Elapsed time for attention_prob_times_values (48x2048x2048x244): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x244): 61.930

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 223.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x245x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x245x2048): 53.403
Elapsed time for attention_prob_times_values (48x2048x2048x245): 0.0018
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x245): 56.097

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 211.814
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x246x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x246x2048): 54.047
Elapsed time for attention_prob_times_values (48x2048x2048x246): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x246): 62.203

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 224.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x247x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x247x2048): 53.749
Elapsed time for attention_prob_times_values (48x2048x2048x247): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x247): 59.857

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 220.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x248x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x248x2048): 55.081
Elapsed time for attention_prob_times_values (48x2048x2048x248): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x248): 61.206

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 226.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 2988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x249x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x249x2048): 53.217
Elapsed time for attention_prob_times_values (48x2048x2048x249): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x249): 53.679

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 209.405
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x250x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x250x2048): 53.553
Elapsed time for attention_prob_times_values (48x2048x2048x250): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x250): 62.895

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 227.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x251x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x251x2048): 53.308
Elapsed time for attention_prob_times_values (48x2048x2048x251): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x251): 54.409

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 212.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x252x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x252x2048): 54.305
Elapsed time for attention_prob_times_values (48x2048x2048x252): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x252): 63.266

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 231.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x253x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x253x2048): 52.770
Elapsed time for attention_prob_times_values (48x2048x2048x253): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x253): 62.213

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 226.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x254x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x254x2048): 54.006
Elapsed time for attention_prob_times_values (48x2048x2048x254): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x254): 63.863

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 232.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x255x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x255x2048): 52.952
Elapsed time for attention_prob_times_values (48x2048x2048x255): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x255): 61.610

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 227.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x256x2048): 0.0014
Elapsed time for attention_key_query_prob (320x2048x176x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x176x2048): 67.786
Elapsed time for attention_prob_times_values (320x2048x2048x176): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x176): 58.790

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 928.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x177x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x177x2048): 63.215
Elapsed time for attention_prob_times_values (320x2048x2048x177): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x177): 56.924

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 888.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x178x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x178x2048): 65.491
Elapsed time for attention_prob_times_values (320x2048x2048x178): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x178): 60.514

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 937.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x179x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x179x2048): 61.188
Elapsed time for attention_prob_times_values (320x2048x2048x179): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x179): 55.647

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 873.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x180x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x180x2048): 66.407
Elapsed time for attention_prob_times_values (320x2048x2048x180): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x180): 59.542

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 945.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x181x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x181x2048): 60.969
Elapsed time for attention_prob_times_values (320x2048x2048x181): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x181): 59.465

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 911.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x182x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x182x2048): 66.850
Elapsed time for attention_prob_times_values (320x2048x2048x182): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x182): 59.260

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 956.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x183x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x183x2048): 62.287
Elapsed time for attention_prob_times_values (320x2048x2048x183): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x183): 56.802

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 908.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x184x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x184x2048): 68.046
Elapsed time for attention_prob_times_values (320x2048x2048x184): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x184): 60.254

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 982.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x185x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x185x2048): 64.206
Elapsed time for attention_prob_times_values (320x2048x2048x185): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x185): 60.555

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 2357.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1198x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1198x2048): 85.392
Elapsed time for attention_prob_times_values (96x2048x2048x1198): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1198): 84.441

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2469.135
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1199x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1199x2048): 83.888
Elapsed time for attention_prob_times_values (96x2048x2048x1199): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1199): 81.429

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2404.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1200x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1200x2048): 84.522
Elapsed time for attention_prob_times_values (96x2048x2048x1200): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1200): 91.357

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 2557.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1201x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1201x2048): 82.194
Elapsed time for attention_prob_times_values (96x2048x2048x1201): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1201): 83.189

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2410.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1202x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1202x2048): 85.040
Elapsed time for attention_prob_times_values (96x2048x2048x1202): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1202): 84.857

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2478.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1203x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1203x2048): 82.803
Elapsed time for attention_prob_times_values (96x2048x2048x1203): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1203): 83.502

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2427.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1204x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1204x2048): 84.180
Elapsed time for attention_prob_times_values (96x2048x2048x1204): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1204): 86.365

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2491.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1205x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1205x2048): 83.456
Elapsed time for attention_prob_times_values (96x2048x2048x1205): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1205): 83.880

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2446.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1206x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1206x2048): 85.280
Elapsed time for attention_prob_times_values (96x2048x2048x1206): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1206): 85.101

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2493.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x256x2048): 73.880
Elapsed time for attention_prob_times_values (48x2048x2048x256): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x256): 67.702

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 282.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x257x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x257x2048): 55.745
Elapsed time for attention_prob_times_values (48x2048x2048x257): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x257): 52.369

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 216.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x258x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x258x2048): 56.346
Elapsed time for attention_prob_times_values (48x2048x2048x258): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x258): 55.120

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 224.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x259x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x259x2048): 55.849
Elapsed time for attention_prob_times_values (48x2048x2048x259): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x259): 52.981

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 219.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x260x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x260x2048): 56.871
Elapsed time for attention_prob_times_values (48x2048x2048x260): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x260): 55.709

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 227.775
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x261x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x261x2048): 55.803
Elapsed time for attention_prob_times_values (48x2048x2048x261): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x261): 53.683

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 222.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x262x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x262x2048): 56.424
Elapsed time for attention_prob_times_values (48x2048x2048x262): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x262): 53.247

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 223.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x263x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x263x2048): 55.632
Elapsed time for attention_prob_times_values (48x2048x2048x263): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x263): 54.055

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 223.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x264x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x264x2048): 52.056
Elapsed time for attention_prob_times_values (48x2048x2048x264): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x264): 53.473

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 215.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x265x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x265x2048): 54.759
Elapsed time for attention_prob_times_values (48x2048x2048x265): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x265): 54.136

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 223.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x266x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x266x2048): 52.659
Elapsed time for attention_prob_times_values (48x2048x2048x266): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x266): 56.743

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 224.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x267x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x267x2048): 55.031
Elapsed time for attention_prob_times_values (48x2048x2048x267): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x267): 54.492

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 226.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x268x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x268x2048): 55.982
Elapsed time for attention_prob_times_values (48x2048x2048x268): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x268): 57.336

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 234.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x269x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x269x2048): 55.579
Elapsed time for attention_prob_times_values (48x2048x2048x269): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x269): 52.703

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 224.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x270x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x270x2048): 55.792
Elapsed time for attention_prob_times_values (48x2048x2048x270): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x270): 57.531

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 235.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x271x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x271x2048): 55.760
Elapsed time for attention_prob_times_values (48x2048x2048x271): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x271): 55.228

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 231.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x272x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x272x2048): 57.407
Elapsed time for attention_prob_times_values (48x2048x2048x272): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x272): 70.585

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 265.141
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x273x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x273x2048): 55.058
Elapsed time for attention_prob_times_values (48x2048x2048x273): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x273): 55.623

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 232.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x274x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x274x2048): 55.894
Elapsed time for attention_prob_times_values (48x2048x2048x274): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x274): 58.207

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 240.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x275x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x275x2048): 55.237
Elapsed time for attention_prob_times_values (48x2048x2048x275): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x275): 55.694

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 234.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x276x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x276x2048): 56.384
Elapsed time for attention_prob_times_values (48x2048x2048x276): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x276): 58.728

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 243.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x277x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x277x2048): 55.798
Elapsed time for attention_prob_times_values (48x2048x2048x277): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x277): 56.390

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 238.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x278x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x278x2048): 56.268
Elapsed time for attention_prob_times_values (48x2048x2048x278): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x278): 58.968

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 245.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x279x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x279x2048): 56.134
Elapsed time for attention_prob_times_values (48x2048x2048x279): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x279): 56.790

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 241.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x280x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x280x2048): 57.319
Elapsed time for attention_prob_times_values (48x2048x2048x280): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x280): 72.206

Attention duration (in seconds): 0.0035
Attention throughput (in TFLOP/s): 273.601
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0035
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x281x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x281x2048): 55.350
Elapsed time for attention_prob_times_values (48x2048x2048x281): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x281): 57.074

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 241.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x282x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x282x2048): 55.840
Elapsed time for attention_prob_times_values (48x2048x2048x282): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x282): 59.801

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 248.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x283x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x283x2048): 55.653
Elapsed time for attention_prob_times_values (48x2048x2048x283): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x283): 57.549

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 244.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x284x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x284x2048): 56.748
Elapsed time for attention_prob_times_values (48x2048x2048x284): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x284): 60.255

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 252.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x285x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x285x2048): 53.261
Elapsed time for attention_prob_times_values (48x2048x2048x285): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x285): 57.630

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 240.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x286x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x286x2048): 56.579
Elapsed time for attention_prob_times_values (48x2048x2048x286): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x286): 60.233

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 253.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x287x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x287x2048): 56.191
Elapsed time for attention_prob_times_values (48x2048x2048x287): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x287): 58.187

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 249.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x288x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x288x2048): 76.093
Elapsed time for attention_prob_times_values (48x2048x2048x288): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x288): 72.509

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 324.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x289x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x289x2048): 59.645
Elapsed time for attention_prob_times_values (48x2048x2048x289): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x289): 58.727

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 259.617
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x290x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x290x2048): 60.495
Elapsed time for attention_prob_times_values (48x2048x2048x290): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x290): 58.704

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 262.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x291x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x291x2048): 59.722
Elapsed time for attention_prob_times_values (48x2048x2048x291): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x291): 59.147

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 262.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x292x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x292x2048): 56.077
Elapsed time for attention_prob_times_values (48x2048x2048x292): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x292): 61.682

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 259.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x293x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x293x2048): 59.698
Elapsed time for attention_prob_times_values (48x2048x2048x293): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x293): 59.626

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 264.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x294x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x294x2048): 60.338
Elapsed time for attention_prob_times_values (48x2048x2048x294): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x294): 61.910

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 271.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x295x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x295x2048): 53.307
Elapsed time for attention_prob_times_values (48x2048x2048x295): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x295): 60.025

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 251.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x296x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x296x2048): 53.215
Elapsed time for attention_prob_times_values (48x2048x2048x296): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x296): 75.823

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 279.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x297x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x297x2048): 51.967
Elapsed time for attention_prob_times_values (48x2048x2048x297): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x297): 60.049

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 249.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x298x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x298x2048): 59.035
Elapsed time for attention_prob_times_values (48x2048x2048x298): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x298): 62.714

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 273.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x299x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x299x2048): 58.765
Elapsed time for attention_prob_times_values (48x2048x2048x299): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x299): 60.555

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 268.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x300x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x300x2048): 60.302
Elapsed time for attention_prob_times_values (48x2048x2048x300): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x300): 54.263

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 257.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x301x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x301x2048): 59.234
Elapsed time for attention_prob_times_values (48x2048x2048x301): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x301): 61.143

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 272.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x302x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x302x2048): 59.898
Elapsed time for attention_prob_times_values (48x2048x2048x302): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x302): 61.539

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 275.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x303x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x303x2048): 57.137
Elapsed time for attention_prob_times_values (48x2048x2048x303): 0.0020
========================================================================================================================
num_attention_heads: 24, hidden_size: 28968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1207x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1207x2048): 84.569
Elapsed time for attention_prob_times_values (96x2048x2048x1207): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1207): 83.905

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2467.186
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1208x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1208x2048): 86.659
Elapsed time for attention_prob_times_values (96x2048x2048x1208): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1208): 91.706

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 2612.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1209x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1209x2048): 82.378
Elapsed time for attention_prob_times_values (96x2048x2048x1209): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1209): 84.220

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 2443.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1210x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1210x2048): 84.754
Elapsed time for attention_prob_times_values (96x2048x2048x1210): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1210): 86.222

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2509.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1211x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1211x2048): 83.871
Elapsed time for attention_prob_times_values (96x2048x2048x1211): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1211): 84.227

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2469.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1212x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1212x2048): 85.917
Elapsed time for attention_prob_times_values (96x2048x2048x1212): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1212): 85.791

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2524.642
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1213x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1213x2048): 81.404
Elapsed time for attention_prob_times_values (96x2048x2048x1213): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1213): 82.587

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2412.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1214x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1214x2048): 84.575
Elapsed time for attention_prob_times_values (96x2048x2048x1214): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1214): 87.043

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2526.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1215x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1215x2048): 84.267
Elapsed time for attention_prob_times_values (96x2048x2048x1215): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1215): 84.570

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2488.358
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1216x2048): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x303): 61.653

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 269.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x304x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x304x2048): 60.723
Elapsed time for attention_prob_times_values (48x2048x2048x304): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x304): 77.054

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 309.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x305x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x305x2048): 58.627
Elapsed time for attention_prob_times_values (48x2048x2048x305): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x305): 61.904

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 275.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x306x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x306x2048): 59.401
Elapsed time for attention_prob_times_values (48x2048x2048x306): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x306): 61.965

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 278.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x307x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x307x2048): 59.018
Elapsed time for attention_prob_times_values (48x2048x2048x307): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x307): 54.104

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 259.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x308x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x308x2048): 58.028
Elapsed time for attention_prob_times_values (48x2048x2048x308): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x308): 65.339

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 283.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x309x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x309x2048): 56.265
Elapsed time for attention_prob_times_values (48x2048x2048x309): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x309): 62.776

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 274.226
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x310x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x310x2048): 59.881
Elapsed time for attention_prob_times_values (48x2048x2048x310): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x310): 65.182

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 289.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x311x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x311x2048): 59.523
Elapsed time for attention_prob_times_values (48x2048x2048x311): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x311): 63.091

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 284.501
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x312x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x312x2048): 61.078
Elapsed time for attention_prob_times_values (48x2048x2048x312): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x312): 79.547

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 321.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention throughput (in TFLOP/s): 963.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x186x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x186x2048): 66.941
Elapsed time for attention_prob_times_values (320x2048x2048x186): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x186): 60.934

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 990.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 14960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x187x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x187x2048): 63.317
Elapsed time for attention_prob_times_values (320x2048x2048x187): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x187): 58.465

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 948.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x188x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x188x2048): 67.622
Elapsed time for attention_prob_times_values (320x2048x2048x188): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x188): 63.183

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1024.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x189x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x189x2048): 66.351
Elapsed time for attention_prob_times_values (320x2048x2048x189): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x189): 61.448

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1005.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x190x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x190x2048): 67.868
Elapsed time for attention_prob_times_values (320x2048x2048x190): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x190): 62.592

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1031.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x191x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x191x2048): 67.190
Elapsed time for attention_prob_times_values (320x2048x2048x191): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x191): 61.089

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1018.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x192x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x192x2048): 77.260
Elapsed time for attention_prob_times_values (320x2048x2048x192): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x192): 65.654

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1135.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x193x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x193x2048): 65.733
Elapsed time for attention_prob_times_values (320x2048x2048x193): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x193): 52.483

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 938.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x194x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x194x2048): 67.415
Elapsed time for attention_prob_times_values (320x2048x2048x194): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x194): 52.955

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 958.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x313x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x313x2048): 58.839
Elapsed time for attention_prob_times_values (48x2048x2048x313): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x313): 62.915

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 283.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x314x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x314x2048): 59.265
Elapsed time for attention_prob_times_values (48x2048x2048x314): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x314): 65.876

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 291.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x315x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x315x2048): 59.241
Elapsed time for attention_prob_times_values (48x2048x2048x315): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x315): 63.703

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 288.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x316x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x316x2048): 60.631
Elapsed time for attention_prob_times_values (48x2048x2048x316): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x316): 66.197

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 297.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x317x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x317x2048): 59.353
Elapsed time for attention_prob_times_values (48x2048x2048x317): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x317): 64.200

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 290.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x318x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x318x2048): 58.274
Elapsed time for attention_prob_times_values (48x2048x2048x318): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x318): 66.690

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 293.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x319x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x319x2048): 59.425
Elapsed time for attention_prob_times_values (48x2048x2048x319): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x319): 64.389

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 292.860
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x320x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x320x2048): 76.941
Elapsed time for attention_prob_times_values (48x2048x2048x320): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x320): 80.183

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 373.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x321x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x321x2048): 61.999
Elapsed time for attention_prob_times_values (48x2048x2048x321): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x321): 56.838

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 282.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x322x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x322x2048): 62.926
Elapsed time for attention_prob_times_values (48x2048x2048x322): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x322): 59.001

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 290.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x323x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x323x2048): 61.967
Elapsed time for attention_prob_times_values (48x2048x2048x323): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x323): 56.965

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 284.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x324x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x324x2048): 59.496
Elapsed time for attention_prob_times_values (48x2048x2048x324): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x324): 59.136

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 284.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x325x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x325x2048): 61.706
Elapsed time for attention_prob_times_values (48x2048x2048x325): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x325): 52.484

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 272.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x326x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x326x2048): 62.197
Elapsed time for attention_prob_times_values (48x2048x2048x326): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x326): 59.650

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 293.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x327x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x327x2048): 61.560
Elapsed time for attention_prob_times_values (48x2048x2048x327): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x327): 57.803

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 288.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x328x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x328x2048): 63.628
Elapsed time for attention_prob_times_values (48x2048x2048x328): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x328): 70.324

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 323.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x329x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x329x2048): 60.725
Elapsed time for attention_prob_times_values (48x2048x2048x329): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x329): 56.651

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 284.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x330x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x330x2048): 61.305
Elapsed time for attention_prob_times_values (48x2048x2048x330): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x330): 60.460

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 296.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x331x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x331x2048): 61.094
Elapsed time for attention_prob_times_values (48x2048x2048x331): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x331): 57.147

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 288.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Elapsed time for attention_prob_times_values (1024x2048x2048x93): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x93): 45.128

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 1156.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x94x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x94x2048): 50.569
Elapsed time for attention_prob_times_values (1024x2048x2048x94): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x94): 47.374

Attention duration (in seconds): 0.0330
Attention throughput (in TFLOP/s): 1198.531
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0330
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x95x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x95x2048): 50.224
Elapsed time for attention_prob_times_values (1024x2048x2048x95): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x95): 45.922

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 1187.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x96x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x96x2048): 63.907
Elapsed time for attention_prob_times_values (1024x2048x2048x96): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x96): 47.661

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 1365.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x97x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x97x2048): 52.114
Elapsed time for attention_prob_times_values (1024x2048x2048x97): 0.0177
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x97): 46.983

Attention duration (in seconds): 0.0337
Attention throughput (in TFLOP/s): 1247.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x98x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x98x2048): 51.182
Elapsed time for attention_prob_times_values (1024x2048x2048x98): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x98): 48.788

Attention duration (in seconds): 0.0337
Attention throughput (in TFLOP/s): 1273.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0337
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x99x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x99x2048): 52.615
Elapsed time for attention_prob_times_values (1024x2048x2048x99): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x99): 47.385

Attention duration (in seconds): 0.0341
Attention throughput (in TFLOP/s): 1283.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0341
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x100x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x100x2048): 53.924
Elapsed time for attention_prob_times_values (1024x2048x2048x100): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x100): 49.299

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 1339.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0334
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x101x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x101x2048): 53.165
Elapsed time for attention_prob_times_values (1024x2048x2048x101): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x101): 47.455

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 1316.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x102x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x102x2048): 51.078
Elapsed time for attention_prob_times_values (1024x2048x2048x102): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x102): 49.805

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 1336.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x332x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x332x2048): 62.599
Elapsed time for attention_prob_times_values (48x2048x2048x332): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x332): 60.932

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 302.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 3996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x333x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x333x2048): 61.222
Elapsed time for attention_prob_times_values (48x2048x2048x333): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x333): 52.757

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 277.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x334x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x334x2048): 61.973
Elapsed time for attention_prob_times_values (48x2048x2048x334): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x334): 61.060

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 302.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x335x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x335x2048): 61.356
Elapsed time for attention_prob_times_values (48x2048x2048x335): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x335): 57.549

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 292.550
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x336x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x336x2048): 62.945
Elapsed time for attention_prob_times_values (48x2048x2048x336): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x336): 71.730

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 331.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x337x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x337x2048): 54.852
Elapsed time for attention_prob_times_values (48x2048x2048x337): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x337): 57.705

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 278.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x338x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x338x2048): 57.089
Elapsed time for attention_prob_times_values (48x2048x2048x338): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x338): 61.340

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 293.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x339x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x339x2048): 60.932
Elapsed time for attention_prob_times_values (48x2048x2048x339): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x339): 58.582

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 297.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x340x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x340x2048): 58.042
Elapsed time for attention_prob_times_values (48x2048x2048x340): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x340): 62.177

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 299.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1216x2048): 97.028
Elapsed time for attention_prob_times_values (96x2048x2048x1216): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1216): 94.098

Attention duration (in seconds): 0.0205
Attention throughput (in TFLOP/s): 2818.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0205
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1217x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1217x2048): 85.853
Elapsed time for attention_prob_times_values (96x2048x2048x1217): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1217): 83.049

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2492.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1218x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1218x2048): 84.908
Elapsed time for attention_prob_times_values (96x2048x2048x1218): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1218): 85.251

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 2513.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1219x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1219x2048): 84.309
Elapsed time for attention_prob_times_values (96x2048x2048x1219): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1219): 84.913

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2501.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1220x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1220x2048): 86.466
Elapsed time for attention_prob_times_values (96x2048x2048x1220): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1220): 85.654

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 2546.774
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1221x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1221x2048): 85.010
Elapsed time for attention_prob_times_values (96x2048x2048x1221): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1221): 83.689

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 2498.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1222x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1222x2048): 85.547
Elapsed time for attention_prob_times_values (96x2048x2048x1222): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1222): 87.506

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 2564.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1223x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1223x2048): 84.420
Elapsed time for attention_prob_times_values (96x2048x2048x1223): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1223): 85.114

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2514.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1224x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1224x2048): 87.968
Elapsed time for attention_prob_times_values (96x2048x2048x1224): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1224): 92.554

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2677.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1225x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1225x2048): 82.740
Elapsed time for attention_prob_times_values (96x2048x2048x1225): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1225): 84.601

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2485.633
MLP duration (in seconds): 0.0000
Elapsed time for attention_key_query_prob (48x2048x341x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x341x2048): 61.409
Elapsed time for attention_prob_times_values (48x2048x2048x341): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x341): 58.319

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 298.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x342x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x342x2048): 61.315
Elapsed time for attention_prob_times_values (48x2048x2048x342): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x342): 61.933

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 308.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x343x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x343x2048): 61.540
Elapsed time for attention_prob_times_values (48x2048x2048x343): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x343): 56.251

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 295.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x344x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x344x2048): 63.327
Elapsed time for attention_prob_times_values (48x2048x2048x344): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x344): 73.521

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 342.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x345x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x345x2048): 60.769
Elapsed time for attention_prob_times_values (48x2048x2048x345): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x345): 58.800

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 301.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x346x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x346x2048): 61.265
Elapsed time for attention_prob_times_values (48x2048x2048x346): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x346): 61.477

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 310.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x347x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x347x2048): 60.893
Elapsed time for attention_prob_times_values (48x2048x2048x347): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x347): 56.891

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 298.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x348x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x348x2048): 62.344
Elapsed time for attention_prob_times_values (48x2048x2048x348): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x348): 61.957

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 315.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x349x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x349x2048): 61.186
Elapsed time for attention_prob_times_values (48x2048x2048x349): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x349): 59.887

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 308.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x350x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x350x2048): 61.889
Elapsed time for attention_prob_times_values (48x2048x2048x350): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x350): 61.643

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 315.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x351x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x351x2048): 61.308
Elapsed time for attention_prob_times_values (48x2048x2048x351): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x351): 59.769

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 309.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x352x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x352x2048): 79.306
Elapsed time for attention_prob_times_values (48x2048x2048x352): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x352): 74.180

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 392.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x353x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x353x2048): 63.659
Elapsed time for attention_prob_times_values (48x2048x2048x353): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x353): 61.562

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 321.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x354x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x354x2048): 64.299
Elapsed time for attention_prob_times_values (48x2048x2048x354): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x354): 62.276

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 325.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x355x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x355x2048): 63.558
Elapsed time for attention_prob_times_values (48x2048x2048x355): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x355): 59.208

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 316.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x356x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x356x2048): 65.195
Elapsed time for attention_prob_times_values (48x2048x2048x356): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x356): 62.943

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 331.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x357x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x357x2048): 63.438
Elapsed time for attention_prob_times_values (48x2048x2048x357): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x357): 62.099

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 325.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x358x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x358x2048): 64.252
Elapsed time for attention_prob_times_values (48x2048x2048x358): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x358): 63.046

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 330.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x359x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x359x2048): 63.228
Elapsed time for attention_prob_times_values (48x2048x2048x359): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x359): 61.949

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 325.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 80, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x195x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x195x2048): 66.243
Elapsed time for attention_prob_times_values (320x2048x2048x195): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x195): 52.274

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 948.662
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x196x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x196x2048): 67.905
Elapsed time for attention_prob_times_values (320x2048x2048x196): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x196): 52.810

Attention duration (in seconds): 0.0177
Attention throughput (in TFLOP/s): 969.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x197x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x197x2048): 66.915
Elapsed time for attention_prob_times_values (320x2048x2048x197): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x197): 52.898

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 968.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x198x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x198x2048): 67.718
Elapsed time for attention_prob_times_values (320x2048x2048x198): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x198): 53.275

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 982.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 15920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x199x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x199x2048): 67.409
Elapsed time for attention_prob_times_values (320x2048x2048x199): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x199): 52.575

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 977.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x200x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x200x2048): 67.699
Elapsed time for attention_prob_times_values (320x2048x2048x200): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x200): 51.117

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 968.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x201x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x201x2048): 66.946
Elapsed time for attention_prob_times_values (320x2048x2048x201): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x201): 51.493

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 972.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x202x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x202x2048): 68.310
Elapsed time for attention_prob_times_values (320x2048x2048x202): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x202): 54.091

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1013.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x203x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x203x2048): 67.371
Elapsed time for attention_prob_times_values (320x2048x2048x203): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x203): 51.879

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 988.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x204x2048): 0.0082
num_attention_heads: 12, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x360x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x360x2048): 64.997
Elapsed time for attention_prob_times_values (48x2048x2048x360): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x360): 75.894

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 365.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x361x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x361x2048): 62.320
Elapsed time for attention_prob_times_values (48x2048x2048x361): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x361): 61.218

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 323.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x362x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x362x2048): 63.151
Elapsed time for attention_prob_times_values (48x2048x2048x362): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x362): 63.763

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 332.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x363x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x363x2048): 62.633
Elapsed time for attention_prob_times_values (48x2048x2048x363): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x363): 61.428

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 325.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x364x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x364x2048): 58.374
Elapsed time for attention_prob_times_values (48x2048x2048x364): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x364): 64.357

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 322.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x365x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x365x2048): 58.061
Elapsed time for attention_prob_times_values (48x2048x2048x365): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x365): 61.903

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 316.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x366x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x366x2048): 63.521
Elapsed time for attention_prob_times_values (48x2048x2048x366): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x366): 64.515

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 338.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x367x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x367x2048): 62.721
Elapsed time for attention_prob_times_values (48x2048x2048x367): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x367): 62.367

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 331.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x368x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x368x2048): 64.990
Elapsed time for attention_prob_times_values (48x2048x2048x368): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x368): 76.586

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 373.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x369x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x369x2048): 62.275
Elapsed time for attention_prob_times_values (48x2048x2048x369): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x369): 62.613

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 332.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x370x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x370x2048): 63.189
Elapsed time for attention_prob_times_values (48x2048x2048x370): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x370): 64.882

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 341.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x371x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x371x2048): 62.450
Elapsed time for attention_prob_times_values (48x2048x2048x371): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x371): 62.820

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 334.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x372x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x372x2048): 63.698
Elapsed time for attention_prob_times_values (48x2048x2048x372): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x372): 65.586

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 346.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x373x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x373x2048): 62.309
Elapsed time for attention_prob_times_values (48x2048x2048x373): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x373): 63.413

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 337.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x374x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x374x2048): 63.201
Elapsed time for attention_prob_times_values (48x2048x2048x374): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x374): 65.481

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 346.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x375x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x375x2048): 62.948
Elapsed time for attention_prob_times_values (48x2048x2048x375): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x375): 63.319

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 340.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x376x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x376x2048): 64.501
Elapsed time for attention_prob_times_values (48x2048x2048x376): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x376): 80.188

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 386.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x377x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x377x2048): 62.372
Elapsed time for attention_prob_times_values (48x2048x2048x377): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x377): 63.479

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 340.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x378x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x378x2048): 62.907
Elapsed time for attention_prob_times_values (48x2048x2048x378): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x378): 66.223

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 350.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1226x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1226x2048): 84.389
Elapsed time for attention_prob_times_values (96x2048x2048x1226): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1226): 87.553

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 2555.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1227x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1227x2048): 83.631
Elapsed time for attention_prob_times_values (96x2048x2048x1227): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1227): 83.391

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2485.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1228x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1228x2048): 84.953
Elapsed time for attention_prob_times_values (96x2048x2048x1228): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1228): 88.010

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 2574.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1229x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1229x2048): 82.557
Elapsed time for attention_prob_times_values (96x2048x2048x1229): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1229): 85.555

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2504.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1230x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1230x2048): 86.107
Elapsed time for attention_prob_times_values (96x2048x2048x1230): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1230): 84.934

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 2550.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1231x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1231x2048): 84.993
Elapsed time for attention_prob_times_values (96x2048x2048x1231): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1231): 78.166

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2431.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1232x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1232x2048): 85.081
Elapsed time for attention_prob_times_values (96x2048x2048x1232): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1232): 94.118

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2669.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1233x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1233x2048): 80.618
Elapsed time for attention_prob_times_values (96x2048x2048x1233): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1233): 84.212

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2462.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1234x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1234x2048): 82.800
Elapsed time for attention_prob_times_values (96x2048x2048x1234): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1234): 85.536

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 2517.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 12, hidden_size: 4548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x379x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x379x2048): 62.379
Elapsed time for attention_prob_times_values (48x2048x2048x379): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x379): 63.784

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 343.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x380x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x380x2048): 62.869
Elapsed time for attention_prob_times_values (48x2048x2048x380): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x380): 66.250

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 351.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x381x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x381x2048): 62.266
Elapsed time for attention_prob_times_values (48x2048x2048x381): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x381): 64.166

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 345.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x382x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x382x2048): 63.293
Elapsed time for attention_prob_times_values (48x2048x2048x382): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x382): 66.799

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 355.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x383x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x383x2048): 62.291
Elapsed time for attention_prob_times_values (48x2048x2048x383): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x383): 63.948

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 346.357
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x384x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x384x2048): 76.557
Elapsed time for attention_prob_times_values (48x2048x2048x384): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x384): 80.873

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 432.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x385x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x385x2048): 64.409
Elapsed time for attention_prob_times_values (48x2048x2048x385): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x385): 56.767

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 332.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x386x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x386x2048): 65.432
Elapsed time for attention_prob_times_values (48x2048x2048x386): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x386): 59.978

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 345.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x387x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x387x2048): 64.731
Elapsed time for attention_prob_times_values (48x2048x2048x387): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x387): 57.302

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 336.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x388x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x388x2048): 65.877
Elapsed time for attention_prob_times_values (48x2048x2048x388): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x388): 60.489

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 349.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x389x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x389x2048): 64.949
Elapsed time for attention_prob_times_values (48x2048x2048x389): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x389): 57.664

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 339.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x390x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x390x2048): 62.592
Elapsed time for attention_prob_times_values (48x2048x2048x390): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x390): 55.154

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 326.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x391x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x391x2048): 61.737
Elapsed time for attention_prob_times_values (48x2048x2048x391): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x391): 57.965

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 333.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x392x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x392x2048): 65.946
Elapsed time for attention_prob_times_values (48x2048x2048x392): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x392): 71.578

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 383.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x393x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x393x2048): 63.844
Elapsed time for attention_prob_times_values (48x2048x2048x393): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x393): 53.336

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 325.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x394x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x394x2048): 64.378
Elapsed time for attention_prob_times_values (48x2048x2048x394): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x394): 61.136

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 352.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x395x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x395x2048): 63.709
Elapsed time for attention_prob_times_values (48x2048x2048x395): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x395): 58.328

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 342.799
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x396x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x396x2048): 65.019
Elapsed time for attention_prob_times_values (48x2048x2048x396): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x396): 61.742

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 357.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x397x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x397x2048): 63.867
Elapsed time for attention_prob_times_values (48x2048x2048x397): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x397): 58.480

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 345.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x398x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x398x2048): 65.069
Elapsed time for attention_prob_times_values (48x2048x2048x398): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x398): 61.649

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 358.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x399x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x399x2048): 64.366
Elapsed time for attention_prob_times_values (48x2048x2048x399): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x399): 58.616

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 348.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x400x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x400x2048): 66.022
Elapsed time for attention_prob_times_values (48x2048x2048x400): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x400): 72.623

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 393.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x401x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x401x2048): 63.286
Elapsed time for attention_prob_times_values (48x2048x2048x401): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x401): 59.074

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 348.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x402x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x402x2048): 64.395
Elapsed time for attention_prob_times_values (48x2048x2048x402): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x402): 62.149

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 361.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x403x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x403x2048): 64.021
Elapsed time for attention_prob_times_values (48x2048x2048x403): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x403): 59.317

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 352.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x404x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x404x2048): 65.024
Elapsed time for attention_prob_times_values (48x2048x2048x404): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x404): 62.665

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 365.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x405x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x405x2048): 64.302
Elapsed time for attention_prob_times_values (48x2048x2048x405): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x405): 59.379

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 354.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x406x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x406x2048): 63.867
Elapsed time for attention_prob_times_values (48x2048x2048x406): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x406): 62.204

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 362.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x204x2048): 66.992
Elapsed time for attention_prob_times_values (320x2048x2048x204): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x204): 53.316

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1005.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x205x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x205x2048): 67.662
Elapsed time for attention_prob_times_values (320x2048x2048x205): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x205): 52.341

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1004.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x206x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x206x2048): 69.168
Elapsed time for attention_prob_times_values (320x2048x2048x206): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x206): 54.788

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1045.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x207x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x207x2048): 66.776
Elapsed time for attention_prob_times_values (320x2048x2048x207): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x207): 52.630

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1010.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x208x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x208x2048): 69.607
Elapsed time for attention_prob_times_values (320x2048x2048x208): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x208): 53.108

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1039.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x209x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x209x2048): 67.016
Elapsed time for attention_prob_times_values (320x2048x2048x209): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x209): 51.441

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1008.579
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x210x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x210x2048): 67.968
Elapsed time for attention_prob_times_values (320x2048x2048x210): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x210): 54.879

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1057.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x211x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x211x2048): 67.223
Elapsed time for attention_prob_times_values (320x2048x2048x211): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x211): 53.007

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1036.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x212x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x212x2048): 66.627
Elapsed time for attention_prob_times_values (320x2048x2048x212): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x212): 56.324

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1072.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x213x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x213x2048): 67.138
Elapsed time for attention_prob_times_values (320x2048x2048x213): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x213): 53.460

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1050.029
MLP duration (in seconds): 0.0000
Elapsed time for attention_key_query_prob (48x2048x407x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x407x2048): 64.590
Elapsed time for attention_prob_times_values (48x2048x2048x407): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x407): 59.807

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 358.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x408x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x408x2048): 65.806
Elapsed time for attention_prob_times_values (48x2048x2048x408): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x408): 74.412

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 403.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x409x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x409x2048): 63.969
Elapsed time for attention_prob_times_values (48x2048x2048x409): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x409): 60.023

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 358.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x410x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x410x2048): 64.743
Elapsed time for attention_prob_times_values (48x2048x2048x410): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x410): 63.000

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 370.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x411x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x411x2048): 64.282
Elapsed time for attention_prob_times_values (48x2048x2048x411): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x411): 60.279

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 361.876
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x412x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x412x2048): 65.029
Elapsed time for attention_prob_times_values (48x2048x2048x412): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x412): 63.561

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 374.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x413x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x413x2048): 64.333
Elapsed time for attention_prob_times_values (48x2048x2048x413): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x413): 54.335

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 344.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x414x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x414x2048): 64.901
Elapsed time for attention_prob_times_values (48x2048x2048x414): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x414): 63.608

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 375.954
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x415x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x415x2048): 64.347
Elapsed time for attention_prob_times_values (48x2048x2048x415): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x415): 60.776

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 366.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x416x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x416x2048): 81.741
Elapsed time for attention_prob_times_values (48x2048x2048x416): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x416): 75.020

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 459.638
num_attention_heads: 24, hidden_size: 29640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1235x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1235x2048): 83.441
Elapsed time for attention_prob_times_values (96x2048x2048x1235): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1235): 80.652

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2456.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1236x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1236x2048): 84.241
Elapsed time for attention_prob_times_values (96x2048x2048x1236): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1236): 85.545

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2543.989
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1237x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1237x2048): 81.717
Elapsed time for attention_prob_times_values (96x2048x2048x1237): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1237): 73.672

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2323.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1238x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1238x2048): 85.660
Elapsed time for attention_prob_times_values (96x2048x2048x1238): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1238): 79.340

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2472.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1239x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1239x2048): 84.945
Elapsed time for attention_prob_times_values (96x2048x2048x1239): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1239): 73.379

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2365.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1240x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1240x2048): 87.020
Elapsed time for attention_prob_times_values (96x2048x2048x1240): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1240): 94.196

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 2719.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1241x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1241x2048): 80.132
Elapsed time for attention_prob_times_values (96x2048x2048x1241): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1241): 75.752

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2343.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1242x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1242x2048): 85.187
Elapsed time for attention_prob_times_values (96x2048x2048x1242): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1242): 81.657

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2510.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1243x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1243x2048): 84.390
Elapsed time for attention_prob_times_values (96x2048x2048x1243): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1243): 70.815

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2320.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1244x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1244x2048): 86.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x417x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x417x2048): 66.170
Elapsed time for attention_prob_times_values (48x2048x2048x417): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x417): 61.357

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 374.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x418x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x418x2048): 68.273
Elapsed time for attention_prob_times_values (48x2048x2048x418): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x418): 62.077

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 383.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x419x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x419x2048): 63.046
Elapsed time for attention_prob_times_values (48x2048x2048x419): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x419): 61.565

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 368.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x420x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x420x2048): 69.067
Elapsed time for attention_prob_times_values (48x2048x2048x420): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x420): 58.997

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 376.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x421x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x421x2048): 67.222
Elapsed time for attention_prob_times_values (48x2048x2048x421): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x421): 61.913

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 382.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x422x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x422x2048): 67.921
Elapsed time for attention_prob_times_values (48x2048x2048x422): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x422): 60.836

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 381.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x423x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x423x2048): 66.859
Elapsed time for attention_prob_times_values (48x2048x2048x423): 0.0027
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x423): 62.229

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 383.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x424x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x424x2048): 68.919
Elapsed time for attention_prob_times_values (48x2048x2048x424): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x424): 72.794

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 422.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x425x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x425x2048): 62.365
Elapsed time for attention_prob_times_values (48x2048x2048x425): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x425): 62.182

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 372.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x426x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x426x2048): 63.775
Elapsed time for attention_prob_times_values (48x2048x2048x426): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x426): 65.150

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 386.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x427x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x427x2048): 61.741
Elapsed time for attention_prob_times_values (48x2048x2048x427): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x427): 62.377

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 372.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x428x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x428x2048): 67.865
Elapsed time for attention_prob_times_values (48x2048x2048x428): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x428): 61.486

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 388.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x429x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x429x2048): 66.628
Elapsed time for attention_prob_times_values (48x2048x2048x429): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x429): 57.929

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 373.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x430x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x430x2048): 67.254
Elapsed time for attention_prob_times_values (48x2048x2048x430): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x430): 61.600

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 388.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x431x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x431x2048): 66.669
Elapsed time for attention_prob_times_values (48x2048x2048x431): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x431): 56.632

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 370.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x432x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x432x2048): 68.307
Elapsed time for attention_prob_times_values (48x2048x2048x432): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x432): 77.385

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 439.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x433x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x433x2048): 65.993
Elapsed time for attention_prob_times_values (48x2048x2048x433): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x433): 63.319

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 392.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x434x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x434x2048): 66.663
Elapsed time for attention_prob_times_values (48x2048x2048x434): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x434): 66.307

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 404.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x435x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x435x2048): 66.292
Elapsed time for attention_prob_times_values (48x2048x2048x435): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x435): 63.365

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 395.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x436x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x436x2048): 67.722
Elapsed time for attention_prob_times_values (48x2048x2048x436): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x436): 66.843

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 411.035
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x437x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x437x2048): 66.543
Elapsed time for attention_prob_times_values (48x2048x2048x437): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x437): 63.871

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 398.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x438x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x438x2048): 67.204
Elapsed time for attention_prob_times_values (48x2048x2048x438): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x438): 66.924

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 411.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x439x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x439x2048): 66.685
Elapsed time for attention_prob_times_values (48x2048x2048x439): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x439): 57.598

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 379.790
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x440x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x440x2048): 68.519
Elapsed time for attention_prob_times_values (48x2048x2048x440): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x440): 79.061

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 451.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x441x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x441x2048): 65.998
Elapsed time for attention_prob_times_values (48x2048x2048x441): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x441): 59.468

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 385.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x442x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x442x2048): 66.566
Elapsed time for attention_prob_times_values (48x2048x2048x442): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x442): 67.365

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 413.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x443x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x443x2048): 66.174
Elapsed time for attention_prob_times_values (48x2048x2048x443): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x443): 64.748

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 405.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x444x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x444x2048): 67.059
Elapsed time for attention_prob_times_values (48x2048x2048x444): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x444): 67.554

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 417.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (96x2048x2048x1244): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1244): 79.140

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2486.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1245x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1245x2048): 84.314
Elapsed time for attention_prob_times_values (96x2048x2048x1245): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1245): 76.243

Attention duration (in seconds): 0.0250
Attention throughput (in TFLOP/s): 2416.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1246x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1246x2048): 85.637
Elapsed time for attention_prob_times_values (96x2048x2048x1246): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1246): 81.800

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2527.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1247x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1247x2048): 84.588
Elapsed time for attention_prob_times_values (96x2048x2048x1247): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1247): 72.988

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2368.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1248x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1248x2048): 98.162
Elapsed time for attention_prob_times_values (96x2048x2048x1248): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1248): 93.304

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 2894.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 29976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1249x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1249x2048): 86.282
Elapsed time for attention_prob_times_values (96x2048x2048x1249): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1249): 76.551

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2455.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1250x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1250x2048): 87.310
Elapsed time for attention_prob_times_values (96x2048x2048x1250): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1250): 82.173

Attention duration (in seconds): 0.0238
Attention throughput (in TFLOP/s): 2565.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1251x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1251x2048): 83.708
Elapsed time for attention_prob_times_values (96x2048x2048x1251): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1251): 76.157

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2418.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1252x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1252x2048): 87.402
Elapsed time for attention_prob_times_values (96x2048x2048x1252): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1252): 79.846

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2532.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1253x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1253x2048): 84.347
Elapsed time for attention_prob_times_values (96x2048x2048x1253): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1253): 71.747

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2354.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
========================================================================================================================
num_attention_heads: 12, hidden_size: 5340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x445x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x445x2048): 66.367
Elapsed time for attention_prob_times_values (48x2048x2048x445): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x445): 64.973

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 408.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x446x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x446x2048): 67.097
Elapsed time for attention_prob_times_values (48x2048x2048x446): 0.0026
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x446): 67.858

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 420.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x447x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x447x2048): 60.374
Elapsed time for attention_prob_times_values (48x2048x2048x447): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x447): 65.169

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 391.014
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x448x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x448x2048): 82.320
Elapsed time for attention_prob_times_values (48x2048x2048x448): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x448): 80.072

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 507.380
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x449x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x449x2048): 68.692
Elapsed time for attention_prob_times_values (48x2048x2048x449): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x449): 59.586

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 399.598
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x450x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x450x2048): 69.538
Elapsed time for attention_prob_times_values (48x2048x2048x450): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x450): 62.151

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 411.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x451x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x451x2048): 68.774
Elapsed time for attention_prob_times_values (48x2048x2048x451): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x451): 59.664

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 401.596
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x452x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x452x2048): 70.557
Elapsed time for attention_prob_times_values (48x2048x2048x452): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x452): 58.152

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 401.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x453x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x453x2048): 68.651
Elapsed time for attention_prob_times_values (48x2048x2048x453): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x453): 59.537

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 402.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x454x2048): 0.0026
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x214x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x214x2048): 69.901
Elapsed time for attention_prob_times_values (320x2048x2048x214): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x214): 56.494

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1107.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x215x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x215x2048): 66.032
Elapsed time for attention_prob_times_values (320x2048x2048x215): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x215): 53.204

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1048.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x216x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x216x2048): 70.446
Elapsed time for attention_prob_times_values (320x2048x2048x216): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x216): 53.118

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1082.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x217x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x217x2048): 65.632
Elapsed time for attention_prob_times_values (320x2048x2048x217): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x217): 51.439

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1035.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x218x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x218x2048): 69.806
Elapsed time for attention_prob_times_values (320x2048x2048x218): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x218): 57.500

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1137.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x219x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x219x2048): 67.011
Elapsed time for attention_prob_times_values (320x2048x2048x219): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x219): 54.997

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1094.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x220x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x220x2048): 66.375
Elapsed time for attention_prob_times_values (320x2048x2048x220): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x220): 58.058

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1126.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x221x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x221x2048): 68.696
Elapsed time for attention_prob_times_values (320x2048x2048x221): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x221): 54.256

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1107.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x222x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x222x2048): 67.978
Elapsed time for attention_prob_times_values (320x2048x2048x222): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x222): 55.458

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1120.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer duration (in seconds): 0.0347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x103x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x103x2048): 52.942
Elapsed time for attention_prob_times_values (1024x2048x2048x103): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x103): 49.470

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 1368.181
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x104x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x104x2048): 55.328
Elapsed time for attention_prob_times_values (1024x2048x2048x104): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x104): 50.161

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 1420.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x105x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x105x2048): 51.633
Elapsed time for attention_prob_times_values (1024x2048x2048x105): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x105): 49.649

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 1379.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0356
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x106x2048): 0.0171
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x106x2048): 53.218
Elapsed time for attention_prob_times_values (1024x2048x2048x106): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x106): 52.152

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 1448.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x107x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x107x2048): 52.437
Elapsed time for attention_prob_times_values (1024x2048x2048x107): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x107): 49.825

Attention duration (in seconds): 0.0360
Attention throughput (in TFLOP/s): 1417.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x108x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x108x2048): 55.555
Elapsed time for attention_prob_times_values (1024x2048x2048x108): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x108): 53.034

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 1519.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x109x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x109x2048): 53.669
Elapsed time for attention_prob_times_values (1024x2048x2048x109): 0.0191
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x109): 48.987

Attention duration (in seconds): 0.0366
Attention throughput (in TFLOP/s): 1447.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0366
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x110x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x110x2048): 55.554
Elapsed time for attention_prob_times_values (1024x2048x2048x110): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x110): 52.045

Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 1531.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x111x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x111x2048): 54.731
Elapsed time for attention_prob_times_values (1024x2048x2048x111): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x111): 51.868

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1531.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x454x2048): 69.344
Elapsed time for attention_prob_times_values (48x2048x2048x454): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x454): 62.428

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 415.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x455x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x455x2048): 68.390
Elapsed time for attention_prob_times_values (48x2048x2048x455): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x455): 59.968

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 404.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x456x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x456x2048): 70.517
Elapsed time for attention_prob_times_values (48x2048x2048x456): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x456): 79.937

Attention duration (in seconds): 0.0049
Attention throughput (in TFLOP/s): 475.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x457x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x457x2048): 67.148
Elapsed time for attention_prob_times_values (48x2048x2048x457): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x457): 59.807

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 402.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x458x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x458x2048): 67.731
Elapsed time for attention_prob_times_values (48x2048x2048x458): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x458): 61.345

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 409.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x459x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x459x2048): 67.713
Elapsed time for attention_prob_times_values (48x2048x2048x459): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x459): 60.148

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 406.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x460x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x460x2048): 69.277
Elapsed time for attention_prob_times_values (48x2048x2048x460): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x460): 63.423

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 423.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x461x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x461x2048): 67.777
Elapsed time for attention_prob_times_values (48x2048x2048x461): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x461): 60.405

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 408.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x462x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x462x2048): 68.527
Elapsed time for attention_prob_times_values (48x2048x2048x462): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x462): 63.368

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 422.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x463x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x463x2048): 67.812
Elapsed time for attention_prob_times_values (48x2048x2048x463): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x463): 60.804

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 412.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x464x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x464x2048): 69.225
Elapsed time for attention_prob_times_values (48x2048x2048x464): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x464): 81.509

Attention duration (in seconds): 0.0050
Attention throughput (in TFLOP/s): 481.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x465x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x465x2048): 67.121
Elapsed time for attention_prob_times_values (48x2048x2048x465): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x465): 61.000

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 412.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x466x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x466x2048): 67.962
Elapsed time for attention_prob_times_values (48x2048x2048x466): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x466): 64.000

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 425.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x467x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x467x2048): 67.318
Elapsed time for attention_prob_times_values (48x2048x2048x467): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x467): 57.979

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 403.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x468x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x468x2048): 68.825
Elapsed time for attention_prob_times_values (48x2048x2048x468): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x468): 61.932

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 422.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x469x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x469x2048): 67.221
Elapsed time for attention_prob_times_values (48x2048x2048x469): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x469): 61.225

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 416.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x470x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x470x2048): 65.164
Elapsed time for attention_prob_times_values (48x2048x2048x470): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x470): 64.324

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 421.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x471x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x471x2048): 67.767
Elapsed time for attention_prob_times_values (48x2048x2048x471): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x471): 59.415

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 412.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x472x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x472x2048): 66.400
Elapsed time for attention_prob_times_values (48x2048x2048x472): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x472): 82.828

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 481.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x473x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x473x2048): 67.134
Elapsed time for attention_prob_times_values (48x2048x2048x473): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x473): 58.223

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 408.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x474x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x474x2048): 67.662
Elapsed time for attention_prob_times_values (48x2048x2048x474): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x474): 64.662

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 433.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x475x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x475x2048): 67.243
Elapsed time for attention_prob_times_values (48x2048x2048x475): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x475): 61.870

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 423.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x476x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x476x2048): 68.674
Elapsed time for attention_prob_times_values (48x2048x2048x476): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x476): 61.876

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 428.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x477x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x477x2048): 63.874
Elapsed time for attention_prob_times_values (48x2048x2048x477): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x477): 62.250

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 415.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x478x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x478x2048): 64.639
Elapsed time for attention_prob_times_values (48x2048x2048x478): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x478): 65.076

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 428.155
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x479x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x479x2048): 67.509
Elapsed time for attention_prob_times_values (48x2048x2048x479): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x479): 61.396

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 425.285
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x480x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x480x2048): 83.866
Elapsed time for attention_prob_times_values (48x2048x2048x480): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x480): 85.045

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 559.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x481x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x481x2048): 69.537
Elapsed time for attention_prob_times_values (48x2048x2048x481): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x481): 62.603

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 437.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x482x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x482x2048): 70.485
Elapsed time for attention_prob_times_values (48x2048x2048x482): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x482): 65.305

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 450.737
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1254x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1254x2048): 86.601
Elapsed time for attention_prob_times_values (96x2048x2048x1254): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1254): 81.761

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2556.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1255x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1255x2048): 82.306
Elapsed time for attention_prob_times_values (96x2048x2048x1255): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1255): 74.955

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2386.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1256x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1256x2048): 87.638
Elapsed time for attention_prob_times_values (96x2048x2048x1256): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1256): 94.906

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 2773.690
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1257x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1257x2048): 82.802
Elapsed time for attention_prob_times_values (96x2048x2048x1257): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1257): 73.209

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2367.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1258x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1258x2048): 83.562
Elapsed time for attention_prob_times_values (96x2048x2048x1258): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1258): 82.488

Attention duration (in seconds): 0.0244
Attention throughput (in TFLOP/s): 2530.872
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1259x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1259x2048): 84.930
Elapsed time for attention_prob_times_values (96x2048x2048x1259): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1259): 76.545

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 2456.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1260x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1260x2048): 86.664
Elapsed time for attention_prob_times_values (96x2048x2048x1260): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1260): 81.750

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2568.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1261x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1261x2048): 84.990
Elapsed time for attention_prob_times_values (96x2048x2048x1261): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1261): 74.723

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2429.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1262x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1262x2048): 86.332
Elapsed time for attention_prob_times_values (96x2048x2048x1262): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1262): 82.667

Attention duration (in seconds): 0.0241
Attention throughput (in TFLOP/s): 2582.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0241
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x483x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x483x2048): 69.216
Elapsed time for attention_prob_times_values (48x2048x2048x483): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x483): 63.004

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 439.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x484x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x484x2048): 70.924
Elapsed time for attention_prob_times_values (48x2048x2048x484): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x484): 64.276

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 449.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x485x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x485x2048): 69.093
Elapsed time for attention_prob_times_values (48x2048x2048x485): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x485): 62.560

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 438.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x486x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x486x2048): 64.139
Elapsed time for attention_prob_times_values (48x2048x2048x486): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x486): 65.641

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 434.401
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x487x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x487x2048): 68.391
Elapsed time for attention_prob_times_values (48x2048x2048x487): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x487): 63.345

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 441.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x488x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x488x2048): 66.739
Elapsed time for attention_prob_times_values (48x2048x2048x488): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x488): 85.352

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 503.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x489x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x489x2048): 67.988
Elapsed time for attention_prob_times_values (48x2048x2048x489): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x489): 60.809

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 432.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x490x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x490x2048): 68.425
Elapsed time for attention_prob_times_values (48x2048x2048x490): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x490): 66.065

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 453.242
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x491x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x491x2048): 68.367
Elapsed time for attention_prob_times_values (48x2048x2048x491): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x491): 63.411

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 444.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x223x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x223x2048): 69.875
Elapsed time for attention_prob_times_values (320x2048x2048x223): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x223): 55.975

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1145.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x224x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x224x2048): 79.892
Elapsed time for attention_prob_times_values (320x2048x2048x224): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x224): 56.979

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1230.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x225x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x225x2048): 66.697
Elapsed time for attention_prob_times_values (320x2048x2048x225): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x225): 55.723

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1128.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x226x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x226x2048): 68.634
Elapsed time for attention_prob_times_values (320x2048x2048x226): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x226): 59.159

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1185.512
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x227x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x227x2048): 67.009
Elapsed time for attention_prob_times_values (320x2048x2048x227): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x227): 56.063

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1143.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x228x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x228x2048): 69.361
Elapsed time for attention_prob_times_values (320x2048x2048x228): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x228): 59.718

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1207.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x229x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x229x2048): 68.968
Elapsed time for attention_prob_times_values (320x2048x2048x229): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x229): 55.276

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1159.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x230x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x230x2048): 70.289
Elapsed time for attention_prob_times_values (320x2048x2048x230): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x230): 59.865

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1226.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x231x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x231x2048): 67.716
Elapsed time for attention_prob_times_values (320x2048x2048x231): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x231): 57.804

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1187.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x232x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x232x2048): 71.735
num_attention_heads: 12, hidden_size: 5904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x492x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x492x2048): 67.852
Elapsed time for attention_prob_times_values (48x2048x2048x492): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x492): 65.195

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 449.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x493x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x493x2048): 65.144
Elapsed time for attention_prob_times_values (48x2048x2048x493): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x493): 63.587

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 436.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x494x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x494x2048): 69.391
Elapsed time for attention_prob_times_values (48x2048x2048x494): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x494): 66.462

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 460.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x495x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x495x2048): 68.160
Elapsed time for attention_prob_times_values (48x2048x2048x495): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x495): 63.962

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 448.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x496x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x496x2048): 70.737
Elapsed time for attention_prob_times_values (48x2048x2048x496): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x496): 87.112

Attention duration (in seconds): 0.0051
Attention throughput (in TFLOP/s): 531.888
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0051
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x497x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x497x2048): 62.935
Elapsed time for attention_prob_times_values (48x2048x2048x497): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x497): 64.269

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 433.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x498x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x498x2048): 68.865
Elapsed time for attention_prob_times_values (48x2048x2048x498): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x498): 66.607

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 462.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 5988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x499x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x499x2048): 67.941
Elapsed time for attention_prob_times_values (48x2048x2048x499): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x499): 64.262

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 452.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x500x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x500x2048): 64.288
Elapsed time for attention_prob_times_values (48x2048x2048x500): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x500): 67.231

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 450.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x501x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x501x2048): 64.495
Elapsed time for attention_prob_times_values (48x2048x2048x501): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x501): 64.114

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 441.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x502x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x502x2048): 69.017
Elapsed time for attention_prob_times_values (48x2048x2048x502): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x502): 66.993

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 467.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x503x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x503x2048): 68.447
Elapsed time for attention_prob_times_values (48x2048x2048x503): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x503): 64.684

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 458.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x504x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x504x2048): 65.546
Elapsed time for attention_prob_times_values (48x2048x2048x504): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x504): 88.324

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 519.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x505x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x505x2048): 67.860
Elapsed time for attention_prob_times_values (48x2048x2048x505): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x505): 58.328

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 433.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x506x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x506x2048): 68.507
Elapsed time for attention_prob_times_values (48x2048x2048x506): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x506): 65.908

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 465.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x507x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x507x2048): 67.289
Elapsed time for attention_prob_times_values (48x2048x2048x507): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x507): 60.740

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 443.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x508x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x508x2048): 69.261
Elapsed time for attention_prob_times_values (48x2048x2048x508): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x508): 62.632

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 457.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x509x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x509x2048): 62.737
Elapsed time for attention_prob_times_values (48x2048x2048x509): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x509): 60.490

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 428.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x510x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x510x2048): 67.900
Elapsed time for attention_prob_times_values (48x2048x2048x510): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x510): 66.872

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 470.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x511x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x511x2048): 67.857
Elapsed time for attention_prob_times_values (48x2048x2048x511): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x511): 63.191

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 457.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x512x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x512x2048): 80.971
Elapsed time for attention_prob_times_values (48x2048x2048x512): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x512): 92.086

Attention duration (in seconds): 0.0048
Attention throughput (in TFLOP/s): 603.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0048
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x513x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x513x2048): 70.056
Elapsed time for attention_prob_times_values (48x2048x2048x513): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x513): 56.611

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 439.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x514x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x514x2048): 73.834
Elapsed time for attention_prob_times_values (48x2048x2048x514): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x514): 62.758

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 476.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x515x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x515x2048): 69.804
Elapsed time for attention_prob_times_values (48x2048x2048x515): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x515): 59.536

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 452.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x516x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x516x2048): 71.465
Elapsed time for attention_prob_times_values (48x2048x2048x516): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x516): 62.998

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 471.893
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x517x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x517x2048): 70.114
Elapsed time for attention_prob_times_values (48x2048x2048x517): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x517): 60.258

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 457.492
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x518x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x518x2048): 70.699
Elapsed time for attention_prob_times_values (48x2048x2048x518): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x518): 63.119

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 471.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x519x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x519x2048): 63.714
Elapsed time for attention_prob_times_values (48x2048x2048x519): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x519): 60.646

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 440.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x520x2048): 0.0029
--------
Elapsed time for attention_key_query_prob (96x2048x1263x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1263x2048): 84.339
Elapsed time for attention_prob_times_values (96x2048x2048x1263): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1263): 73.137

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2397.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1264x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1264x2048): 85.704
Elapsed time for attention_prob_times_values (96x2048x2048x1264): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1264): 95.142

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 2761.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1265x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1265x2048): 84.279
Elapsed time for attention_prob_times_values (96x2048x2048x1265): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1265): 75.159

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2435.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1266x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1266x2048): 84.334
Elapsed time for attention_prob_times_values (96x2048x2048x1266): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1266): 81.243

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2538.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1267x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1267x2048): 82.263
Elapsed time for attention_prob_times_values (96x2048x2048x1267): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1267): 76.112

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2427.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1268x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1268x2048): 85.940
Elapsed time for attention_prob_times_values (96x2048x2048x1268): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1268): 83.072

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2595.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1269x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1269x2048): 84.413
Elapsed time for attention_prob_times_values (96x2048x2048x1269): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1269): 75.915

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2457.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1270x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1270x2048): 85.849
Elapsed time for attention_prob_times_values (96x2048x2048x1270): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1270): 82.661

Attention duration (in seconds): 0.0243
Attention throughput (in TFLOP/s): 2591.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0243
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1271x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1271x2048): 82.758
Elapsed time for attention_prob_times_values (96x2048x2048x1271): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1271): 75.931

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2438.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1272x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1272x2048): 86.818
Elapsed time for attention_prob_times_values (96x2048x2048x1272): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1272): 96.261

Attention duration (in seconds): 0.0224
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x520x2048): 71.219
Elapsed time for attention_prob_times_values (48x2048x2048x520): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x520): 73.786

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 514.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x521x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x521x2048): 69.081
Elapsed time for attention_prob_times_values (48x2048x2048x521): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x521): 59.718

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 455.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x522x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x522x2048): 69.795
Elapsed time for attention_prob_times_values (48x2048x2048x522): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x522): 63.487

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 473.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x523x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x523x2048): 69.379
Elapsed time for attention_prob_times_values (48x2048x2048x523): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x523): 59.789

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 457.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x524x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x524x2048): 70.285
Elapsed time for attention_prob_times_values (48x2048x2048x524): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x524): 64.123

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 478.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x525x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x525x2048): 68.812
Elapsed time for attention_prob_times_values (48x2048x2048x525): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x525): 60.238

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 459.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x526x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x526x2048): 70.302
Elapsed time for attention_prob_times_values (48x2048x2048x526): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x526): 64.200

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 480.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x527x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x527x2048): 69.182
Elapsed time for attention_prob_times_values (48x2048x2048x527): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x527): 60.653

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 463.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x528x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x528x2048): 68.054
Elapsed time for attention_prob_times_values (48x2048x2048x528): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x528): 75.206

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 513.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x529x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x529x2048): 69.010
Elapsed time for attention_prob_times_values (48x2048x2048x529): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x529): 60.788

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 465.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x530x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x530x2048): 69.695
Elapsed time for attention_prob_times_values (48x2048x2048x530): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x530): 64.349

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 482.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x531x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x531x2048): 69.260
Elapsed time for attention_prob_times_values (48x2048x2048x531): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x531): 60.565

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 466.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x532x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x532x2048): 70.198
Elapsed time for attention_prob_times_values (48x2048x2048x532): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x532): 64.825

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 487.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x533x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x533x2048): 69.478
Elapsed time for attention_prob_times_values (48x2048x2048x533): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x533): 60.858

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 470.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x534x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x534x2048): 70.112
Elapsed time for attention_prob_times_values (48x2048x2048x534): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x534): 64.900

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 489.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x535x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x535x2048): 69.733
Elapsed time for attention_prob_times_values (48x2048x2048x535): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x535): 61.062

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 473.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x536x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x536x2048): 63.130
Elapsed time for attention_prob_times_values (48x2048x2048x536): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x536): 69.329

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 481.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x537x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x537x2048): 68.914
Elapsed time for attention_prob_times_values (48x2048x2048x537): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x537): 59.233

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 464.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x538x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x538x2048): 69.185
Elapsed time for attention_prob_times_values (48x2048x2048x538): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x538): 64.536

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 487.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_prob_times_values (320x2048x2048x232): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x232): 58.606

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1233.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x233x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x233x2048): 67.471
Elapsed time for attention_prob_times_values (320x2048x2048x233): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x233): 57.940

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1197.188
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x234x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x234x2048): 70.053
Elapsed time for attention_prob_times_values (320x2048x2048x234): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x234): 56.568

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1206.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x235x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x235x2048): 68.679
Elapsed time for attention_prob_times_values (320x2048x2048x235): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x235): 57.080

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1206.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x236x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x236x2048): 68.107
Elapsed time for attention_prob_times_values (320x2048x2048x236): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x236): 59.768

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1237.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x237x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x237x2048): 69.404
Elapsed time for attention_prob_times_values (320x2048x2048x237): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x237): 58.827

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1242.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x238x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x238x2048): 70.836
Elapsed time for attention_prob_times_values (320x2048x2048x238): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x238): 61.429

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1289.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x239x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x239x2048): 66.491
Elapsed time for attention_prob_times_values (320x2048x2048x239): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x239): 56.276

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1199.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x240x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x240x2048): 54.104
Elapsed time for attention_prob_times_values (320x2048x2048x240): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x240): 59.348

Attention duration (in seconds): 0.0228
Attention throughput (in TFLOP/s): 1117.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0228
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x241x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x241x2048): 52.433
Elapsed time for attention_prob_times_values (320x2048x2048x241): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x241): 59.619

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1106.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Elapsed time for attention_key_query_prob (48x2048x539x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x539x2048): 69.171
Elapsed time for attention_prob_times_values (48x2048x2048x539): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x539): 57.244

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 458.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x540x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x540x2048): 67.514
Elapsed time for attention_prob_times_values (48x2048x2048x540): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x540): 65.207

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 486.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x541x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x541x2048): 69.280
Elapsed time for attention_prob_times_values (48x2048x2048x541): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x541): 61.394

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 477.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x542x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x542x2048): 69.869
Elapsed time for attention_prob_times_values (48x2048x2048x542): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x542): 64.621

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 493.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x543x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x543x2048): 69.394
Elapsed time for attention_prob_times_values (48x2048x2048x543): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x543): 61.741

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 481.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x544x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x544x2048): 85.368
Elapsed time for attention_prob_times_values (48x2048x2048x544): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x544): 73.434

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 582.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x545x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x545x2048): 71.998
Elapsed time for attention_prob_times_values (48x2048x2048x545): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x545): 58.130

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 475.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x546x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x546x2048): 72.923
Elapsed time for attention_prob_times_values (48x2048x2048x546): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x546): 64.832

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 507.830
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x547x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x547x2048): 71.886
Elapsed time for attention_prob_times_values (48x2048x2048x547): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x547): 62.419

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 495.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x548x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x548x2048): 73.434
Elapsed time for attention_prob_times_values (48x2048x2048x548): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x548): 60.980

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 494.519
Attention throughput (in TFLOP/s): 2813.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1273x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1273x2048): 84.041
Elapsed time for attention_prob_times_values (96x2048x2048x1273): 0.0138
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1273): 74.243

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2431.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1274x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1274x2048): 84.165
Elapsed time for attention_prob_times_values (96x2048x2048x1274): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1274): 81.403

Attention duration (in seconds): 0.0248
Attention throughput (in TFLOP/s): 2553.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0248
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1275x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1275x2048): 83.936
Elapsed time for attention_prob_times_values (96x2048x2048x1275): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1275): 75.739

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2459.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1276x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1276x2048): 83.988
Elapsed time for attention_prob_times_values (96x2048x2048x1276): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1276): 82.794

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2577.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1277x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1277x2048): 79.740
Elapsed time for attention_prob_times_values (96x2048x2048x1277): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1277): 75.863

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2404.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1278x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1278x2048): 84.401
Elapsed time for attention_prob_times_values (96x2048x2048x1278): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1278): 82.425

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 2581.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1279x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1279x2048): 83.539
Elapsed time for attention_prob_times_values (96x2048x2048x1279): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1279): 75.915

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2464.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1280x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1280x2048): 91.366
Elapsed time for attention_prob_times_values (96x2048x2048x1280): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1280): 97.366

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 2922.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1281x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1281x2048): 82.247
Elapsed time for attention_prob_times_values (96x2048x2048x1281): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1281): 71.279

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2369.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x549x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x549x2048): 19.162
Elapsed time for attention_prob_times_values (48x2048x2048x549): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x549): 62.838

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 218.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x550x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x550x2048): 67.564
Elapsed time for attention_prob_times_values (48x2048x2048x550): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x550): 65.211

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 494.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x551x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x551x2048): 71.499
Elapsed time for attention_prob_times_values (48x2048x2048x551): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x551): 62.978

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 499.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x552x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x552x2048): 73.422
Elapsed time for attention_prob_times_values (48x2048x2048x552): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x552): 78.163

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 565.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x553x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x553x2048): 66.175
Elapsed time for attention_prob_times_values (48x2048x2048x553): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x553): 62.874

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 482.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x554x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x554x2048): 71.373
Elapsed time for attention_prob_times_values (48x2048x2048x554): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x554): 65.566

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 512.061
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x555x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x555x2048): 70.661
Elapsed time for attention_prob_times_values (48x2048x2048x555): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x555): 62.953

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 499.645
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x556x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x556x2048): 72.493
Elapsed time for attention_prob_times_values (48x2048x2048x556): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x556): 66.487

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 521.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x557x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x557x2048): 71.065
Elapsed time for attention_prob_times_values (48x2048x2048x557): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x557): 63.945

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 506.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x558x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x558x2048): 72.075
Elapsed time for attention_prob_times_values (48x2048x2048x558): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x558): 66.731

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 522.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x559x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x559x2048): 70.469
Elapsed time for attention_prob_times_values (48x2048x2048x559): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x559): 64.525

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 508.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x560x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x560x2048): 70.992
Elapsed time for attention_prob_times_values (48x2048x2048x560): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x560): 79.616

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 567.619
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x561x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x561x2048): 70.285
Elapsed time for attention_prob_times_values (48x2048x2048x561): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x561): 64.151

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 508.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x562x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x562x2048): 65.775
Elapsed time for attention_prob_times_values (48x2048x2048x562): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x562): 66.807

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 502.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x563x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x563x2048): 70.504
Elapsed time for attention_prob_times_values (48x2048x2048x563): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x563): 64.050

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 509.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x564x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x564x2048): 72.215
Elapsed time for attention_prob_times_values (48x2048x2048x564): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x564): 68.163

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 533.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x565x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x565x2048): 70.765
Elapsed time for attention_prob_times_values (48x2048x2048x565): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x565): 64.530

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 514.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x566x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x566x2048): 71.693
Elapsed time for attention_prob_times_values (48x2048x2048x566): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x566): 61.271

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 504.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x567x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x567x2048): 70.879
Elapsed time for attention_prob_times_values (48x2048x2048x567): 0.0036
num_attention_heads: 256, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x112x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x112x2048): 57.781
Elapsed time for attention_prob_times_values (1024x2048x2048x112): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x112): 53.817

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 1616.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0345
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x113x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x113x2048): 55.396
Elapsed time for attention_prob_times_values (1024x2048x2048x113): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x113): 52.617

Attention duration (in seconds): 0.0360
Attention throughput (in TFLOP/s): 1578.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x114x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x114x2048): 56.094
Elapsed time for attention_prob_times_values (1024x2048x2048x114): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x114): 53.651

Attention duration (in seconds): 0.0357
Attention throughput (in TFLOP/s): 1617.926
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0357
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x115x2048): 0.0181
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x115x2048): 54.703
Elapsed time for attention_prob_times_values (1024x2048x2048x115): 0.0185
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x115): 53.533

Attention duration (in seconds): 0.0365
Attention throughput (in TFLOP/s): 1609.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x116x2048): 0.0175
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x116x2048): 57.012
Elapsed time for attention_prob_times_values (1024x2048x2048x116): 0.0184
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x116): 54.154

Attention duration (in seconds): 0.0359
Attention throughput (in TFLOP/s): 1666.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x117x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x117x2048): 55.685
Elapsed time for attention_prob_times_values (1024x2048x2048x117): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x117): 51.071

Attention duration (in seconds): 0.0377
Attention throughput (in TFLOP/s): 1611.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0377
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x118x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x118x2048): 56.968
Elapsed time for attention_prob_times_values (1024x2048x2048x118): 0.0181
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x118): 56.026

Attention duration (in seconds): 0.0359
Attention throughput (in TFLOP/s): 1723.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x119x2048): 0.0178
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x119x2048): 57.476
Elapsed time for attention_prob_times_values (1024x2048x2048x119): 0.0194
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x119): 52.801

Attention duration (in seconds): 0.0371
Attention throughput (in TFLOP/s): 1692.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x120x2048): 0.0176
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x120x2048): 58.598
Elapsed time for attention_prob_times_values (1024x2048x2048x120): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x120): 56.758

Attention duration (in seconds): 0.0358
Attention throughput (in TFLOP/s): 1787.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0358
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x121x2048): 0.0183
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x121x2048): 56.766
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x567): 62.916

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 509.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x568x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x568x2048): 72.677
Elapsed time for attention_prob_times_values (48x2048x2048x568): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x568): 80.394

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 584.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x569x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x569x2048): 70.395
Elapsed time for attention_prob_times_values (48x2048x2048x569): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x569): 64.711

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 517.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x570x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x570x2048): 64.846
Elapsed time for attention_prob_times_values (48x2048x2048x570): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x570): 67.663

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 508.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x571x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x571x2048): 70.637
Elapsed time for attention_prob_times_values (48x2048x2048x571): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x571): 62.518

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 510.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x572x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x572x2048): 70.854
Elapsed time for attention_prob_times_values (48x2048x2048x572): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x572): 67.653

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 533.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x573x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x573x2048): 70.755
Elapsed time for attention_prob_times_values (48x2048x2048x573): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x573): 65.214

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 523.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x574x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x574x2048): 71.379
Elapsed time for attention_prob_times_values (48x2048x2048x574): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x574): 67.885

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 537.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x575x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x575x2048): 70.769
Elapsed time for attention_prob_times_values (48x2048x2048x575): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x575): 65.310

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 525.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x576x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x576x2048): 85.594
Elapsed time for attention_prob_times_values (48x2048x2048x576): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x576): 81.760

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 648.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x242x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x242x2048): 53.841
Elapsed time for attention_prob_times_values (320x2048x2048x242): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x242): 61.426

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 1142.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0226
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x243x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x243x2048): 53.097
Elapsed time for attention_prob_times_values (320x2048x2048x243): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x243): 58.215

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1109.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x244x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x244x2048): 54.789
Elapsed time for attention_prob_times_values (320x2048x2048x244): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x244): 62.960

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1175.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x245x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x245x2048): 54.180
Elapsed time for attention_prob_times_values (320x2048x2048x245): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x245): 59.248

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1139.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x246x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x246x2048): 53.037
Elapsed time for attention_prob_times_values (320x2048x2048x246): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x246): 63.050

Attention duration (in seconds): 0.0229
Attention throughput (in TFLOP/s): 1164.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0229
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x247x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x247x2048): 54.465
Elapsed time for attention_prob_times_values (320x2048x2048x247): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x247): 60.808

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1166.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x248x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x248x2048): 53.374
Elapsed time for attention_prob_times_values (320x2048x2048x248): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x248): 59.763

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1148.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x249x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x249x2048): 52.605
Elapsed time for attention_prob_times_values (320x2048x2048x249): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x249): 62.448

Attention duration (in seconds): 0.0234
Attention throughput (in TFLOP/s): 1167.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x250x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x250x2048): 53.331
Elapsed time for attention_prob_times_values (320x2048x2048x250): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x250): 63.990

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1194.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
========================================================================================================================
num_attention_heads: 24, hidden_size: 30768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1282x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1282x2048): 86.362
Elapsed time for attention_prob_times_values (96x2048x2048x1282): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1282): 77.555

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2537.217
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1283x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1283x2048): 82.726
Elapsed time for attention_prob_times_values (96x2048x2048x1283): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1283): 71.288

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2379.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1284x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1284x2048): 87.358
Elapsed time for attention_prob_times_values (96x2048x2048x1284): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1284): 75.817

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2524.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1285x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1285x2048): 84.980
Elapsed time for attention_prob_times_values (96x2048x2048x1285): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1285): 71.702

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2420.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1286x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1286x2048): 86.594
Elapsed time for attention_prob_times_values (96x2048x2048x1286): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1286): 77.682

Attention duration (in seconds): 0.0253
Attention throughput (in TFLOP/s): 2550.292
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1287x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1287x2048): 85.200
Elapsed time for attention_prob_times_values (96x2048x2048x1287): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1287): 70.985

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2413.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1288x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1288x2048): 87.794
Elapsed time for attention_prob_times_values (96x2048x2048x1288): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1288): 86.024

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 2710.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1289x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1289x2048): 82.779
Elapsed time for attention_prob_times_values (96x2048x2048x1289): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1289): 72.161

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2406.565
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1290x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1290x2048): 85.740
Elapsed time for attention_prob_times_values (96x2048x2048x1290): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1290): 76.534

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2526.110
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 30984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1291x2048): 0.0125
========================================================================================================================
num_attention_heads: 12, hidden_size: 6924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x577x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x577x2048): 72.683
Elapsed time for attention_prob_times_values (48x2048x2048x577): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x577): 60.756

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 513.722
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x578x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x578x2048): 73.823
Elapsed time for attention_prob_times_values (48x2048x2048x578): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x578): 63.341

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 530.005
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x579x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x579x2048): 72.670
Elapsed time for attention_prob_times_values (48x2048x2048x579): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x579): 57.338

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 499.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x580x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x580x2048): 74.337
Elapsed time for attention_prob_times_values (48x2048x2048x580): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x580): 63.408

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 533.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x581x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x581x2048): 71.565
Elapsed time for attention_prob_times_values (48x2048x2048x581): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x581): 61.288

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 515.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x582x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x582x2048): 73.144
Elapsed time for attention_prob_times_values (48x2048x2048x582): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x582): 63.718

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 532.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 6996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x583x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x583x2048): 74.242
Elapsed time for attention_prob_times_values (48x2048x2048x583): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x583): 61.441

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 526.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x584x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x584x2048): 68.971
Elapsed time for attention_prob_times_values (48x2048x2048x584): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x584): 82.735

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 590.077
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x585x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x585x2048): 71.587
Elapsed time for attention_prob_times_values (48x2048x2048x585): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x585): 61.297

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 518.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x586x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x586x2048): 71.840
Elapsed time for attention_prob_times_values (48x2048x2048x586): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x586): 64.235

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 533.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x587x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x587x2048): 71.915
Elapsed time for attention_prob_times_values (48x2048x2048x587): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x587): 61.384

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 521.850
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x588x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x588x2048): 74.163
Elapsed time for attention_prob_times_values (48x2048x2048x588): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x588): 61.035

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 528.371
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x589x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x589x2048): 71.863
Elapsed time for attention_prob_times_values (48x2048x2048x589): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x589): 61.211

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 522.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x590x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x590x2048): 72.493
Elapsed time for attention_prob_times_values (48x2048x2048x590): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x590): 64.211

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 538.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x591x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x591x2048): 71.781
Elapsed time for attention_prob_times_values (48x2048x2048x591): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x591): 61.937

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 527.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x592x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x592x2048): 73.704
Elapsed time for attention_prob_times_values (48x2048x2048x592): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x592): 84.151

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 623.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x593x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x593x2048): 70.932
Elapsed time for attention_prob_times_values (48x2048x2048x593): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x593): 62.262

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 527.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x594x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x594x2048): 68.003
Elapsed time for attention_prob_times_values (48x2048x2048x594): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x594): 64.801

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 528.312
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x595x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x595x2048): 71.424
Elapsed time for attention_prob_times_values (48x2048x2048x595): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x595): 62.317

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 530.665
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x596x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x596x2048): 72.854
Elapsed time for attention_prob_times_values (48x2048x2048x596): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x596): 64.960

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 548.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x597x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x597x2048): 71.578
Elapsed time for attention_prob_times_values (48x2048x2048x597): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x597): 62.469

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 533.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x598x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x598x2048): 70.095
Elapsed time for attention_prob_times_values (48x2048x2048x598): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x598): 64.827

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 539.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x599x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x599x2048): 71.848
Elapsed time for attention_prob_times_values (48x2048x2048x599): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x599): 62.834

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 537.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x600x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x600x2048): 73.535
Elapsed time for attention_prob_times_values (48x2048x2048x600): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x600): 84.888

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 632.900
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x601x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x601x2048): 70.855
Elapsed time for attention_prob_times_values (48x2048x2048x601): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x601): 62.957

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 536.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x602x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x602x2048): 71.648
Elapsed time for attention_prob_times_values (48x2048x2048x602): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x602): 64.617

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 547.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x603x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x603x2048): 71.280
Elapsed time for attention_prob_times_values (48x2048x2048x603): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x603): 62.781

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 538.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x604x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x604x2048): 72.489
Elapsed time for attention_prob_times_values (48x2048x2048x604): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x604): 65.612

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 556.414
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1291x2048): 83.246
Elapsed time for attention_prob_times_values (96x2048x2048x1291): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1291): 72.375

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2420.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1292x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1292x2048): 86.308
Elapsed time for attention_prob_times_values (96x2048x2048x1292): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1292): 77.388

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2552.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1293x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1293x2048): 84.270
Elapsed time for attention_prob_times_values (96x2048x2048x1293): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1293): 68.395

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2363.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1294x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1294x2048): 85.191
Elapsed time for attention_prob_times_values (96x2048x2048x1294): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1294): 77.757

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2547.127
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1295x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1295x2048): 85.221
Elapsed time for attention_prob_times_values (96x2048x2048x1295): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1295): 72.660

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2459.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1296x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1296x2048): 88.156
Elapsed time for attention_prob_times_values (96x2048x2048x1296): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1296): 89.135

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 2781.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1297x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1297x2048): 84.799
Elapsed time for attention_prob_times_values (96x2048x2048x1297): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1297): 71.184

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2430.152
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1298x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1298x2048): 83.706
Elapsed time for attention_prob_times_values (96x2048x2048x1298): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1298): 76.240

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2507.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1299x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1299x2048): 85.101
Elapsed time for attention_prob_times_values (96x2048x2048x1299): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1299): 71.449

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2442.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1300x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1300x2048): 86.367
Elapsed time for attention_prob_times_values (96x2048x2048x1300): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1300): 77.375

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2568.609
MLP duration (in seconds): 0.0000
Elapsed time for attention_key_query_prob (48x2048x605x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x605x2048): 71.366
Elapsed time for attention_prob_times_values (48x2048x2048x605): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x605): 63.251

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 542.538
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x606x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x606x2048): 72.104
Elapsed time for attention_prob_times_values (48x2048x2048x606): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x606): 65.496

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 556.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x607x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x607x2048): 71.314
Elapsed time for attention_prob_times_values (48x2048x2048x607): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x607): 63.401

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 544.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x608x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x608x2048): 86.791
Elapsed time for attention_prob_times_values (48x2048x2048x608): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x608): 87.004

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 706.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x609x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x609x2048): 72.980
Elapsed time for attention_prob_times_values (48x2048x2048x609): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x609): 63.823

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 554.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x610x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x610x2048): 74.072
Elapsed time for attention_prob_times_values (48x2048x2048x610): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x610): 65.689

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 567.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x611x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x611x2048): 72.868
Elapsed time for attention_prob_times_values (48x2048x2048x611): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x611): 60.878

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 541.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x612x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x612x2048): 74.519
Elapsed time for attention_prob_times_values (48x2048x2048x612): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x612): 66.145

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 572.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x613x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x613x2048): 72.727
Elapsed time for attention_prob_times_values (48x2048x2048x613): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x613): 64.008

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 557.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x614x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x614x2048): 72.850
Elapsed time for attention_prob_times_values (48x2048x2048x614): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x614): 65.901

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 567.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x615x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x615x2048): 69.447
Elapsed time for attention_prob_times_values (48x2048x2048x615): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x615): 62.430

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 539.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x616x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x616x2048): 74.432
Elapsed time for attention_prob_times_values (48x2048x2048x616): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x616): 87.064

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 659.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x617x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x617x2048): 70.274
Elapsed time for attention_prob_times_values (48x2048x2048x617): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x617): 64.382

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 553.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x618x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x618x2048): 72.535
Elapsed time for attention_prob_times_values (48x2048x2048x618): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x618): 66.297

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 570.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x619x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x619x2048): 71.977
Elapsed time for attention_prob_times_values (48x2048x2048x619): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x619): 64.483

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 561.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x620x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x620x2048): 70.668
Elapsed time for attention_prob_times_values (48x2048x2048x620): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x620): 66.864

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 567.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x621x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x621x2048): 72.130
Elapsed time for attention_prob_times_values (48x2048x2048x621): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x621): 64.711

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 564.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x622x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x622x2048): 73.010
Elapsed time for attention_prob_times_values (48x2048x2048x622): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x622): 66.688

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 577.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x623x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x623x2048): 71.986
Elapsed time for attention_prob_times_values (48x2048x2048x623): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x623): 64.924

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 566.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (320x2048x251x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x251x2048): 53.376
Elapsed time for attention_prob_times_values (320x2048x2048x251): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x251): 44.402

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 999.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x252x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x252x2048): 51.545
Elapsed time for attention_prob_times_values (320x2048x2048x252): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x252): 64.315

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 1183.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0236
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x253x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x253x2048): 52.360
Elapsed time for attention_prob_times_values (320x2048x2048x253): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x253): 56.187

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 1125.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x254x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x254x2048): 54.114
Elapsed time for attention_prob_times_values (320x2048x2048x254): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x254): 64.561

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1227.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x255x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x255x2048): 53.565
Elapsed time for attention_prob_times_values (320x2048x2048x255): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x255): 66.255

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1239.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x256x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x256x2048): 74.702
Elapsed time for attention_prob_times_values (320x2048x2048x256): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x256): 67.149

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1485.213
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x257x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x257x2048): 55.672
Elapsed time for attention_prob_times_values (320x2048x2048x257): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x257): 50.598

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1117.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x258x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x258x2048): 54.366
Elapsed time for attention_prob_times_values (320x2048x2048x258): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x258): 55.522

Attention duration (in seconds): 0.0252
Attention throughput (in TFLOP/s): 1162.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x259x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x259x2048): 56.729
Elapsed time for attention_prob_times_values (320x2048x2048x259): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x259): 53.923

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 1174.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x260x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x260x2048): 57.717
Elapsed time for attention_prob_times_values (320x2048x2048x260): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x260): 56.622

Attention duration (in seconds): 0.0244
num_attention_heads: 12, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x624x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x624x2048): 70.482
Elapsed time for attention_prob_times_values (48x2048x2048x624): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x624): 88.633

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 652.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x625x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x625x2048): 71.552
Elapsed time for attention_prob_times_values (48x2048x2048x625): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x625): 65.168

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 567.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x626x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x626x2048): 66.122
Elapsed time for attention_prob_times_values (48x2048x2048x626): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x626): 66.913

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 554.463
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x627x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x627x2048): 71.737
Elapsed time for attention_prob_times_values (48x2048x2048x627): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x627): 64.029

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 564.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x628x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x628x2048): 73.170
Elapsed time for attention_prob_times_values (48x2048x2048x628): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x628): 65.067

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 575.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x629x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x629x2048): 71.932
Elapsed time for attention_prob_times_values (48x2048x2048x629): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x629): 61.518

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 555.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x630x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x630x2048): 72.775
Elapsed time for attention_prob_times_values (48x2048x2048x630): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x630): 67.323

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 586.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x631x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x631x2048): 72.014
Elapsed time for attention_prob_times_values (48x2048x2048x631): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x631): 63.327

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 565.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x632x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x632x2048): 73.784
Elapsed time for attention_prob_times_values (48x2048x2048x632): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x632): 89.460

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 679.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x633x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x633x2048): 70.953
Elapsed time for attention_prob_times_values (48x2048x2048x633): 0.0039
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1301x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1301x2048): 84.373
Elapsed time for attention_prob_times_values (96x2048x2048x1301): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1301): 73.071

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2466.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1302x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1302x2048): 84.988
Elapsed time for attention_prob_times_values (96x2048x2048x1302): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1302): 78.443

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2571.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1303x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1303x2048): 85.204
Elapsed time for attention_prob_times_values (96x2048x2048x1303): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1303): 71.703

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2456.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1304x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1304x2048): 85.980
Elapsed time for attention_prob_times_values (96x2048x2048x1304): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1304): 87.409

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2736.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1305x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1305x2048): 84.631
Elapsed time for attention_prob_times_values (96x2048x2048x1305): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1305): 71.885

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2455.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1306x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1306x2048): 85.736
Elapsed time for attention_prob_times_values (96x2048x2048x1306): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1306): 77.696

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2576.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1307x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1307x2048): 85.103
Elapsed time for attention_prob_times_values (96x2048x2048x1307): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1307): 72.166

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2470.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1308x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1308x2048): 86.381
Elapsed time for attention_prob_times_values (96x2048x2048x1308): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1308): 78.491

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2603.638
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1309x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1309x2048): 83.535
Elapsed time for attention_prob_times_values (96x2048x2048x1309): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1309): 73.780

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2482.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x633): 65.229

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 572.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x634x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x634x2048): 72.079
Elapsed time for attention_prob_times_values (48x2048x2048x634): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x634): 67.329

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 586.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x635x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x635x2048): 71.328
Elapsed time for attention_prob_times_values (48x2048x2048x635): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x635): 61.929

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 559.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x636x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x636x2048): 68.128
Elapsed time for attention_prob_times_values (48x2048x2048x636): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x636): 67.338

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 572.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x637x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x637x2048): 71.250
Elapsed time for attention_prob_times_values (48x2048x2048x637): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x637): 65.390

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 577.253
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x638x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x638x2048): 72.247
Elapsed time for attention_prob_times_values (48x2048x2048x638): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x638): 67.924

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 593.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x639x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x639x2048): 71.003
Elapsed time for attention_prob_times_values (48x2048x2048x639): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x639): 65.018

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 576.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x640x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x640x2048): 82.946
Elapsed time for attention_prob_times_values (48x2048x2048x640): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x640): 86.754

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 720.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x641x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x641x2048): 73.175
Elapsed time for attention_prob_times_values (48x2048x2048x641): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x641): 60.005

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 561.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x642x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x642x2048): 74.248
Elapsed time for attention_prob_times_values (48x2048x2048x642): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x642): 62.953

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 580.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x643x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x643x2048): 73.048
Elapsed time for attention_prob_times_values (48x2048x2048x643): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x643): 60.823

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 566.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x644x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x644x2048): 74.689
Elapsed time for attention_prob_times_values (48x2048x2048x644): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x644): 63.047

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 584.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x645x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x645x2048): 73.473
Elapsed time for attention_prob_times_values (48x2048x2048x645): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x645): 59.247

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 561.420
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x646x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x646x2048): 74.071
Elapsed time for attention_prob_times_values (48x2048x2048x646): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x646): 63.140

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 584.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x647x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x647x2048): 73.276
Elapsed time for attention_prob_times_values (48x2048x2048x647): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x647): 61.140

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 572.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x648x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x648x2048): 74.722
Elapsed time for attention_prob_times_values (48x2048x2048x648): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x648): 76.514

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 649.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x649x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x649x2048): 72.510
Elapsed time for attention_prob_times_values (48x2048x2048x649): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x649): 59.767

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 563.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x650x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x650x2048): 73.144
Elapsed time for attention_prob_times_values (48x2048x2048x650): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x650): 63.638

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 586.493
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x651x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x651x2048): 72.705
Elapsed time for attention_prob_times_values (48x2048x2048x651): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x651): 61.373

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 574.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x652x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x652x2048): 73.778
Elapsed time for attention_prob_times_values (48x2048x2048x652): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x652): 64.139

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 592.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x653x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x653x2048): 72.822
Elapsed time for attention_prob_times_values (48x2048x2048x653): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x653): 61.435

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 576.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x654x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x654x2048): 69.066
Elapsed time for attention_prob_times_values (48x2048x2048x654): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x654): 64.055

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 575.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x655x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x655x2048): 72.901
Elapsed time for attention_prob_times_values (48x2048x2048x655): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x655): 58.572

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 563.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x656x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x656x2048): 69.819
Elapsed time for attention_prob_times_values (48x2048x2048x656): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x656): 77.812

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 639.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x657x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x657x2048): 71.401
Elapsed time for attention_prob_times_values (48x2048x2048x657): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x657): 62.019

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 577.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x658x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x658x2048): 73.148
Elapsed time for attention_prob_times_values (48x2048x2048x658): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x658): 63.858

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 593.983
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x659x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x659x2048): 71.142
Elapsed time for attention_prob_times_values (48x2048x2048x659): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x659): 62.190

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 578.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x660x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x660x2048): 73.661
Elapsed time for attention_prob_times_values (48x2048x2048x660): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x660): 64.557

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 601.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x661x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x661x2048): 72.645
Elapsed time for attention_prob_times_values (48x2048x2048x661): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x661): 62.340

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 586.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Attention throughput (in TFLOP/s): 1218.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0244
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x261x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x261x2048): 52.874
Elapsed time for attention_prob_times_values (320x2048x2048x261): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x261): 53.491

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1137.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x262x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x262x2048): 57.059
Elapsed time for attention_prob_times_values (320x2048x2048x262): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x262): 57.003

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 1224.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0247
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x263x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x263x2048): 55.329
Elapsed time for attention_prob_times_values (320x2048x2048x263): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x263): 54.843

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 1186.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x264x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x264x2048): 57.614
Elapsed time for attention_prob_times_values (320x2048x2048x264): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x264): 53.884

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1204.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x265x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x265x2048): 53.642
Elapsed time for attention_prob_times_values (320x2048x2048x265): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x265): 54.470

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1173.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x266x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x266x2048): 53.709
Elapsed time for attention_prob_times_values (320x2048x2048x266): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x266): 57.668

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1211.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x267x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x267x2048): 54.485
Elapsed time for attention_prob_times_values (320x2048x2048x267): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x267): 55.125

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1197.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x268x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x268x2048): 56.727
Elapsed time for attention_prob_times_values (320x2048x2048x268): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x268): 56.731

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1244.491
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x269x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x269x2048): 56.215
Elapsed time for attention_prob_times_values (320x2048x2048x269): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x269): 55.646

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 1231.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x662x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x662x2048): 73.399
Elapsed time for attention_prob_times_values (48x2048x2048x662): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x662): 64.813

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 602.881
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x663x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x663x2048): 72.831
Elapsed time for attention_prob_times_values (48x2048x2048x663): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x663): 62.292

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 588.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x664x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x664x2048): 74.358
Elapsed time for attention_prob_times_values (48x2048x2048x664): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x664): 78.299

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 669.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x665x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x665x2048): 72.289
Elapsed time for attention_prob_times_values (48x2048x2048x665): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x665): 62.390

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 588.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 7992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x666x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x666x2048): 72.985
Elapsed time for attention_prob_times_values (48x2048x2048x666): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x666): 64.975

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 605.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x667x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x667x2048): 72.413
Elapsed time for attention_prob_times_values (48x2048x2048x667): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x667): 62.630

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 592.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x668x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x668x2048): 73.601
Elapsed time for attention_prob_times_values (48x2048x2048x668): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x668): 65.001

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 609.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x669x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x669x2048): 72.575
Elapsed time for attention_prob_times_values (48x2048x2048x669): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x669): 62.626

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 594.342
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x670x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x670x2048): 73.252
Elapsed time for attention_prob_times_values (48x2048x2048x670): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x670): 64.978

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 609.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
num_attention_heads: 24, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1310x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1310x2048): 86.176
Elapsed time for attention_prob_times_values (96x2048x2048x1310): 0.0141
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1310): 74.619

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 2535.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1311x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1311x2048): 85.370
Elapsed time for attention_prob_times_values (96x2048x2048x1311): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1311): 71.996

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2478.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1312x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1312x2048): 98.363
Elapsed time for attention_prob_times_values (96x2048x2048x1312): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1312): 91.359

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 3007.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1313x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1313x2048): 86.330
Elapsed time for attention_prob_times_values (96x2048x2048x1313): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1313): 74.194

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 2535.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1314x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1314x2048): 87.135
Elapsed time for attention_prob_times_values (96x2048x2048x1314): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1314): 78.973

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2634.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1315x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1315x2048): 86.286
Elapsed time for attention_prob_times_values (96x2048x2048x1315): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1315): 73.876

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 2532.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1316x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1316x2048): 86.756
Elapsed time for attention_prob_times_values (96x2048x2048x1316): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1316): 79.268

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2638.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1317x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1317x2048): 86.056
Elapsed time for attention_prob_times_values (96x2048x2048x1317): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1317): 73.927

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2534.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1318x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1318x2048): 87.039
Elapsed time for attention_prob_times_values (96x2048x2048x1318): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1318): 79.111

Attention duration (in seconds): 0.0256
Attention throughput (in TFLOP/s): 2643.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1319x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1319x2048): 86.130
Elapsed time for attention_key_query_prob (48x2048x671x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x671x2048): 72.721
Elapsed time for attention_prob_times_values (48x2048x2048x671): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x671): 62.988

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 598.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x672x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x672x2048): 87.739
Elapsed time for attention_prob_times_values (48x2048x2048x672): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x672): 80.234

Attention duration (in seconds): 0.0065
Attention throughput (in TFLOP/s): 743.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0065
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x673x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x673x2048): 74.732
Elapsed time for attention_prob_times_values (48x2048x2048x673): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x673): 63.285

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 609.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x674x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x674x2048): 71.717
Elapsed time for attention_prob_times_values (48x2048x2048x674): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x674): 65.291

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 608.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x675x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x675x2048): 74.752
Elapsed time for attention_prob_times_values (48x2048x2048x675): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x675): 63.307

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 610.839
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x676x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x676x2048): 76.370
Elapsed time for attention_prob_times_values (48x2048x2048x676): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x676): 65.685

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 630.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x677x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x677x2048): 74.670
Elapsed time for attention_prob_times_values (48x2048x2048x677): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x677): 63.347

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 612.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x678x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x678x2048): 75.391
Elapsed time for attention_prob_times_values (48x2048x2048x678): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x678): 65.513

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 627.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x679x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x679x2048): 74.662
Elapsed time for attention_prob_times_values (48x2048x2048x679): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x679): 63.728

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 615.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x680x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x680x2048): 79.873
Elapsed time for attention_prob_times_values (48x2048x2048x680): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x680): 80.197

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 717.811
Elapsed time for attention_prob_times_values (1024x2048x2048x121): 0.0245
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x121): 42.503

Attention duration (in seconds): 0.0428
Attention throughput (in TFLOP/s): 1519.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0428
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x122x2048): 0.0180
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x122x2048): 58.105
Elapsed time for attention_prob_times_values (1024x2048x2048x122): 0.0189
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x122): 55.434

Attention duration (in seconds): 0.0369
Attention throughput (in TFLOP/s): 1787.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0369
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x123x2048): 0.0187
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x123x2048): 56.436
Elapsed time for attention_prob_times_values (1024x2048x2048x123): 0.0238
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x123): 44.456

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 1579.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x124x2048): 0.0185
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x124x2048): 57.567
Elapsed time for attention_prob_times_values (1024x2048x2048x124): 0.0185
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x124): 57.519

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 1841.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0370
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x125x2048): 0.0188
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x125x2048): 57.078
Elapsed time for attention_prob_times_values (1024x2048x2048x125): 0.0246
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x125): 43.648

Attention duration (in seconds): 0.0434
Attention throughput (in TFLOP/s): 1595.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0434
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x126x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x126x2048): 56.715
Elapsed time for attention_prob_times_values (1024x2048x2048x126): 0.0182
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x126): 59.453

Attention duration (in seconds): 0.0373
Attention throughput (in TFLOP/s): 1886.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0373
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 256, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (1024x2048x127x2048): 0.0189
Throughput (in TFLOP/s) for attention_key_query_prob (1024x2048x127x2048): 57.584
Elapsed time for attention_prob_times_values (1024x2048x2048x127): 0.0247
Throughput (in TFLOP/s) for attention_prob_times_values (1024x2048x2048x127): 44.092

Attention duration (in seconds): 0.0437
Attention throughput (in TFLOP/s): 1635.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0437
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x1x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x1x2048): 1.059
Elapsed time for attention_prob_times_values (2048x2048x2048x1): 0.0788
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x1): 0.218

Attention duration (in seconds): 0.0950
Attention throughput (in TFLOP/s): 0.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0950
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x2x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x2x2048): 1.997
Elapsed time for attention_prob_times_values (2048x2048x2048x2): 0.0381
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x2): 0.903

Attention duration (in seconds): 0.0553
Attention throughput (in TFLOP/s): 2.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0553
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x3x2048): 0.0190
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x3x2048): 2.713
Elapsed time for attention_prob_times_values (2048x2048x2048x3): 0.0252
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x3): 2.042

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 5.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x681x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x681x2048): 74.000
Elapsed time for attention_prob_times_values (48x2048x2048x681): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x681): 63.465

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 613.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x682x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x682x2048): 74.647
Elapsed time for attention_prob_times_values (48x2048x2048x682): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x682): 65.757

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 628.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x683x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x683x2048): 69.714
Elapsed time for attention_prob_times_values (48x2048x2048x683): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x683): 63.108

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 596.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x684x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x684x2048): 75.699
Elapsed time for attention_prob_times_values (48x2048x2048x684): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x684): 66.248

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 637.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x685x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x685x2048): 74.025
Elapsed time for attention_prob_times_values (48x2048x2048x685): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x685): 61.611

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 607.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x686x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x686x2048): 75.036
Elapsed time for attention_prob_times_values (48x2048x2048x686): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x686): 66.412

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 636.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x687x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x687x2048): 75.640
Elapsed time for attention_prob_times_values (48x2048x2048x687): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x687): 64.187

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 628.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x688x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x688x2048): 75.706
Elapsed time for attention_prob_times_values (48x2048x2048x688): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x688): 81.550

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 711.581
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x689x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x689x2048): 73.563
Elapsed time for attention_prob_times_values (48x2048x2048x689): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x689): 61.665

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 608.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (96x2048x2048x1319): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1319): 73.173

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2525.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1320x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1320x2048): 83.444
Elapsed time for attention_prob_times_values (96x2048x2048x1320): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1320): 89.637

Attention duration (in seconds): 0.0246
Attention throughput (in TFLOP/s): 2760.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0246
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1321x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1321x2048): 85.650
Elapsed time for attention_prob_times_values (96x2048x2048x1321): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1321): 72.534

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2510.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1322x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1322x2048): 86.551
Elapsed time for attention_prob_times_values (96x2048x2048x1322): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1322): 79.371

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2648.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1323x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1323x2048): 85.512
Elapsed time for attention_prob_times_values (96x2048x2048x1323): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1323): 73.928

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2538.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1324x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1324x2048): 86.877
Elapsed time for attention_prob_times_values (96x2048x2048x1324): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1324): 78.934

Attention duration (in seconds): 0.0258
Attention throughput (in TFLOP/s): 2649.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0258
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1325x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1325x2048): 85.858
Elapsed time for attention_prob_times_values (96x2048x2048x1325): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1325): 72.617

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 2522.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1326x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1326x2048): 84.415
Elapsed time for attention_prob_times_values (96x2048x2048x1326): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1326): 78.774

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2614.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1327x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1327x2048): 84.736
Elapsed time for attention_prob_times_values (96x2048x2048x1327): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1327): 73.185

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2521.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1328x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1328x2048): 86.891
Elapsed time for attention_prob_times_values (96x2048x2048x1328): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1328): 91.777

Attention duration (in seconds): 0.0240
Attention throughput (in TFLOP/s): 2867.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0240
num_attention_heads: 12, hidden_size: 8280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x690x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x690x2048): 74.432
Elapsed time for attention_prob_times_values (48x2048x2048x690): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x690): 66.677

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 639.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x691x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x691x2048): 73.498
Elapsed time for attention_prob_times_values (48x2048x2048x691): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x691): 64.446

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 624.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x692x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x692x2048): 75.273
Elapsed time for attention_prob_times_values (48x2048x2048x692): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x692): 65.133

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 636.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x693x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x693x2048): 74.004
Elapsed time for attention_prob_times_values (48x2048x2048x693): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x693): 63.005

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 620.807
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x694x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x694x2048): 73.240
Elapsed time for attention_prob_times_values (48x2048x2048x694): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x694): 66.975

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 639.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x695x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x695x2048): 73.984
Elapsed time for attention_prob_times_values (48x2048x2048x695): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x695): 64.734

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 631.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x696x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x696x2048): 75.854
Elapsed time for attention_prob_times_values (48x2048x2048x696): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x696): 82.135

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 722.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x697x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x697x2048): 73.328
Elapsed time for attention_prob_times_values (48x2048x2048x697): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x697): 64.963

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 631.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x698x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x698x2048): 74.051
Elapsed time for attention_prob_times_values (48x2048x2048x698): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x698): 67.365

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 647.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x699x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x699x2048): 68.822
Elapsed time for attention_prob_times_values (48x2048x2048x699): 0.0044
========================================================================================================================
num_attention_heads: 80, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x270x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x270x2048): 56.225
Elapsed time for attention_prob_times_values (320x2048x2048x270): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x270): 57.373

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1254.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x271x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x271x2048): 56.218
Elapsed time for attention_prob_times_values (320x2048x2048x271): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x271): 55.830

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1242.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x272x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x272x2048): 58.234
Elapsed time for attention_prob_times_values (320x2048x2048x272): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x272): 72.025

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1432.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x273x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x273x2048): 54.125
Elapsed time for attention_prob_times_values (320x2048x2048x273): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x273): 56.386

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1233.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x274x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x274x2048): 56.456
Elapsed time for attention_prob_times_values (320x2048x2048x274): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x274): 58.746

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 1290.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x275x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x275x2048): 54.118
Elapsed time for attention_prob_times_values (320x2048x2048x275): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x275): 55.769

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1235.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x276x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x276x2048): 55.457
Elapsed time for attention_prob_times_values (320x2048x2048x276): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x276): 58.521

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1284.890
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x277x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x277x2048): 55.932
Elapsed time for attention_prob_times_values (320x2048x2048x277): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x277): 56.967

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1277.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x278x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x278x2048): 57.015
Elapsed time for attention_prob_times_values (320x2048x2048x278): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x278): 57.450

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1300.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x279x2048): 0.0132
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x699): 64.540

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 612.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x700x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x700x2048): 74.898
Elapsed time for attention_prob_times_values (48x2048x2048x700): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x700): 67.385

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 652.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x701x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x701x2048): 73.346
Elapsed time for attention_prob_times_values (48x2048x2048x701): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x701): 65.509

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 637.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x702x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x702x2048): 71.052
Elapsed time for attention_prob_times_values (48x2048x2048x702): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x702): 67.639

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 639.432
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x703x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x703x2048): 73.656
Elapsed time for attention_prob_times_values (48x2048x2048x703): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x703): 61.904

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 621.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x704x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x704x2048): 87.352
Elapsed time for attention_prob_times_values (48x2048x2048x704): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x704): 84.521

Attention duration (in seconds): 0.0066
Attention throughput (in TFLOP/s): 794.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x705x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x705x2048): 75.606
Elapsed time for attention_prob_times_values (48x2048x2048x705): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x705): 62.113

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 631.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x706x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x706x2048): 76.461
Elapsed time for attention_prob_times_values (48x2048x2048x706): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x706): 61.727

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 633.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x707x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x707x2048): 75.566
Elapsed time for attention_prob_times_values (48x2048x2048x707): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x707): 58.755

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 613.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x708x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x708x2048): 77.277
Elapsed time for attention_prob_times_values (48x2048x2048x708): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x708): 65.111

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 657.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x709x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x709x2048): 75.194
Elapsed time for attention_prob_times_values (48x2048x2048x709): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x709): 62.338

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 634.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x710x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x710x2048): 76.123
Elapsed time for attention_prob_times_values (48x2048x2048x710): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x710): 64.892

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 652.985
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x711x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x711x2048): 75.128
Elapsed time for attention_prob_times_values (48x2048x2048x711): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x711): 62.521

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 636.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x712x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x712x2048): 77.022
Elapsed time for attention_prob_times_values (48x2048x2048x712): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x712): 83.988

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 750.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x713x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x713x2048): 74.436
Elapsed time for attention_prob_times_values (48x2048x2048x713): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x713): 62.333

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 634.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x714x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x714x2048): 75.135
Elapsed time for attention_prob_times_values (48x2048x2048x714): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x714): 65.375

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 654.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x715x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x715x2048): 74.603
Elapsed time for attention_prob_times_values (48x2048x2048x715): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x715): 62.363

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 637.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x716x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x716x2048): 76.133
Elapsed time for attention_prob_times_values (48x2048x2048x716): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x716): 63.734

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 651.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x717x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x717x2048): 74.439
Elapsed time for attention_prob_times_values (48x2048x2048x717): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x717): 61.021

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 630.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x718x2048): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1329x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1329x2048): 85.329
Elapsed time for attention_prob_times_values (96x2048x2048x1329): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1329): 71.278

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 2497.070
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1330x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1330x2048): 84.958
Elapsed time for attention_prob_times_values (96x2048x2048x1330): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1330): 79.834

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 2648.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1331x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1331x2048): 83.460
Elapsed time for attention_prob_times_values (96x2048x2048x1331): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1331): 74.099

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2527.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1332x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1332x2048): 87.263
Elapsed time for attention_prob_times_values (96x2048x2048x1332): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1332): 80.125

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2691.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 31992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1333x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1333x2048): 85.576
Elapsed time for attention_prob_times_values (96x2048x2048x1333): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1333): 74.029

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 2559.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1334x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1334x2048): 84.798
Elapsed time for attention_prob_times_values (96x2048x2048x1334): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1334): 79.960

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 2655.733
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1335x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1335x2048): 83.348
Elapsed time for attention_prob_times_values (96x2048x2048x1335): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1335): 72.858

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2510.497
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1336x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1336x2048): 87.308
Elapsed time for attention_prob_times_values (96x2048x2048x1336): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1336): 90.642

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2873.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1337x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1337x2048): 85.125
Elapsed time for attention_prob_times_values (96x2048x2048x1337): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1337): 73.940

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 2559.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x718x2048): 73.366
Elapsed time for attention_prob_times_values (48x2048x2048x718): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x718): 65.574

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 651.938
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x719x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x719x2048): 74.744
Elapsed time for attention_prob_times_values (48x2048x2048x719): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x719): 62.576

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 642.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x720x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x720x2048): 76.761
Elapsed time for attention_prob_times_values (48x2048x2048x720): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x720): 81.928

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 748.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x721x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x721x2048): 71.843
Elapsed time for attention_prob_times_values (48x2048x2048x721): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x721): 62.734

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 632.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x722x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x722x2048): 74.977
Elapsed time for attention_prob_times_values (48x2048x2048x722): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x722): 65.744

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 662.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x723x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x723x2048): 74.322
Elapsed time for attention_prob_times_values (48x2048x2048x723): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x723): 62.940

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 645.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x724x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x724x2048): 75.828
Elapsed time for attention_prob_times_values (48x2048x2048x724): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x724): 66.168

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 670.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x725x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x725x2048): 74.410
Elapsed time for attention_prob_times_values (48x2048x2048x725): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x725): 63.072

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 648.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x726x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x726x2048): 75.147
Elapsed time for attention_prob_times_values (48x2048x2048x726): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x726): 65.059

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 663.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x727x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x727x2048): 74.448
Elapsed time for attention_prob_times_values (48x2048x2048x727): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x727): 63.132

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 650.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x728x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x728x2048): 76.271
Elapsed time for attention_prob_times_values (48x2048x2048x728): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x728): 85.750

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 769.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x729x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x729x2048): 73.998
Elapsed time for attention_prob_times_values (48x2048x2048x729): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x729): 63.286

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 651.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x730x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x730x2048): 74.705
Elapsed time for attention_prob_times_values (48x2048x2048x730): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x730): 65.345

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 666.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x731x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x731x2048): 74.045
Elapsed time for attention_prob_times_values (48x2048x2048x731): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x731): 63.581

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 654.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x732x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x732x2048): 75.275
Elapsed time for attention_prob_times_values (48x2048x2048x732): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x732): 65.562

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 671.269
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x733x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x733x2048): 74.238
Elapsed time for attention_prob_times_values (48x2048x2048x733): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x733): 63.607

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 657.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x734x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x734x2048): 75.005
Elapsed time for attention_prob_times_values (48x2048x2048x734): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x734): 65.346

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 670.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x735x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x735x2048): 74.272
Elapsed time for attention_prob_times_values (48x2048x2048x735): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x735): 63.644

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 658.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x736x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x736x2048): 88.476
Elapsed time for attention_prob_times_values (48x2048x2048x736): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x736): 87.807

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 848.347
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x279x2048): 56.655
Elapsed time for attention_prob_times_values (320x2048x2048x279): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x279): 57.384

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1299.819
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x280x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x280x2048): 56.511
Elapsed time for attention_prob_times_values (320x2048x2048x280): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x280): 75.304

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1476.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x281x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x281x2048): 56.128
Elapsed time for attention_prob_times_values (320x2048x2048x281): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x281): 57.962

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1309.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x282x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x282x2048): 54.721
Elapsed time for attention_prob_times_values (320x2048x2048x282): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x282): 60.394

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 1322.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x283x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x283x2048): 56.379
Elapsed time for attention_prob_times_values (320x2048x2048x283): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x283): 57.277

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1313.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x284x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x284x2048): 55.563
Elapsed time for attention_prob_times_values (320x2048x2048x284): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x284): 60.992

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1348.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x285x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x285x2048): 56.755
Elapsed time for attention_prob_times_values (320x2048x2048x285): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x285): 58.766

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1343.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x286x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x286x2048): 57.374
Elapsed time for attention_prob_times_values (320x2048x2048x286): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x286): 60.499

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1374.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 22960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x287x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x287x2048): 54.494
Elapsed time for attention_prob_times_values (320x2048x2048x287): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x287): 57.702

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1312.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x288x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x288x2048): 77.428
Elapsed time for attention_prob_times_values (320x2048x2048x288): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x288): 77.919

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1825.303
MLP duration (in seconds): 0.0000
Elapsed time for attention_key_query_prob (48x2048x737x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x737x2048): 75.909
Elapsed time for attention_prob_times_values (48x2048x2048x737): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x737): 64.024

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 669.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x738x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x738x2048): 76.766
Elapsed time for attention_prob_times_values (48x2048x2048x738): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x738): 65.588

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 682.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x739x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x739x2048): 75.847
Elapsed time for attention_prob_times_values (48x2048x2048x739): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x739): 64.010

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 670.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x740x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x740x2048): 77.250
Elapsed time for attention_prob_times_values (48x2048x2048x740): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x740): 65.908

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 687.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x741x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x741x2048): 75.725
Elapsed time for attention_prob_times_values (48x2048x2048x741): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x741): 61.240

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 655.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x742x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x742x2048): 76.550
Elapsed time for attention_prob_times_values (48x2048x2048x742): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x742): 65.860

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 686.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x743x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x743x2048): 75.632
Elapsed time for attention_prob_times_values (48x2048x2048x743): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x743): 64.359

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 675.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x744x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x744x2048): 77.342
Elapsed time for attention_prob_times_values (48x2048x2048x744): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x744): 81.841

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 772.914
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x745x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x745x2048): 74.763
Elapsed time for attention_prob_times_values (48x2048x2048x745): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x745): 64.325

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 672.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x746x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x746x2048): 75.782
Elapsed time for attention_prob_times_values (48x2048x2048x746): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x746): 66.198

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 688.449
--------
Elapsed time for attention_key_query_prob (96x2048x1338x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1338x2048): 86.292
Elapsed time for attention_prob_times_values (96x2048x2048x1338): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1338): 80.176

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2689.763
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1339x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1339x2048): 83.685
Elapsed time for attention_prob_times_values (96x2048x2048x1339): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1339): 73.594

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2536.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1340x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1340x2048): 84.923
Elapsed time for attention_prob_times_values (96x2048x2048x1340): 0.0139
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1340): 77.383

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 2624.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1341x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1341x2048): 84.725
Elapsed time for attention_prob_times_values (96x2048x2048x1341): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1341): 73.393

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2550.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1342x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1342x2048): 80.816
Elapsed time for attention_prob_times_values (96x2048x2048x1342): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1342): 79.931

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 2608.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1343x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1343x2048): 85.609
Elapsed time for attention_prob_times_values (96x2048x2048x1343): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1343): 73.361

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2566.068
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1344x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1344x2048): 96.872
Elapsed time for attention_prob_times_values (96x2048x2048x1344): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1344): 86.154

Attention duration (in seconds): 0.0237
Attention throughput (in TFLOP/s): 2963.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1345x2048): 0.0254
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1345x2048): 42.650
Elapsed time for attention_prob_times_values (96x2048x2048x1345): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1345): 70.266

Attention duration (in seconds): 0.0408
Attention throughput (in TFLOP/s): 1726.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0408
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1346x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1346x2048): 87.229
Elapsed time for attention_prob_times_values (96x2048x2048x1346): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1346): 80.635

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2727.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1347x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1347x2048): 87.645
Elapsed time for attention_prob_times_values (96x2048x2048x1347): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1347): 73.427

Attention duration (in seconds): 0.0271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x747x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x747x2048): 74.853
Elapsed time for attention_prob_times_values (48x2048x2048x747): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x747): 62.805

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 666.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x748x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x748x2048): 75.409
Elapsed time for attention_prob_times_values (48x2048x2048x748): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x748): 66.747

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 691.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 8988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x749x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x749x2048): 74.947
Elapsed time for attention_prob_times_values (48x2048x2048x749): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x749): 64.710

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 679.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x750x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x750x2048): 75.915
Elapsed time for attention_prob_times_values (48x2048x2048x750): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x750): 66.599

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 694.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x751x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x751x2048): 75.027
Elapsed time for attention_prob_times_values (48x2048x2048x751): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x751): 62.624

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 669.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x752x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x752x2048): 77.195
Elapsed time for attention_prob_times_values (48x2048x2048x752): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x752): 89.005

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 811.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x753x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x753x2048): 74.425
Elapsed time for attention_prob_times_values (48x2048x2048x753): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x753): 64.959

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 681.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x754x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x754x2048): 75.277
Elapsed time for attention_prob_times_values (48x2048x2048x754): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x754): 66.705

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 695.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x755x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x755x2048): 74.518
Elapsed time for attention_prob_times_values (48x2048x2048x755): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x755): 65.012

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 683.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x756x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x756x2048): 75.705
Elapsed time for attention_prob_times_values (48x2048x2048x756): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x756): 67.029

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 701.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x757x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x757x2048): 74.676
Elapsed time for attention_prob_times_values (48x2048x2048x757): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x757): 65.351

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 688.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x758x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x758x2048): 75.400
Elapsed time for attention_prob_times_values (48x2048x2048x758): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x758): 66.967

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 701.029
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x759x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x759x2048): 74.847
Elapsed time for attention_prob_times_values (48x2048x2048x759): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x759): 65.263

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 689.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x760x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x760x2048): 76.214
Elapsed time for attention_prob_times_values (48x2048x2048x760): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x760): 89.532

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 815.661
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x761x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x761x2048): 74.089
Elapsed time for attention_prob_times_values (48x2048x2048x761): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x761): 64.056

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 681.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x762x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x762x2048): 74.881
Elapsed time for attention_prob_times_values (48x2048x2048x762): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x762): 66.852

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 701.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x763x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x763x2048): 69.721
Elapsed time for attention_prob_times_values (48x2048x2048x763): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x763): 65.541

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 671.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x764x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x764x2048): 75.526
Elapsed time for attention_prob_times_values (48x2048x2048x764): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x764): 67.335

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 708.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x765x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x765x2048): 74.345
Elapsed time for attention_prob_times_values (48x2048x2048x765): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x765): 64.881

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 690.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x766x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x766x2048): 75.030
Elapsed time for attention_prob_times_values (48x2048x2048x766): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x766): 67.126

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 706.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x767x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x767x2048): 74.253
Elapsed time for attention_prob_times_values (48x2048x2048x767): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x767): 61.104

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 669.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x768x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x768x2048): 85.202
Elapsed time for attention_prob_times_values (48x2048x2048x768): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x768): 92.566

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 887.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x769x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x769x2048): 76.100
Elapsed time for attention_prob_times_values (48x2048x2048x769): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x769): 60.392

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 674.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x770x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x770x2048): 77.025
Elapsed time for attention_prob_times_values (48x2048x2048x770): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x770): 63.730

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 699.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x771x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x771x2048): 75.752
Elapsed time for attention_prob_times_values (48x2048x2048x771): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x771): 60.456

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 674.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x772x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x772x2048): 77.510
Elapsed time for attention_prob_times_values (48x2048x2048x772): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x772): 63.952

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 704.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x773x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x773x2048): 76.126
Elapsed time for attention_prob_times_values (48x2048x2048x773): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x773): 61.067

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 681.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x774x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x774x2048): 76.251
Elapsed time for attention_prob_times_values (48x2048x2048x774): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x774): 62.523

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 691.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x289x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x289x2048): 60.492
Elapsed time for attention_prob_times_values (320x2048x2048x289): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x289): 59.545

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1415.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x290x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x290x2048): 61.296
Elapsed time for attention_prob_times_values (320x2048x2048x290): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x290): 61.249

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 1449.473
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x291x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x291x2048): 56.496
Elapsed time for attention_prob_times_values (320x2048x2048x291): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x291): 57.144

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 1348.546
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x292x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x292x2048): 62.188
Elapsed time for attention_prob_times_values (320x2048x2048x292): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x292): 62.669

Attention duration (in seconds): 0.0251
Attention throughput (in TFLOP/s): 1486.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0251
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x293x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x293x2048): 59.170
Elapsed time for attention_prob_times_values (320x2048x2048x293): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x293): 60.488

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1429.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x294x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x294x2048): 59.588
Elapsed time for attention_prob_times_values (320x2048x2048x294): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x294): 61.226

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 1447.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0261
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x295x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x295x2048): 57.866
Elapsed time for attention_prob_times_values (320x2048x2048x295): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x295): 58.406

Attention duration (in seconds): 0.0272
Attention throughput (in TFLOP/s): 1397.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0272
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x296x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x296x2048): 61.024
Elapsed time for attention_prob_times_values (320x2048x2048x296): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x296): 78.911

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1660.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x297x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x297x2048): 59.218
Elapsed time for attention_prob_times_values (320x2048x2048x297): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x297): 59.347

Attention duration (in seconds): 0.0269
Attention throughput (in TFLOP/s): 1434.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 2602.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1348x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1348x2048): 88.164
Elapsed time for attention_prob_times_values (96x2048x2048x1348): 0.0134
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1348): 80.950

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 2751.022
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1349x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1349x2048): 86.730
Elapsed time for attention_prob_times_values (96x2048x2048x1349): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1349): 72.643

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2578.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1350x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1350x2048): 85.358
Elapsed time for attention_prob_times_values (96x2048x2048x1350): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1350): 80.528

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2705.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1351x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1351x2048): 86.282
Elapsed time for attention_prob_times_values (96x2048x2048x1351): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1351): 73.523

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2593.298
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1352x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1352x2048): 87.868
Elapsed time for attention_prob_times_values (96x2048x2048x1352): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1352): 92.194

Attention duration (in seconds): 0.0242
Attention throughput (in TFLOP/s): 2941.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0242
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1353x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1353x2048): 85.875
Elapsed time for attention_prob_times_values (96x2048x2048x1353): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1353): 73.360

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2588.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1354x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1354x2048): 86.962
Elapsed time for attention_prob_times_values (96x2048x2048x1354): 0.0137
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1354): 79.621

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 2721.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1355x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1355x2048): 85.888
Elapsed time for attention_prob_times_values (96x2048x2048x1355): 0.0146
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1355): 74.511

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2613.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1356x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1356x2048): 91.457
Elapsed time for attention_prob_times_values (96x2048x2048x1356): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1356): 81.025

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 2816.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0254
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x775x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x775x2048): 75.955
Elapsed time for attention_prob_times_values (48x2048x2048x775): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x775): 58.314

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 665.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x776x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x776x2048): 77.373
Elapsed time for attention_prob_times_values (48x2048x2048x776): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x776): 80.743

Attention duration (in seconds): 0.0079
Attention throughput (in TFLOP/s): 797.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0079
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x777x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x777x2048): 75.243
Elapsed time for attention_prob_times_values (48x2048x2048x777): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x777): 60.980

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 680.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x778x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x778x2048): 75.941
Elapsed time for attention_prob_times_values (48x2048x2048x778): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x778): 64.188

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 703.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x779x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x779x2048): 75.260
Elapsed time for attention_prob_times_values (48x2048x2048x779): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x779): 61.294

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 684.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x780x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x780x2048): 76.405
Elapsed time for attention_prob_times_values (48x2048x2048x780): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x780): 64.745

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 710.792
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x781x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x781x2048): 75.557
Elapsed time for attention_prob_times_values (48x2048x2048x781): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x781): 61.118

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 686.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x782x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x782x2048): 76.181
Elapsed time for attention_prob_times_values (48x2048x2048x782): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x782): 64.383

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 709.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x783x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x783x2048): 75.092
Elapsed time for attention_prob_times_values (48x2048x2048x783): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x783): 61.528

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 688.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x784x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x784x2048): 77.494
Elapsed time for attention_prob_times_values (48x2048x2048x784): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x784): 79.214

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 798.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x785x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x785x2048): 74.956
Elapsed time for attention_prob_times_values (48x2048x2048x785): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x785): 58.825

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 672.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x786x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x786x2048): 75.316
Elapsed time for attention_prob_times_values (48x2048x2048x786): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x786): 64.643

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 710.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x787x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x787x2048): 71.042
Elapsed time for attention_prob_times_values (48x2048x2048x787): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x787): 61.688

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 675.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x788x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x788x2048): 76.448
Elapsed time for attention_prob_times_values (48x2048x2048x788): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x788): 64.930

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 718.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x789x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x789x2048): 75.304
Elapsed time for attention_prob_times_values (48x2048x2048x789): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x789): 61.856

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 695.922
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x790x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x790x2048): 76.014
Elapsed time for attention_prob_times_values (48x2048x2048x790): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x790): 64.902

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 718.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x791x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x791x2048): 75.435
Elapsed time for attention_prob_times_values (48x2048x2048x791): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x791): 60.518

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 689.685
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x792x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x792x2048): 76.656
Elapsed time for attention_prob_times_values (48x2048x2048x792): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x792): 82.211

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 815.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x793x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x793x2048): 74.924
Elapsed time for attention_prob_times_values (48x2048x2048x793): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x793): 59.796

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 684.592
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x794x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x794x2048): 75.512
Elapsed time for attention_prob_times_values (48x2048x2048x794): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x794): 62.899

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 707.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x795x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x795x2048): 75.008
Elapsed time for attention_prob_times_values (48x2048x2048x795): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x795): 62.208

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 701.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x796x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x796x2048): 76.391
Elapsed time for attention_prob_times_values (48x2048x2048x796): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x796): 64.566

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 722.786
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x797x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x797x2048): 70.746
Elapsed time for attention_prob_times_values (48x2048x2048x797): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x797): 62.437

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 685.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x798x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x798x2048): 75.735
Elapsed time for attention_prob_times_values (48x2048x2048x798): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x798): 65.207

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 725.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x799x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x799x2048): 75.221
Elapsed time for attention_prob_times_values (48x2048x2048x799): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x799): 62.466

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 707.322
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x800x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x800x2048): 89.127
Elapsed time for attention_prob_times_values (48x2048x2048x800): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x800): 83.596

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 895.083
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x801x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x801x2048): 73.190
Elapsed time for attention_prob_times_values (48x2048x2048x801): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x801): 62.855

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 702.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x802x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x802x2048): 73.186
Elapsed time for attention_prob_times_values (48x2048x2048x802): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x802): 65.303

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 717.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 24, hidden_size: 32568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1357x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1357x2048): 89.940
Elapsed time for attention_prob_times_values (96x2048x2048x1357): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1357): 74.487

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2673.165
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1358x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1358x2048): 89.061
Elapsed time for attention_prob_times_values (96x2048x2048x1358): 0.0136
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1358): 80.476

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2775.666
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1359x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1359x2048): 88.854
Elapsed time for attention_prob_times_values (96x2048x2048x1359): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1359): 72.661

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2626.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1360x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1360x2048): 91.329
Elapsed time for attention_prob_times_values (96x2048x2048x1360): 0.0207
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1360): 52.899

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 2202.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0327
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1361x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1361x2048): 87.990
Elapsed time for attention_prob_times_values (96x2048x2048x1361): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1361): 73.461

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 2634.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1362x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1362x2048): 89.344
Elapsed time for attention_prob_times_values (96x2048x2048x1362): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1362): 78.491

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 2751.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1363x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1363x2048): 86.460
Elapsed time for attention_prob_times_values (96x2048x2048x1363): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1363): 73.296

Attention duration (in seconds): 0.0277
Attention throughput (in TFLOP/s): 2613.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0277
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1364x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1364x2048): 91.336
Elapsed time for attention_prob_times_values (96x2048x2048x1364): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1364): 81.363

Attention duration (in seconds): 0.0255
Attention throughput (in TFLOP/s): 2837.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 24, hidden_size: 32760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (96x2048x1365x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (96x2048x1365x2048): 81.668
Elapsed time for attention_prob_times_values (96x2048x2048x1365): 0.0147
Throughput (in TFLOP/s) for attention_prob_times_values (96x2048x2048x1365): 75.004

Attention duration (in seconds): 0.0281
Attention throughput (in TFLOP/s): 2579.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0281
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 32, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x1x2048): 0.0011
Elapsed time for attention_key_query_prob (48x2048x803x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x803x2048): 77.014
Elapsed time for attention_prob_times_values (48x2048x2048x803): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x803): 62.654

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 719.300
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x804x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x804x2048): 78.356
Elapsed time for attention_prob_times_values (48x2048x2048x804): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x804): 61.725

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 719.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x805x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x805x2048): 76.207
Elapsed time for attention_prob_times_values (48x2048x2048x805): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x805): 63.137

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 720.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x806x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x806x2048): 74.940
Elapsed time for attention_prob_times_values (48x2048x2048x806): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x806): 65.639

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 730.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x807x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x807x2048): 76.332
Elapsed time for attention_prob_times_values (48x2048x2048x807): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x807): 63.182

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 722.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x808x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x808x2048): 78.560
Elapsed time for attention_prob_times_values (48x2048x2048x808): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x808): 77.052

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 814.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x809x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x809x2048): 72.016
Elapsed time for attention_prob_times_values (48x2048x2048x809): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x809): 63.201

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 705.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x810x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x810x2048): 76.822
Elapsed time for attention_prob_times_values (48x2048x2048x810): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x810): 64.630

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 736.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x811x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x811x2048): 76.257
Elapsed time for attention_prob_times_values (48x2048x2048x811): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x811): 63.112

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 725.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x812x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x812x2048): 77.984
Elapsed time for attention_prob_times_values (48x2048x2048x812): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x812): 66.477

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 754.727
num_attention_heads: 80, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x298x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x298x2048): 59.808
Elapsed time for attention_prob_times_values (320x2048x2048x298): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x298): 63.668

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 1497.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 23920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x299x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x299x2048): 59.191
Elapsed time for attention_prob_times_values (320x2048x2048x299): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x299): 61.358

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1467.771
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x300x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x300x2048): 61.008
Elapsed time for attention_prob_times_values (320x2048x2048x300): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x300): 64.476

Attention duration (in seconds): 0.0257
Attention throughput (in TFLOP/s): 1532.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0257
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x301x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x301x2048): 57.757
Elapsed time for attention_prob_times_values (320x2048x2048x301): 0.0133
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x301): 60.574

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1449.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x302x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x302x2048): 59.231
Elapsed time for attention_prob_times_values (320x2048x2048x302): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x302): 64.656

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1520.507
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x303x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x303x2048): 60.036
Elapsed time for attention_prob_times_values (320x2048x2048x303): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x303): 63.864

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1526.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x304x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x304x2048): 61.455
Elapsed time for attention_prob_times_values (320x2048x2048x304): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x304): 81.872

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1737.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x305x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x305x2048): 57.898
Elapsed time for attention_prob_times_values (320x2048x2048x305): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x305): 63.824

Attention duration (in seconds): 0.0270
Attention throughput (in TFLOP/s): 1507.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0270
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x306x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x306x2048): 58.062
Elapsed time for attention_prob_times_values (320x2048x2048x306): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x306): 65.355

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1531.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x307x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x307x2048): 57.852
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x1x2048): 1.012
Elapsed time for attention_prob_times_values (128x2048x2048x1): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x1): 0.210

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 0.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 64, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x2x2048): 0.0011
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x2x2048): 1.874
Elapsed time for attention_prob_times_values (128x2048x2048x2): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x2): 1.771

Attention duration (in seconds): 0.0024
Attention throughput (in TFLOP/s): 1.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 96, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x3x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x3x2048): 2.625
Elapsed time for attention_prob_times_values (128x2048x2048x3): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x3): 2.456

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 2.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x4x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x4x2048): 3.367
Elapsed time for attention_prob_times_values (128x2048x2048x4): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x4): 3.098

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 3.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x5x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x5x2048): 4.172
Elapsed time for attention_prob_times_values (128x2048x2048x5): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x5): 3.731

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 4.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x6x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x6x2048): 4.972
Elapsed time for attention_prob_times_values (128x2048x2048x6): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x6): 4.517

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 5.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x7x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x7x2048): 5.796
Elapsed time for attention_prob_times_values (128x2048x2048x7): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x7): 4.672

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 6.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x8x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x8x2048): 6.602
Elapsed time for attention_prob_times_values (128x2048x2048x8): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x8): 7.069

Attention duration (in seconds): 0.0025
Attention throughput (in TFLOP/s): 8.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0025
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x9x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x9x2048): 7.115
Elapsed time for attention_prob_times_values (128x2048x2048x9): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x9): 7.537

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 9.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x10x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x10x2048): 7.867
Elapsed time for attention_prob_times_values (128x2048x2048x10): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x10): 8.682

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 10.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x813x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x813x2048): 76.531
Elapsed time for attention_prob_times_values (48x2048x2048x813): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x813): 61.505

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 717.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x814x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x814x2048): 77.423
Elapsed time for attention_prob_times_values (48x2048x2048x814): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x814): 78.569

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 821.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x815x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x815x2048): 74.623
Elapsed time for attention_prob_times_values (48x2048x2048x815): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x815): 76.167

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 795.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x816x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x816x2048): 81.501
Elapsed time for attention_prob_times_values (48x2048x2048x816): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x816): 82.299

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 865.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x817x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x817x2048): 76.173
Elapsed time for attention_prob_times_values (48x2048x2048x817): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x817): 76.515

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 807.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x818x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x818x2048): 76.847
Elapsed time for attention_prob_times_values (48x2048x2048x818): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x818): 78.892

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 824.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x819x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x819x2048): 72.802
Elapsed time for attention_prob_times_values (48x2048x2048x819): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x819): 76.566

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 790.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x820x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x820x2048): 78.130
Elapsed time for attention_prob_times_values (48x2048x2048x820): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x820): 78.801

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 832.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x821x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x821x2048): 78.416
Elapsed time for attention_prob_times_values (48x2048x2048x821): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x821): 76.694

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 823.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x4x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x4x2048): 3.450
Elapsed time for attention_prob_times_values (2048x2048x2048x4): 0.0223
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x4): 3.082

Attention duration (in seconds): 0.0422
Attention throughput (in TFLOP/s): 9.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0422
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x5x2048): 0.0204
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x5x2048): 4.211
Elapsed time for attention_prob_times_values (2048x2048x2048x5): 0.0236
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x5): 3.647

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 13.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x6x2048): 0.0201
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x6x2048): 5.125
Elapsed time for attention_prob_times_values (2048x2048x2048x6): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x6): 4.511

Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 19.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0430
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x7x2048): 0.0197
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x7x2048): 6.094
Elapsed time for attention_prob_times_values (2048x2048x2048x7): 0.0232
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x7): 5.193

Attention duration (in seconds): 0.0429
Attention throughput (in TFLOP/s): 25.235
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0429
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x8x2048): 0.0210
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x8x2048): 6.541
Elapsed time for attention_prob_times_values (2048x2048x2048x8): 0.0189
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x8): 7.276

Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 34.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0399
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x9x2048): 0.0300
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x9x2048): 5.147
Elapsed time for attention_prob_times_values (2048x2048x2048x9): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x9): 7.758

Attention duration (in seconds): 0.0500
Attention throughput (in TFLOP/s): 34.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0500
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x10x2048): 0.0220
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x10x2048): 7.801
Elapsed time for attention_prob_times_values (2048x2048x2048x10): 0.0193
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x10): 8.923

Attention duration (in seconds): 0.0413
Attention throughput (in TFLOP/s): 49.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0413
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x11x2048): 0.0214
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x11x2048): 8.812
Elapsed time for attention_prob_times_values (2048x2048x2048x11): 0.0202
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x11): 9.341

Attention duration (in seconds): 0.0417
Attention throughput (in TFLOP/s): 58.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0417
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x12x2048): 0.0215
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x12x2048): 9.582
Elapsed time for attention_prob_times_values (2048x2048x2048x12): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x12): 10.467

Attention duration (in seconds): 0.0412
Attention throughput (in TFLOP/s): 70.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0412
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x11x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x11x2048): 8.634
Elapsed time for attention_prob_times_values (128x2048x2048x11): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x11): 9.234

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 11.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x12x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x12x2048): 9.378
Elapsed time for attention_prob_times_values (128x2048x2048x12): 0.0012
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x12): 10.381

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 13.549
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x13x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x13x2048): 10.145
Elapsed time for attention_prob_times_values (128x2048x2048x13): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x13): 10.828

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 14.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x14x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x14x2048): 10.914
Elapsed time for attention_prob_times_values (128x2048x2048x14): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x14): 12.014

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 16.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x15x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x15x2048): 10.509
Elapsed time for attention_prob_times_values (128x2048x2048x15): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x15): 12.386

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 16.701
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x16x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x16x2048): 12.512
Elapsed time for attention_prob_times_values (128x2048x2048x16): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x16): 13.637

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 19.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x17x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x17x2048): 12.716
Elapsed time for attention_prob_times_values (128x2048x2048x17): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x17): 13.885

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 20.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x18x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x18x2048): 12.730
Elapsed time for attention_prob_times_values (128x2048x2048x18): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x18): 15.141

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 21.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x19x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x19x2048): 14.238
Elapsed time for attention_prob_times_values (128x2048x2048x19): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x19): 15.513

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 23.664
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x20x2048): 0.0015
num_attention_heads: 12, hidden_size: 9864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x822x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x822x2048): 77.395
Elapsed time for attention_prob_times_values (48x2048x2048x822): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x822): 75.885

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 814.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x823x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x823x2048): 76.791
Elapsed time for attention_prob_times_values (48x2048x2048x823): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x823): 74.455

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 804.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x824x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x824x2048): 78.084
Elapsed time for attention_prob_times_values (48x2048x2048x824): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x824): 82.755

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 856.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x825x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x825x2048): 75.789
Elapsed time for attention_prob_times_values (48x2048x2048x825): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x825): 76.829

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 814.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x826x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x826x2048): 76.509
Elapsed time for attention_prob_times_values (48x2048x2048x826): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x826): 79.243

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 831.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x827x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x827x2048): 75.882
Elapsed time for attention_prob_times_values (48x2048x2048x827): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x827): 77.357

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 819.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x828x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x828x2048): 77.584
Elapsed time for attention_prob_times_values (48x2048x2048x828): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x828): 79.784

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 841.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x829x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x829x2048): 75.703
Elapsed time for attention_prob_times_values (48x2048x2048x829): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x829): 77.591

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 821.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x830x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x830x2048): 76.657
Elapsed time for attention_prob_times_values (48x2048x2048x830): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x830): 79.963

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 839.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x831x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x831x2048): 72.049
Elapsed time for attention_prob_times_values (48x2048x2048x831): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x20x2048): 14.052
Elapsed time for attention_prob_times_values (128x2048x2048x20): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x20): 16.739

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 24.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x21x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x21x2048): 15.702
Elapsed time for attention_prob_times_values (128x2048x2048x21): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x21): 17.019

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 27.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x22x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x22x2048): 16.431
Elapsed time for attention_prob_times_values (128x2048x2048x22): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x22): 18.060

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 29.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x23x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x23x2048): 17.192
Elapsed time for attention_prob_times_values (128x2048x2048x23): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x23): 18.510

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 30.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x24x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x24x2048): 17.925
Elapsed time for attention_prob_times_values (128x2048x2048x24): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x24): 16.729

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 30.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x25x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x25x2048): 15.734
Elapsed time for attention_prob_times_values (128x2048x2048x25): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x25): 19.728

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 31.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x26x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x26x2048): 18.917
Elapsed time for attention_prob_times_values (128x2048x2048x26): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x26): 21.132

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 36.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x27x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x27x2048): 19.600
Elapsed time for attention_prob_times_values (128x2048x2048x27): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x27): 21.518

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 37.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x28x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x28x2048): 20.264
Elapsed time for attention_prob_times_values (128x2048x2048x28): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x28): 22.680

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 40.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x29x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x29x2048): 20.923
Elapsed time for attention_prob_times_values (128x2048x2048x29): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x29): 22.945

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 41.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x831): 77.731

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 803.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x832x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x832x2048): 83.326
Elapsed time for attention_prob_times_values (48x2048x2048x832): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x832): 87.519

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 917.739
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 9996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x833x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x833x2048): 67.691
Elapsed time for attention_prob_times_values (48x2048x2048x833): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x833): 72.542

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 753.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x834x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x834x2048): 77.831
Elapsed time for attention_prob_times_values (48x2048x2048x834): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x834): 80.224

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 851.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x835x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x835x2048): 77.611
Elapsed time for attention_prob_times_values (48x2048x2048x835): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x835): 76.716

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 832.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x836x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x836x2048): 79.319
Elapsed time for attention_prob_times_values (48x2048x2048x836): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x836): 80.454

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 862.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x837x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x837x2048): 74.966
Elapsed time for attention_prob_times_values (48x2048x2048x837): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x837): 78.206

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 827.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x838x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x838x2048): 78.286
Elapsed time for attention_prob_times_values (48x2048x2048x838): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x838): 80.039

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 856.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x839x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x839x2048): 77.171
Elapsed time for attention_prob_times_values (48x2048x2048x839): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x839): 76.970

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 834.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x840x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x840x2048): 79.205
Elapsed time for attention_prob_times_values (48x2048x2048x840): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x840): 87.017

Attention duration (in seconds): 0.0082
Attention throughput (in TFLOP/s): 899.241
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x30x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x30x2048): 21.476
Elapsed time for attention_prob_times_values (128x2048x2048x30): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x30): 24.148

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 44.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x31x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x31x2048): 22.334
Elapsed time for attention_prob_times_values (128x2048x2048x31): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x31): 24.310

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 45.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x32x2048): 0.0009
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x32x2048): 39.866
Elapsed time for attention_prob_times_values (128x2048x2048x32): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x32): 26.141

Attention duration (in seconds): 0.0022
Attention throughput (in TFLOP/s): 63.153
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0022
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x33x2048): 0.0013
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x33x2048): 26.370
Elapsed time for attention_prob_times_values (128x2048x2048x33): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x33): 25.960

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 53.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x34x2048): 0.0014
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x34x2048): 26.667
Elapsed time for attention_prob_times_values (128x2048x2048x34): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x34): 26.939

Attention duration (in seconds): 0.0027
Attention throughput (in TFLOP/s): 55.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x35x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x35x2048): 25.701
Elapsed time for attention_prob_times_values (128x2048x2048x35): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x35): 27.437

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 55.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x36x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x36x2048): 26.522
Elapsed time for attention_prob_times_values (128x2048x2048x36): 0.0016
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x36): 23.895

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 53.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x37x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x37x2048): 26.479
Elapsed time for attention_prob_times_values (128x2048x2048x37): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x37): 26.725

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 57.360
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x38x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x38x2048): 27.303
Elapsed time for attention_prob_times_values (128x2048x2048x38): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x38): 29.940

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 62.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_prob_times_values (320x2048x2048x307): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x307): 63.365

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1511.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x308x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x308x2048): 61.180
Elapsed time for attention_prob_times_values (320x2048x2048x308): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x308): 66.073

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 1592.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x309x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x309x2048): 60.043
Elapsed time for attention_prob_times_values (320x2048x2048x309): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x309): 65.133

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1570.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x310x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x310x2048): 60.641
Elapsed time for attention_prob_times_values (320x2048x2048x310): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x310): 66.174

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1596.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x311x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x311x2048): 60.360
Elapsed time for attention_prob_times_values (320x2048x2048x311): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x311): 65.559

Attention duration (in seconds): 0.0266
Attention throughput (in TFLOP/s): 1589.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x312x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x312x2048): 61.580
Elapsed time for attention_prob_times_values (320x2048x2048x312): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x312): 81.519

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 1780.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x313x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x313x2048): 57.123
Elapsed time for attention_prob_times_values (320x2048x2048x313): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x313): 64.370

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1540.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x314x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x314x2048): 59.377
Elapsed time for attention_prob_times_values (320x2048x2048x314): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x314): 65.232

Attention duration (in seconds): 0.0271
Attention throughput (in TFLOP/s): 1587.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0271
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x315x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x315x2048): 57.719
Elapsed time for attention_prob_times_values (320x2048x2048x315): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x315): 65.289

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 1569.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x316x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x316x2048): 60.142
Elapsed time for attention_prob_times_values (320x2048x2048x316): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x316): 67.196

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1630.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
========================================================================================================================
num_attention_heads: 12, hidden_size: 10092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x841x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x841x2048): 74.168
Elapsed time for attention_prob_times_values (48x2048x2048x841): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x841): 78.723

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 829.118
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x842x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x842x2048): 77.384
Elapsed time for attention_prob_times_values (48x2048x2048x842): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x842): 80.994

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 860.116
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x843x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x843x2048): 76.729
Elapsed time for attention_prob_times_values (48x2048x2048x843): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x843): 78.941

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 846.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x844x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x844x2048): 78.396
Elapsed time for attention_prob_times_values (48x2048x2048x844): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x844): 77.505

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 848.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x845x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x845x2048): 76.519
Elapsed time for attention_prob_times_values (48x2048x2048x845): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x845): 78.783

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 846.398
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x846x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x846x2048): 73.203
Elapsed time for attention_prob_times_values (48x2048x2048x846): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x846): 81.332

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 840.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x847x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x847x2048): 72.234
Elapsed time for attention_prob_times_values (48x2048x2048x847): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x847): 79.223

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 825.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x848x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x848x2048): 77.005
Elapsed time for attention_prob_times_values (48x2048x2048x848): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x848): 88.192

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 899.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x849x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x849x2048): 76.126
Elapsed time for attention_prob_times_values (48x2048x2048x849): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x849): 79.362

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 850.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x850x2048): 0.0044
Elapsed time for attention_key_query_prob (128x2048x39x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x39x2048): 27.431
Elapsed time for attention_prob_times_values (128x2048x2048x39): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x39): 30.288

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 63.875
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x40x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x40x2048): 28.365
Elapsed time for attention_prob_times_values (128x2048x2048x40): 0.0013
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x40): 32.174

Attention duration (in seconds): 0.0028
Attention throughput (in TFLOP/s): 67.837
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0028
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x41x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x41x2048): 28.019
Elapsed time for attention_prob_times_values (128x2048x2048x41): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x41): 31.580

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 67.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x42x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x42x2048): 28.816
Elapsed time for attention_prob_times_values (128x2048x2048x42): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x42): 32.483

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 70.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x43x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x43x2048): 29.275
Elapsed time for attention_prob_times_values (128x2048x2048x43): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x43): 32.629

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 72.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x44x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x44x2048): 30.143
Elapsed time for attention_prob_times_values (128x2048x2048x44): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x44): 34.083

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 75.981
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x45x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x45x2048): 30.577
Elapsed time for attention_prob_times_values (128x2048x2048x45): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x45): 33.935

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 77.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x46x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x46x2048): 31.498
Elapsed time for attention_prob_times_values (128x2048x2048x46): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x46): 35.346

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 81.197
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x47x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x47x2048): 31.907
Elapsed time for attention_prob_times_values (128x2048x2048x47): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x47): 33.545

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 80.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x48x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x48x2048): 32.645
Elapsed time for attention_prob_times_values (128x2048x2048x48): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x48): 37.998

Attention duration (in seconds): 0.0029
Attention throughput (in TFLOP/s): 87.797
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x850x2048): 77.125
Elapsed time for attention_prob_times_values (48x2048x2048x850): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x850): 81.638

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 869.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x851x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x851x2048): 76.172
Elapsed time for attention_prob_times_values (48x2048x2048x851): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x851): 79.672

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 854.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x852x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x852x2048): 77.888
Elapsed time for attention_prob_times_values (48x2048x2048x852): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x852): 79.191

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 862.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x853x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x853x2048): 76.562
Elapsed time for attention_prob_times_values (48x2048x2048x853): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x853): 79.727

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 858.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x854x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x854x2048): 77.359
Elapsed time for attention_prob_times_values (48x2048x2048x854): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x854): 81.850

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 875.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x855x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x855x2048): 76.631
Elapsed time for attention_prob_times_values (48x2048x2048x855): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x855): 79.963

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 862.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x856x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x856x2048): 78.684
Elapsed time for attention_prob_times_values (48x2048x2048x856): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x856): 85.009

Attention duration (in seconds): 0.0084
Attention throughput (in TFLOP/s): 901.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0084
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x857x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x857x2048): 76.040
Elapsed time for attention_prob_times_values (48x2048x2048x857): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x857): 77.714

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 848.849
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x858x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x858x2048): 76.933
Elapsed time for attention_prob_times_values (48x2048x2048x858): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x858): 82.411

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 879.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x859x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x859x2048): 76.354
Elapsed time for attention_prob_times_values (48x2048x2048x859): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x859): 80.239

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 865.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0029
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x49x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x49x2048): 32.902
Elapsed time for attention_prob_times_values (128x2048x2048x49): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x49): 36.669

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 87.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x50x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x50x2048): 33.740
Elapsed time for attention_prob_times_values (128x2048x2048x50): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x50): 37.674

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 91.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x51x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x51x2048): 34.230
Elapsed time for attention_prob_times_values (128x2048x2048x51): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x51): 37.864

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 93.260
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x52x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x52x2048): 35.063
Elapsed time for attention_prob_times_values (128x2048x2048x52): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x52): 39.619

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 97.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x53x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x53x2048): 35.663
Elapsed time for attention_prob_times_values (128x2048x2048x53): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x53): 37.162

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 96.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x54x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x54x2048): 35.032
Elapsed time for attention_prob_times_values (128x2048x2048x54): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x54): 41.122

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 101.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x55x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x55x2048): 32.562
Elapsed time for attention_prob_times_values (128x2048x2048x55): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x55): 40.527

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 98.175
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x56x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x56x2048): 37.755
Elapsed time for attention_prob_times_values (128x2048x2048x56): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x56): 43.854

Attention duration (in seconds): 0.0030
Attention throughput (in TFLOP/s): 111.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0030
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x57x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x57x2048): 37.296
Elapsed time for attention_prob_times_values (128x2048x2048x57): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x57): 42.445

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 110.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x860x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x860x2048): 77.973
Elapsed time for attention_prob_times_values (48x2048x2048x860): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x860): 80.833

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 879.355
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x861x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x861x2048): 76.267
Elapsed time for attention_prob_times_values (48x2048x2048x861): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x861): 80.297

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 867.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x862x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x862x2048): 77.050
Elapsed time for attention_prob_times_values (48x2048x2048x862): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x862): 77.514

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 857.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x863x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x863x2048): 76.416
Elapsed time for attention_prob_times_values (48x2048x2048x863): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x863): 80.284

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 870.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x864x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x864x2048): 90.100
Elapsed time for attention_prob_times_values (48x2048x2048x864): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x864): 87.976

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 990.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x865x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x865x2048): 78.054
Elapsed time for attention_prob_times_values (48x2048x2048x865): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x865): 80.679

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 883.640
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x866x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x866x2048): 78.907
Elapsed time for attention_prob_times_values (48x2048x2048x866): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x866): 82.967

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 901.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x867x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x867x2048): 76.855
Elapsed time for attention_prob_times_values (48x2048x2048x867): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x867): 80.933

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 879.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x868x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x868x2048): 79.286
Elapsed time for attention_prob_times_values (48x2048x2048x868): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x868): 82.989

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 905.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------

Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x58x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x58x2048): 37.878
Elapsed time for attention_prob_times_values (128x2048x2048x58): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x58): 43.578

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 113.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x59x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x59x2048): 34.779
Elapsed time for attention_prob_times_values (128x2048x2048x59): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x59): 43.226

Attention duration (in seconds): 0.0033
Attention throughput (in TFLOP/s): 109.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0033
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x60x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x60x2048): 39.122
Elapsed time for attention_prob_times_values (128x2048x2048x60): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x60): 42.201

Attention duration (in seconds): 0.0032
Attention throughput (in TFLOP/s): 116.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0032
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x61x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x61x2048): 39.454
Elapsed time for attention_prob_times_values (128x2048x2048x61): 0.0015
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x61): 44.403

Attention duration (in seconds): 0.0031
Attention throughput (in TFLOP/s): 121.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0031
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 1984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x62x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x62x2048): 40.245
Elapsed time for attention_prob_times_values (128x2048x2048x62): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x62): 5.224

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 27.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x63x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x63x2048): 40.417
Elapsed time for attention_prob_times_values (128x2048x2048x63): 0.0017
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x63): 39.599

Attention duration (in seconds): 0.0034
Attention throughput (in TFLOP/s): 118.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x64x2048): 0.0012
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x64x2048): 56.772
Elapsed time for attention_prob_times_values (128x2048x2048x64): 0.0014
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x64): 49.867

Attention duration (in seconds): 0.0026
Attention throughput (in TFLOP/s): 159.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x65x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x65x2048): 43.575
Elapsed time for attention_prob_times_values (128x2048x2048x65): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x65): 30.207

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 108.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x66x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x66x2048): 44.291
Elapsed time for attention_prob_times_values (128x2048x2048x66): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x66): 35.168

Attention duration (in seconds): 0.0036
Attention throughput (in TFLOP/s): 120.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0036
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x67x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x67x2048): 43.934
Elapsed time for attention_prob_times_values (128x2048x2048x67): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x67): 34.544

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 119.658

Elapsed time for attention_key_query_prob (48x2048x869x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x869x2048): 77.459
Elapsed time for attention_prob_times_values (48x2048x2048x869): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x869): 80.820

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 884.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x870x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x870x2048): 78.308
Elapsed time for attention_prob_times_values (48x2048x2048x870): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x870): 83.190

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 903.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x871x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x871x2048): 77.148
Elapsed time for attention_prob_times_values (48x2048x2048x871): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x871): 80.904

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 885.147
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x872x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x872x2048): 79.215
Elapsed time for attention_prob_times_values (48x2048x2048x872): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x872): 90.066

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 945.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x873x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x873x2048): 76.798
Elapsed time for attention_prob_times_values (48x2048x2048x873): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x873): 81.005

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 885.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x874x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x874x2048): 77.589
Elapsed time for attention_prob_times_values (48x2048x2048x874): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x874): 83.492

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 904.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x875x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x875x2048): 77.014
Elapsed time for attention_prob_times_values (48x2048x2048x875): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x875): 81.248

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 889.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x876x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x876x2048): 78.207
Elapsed time for attention_prob_times_values (48x2048x2048x876): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x876): 83.513

Attention duration (in seconds): 0.0087
Attention throughput (in TFLOP/s): 909.955
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0087
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x877x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x877x2048): 75.111
Elapsed time for attention_prob_times_values (48x2048x2048x877): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x877): 80.702

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 877.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x878x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x878x2048): 77.809
Elapsed time for attention_prob_times_values (48x2048x2048x878): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x878): 83.539

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 909.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x68x2048): 0.0016
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x68x2048): 44.924
Elapsed time for attention_prob_times_values (128x2048x2048x68): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x68): 33.521

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 119.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x69x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x69x2048): 44.571
Elapsed time for attention_prob_times_values (128x2048x2048x69): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x69): 32.655

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 118.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x70x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x70x2048): 45.543
Elapsed time for attention_prob_times_values (128x2048x2048x70): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x70): 35.005

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 126.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x71x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x71x2048): 45.202
Elapsed time for attention_prob_times_values (128x2048x2048x71): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x71): 34.316

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 125.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x72x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x72x2048): 46.629
Elapsed time for attention_prob_times_values (128x2048x2048x72): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x72): 36.323

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 132.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x73x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x73x2048): 45.290
Elapsed time for attention_prob_times_values (128x2048x2048x73): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x73): 37.010

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 133.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x74x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x74x2048): 46.468
Elapsed time for attention_prob_times_values (128x2048x2048x74): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x74): 38.892

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 140.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x75x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x75x2048): 46.087
Elapsed time for attention_prob_times_values (128x2048x2048x75): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x75): 38.231

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 139.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x76x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x76x2048): 47.413
Elapsed time for attention_prob_times_values (128x2048x2048x76): 0.0020
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x76): 39.855

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 146.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x317x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x317x2048): 59.218
Elapsed time for attention_prob_times_values (320x2048x2048x317): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x317): 65.627

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 1604.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x318x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x318x2048): 60.735
Elapsed time for attention_prob_times_values (320x2048x2048x318): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x318): 67.707

Attention duration (in seconds): 0.0267
Attention throughput (in TFLOP/s): 1654.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0267
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x319x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x319x2048): 58.692
Elapsed time for attention_prob_times_values (320x2048x2048x319): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x319): 66.897

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 1620.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x320x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x320x2048): 78.289
Elapsed time for attention_prob_times_values (320x2048x2048x320): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x320): 80.429

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 2062.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x321x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x321x2048): 62.807
Elapsed time for attention_prob_times_values (320x2048x2048x321): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x321): 58.195

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1575.454
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x322x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x322x2048): 63.514
Elapsed time for attention_prob_times_values (320x2048x2048x322): 0.0142
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x322): 60.740

Attention duration (in seconds): 0.0278
Attention throughput (in TFLOP/s): 1624.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x323x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x323x2048): 62.851
Elapsed time for attention_prob_times_values (320x2048x2048x323): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x323): 58.488

Attention duration (in seconds): 0.0286
Attention throughput (in TFLOP/s): 1589.560
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0286
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x324x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x324x2048): 61.233
Elapsed time for attention_prob_times_values (320x2048x2048x324): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x324): 60.740

Attention duration (in seconds): 0.0285
Attention throughput (in TFLOP/s): 1604.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x325x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x325x2048): 62.669
Elapsed time for attention_prob_times_values (320x2048x2048x325): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x325): 58.006

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 1589.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x879x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x879x2048): 76.829
Elapsed time for attention_prob_times_values (48x2048x2048x879): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x879): 74.692

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 855.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x880x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x880x2048): 79.318
Elapsed time for attention_prob_times_values (48x2048x2048x880): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x880): 85.154

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 929.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x881x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x881x2048): 76.778
Elapsed time for attention_prob_times_values (48x2048x2048x881): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x881): 81.751

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 896.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x882x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x882x2048): 77.485
Elapsed time for attention_prob_times_values (48x2048x2048x882): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x882): 83.957

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 913.576
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x883x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x883x2048): 70.897
Elapsed time for attention_prob_times_values (48x2048x2048x883): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x883): 81.654

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 861.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x884x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x884x2048): 77.912
Elapsed time for attention_prob_times_values (48x2048x2048x884): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x884): 81.213

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 903.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x885x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x885x2048): 76.857
Elapsed time for attention_prob_times_values (48x2048x2048x885): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x885): 82.055

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 902.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x886x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x886x2048): 77.526
Elapsed time for attention_prob_times_values (48x2048x2048x886): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x886): 84.115

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 918.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x887x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x887x2048): 73.031
Elapsed time for attention_prob_times_values (48x2048x2048x887): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x887): 82.231

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 881.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x77x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x77x2048): 45.605
Elapsed time for attention_prob_times_values (128x2048x2048x77): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x77): 36.089

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 137.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x78x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x78x2048): 47.917
Elapsed time for attention_prob_times_values (128x2048x2048x78): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x78): 40.396

Attention duration (in seconds): 0.0038
Attention throughput (in TFLOP/s): 150.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0038
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x79x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x79x2048): 48.085
Elapsed time for attention_prob_times_values (128x2048x2048x79): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x79): 39.628

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 150.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x80x2048): 0.0017
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x80x2048): 49.936
Elapsed time for attention_prob_times_values (128x2048x2048x80): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x80): 38.658

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 152.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x81x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x81x2048): 46.894
Elapsed time for attention_prob_times_values (128x2048x2048x81): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x81): 39.322

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 151.052
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x82x2048): 0.0018
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x82x2048): 47.676
Elapsed time for attention_prob_times_values (128x2048x2048x82): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x82): 42.380

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 159.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x83x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x83x2048): 47.666
Elapsed time for attention_prob_times_values (128x2048x2048x83): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x83): 41.395

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 159.239
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x84x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x84x2048): 48.488
Elapsed time for attention_prob_times_values (128x2048x2048x84): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x84): 43.398

Attention duration (in seconds): 0.0039
Attention throughput (in TFLOP/s): 166.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0039
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x85x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x85x2048): 48.316
Elapsed time for attention_prob_times_values (128x2048x2048x85): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x85): 42.108

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 164.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x86x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x86x2048): 49.263
Elapsed time for attention_prob_times_values (128x2048x2048x86): 0.0021
num_attention_heads: 12, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x888x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x888x2048): 78.473
Elapsed time for attention_prob_times_values (48x2048x2048x888): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x888): 91.389

Attention duration (in seconds): 0.0085
Attention throughput (in TFLOP/s): 963.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0085
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x889x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x889x2048): 76.344
Elapsed time for attention_prob_times_values (48x2048x2048x889): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x889): 82.106

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 903.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x890x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x890x2048): 77.212
Elapsed time for attention_prob_times_values (48x2048x2048x890): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x890): 84.523

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 922.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x891x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x891x2048): 76.520
Elapsed time for attention_prob_times_values (48x2048x2048x891): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x891): 82.048

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 906.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x892x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x892x2048): 77.724
Elapsed time for attention_prob_times_values (48x2048x2048x892): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x892): 85.005

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 930.013
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x893x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x893x2048): 76.196
Elapsed time for attention_prob_times_values (48x2048x2048x893): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x893): 82.525

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 908.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x894x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x894x2048): 76.942
Elapsed time for attention_prob_times_values (48x2048x2048x894): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x894): 84.851

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 926.193
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x895x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x895x2048): 75.941
Elapsed time for attention_prob_times_values (48x2048x2048x895): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x895): 82.678

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 909.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x896x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x896x2048): 85.434
Elapsed time for attention_prob_times_values (48x2048x2048x896): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x896): 94.222

Attention duration (in seconds): 0.0081
Attention throughput (in TFLOP/s): 1030.552
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0081
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x897x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x897x2048): 77.851
Elapsed time for attention_prob_times_values (48x2048x2048x897): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x86): 44.073

Attention duration (in seconds): 0.0040
Attention throughput (in TFLOP/s): 171.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0040
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x87x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x87x2048): 49.353
Elapsed time for attention_prob_times_values (128x2048x2048x87): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x87): 43.175

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 171.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x88x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x88x2048): 50.377
Elapsed time for attention_prob_times_values (128x2048x2048x88): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x88): 43.247

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 174.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x89x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x89x2048): 48.970
Elapsed time for attention_prob_times_values (128x2048x2048x89): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x89): 43.959

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 175.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x90x2048): 0.0019
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x90x2048): 49.831
Elapsed time for attention_prob_times_values (128x2048x2048x90): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x90): 42.987

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 175.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x91x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x91x2048): 49.383
Elapsed time for attention_prob_times_values (128x2048x2048x91): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x91): 44.795

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 180.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x92x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x92x2048): 50.516
Elapsed time for attention_prob_times_values (128x2048x2048x92): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x92): 46.570

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 187.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 2976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x93x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x93x2048): 50.682
Elapsed time for attention_prob_times_values (128x2048x2048x93): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x93): 45.511

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 187.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x94x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x94x2048): 46.996
Elapsed time for attention_prob_times_values (128x2048x2048x94): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x94): 47.374

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 185.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x95x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x95x2048): 51.547
Elapsed time for attention_prob_times_values (128x2048x2048x95): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x95): 46.178

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 193.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x897): 74.242

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 874.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x898x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x898x2048): 78.255
Elapsed time for attention_prob_times_values (48x2048x2048x898): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x898): 72.394

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 866.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x899x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x899x2048): 74.341
Elapsed time for attention_prob_times_values (48x2048x2048x899): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x899): 74.726

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 859.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x900x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x900x2048): 79.471
Elapsed time for attention_prob_times_values (48x2048x2048x900): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x900): 76.630

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 900.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x901x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x901x2048): 78.210
Elapsed time for attention_prob_times_values (48x2048x2048x901): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x901): 74.984

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 884.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x902x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x902x2048): 78.934
Elapsed time for attention_prob_times_values (48x2048x2048x902): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x902): 76.720

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 900.296
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x903x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x903x2048): 78.027
Elapsed time for attention_prob_times_values (48x2048x2048x903): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x903): 70.571

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 858.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x904x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x904x2048): 79.199
Elapsed time for attention_prob_times_values (48x2048x2048x904): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x904): 81.875

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 933.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x905x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x905x2048): 77.486
Elapsed time for attention_prob_times_values (48x2048x2048x905): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x905): 75.224

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 885.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x906x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x906x2048): 78.286
Elapsed time for attention_prob_times_values (48x2048x2048x906): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x906): 73.137

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 878.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x96x2048): 0.0015
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x96x2048): 66.916
Elapsed time for attention_prob_times_values (128x2048x2048x96): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x96): 48.124

Attention duration (in seconds): 0.0037
Attention throughput (in TFLOP/s): 223.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0037
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x97x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x97x2048): 52.493
Elapsed time for attention_prob_times_values (128x2048x2048x97): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x97): 47.227

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 200.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x98x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x98x2048): 53.699
Elapsed time for attention_prob_times_values (128x2048x2048x98): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x98): 49.072

Attention duration (in seconds): 0.0041
Attention throughput (in TFLOP/s): 208.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0041
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x99x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x99x2048): 53.218
Elapsed time for attention_prob_times_values (128x2048x2048x99): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x99): 48.143

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 206.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x100x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x100x2048): 50.079
Elapsed time for attention_prob_times_values (128x2048x2048x100): 0.0021
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x100): 49.962

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 206.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x101x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x101x2048): 50.868
Elapsed time for attention_prob_times_values (128x2048x2048x101): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x101): 44.520

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 197.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x102x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x102x2048): 54.563
Elapsed time for attention_prob_times_values (128x2048x2048x102): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x102): 44.921

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 206.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x103x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x103x2048): 54.001
Elapsed time for attention_prob_times_values (128x2048x2048x103): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x103): 44.069

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 204.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x104x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x104x2048): 51.800
Elapsed time for attention_prob_times_values (128x2048x2048x104): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x104): 46.749

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 208.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x105x2048): 0.0021
========================================================================================================================
num_attention_heads: 12, hidden_size: 10884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x907x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x907x2048): 77.854
Elapsed time for attention_prob_times_values (48x2048x2048x907): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x907): 75.305

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 890.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x908x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x908x2048): 79.082
Elapsed time for attention_prob_times_values (48x2048x2048x908): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x908): 77.195

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 909.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x909x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x909x2048): 77.714
Elapsed time for attention_prob_times_values (48x2048x2048x909): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x909): 75.338

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 891.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x910x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x910x2048): 78.555
Elapsed time for attention_prob_times_values (48x2048x2048x910): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x910): 76.927

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 906.679
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x911x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x911x2048): 77.728
Elapsed time for attention_prob_times_values (48x2048x2048x911): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x911): 74.763

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 889.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x912x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x912x2048): 79.784
Elapsed time for attention_prob_times_values (48x2048x2048x912): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x912): 82.973

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 950.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x913x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x913x2048): 77.914
Elapsed time for attention_prob_times_values (48x2048x2048x913): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x913): 75.904

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 899.624
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x914x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x914x2048): 73.244
Elapsed time for attention_prob_times_values (48x2048x2048x914): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x914): 77.607

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 882.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x915x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x915x2048): 77.400
Elapsed time for attention_prob_times_values (48x2048x2048x915): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x915): 74.949

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 892.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 10992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x916x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x105x2048): 53.816
Elapsed time for attention_prob_times_values (128x2048x2048x105): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x105): 50.328

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 222.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x106x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x106x2048): 54.790
Elapsed time for attention_prob_times_values (128x2048x2048x106): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x106): 52.442

Attention duration (in seconds): 0.0042
Attention throughput (in TFLOP/s): 231.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0042
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x107x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x107x2048): 54.132
Elapsed time for attention_prob_times_values (128x2048x2048x107): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x107): 51.042

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 228.228
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x108x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x108x2048): 55.467
Elapsed time for attention_prob_times_values (128x2048x2048x108): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x108): 51.678

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 234.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x109x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x109x2048): 54.218
Elapsed time for attention_prob_times_values (128x2048x2048x109): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x109): 51.673

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 233.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x110x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x110x2048): 56.244
Elapsed time for attention_prob_times_values (128x2048x2048x110): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x110): 53.900

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 244.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x111x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x111x2048): 55.886
Elapsed time for attention_prob_times_values (128x2048x2048x111): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x111): 52.422

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 241.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x112x2048): 0.0021
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x112x2048): 57.851
Elapsed time for attention_prob_times_values (128x2048x2048x112): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x112): 54.266

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 252.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x113x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x113x2048): 55.503
Elapsed time for attention_prob_times_values (128x2048x2048x113): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x113): 52.682

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 244.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x114x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x114x2048): 56.894
Elapsed time for attention_prob_times_values (128x2048x2048x114): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x114): 55.265

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 255.808
MLP duration (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (320x2048x326x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x326x2048): 63.179
Elapsed time for attention_prob_times_values (320x2048x2048x326): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x326): 60.214

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 1632.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x327x2048): 0.0141
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x327x2048): 62.293
Elapsed time for attention_prob_times_values (320x2048x2048x327): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x327): 56.816

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1577.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x328x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x328x2048): 61.949
Elapsed time for attention_prob_times_values (320x2048x2048x328): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x328): 72.654

Attention duration (in seconds): 0.0263
Attention throughput (in TFLOP/s): 1780.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x329x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x329x2048): 60.311
Elapsed time for attention_prob_times_values (320x2048x2048x329): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x329): 57.477

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1571.748
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x330x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x330x2048): 61.240
Elapsed time for attention_prob_times_values (320x2048x2048x330): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x330): 61.328

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 1641.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x331x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x331x2048): 61.472
Elapsed time for attention_prob_times_values (320x2048x2048x331): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x331): 58.423

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1609.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x332x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x332x2048): 61.570
Elapsed time for attention_prob_times_values (320x2048x2048x332): 0.0143
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x332): 62.311

Attention duration (in seconds): 0.0288
Attention throughput (in TFLOP/s): 1668.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0288
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x333x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x333x2048): 61.968
Elapsed time for attention_prob_times_values (320x2048x2048x333): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x333): 57.084

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1605.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x334x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x334x2048): 61.905
Elapsed time for attention_prob_times_values (320x2048x2048x334): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x334): 62.247

Attention duration (in seconds): 0.0289
Attention throughput (in TFLOP/s): 1681.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0289
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x335x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x335x2048): 60.622
Elapsed time for attention_prob_times_values (320x2048x2048x335): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x335): 58.635

Attention duration (in seconds): 0.0302
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x916x2048): 78.667
Elapsed time for attention_prob_times_values (48x2048x2048x916): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x916): 77.879

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 918.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x917x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x917x2048): 77.484
Elapsed time for attention_prob_times_values (48x2048x2048x917): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x917): 75.427

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 897.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x918x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x918x2048): 78.175
Elapsed time for attention_prob_times_values (48x2048x2048x918): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x918): 77.604

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 915.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x919x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x919x2048): 75.544
Elapsed time for attention_prob_times_values (48x2048x2048x919): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x919): 76.313

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 893.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x920x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x920x2048): 79.354
Elapsed time for attention_prob_times_values (48x2048x2048x920): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x920): 83.382

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 958.027
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x921x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x921x2048): 76.217
Elapsed time for attention_prob_times_values (48x2048x2048x921): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x921): 76.456

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 900.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x922x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x922x2048): 77.767
Elapsed time for attention_prob_times_values (48x2048x2048x922): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x922): 78.500

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 922.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x923x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x923x2048): 77.169
Elapsed time for attention_prob_times_values (48x2048x2048x923): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x923): 76.596

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 908.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x924x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x924x2048): 78.350
Elapsed time for attention_prob_times_values (48x2048x2048x924): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x924): 78.809

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 929.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x925x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x925x2048): 76.830
Elapsed time for attention_prob_times_values (48x2048x2048x925): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x925): 72.741

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 884.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x115x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x115x2048): 56.381
Elapsed time for attention_prob_times_values (128x2048x2048x115): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x115): 49.833

Attention duration (in seconds): 0.0047
Attention throughput (in TFLOP/s): 243.033
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x116x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x116x2048): 57.649
Elapsed time for attention_prob_times_values (128x2048x2048x116): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x116): 56.089

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 262.971
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x117x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x117x2048): 57.051
Elapsed time for attention_prob_times_values (128x2048x2048x117): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x117): 53.571

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 257.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x118x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x118x2048): 58.016
Elapsed time for attention_prob_times_values (128x2048x2048x118): 0.0022
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x118): 56.462

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 268.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x119x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x119x2048): 56.881
Elapsed time for attention_prob_times_values (128x2048x2048x119): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x119): 55.046

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 264.006
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x120x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x120x2048): 59.401
Elapsed time for attention_prob_times_values (128x2048x2048x120): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x120): 56.822

Attention duration (in seconds): 0.0044
Attention throughput (in TFLOP/s): 275.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x121x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x121x2048): 57.556
Elapsed time for attention_prob_times_values (128x2048x2048x121): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x121): 43.536

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 237.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x122x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x122x2048): 58.613
Elapsed time for attention_prob_times_values (128x2048x2048x122): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x122): 58.179

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 281.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 3936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x123x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x123x2048): 57.214
Elapsed time for attention_prob_times_values (128x2048x2048x123): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x123): 44.883

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 243.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x926x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x926x2048): 78.178
Elapsed time for attention_prob_times_values (48x2048x2048x926): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x926): 78.868

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 930.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x927x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x927x2048): 77.239
Elapsed time for attention_prob_times_values (48x2048x2048x927): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x927): 76.909

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 914.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x928x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x928x2048): 89.896
Elapsed time for attention_prob_times_values (48x2048x2048x928): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x928): 84.831

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 1036.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x929x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x929x2048): 77.113
Elapsed time for attention_prob_times_values (48x2048x2048x929): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x929): 76.240

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 911.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x930x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x930x2048): 78.938
Elapsed time for attention_prob_times_values (48x2048x2048x930): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x930): 78.736

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 938.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x931x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x931x2048): 78.944
Elapsed time for attention_prob_times_values (48x2048x2048x931): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x931): 77.116

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 929.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x932x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x932x2048): 80.648
Elapsed time for attention_prob_times_values (48x2048x2048x932): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x932): 78.898

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 950.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x933x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x933x2048): 75.180
Elapsed time for attention_prob_times_values (48x2048x2048x933): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x933): 77.244

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 909.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x934x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x934x2048): 79.602
Elapsed time for attention_prob_times_values (48x2048x2048x934): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x934): 74.896

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 921.906
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 32, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x124x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x124x2048): 59.447
Elapsed time for attention_prob_times_values (128x2048x2048x124): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x124): 58.833

Attention duration (in seconds): 0.0045
Attention throughput (in TFLOP/s): 288.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0045
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x125x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x125x2048): 58.519
Elapsed time for attention_prob_times_values (128x2048x2048x125): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x125): 43.929

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 246.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x126x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x126x2048): 59.202
Elapsed time for attention_prob_times_values (128x2048x2048x126): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x126): 59.638

Attention duration (in seconds): 0.0046
Attention throughput (in TFLOP/s): 293.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0046
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x127x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x127x2048): 55.551
Elapsed time for attention_prob_times_values (128x2048x2048x127): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x127): 44.010

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 244.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x128x2048): 0.0020
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x128x2048): 69.045
Elapsed time for attention_prob_times_values (128x2048x2048x128): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x128): 58.709

Attention duration (in seconds): 0.0043
Attention throughput (in TFLOP/s): 317.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0043
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x129x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x129x2048): 57.980
Elapsed time for attention_prob_times_values (128x2048x2048x129): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x129): 40.508

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 239.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x130x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x130x2048): 59.138
Elapsed time for attention_prob_times_values (128x2048x2048x130): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x130): 47.922

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 268.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x131x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x131x2048): 56.338
Elapsed time for attention_prob_times_values (128x2048x2048x131): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x131): 42.860

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 247.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x132x2048): 0.0023
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x132x2048): 60.383
Elapsed time for attention_prob_times_values (128x2048x2048x132): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x132): 48.676

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 276.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x133x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x133x2048): 56.401
Elapsed time for attention_prob_times_values (128x2048x2048x133): 0.0030
--------
Elapsed time for attention_key_query_prob (48x2048x935x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x935x2048): 78.718
Elapsed time for attention_prob_times_values (48x2048x2048x935): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x935): 77.315

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 932.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x936x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x936x2048): 80.544
Elapsed time for attention_prob_times_values (48x2048x2048x936): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x936): 84.546

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 987.384
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x937x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x937x2048): 78.161
Elapsed time for attention_prob_times_values (48x2048x2048x937): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x937): 77.419

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 931.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x938x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x938x2048): 75.655
Elapsed time for attention_prob_times_values (48x2048x2048x938): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x938): 79.251

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 928.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x939x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x939x2048): 78.371
Elapsed time for attention_prob_times_values (48x2048x2048x939): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x939): 77.260

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 934.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x940x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x940x2048): 79.956
Elapsed time for attention_prob_times_values (48x2048x2048x940): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x940): 76.915

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 942.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x941x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x941x2048): 75.941
Elapsed time for attention_prob_times_values (48x2048x2048x941): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x941): 77.700

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 923.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x942x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x942x2048): 79.118
Elapsed time for attention_prob_times_values (48x2048x2048x942): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x942): 79.648

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 955.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x943x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x943x2048): 78.091
Elapsed time for attention_prob_times_values (48x2048x2048x943): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x943): 77.873

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 939.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x944x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x944x2048): 79.805
Elapsed time for attention_prob_times_values (48x2048x2048x944): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x944): 85.704

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 996.964
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x133): 46.868

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 263.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x134x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x134x2048): 60.451
Elapsed time for attention_prob_times_values (128x2048x2048x134): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x134): 49.224

Attention duration (in seconds): 0.0053
Attention throughput (in TFLOP/s): 281.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0053
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x135x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x135x2048): 57.217
Elapsed time for attention_prob_times_values (128x2048x2048x135): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x135): 45.730

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 265.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x136x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x136x2048): 61.631
Elapsed time for attention_prob_times_values (128x2048x2048x136): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x136): 45.732

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 275.648
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x137x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x137x2048): 59.653
Elapsed time for attention_prob_times_values (128x2048x2048x137): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x137): 47.688

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 279.927
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x138x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x138x2048): 60.628
Elapsed time for attention_prob_times_values (128x2048x2048x138): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x138): 50.475

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 292.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x139x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x139x2048): 60.094
Elapsed time for attention_prob_times_values (128x2048x2048x139): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x139): 48.273

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 286.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x140x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x140x2048): 61.507
Elapsed time for attention_prob_times_values (128x2048x2048x140): 0.0029
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x140): 51.386

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 300.960
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x141x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x141x2048): 60.918
Elapsed time for attention_prob_times_values (128x2048x2048x141): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x141): 49.077

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 293.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x142x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x142x2048): 62.082
Elapsed time for attention_prob_times_values (128x2048x2048x142): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x142): 51.412

Attention duration (in seconds): 0.0054
Attention throughput (in TFLOP/s): 305.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x945x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x945x2048): 75.944
Elapsed time for attention_prob_times_values (48x2048x2048x945): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x945): 76.888

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 922.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x946x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x946x2048): 76.337
Elapsed time for attention_prob_times_values (48x2048x2048x946): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x946): 79.743

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 942.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x947x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x947x2048): 78.173
Elapsed time for attention_prob_times_values (48x2048x2048x947): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x947): 78.223

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 946.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x948x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x948x2048): 79.760
Elapsed time for attention_prob_times_values (48x2048x2048x948): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x948): 79.709

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 965.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x949x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x949x2048): 78.361
Elapsed time for attention_prob_times_values (48x2048x2048x949): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x949): 74.890

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 928.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x950x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x950x2048): 79.074
Elapsed time for attention_prob_times_values (48x2048x2048x950): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x950): 77.948

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 952.510
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x951x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x951x2048): 78.209
Elapsed time for attention_prob_times_values (48x2048x2048x951): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x951): 74.674

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 927.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x952x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x952x2048): 80.209
Elapsed time for attention_prob_times_values (48x2048x2048x952): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x952): 81.489

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 982.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x953x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x953x2048): 77.756
Elapsed time for attention_prob_times_values (48x2048x2048x953): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x953): 78.175

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 948.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1619.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0302
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x336x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x336x2048): 63.367
Elapsed time for attention_prob_times_values (320x2048x2048x336): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x336): 75.463

Attention duration (in seconds): 0.0262
Attention throughput (in TFLOP/s): 1877.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0262
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 26960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x337x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x337x2048): 61.284
Elapsed time for attention_prob_times_values (320x2048x2048x337): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x337): 58.870

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1641.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x338x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x338x2048): 62.014
Elapsed time for attention_prob_times_values (320x2048x2048x338): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x338): 62.889

Attention duration (in seconds): 0.0291
Attention throughput (in TFLOP/s): 1711.476
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0291
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x339x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x339x2048): 61.738
Elapsed time for attention_prob_times_values (320x2048x2048x339): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x339): 59.851

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1670.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x340x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x340x2048): 61.974
Elapsed time for attention_prob_times_values (320x2048x2048x340): 0.0145
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x340): 62.783

Attention duration (in seconds): 0.0293
Attention throughput (in TFLOP/s): 1719.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0293
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x341x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x341x2048): 61.272
Elapsed time for attention_prob_times_values (320x2048x2048x341): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x341): 60.474

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1682.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x342x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x342x2048): 61.407
Elapsed time for attention_prob_times_values (320x2048x2048x342): 0.0148
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x342): 62.084

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1711.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x343x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x343x2048): 60.221
Elapsed time for attention_prob_times_values (320x2048x2048x343): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x343): 60.242

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1674.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x344x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x344x2048): 63.668
Elapsed time for attention_prob_times_values (320x2048x2048x344): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x344): 76.879

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 1941.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0265
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x143x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x143x2048): 61.499
Elapsed time for attention_prob_times_values (128x2048x2048x143): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x143): 49.479

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 299.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x144x2048): 0.0024
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x144x2048): 63.518
Elapsed time for attention_prob_times_values (128x2048x2048x144): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x144): 49.898

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 307.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x145x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x145x2048): 61.236
Elapsed time for attention_prob_times_values (128x2048x2048x145): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x145): 50.380

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 305.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x146x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x146x2048): 59.873
Elapsed time for attention_prob_times_values (128x2048x2048x146): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x146): 50.676

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 305.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x147x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x147x2048): 61.756
Elapsed time for attention_prob_times_values (128x2048x2048x147): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x147): 50.781

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 311.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x148x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x148x2048): 63.274
Elapsed time for attention_prob_times_values (128x2048x2048x148): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x148): 53.593

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 326.433
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x149x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x149x2048): 62.463
Elapsed time for attention_prob_times_values (128x2048x2048x149): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x149): 51.405

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 318.994
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x150x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x150x2048): 63.670
Elapsed time for attention_prob_times_values (128x2048x2048x150): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x150): 54.065

Attention duration (in seconds): 0.0055
Attention throughput (in TFLOP/s): 332.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0055
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x151x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x151x2048): 63.107
Elapsed time for attention_prob_times_values (128x2048x2048x151): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x151): 52.035

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 326.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x152x2048): 0.0025
Elapsed time for attention_key_query_prob (2048x2048x13x2048): 0.0216
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x13x2048): 10.329
Elapsed time for attention_prob_times_values (2048x2048x2048x13): 0.0206
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x13): 10.816

Attention duration (in seconds): 0.0423
Attention throughput (in TFLOP/s): 79.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0423
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x14x2048): 0.0219
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x14x2048): 10.980
Elapsed time for attention_prob_times_values (2048x2048x2048x14): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x14): 12.094

Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 92.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0418
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x15x2048): 0.0222
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x15x2048): 11.593
Elapsed time for attention_prob_times_values (2048x2048x2048x15): 0.0202
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x15): 12.740

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 103.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x16x2048): 0.0221
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x16x2048): 12.449
Elapsed time for attention_prob_times_values (2048x2048x2048x16): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x16): 13.961

Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 118.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0418
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x17x2048): 0.0223
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x17x2048): 13.111
Elapsed time for attention_prob_times_values (2048x2048x2048x17): 0.0203
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x17): 14.412

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 130.440
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x18x2048): 0.0227
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x18x2048): 13.594
Elapsed time for attention_prob_times_values (2048x2048x2048x18): 0.0197
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x18): 15.684

Attention duration (in seconds): 0.0425
Attention throughput (in TFLOP/s): 145.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0425
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x19x2048): 0.0234
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x19x2048): 13.935
Elapsed time for attention_prob_times_values (2048x2048x2048x19): 0.0202
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x19): 16.145

Attention duration (in seconds): 0.0436
Attention throughput (in TFLOP/s): 157.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0436
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x20x2048): 0.0234
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x20x2048): 14.713
Elapsed time for attention_prob_times_values (2048x2048x2048x20): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x20): 17.279

Attention duration (in seconds): 0.0432
Attention throughput (in TFLOP/s): 174.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0432
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x21x2048): 0.0226
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x21x2048): 15.970
Elapsed time for attention_prob_times_values (2048x2048x2048x21): 0.0204
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x21): 17.681

Attention duration (in seconds): 0.0430
Attention throughput (in TFLOP/s): 192.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0430
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x22x2048): 0.0231
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x22x2048): 16.357
Elapsed time for attention_prob_times_values (2048x2048x2048x22): 0.0199
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x22): 18.977

Attention duration (in seconds): 0.0430
num_attention_heads: 12, hidden_size: 11448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x954x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x954x2048): 78.628
Elapsed time for attention_prob_times_values (48x2048x2048x954): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x954): 80.530

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 969.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x955x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x955x2048): 77.904
Elapsed time for attention_prob_times_values (48x2048x2048x955): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x955): 78.438

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 953.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x956x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x956x2048): 79.503
Elapsed time for attention_prob_times_values (48x2048x2048x956): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x956): 80.610

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 976.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x957x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x957x2048): 77.480
Elapsed time for attention_prob_times_values (48x2048x2048x957): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x957): 78.332

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 951.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x958x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x958x2048): 78.758
Elapsed time for attention_prob_times_values (48x2048x2048x958): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x958): 80.638

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 974.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x959x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x959x2048): 77.963
Elapsed time for attention_prob_times_values (48x2048x2048x959): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x959): 75.591

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 939.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x960x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x960x2048): 89.219
Elapsed time for attention_prob_times_values (48x2048x2048x960): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x960): 83.805

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 1058.738
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x961x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x961x2048): 78.299
Elapsed time for attention_prob_times_values (48x2048x2048x961): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x961): 79.139

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 965.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x962x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x962x2048): 79.137
Elapsed time for attention_prob_times_values (48x2048x2048x962): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x962): 80.938

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 982.210
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x963x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x963x2048): 79.428
Elapsed time for attention_prob_times_values (48x2048x2048x963): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x152x2048): 64.733
Elapsed time for attention_prob_times_values (128x2048x2048x152): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x152): 48.150

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 317.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x153x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x153x2048): 62.641
Elapsed time for attention_prob_times_values (128x2048x2048x153): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x153): 52.328

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 329.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x154x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x154x2048): 60.383
Elapsed time for attention_prob_times_values (128x2048x2048x154): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x154): 53.490

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 329.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x155x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x155x2048): 63.339
Elapsed time for attention_prob_times_values (128x2048x2048x155): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x155): 53.254

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 338.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x156x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x156x2048): 64.758
Elapsed time for attention_prob_times_values (128x2048x2048x156): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x156): 56.138

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 353.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x157x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x157x2048): 60.949
Elapsed time for attention_prob_times_values (128x2048x2048x157): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x157): 53.737

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 337.340
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x158x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x158x2048): 65.359
Elapsed time for attention_prob_times_values (128x2048x2048x158): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x158): 56.331

Attention duration (in seconds): 0.0056
Attention throughput (in TFLOP/s): 359.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0056
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x159x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x159x2048): 64.407
Elapsed time for attention_prob_times_values (128x2048x2048x159): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x159): 52.510

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 345.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x160x2048): 0.0022
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x160x2048): 78.907
Elapsed time for attention_prob_times_values (128x2048x2048x160): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x160): 56.247

Attention duration (in seconds): 0.0052
Attention throughput (in TFLOP/s): 394.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0052
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x161x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x161x2048): 63.514
Elapsed time for attention_prob_times_values (128x2048x2048x161): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x161): 55.409

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 356.962
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x963): 77.350

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 962.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x964x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x964x2048): 78.148
Elapsed time for attention_prob_times_values (48x2048x2048x964): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x964): 81.183

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 979.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x965x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x965x2048): 76.292
Elapsed time for attention_prob_times_values (48x2048x2048x965): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x965): 78.991

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 955.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x966x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x966x2048): 80.184
Elapsed time for attention_prob_times_values (48x2048x2048x966): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x966): 81.178

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 993.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x967x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x967x2048): 79.178
Elapsed time for attention_prob_times_values (48x2048x2048x967): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x967): 78.478

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 972.085
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x968x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x968x2048): 81.160
Elapsed time for attention_prob_times_values (48x2048x2048x968): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x968): 87.339

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1038.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x969x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x969x2048): 78.508
Elapsed time for attention_prob_times_values (48x2048x2048x969): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x969): 79.530

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 976.277
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x970x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x970x2048): 79.369
Elapsed time for attention_prob_times_values (48x2048x2048x970): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x970): 81.396

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 993.952
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x971x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x971x2048): 78.734
Elapsed time for attention_prob_times_values (48x2048x2048x971): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x971): 79.714

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 980.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x972x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x972x2048): 80.076
Elapsed time for attention_prob_times_values (48x2048x2048x972): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x972): 81.918

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1003.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x162x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x162x2048): 64.938
Elapsed time for attention_prob_times_values (128x2048x2048x162): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x162): 57.506

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 369.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x163x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x163x2048): 60.698
Elapsed time for attention_prob_times_values (128x2048x2048x163): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x163): 53.353

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 346.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x164x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x164x2048): 65.957
Elapsed time for attention_prob_times_values (128x2048x2048x164): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x164): 58.345

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 379.249
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x165x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x165x2048): 64.667
Elapsed time for attention_prob_times_values (128x2048x2048x165): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x165): 56.660

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 371.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x166x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x166x2048): 65.955
Elapsed time for attention_prob_times_values (128x2048x2048x166): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x166): 58.644

Attention duration (in seconds): 0.0057
Attention throughput (in TFLOP/s): 384.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0057
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x167x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x167x2048): 65.253
Elapsed time for attention_prob_times_values (128x2048x2048x167): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x167): 57.264

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 379.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x168x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x168x2048): 67.254
Elapsed time for attention_prob_times_values (128x2048x2048x168): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x168): 54.803

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 377.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x169x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x169x2048): 64.875
Elapsed time for attention_prob_times_values (128x2048x2048x169): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x169): 55.285

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 374.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x170x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x170x2048): 63.956
Elapsed time for attention_prob_times_values (128x2048x2048x170): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x170): 59.813

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 390.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 12, hidden_size: 11676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x973x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x973x2048): 78.749
Elapsed time for attention_prob_times_values (48x2048x2048x973): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x973): 80.174

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 985.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x974x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x974x2048): 78.781
Elapsed time for attention_prob_times_values (48x2048x2048x974): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x974): 81.911

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 997.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x975x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x975x2048): 78.782
Elapsed time for attention_prob_times_values (48x2048x2048x975): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x975): 79.844

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 985.485
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x976x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x976x2048): 80.951
Elapsed time for attention_prob_times_values (48x2048x2048x976): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x976): 88.553

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 1051.978
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x977x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x977x2048): 78.259
Elapsed time for attention_prob_times_values (48x2048x2048x977): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x977): 78.617

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 976.483
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x978x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x978x2048): 79.169
Elapsed time for attention_prob_times_values (48x2048x2048x978): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x978): 82.211

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1005.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x979x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x979x2048): 78.423
Elapsed time for attention_prob_times_values (48x2048x2048x979): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x979): 80.036

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 988.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x980x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x980x2048): 80.005
Elapsed time for attention_prob_times_values (48x2048x2048x980): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x980): 79.559

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 996.026
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x981x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x981x2048): 78.378
Elapsed time for attention_prob_times_values (48x2048x2048x981): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x981): 80.869

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 994.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x982x2048): 0.0051
num_attention_heads: 32, hidden_size: 5472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x171x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x171x2048): 63.646
Elapsed time for attention_prob_times_values (128x2048x2048x171): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x171): 57.864

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 384.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x172x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x172x2048): 67.026
Elapsed time for attention_prob_times_values (128x2048x2048x172): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x172): 60.484

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 405.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x173x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x173x2048): 65.938
Elapsed time for attention_prob_times_values (128x2048x2048x173): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x173): 58.414

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 396.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x174x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x174x2048): 67.253
Elapsed time for attention_prob_times_values (128x2048x2048x174): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x174): 61.073

Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 412.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0058
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x175x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x175x2048): 66.554
Elapsed time for attention_prob_times_values (128x2048x2048x175): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x175): 59.058

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 404.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x176x2048): 0.0027
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x176x2048): 69.676
Elapsed time for attention_prob_times_values (128x2048x2048x176): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x176): 60.075

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 419.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x177x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x177x2048): 63.150
Elapsed time for attention_prob_times_values (128x2048x2048x177): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x177): 55.650

Attention duration (in seconds): 0.0064
Attention throughput (in TFLOP/s): 386.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x178x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x178x2048): 67.626
Elapsed time for attention_prob_times_values (128x2048x2048x178): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x178): 62.210

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 425.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x179x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x179x2048): 66.800
Elapsed time for attention_prob_times_values (128x2048x2048x179): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x179): 60.345

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 418.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x180x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x180x2048): 68.455
Elapsed time for attention_prob_times_values (128x2048x2048x180): 0.0034
========================================================================================================================
num_attention_heads: 80, hidden_size: 27600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x345x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x345x2048): 61.202
Elapsed time for attention_prob_times_values (320x2048x2048x345): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x345): 57.948

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1664.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x346x2048): 0.0151
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x346x2048): 61.617
Elapsed time for attention_prob_times_values (320x2048x2048x346): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x346): 62.230

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1735.755
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x347x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x347x2048): 60.672
Elapsed time for attention_prob_times_values (320x2048x2048x347): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x347): 60.465

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1702.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x348x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x348x2048): 63.092
Elapsed time for attention_prob_times_values (320x2048x2048x348): 0.0149
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x348): 62.763

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1773.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 27920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x349x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x349x2048): 59.796
Elapsed time for attention_prob_times_values (320x2048x2048x349): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x349): 60.574

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1701.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x350x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x350x2048): 62.484
Elapsed time for attention_prob_times_values (320x2048x2048x350): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x350): 62.747

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 1774.747
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x351x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x351x2048): 61.160
Elapsed time for attention_prob_times_values (320x2048x2048x351): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x351): 61.239

Attention duration (in seconds): 0.0308
Attention throughput (in TFLOP/s): 1739.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0308
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x352x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x352x2048): 79.195
Elapsed time for attention_prob_times_values (320x2048x2048x352): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x352): 72.629

Attention duration (in seconds): 0.0249
Attention throughput (in TFLOP/s): 2159.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x353x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x353x2048): 64.487
Elapsed time for attention_prob_times_values (320x2048x2048x353): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x353): 63.198

Attention duration (in seconds): 0.0297
Attention throughput (in TFLOP/s): 1824.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0297
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x354x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x982x2048): 78.137
Elapsed time for attention_prob_times_values (48x2048x2048x982): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x982): 82.309

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1002.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x983x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x983x2048): 78.572
Elapsed time for attention_prob_times_values (48x2048x2048x983): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x983): 80.507

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 995.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x984x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x984x2048): 78.274
Elapsed time for attention_prob_times_values (48x2048x2048x984): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x984): 88.721

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 1042.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x985x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x985x2048): 74.457
Elapsed time for attention_prob_times_values (48x2048x2048x985): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x985): 80.628

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 971.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x986x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x986x2048): 77.175
Elapsed time for attention_prob_times_values (48x2048x2048x986): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x986): 80.593

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 989.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x987x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x987x2048): 78.064
Elapsed time for attention_prob_times_values (48x2048x2048x987): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x987): 80.659

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 997.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x988x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x988x2048): 79.810
Elapsed time for attention_prob_times_values (48x2048x2048x988): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x988): 80.608

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1008.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x989x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x989x2048): 78.310
Elapsed time for attention_prob_times_values (48x2048x2048x989): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x989): 80.952

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1002.263
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x990x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x990x2048): 78.918
Elapsed time for attention_prob_times_values (48x2048x2048x990): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x990): 82.918

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1019.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x991x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x991x2048): 78.186
Elapsed time for attention_prob_times_values (48x2048x2048x991): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x991): 81.479

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1006.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x180): 56.436

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 409.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x181x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x181x2048): 67.307
Elapsed time for attention_prob_times_values (128x2048x2048x181): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x181): 60.814

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 425.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x182x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x182x2048): 68.665
Elapsed time for attention_prob_times_values (128x2048x2048x182): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x182): 59.348

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 425.776
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x183x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x183x2048): 67.838
Elapsed time for attention_prob_times_values (128x2048x2048x183): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x183): 61.330

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 432.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x184x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x184x2048): 68.575
Elapsed time for attention_prob_times_values (128x2048x2048x184): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x184): 61.310

Attention duration (in seconds): 0.0061
Attention throughput (in TFLOP/s): 436.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x185x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x185x2048): 67.633
Elapsed time for attention_prob_times_values (128x2048x2048x185): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x185): 59.764

Attention duration (in seconds): 0.0063
Attention throughput (in TFLOP/s): 430.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x186x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x186x2048): 68.842
Elapsed time for attention_prob_times_values (128x2048x2048x186): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x186): 64.332

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 453.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 5984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x187x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x187x2048): 68.054
Elapsed time for attention_prob_times_values (128x2048x2048x187): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x187): 62.629

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 446.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x188x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x188x2048): 69.662
Elapsed time for attention_prob_times_values (128x2048x2048x188): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x188): 64.583

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 460.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x189x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x189x2048): 68.362
Elapsed time for attention_prob_times_values (128x2048x2048x189): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x189): 63.060

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 453.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x992x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x992x2048): 91.254
Elapsed time for attention_prob_times_values (48x2048x2048x992): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x992): 90.354

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 1146.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x993x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x993x2048): 79.817
Elapsed time for attention_prob_times_values (48x2048x2048x993): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x993): 81.401

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1018.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x994x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x994x2048): 80.646
Elapsed time for attention_prob_times_values (48x2048x2048x994): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x994): 83.282

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1036.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x995x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x995x2048): 79.527
Elapsed time for attention_prob_times_values (48x2048x2048x995): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x995): 81.238

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1017.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x996x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x996x2048): 81.170
Elapsed time for attention_prob_times_values (48x2048x2048x996): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x996): 83.458

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1042.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x997x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x997x2048): 74.805
Elapsed time for attention_prob_times_values (48x2048x2048x997): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x997): 81.103

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 987.124
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x998x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x998x2048): 80.040
Elapsed time for attention_prob_times_values (48x2048x2048x998): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x998): 83.452

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1037.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 11988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x999x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x999x2048): 79.028
Elapsed time for attention_prob_times_values (48x2048x2048x999): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x999): 81.691

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1020.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1000x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1000x2048): 75.304
Elapsed time for attention_prob_times_values (48x2048x2048x1000): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1000): 90.110

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1043.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1
========================================================================================================================
num_attention_heads: 32, hidden_size: 6080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x190x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x190x2048): 69.829
Elapsed time for attention_prob_times_values (128x2048x2048x190): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x190): 65.255

Attention duration (in seconds): 0.0060
Attention throughput (in TFLOP/s): 468.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0060
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x191x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x191x2048): 69.064
Elapsed time for attention_prob_times_values (128x2048x2048x191): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x191): 63.310

Attention duration (in seconds): 0.0062
Attention throughput (in TFLOP/s): 460.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0062
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x192x2048): 0.0026
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x192x2048): 80.257
Elapsed time for attention_prob_times_values (128x2048x2048x192): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x192): 61.535

Attention duration (in seconds): 0.0059
Attention throughput (in TFLOP/s): 487.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x193x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x193x2048): 67.911
Elapsed time for attention_prob_times_values (128x2048x2048x193): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x193): 53.568

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 421.121
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x194x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x194x2048): 69.341
Elapsed time for attention_prob_times_values (128x2048x2048x194): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x194): 56.375

Attention duration (in seconds): 0.0067
Attention throughput (in TFLOP/s): 439.212
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x195x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x195x2048): 68.295
Elapsed time for attention_prob_times_values (128x2048x2048x195): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x195): 53.144

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 424.023
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x196x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x196x2048): 68.730
Elapsed time for attention_prob_times_values (128x2048x2048x196): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x196): 54.888

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 434.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x197x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x197x2048): 68.606
Elapsed time for attention_prob_times_values (128x2048x2048x197): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x197): 53.537

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 430.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x198x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x198x2048): 70.142
Elapsed time for attention_prob_times_values (128x2048x2048x198): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x198): 56.153

Attention duration (in seconds): 0.0068
Attention throughput (in TFLOP/s): 448.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0068
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x199x2048): 0.0031


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1001x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1001x2048): 77.765
Elapsed time for attention_prob_times_values (48x2048x2048x1001): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1001): 81.586

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1013.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1002x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1002x2048): 79.339
Elapsed time for attention_prob_times_values (48x2048x2048x1002): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1002): 83.761

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1038.365
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1003x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1003x2048): 78.736
Elapsed time for attention_prob_times_values (48x2048x2048x1003): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1003): 81.513

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1021.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1004x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1004x2048): 80.433
Elapsed time for attention_prob_times_values (48x2048x2048x1004): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1004): 83.879

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1048.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1005x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1005x2048): 78.823
Elapsed time for attention_prob_times_values (48x2048x2048x1005): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1005): 81.612

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1024.659
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1006x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1006x2048): 79.626
Elapsed time for attention_prob_times_values (48x2048x2048x1006): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1006): 83.896

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1044.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1007x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1007x2048): 75.632
Elapsed time for attention_prob_times_values (48x2048x2048x1007): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1007): 81.704

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1005.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1008x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1008x2048): 81.103
Elapsed time for attention_prob_times_values (48x2048x2048x1008): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1008): 90.962

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 1098.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1009x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1009x2048): 78.282
Elapsed time for attention_prob_times_values (48x2048x2048x1009): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1009): 81.973

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1027.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1010x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1010x2048): 79.120
Elapsed time for attention_prob_times_values (48x2048x2048x1010): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1010): 84.288

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1047.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1011x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1011x2048): 73.286
Elapsed time for attention_prob_times_values (48x2048x2048x1011): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1011): 81.962

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 994.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1012x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1012x2048): 80.372
Elapsed time for attention_prob_times_values (48x2048x2048x1012): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1012): 84.405

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 1058.831
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1013x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1013x2048): 78.679
Elapsed time for attention_prob_times_values (48x2048x2048x1013): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1013): 81.548

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1030.823
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1014x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1014x2048): 79.179
Elapsed time for attention_prob_times_values (48x2048x2048x1014): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1014): 82.099

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 1038.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1015x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1015x2048): 77.907
Elapsed time for attention_prob_times_values (48x2048x2048x1015): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1015): 82.191

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1031.456
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1016x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1016x2048): 77.754
Elapsed time for attention_prob_times_values (48x2048x2048x1016): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1016): 91.459

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 1084.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1017x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1017x2048): 77.728
Elapsed time for attention_prob_times_values (48x2048x2048x1017): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1017): 82.345

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1033.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1018x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1018x2048): 76.540
Elapsed time for attention_prob_times_values (48x2048x2048x1018): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1018): 84.734

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1039.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1019x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1019x2048): 78.841
Elapsed time for attention_prob_times_values (48x2048x2048x1019): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1019): 82.326

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1042.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x199x2048): 68.966
Elapsed time for attention_prob_times_values (128x2048x2048x199): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x199): 53.372

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 434.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x200x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x200x2048): 71.639
Elapsed time for attention_prob_times_values (128x2048x2048x200): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x200): 52.416

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 438.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x201x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x201x2048): 68.844
Elapsed time for attention_prob_times_values (128x2048x2048x201): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x201): 52.817

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 435.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x202x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x202x2048): 69.979
Elapsed time for attention_prob_times_values (128x2048x2048x202): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x202): 56.271

Attention duration (in seconds): 0.0070
Attention throughput (in TFLOP/s): 456.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x203x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x203x2048): 69.255
Elapsed time for attention_prob_times_values (128x2048x2048x203): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x203): 53.299

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 442.372
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x204x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x204x2048): 71.038
Elapsed time for attention_prob_times_values (128x2048x2048x204): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x204): 56.775

Attention duration (in seconds): 0.0069
Attention throughput (in TFLOP/s): 465.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x205x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x205x2048): 69.741
Elapsed time for attention_prob_times_values (128x2048x2048x205): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x205): 51.251

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 437.587
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x206x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x206x2048): 71.059
Elapsed time for attention_prob_times_values (128x2048x2048x206): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x206): 53.901

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 455.932
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x207x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x207x2048): 67.373
Elapsed time for attention_prob_times_values (128x2048x2048x207): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x207): 54.122

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 448.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x208x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x208x2048): 73.401
Elapsed time for attention_prob_times_values (128x2048x2048x208): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x208): 52.858

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 460.938
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x354x2048): 65.031
Elapsed time for attention_prob_times_values (320x2048x2048x354): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x354): 62.220

Attention duration (in seconds): 0.0299
Attention throughput (in TFLOP/s): 1822.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0299
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x355x2048): 0.0153
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x355x2048): 62.444
Elapsed time for attention_prob_times_values (320x2048x2048x355): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x355): 62.761

Attention duration (in seconds): 0.0304
Attention throughput (in TFLOP/s): 1798.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x356x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x356x2048): 65.812
Elapsed time for attention_prob_times_values (320x2048x2048x356): 0.0150
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x356): 63.918

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 1868.515
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0295
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x357x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x357x2048): 64.224
Elapsed time for attention_prob_times_values (320x2048x2048x357): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x357): 63.076

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 1838.741
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x358x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x358x2048): 61.271
Elapsed time for attention_prob_times_values (320x2048x2048x358): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x358): 59.436

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 1747.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x359x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x359x2048): 60.218
Elapsed time for attention_prob_times_values (320x2048x2048x359): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x359): 61.946

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1773.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x360x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x360x2048): 65.476
Elapsed time for attention_prob_times_values (320x2048x2048x360): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x360): 80.183

Attention duration (in seconds): 0.0268
Attention throughput (in TFLOP/s): 2099.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x361x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x361x2048): 62.081
Elapsed time for attention_prob_times_values (320x2048x2048x361): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x361): 61.815

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 1809.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0313
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 28960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x362x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x362x2048): 63.825
Elapsed time for attention_prob_times_values (320x2048x2048x362): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x362): 64.513

Attention duration (in seconds): 0.0303
Attention throughput (in TFLOP/s): 1878.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x363x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x363x2048): 61.077
Elapsed time for attention_prob_times_values (320x2048x2048x363): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x363): 61.967

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1806.147
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1020x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1020x2048): 83.513
Elapsed time for attention_prob_times_values (48x2048x2048x1020): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1020): 84.696

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1089.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1021x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1021x2048): 78.478
Elapsed time for attention_prob_times_values (48x2048x2048x1021): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1021): 82.389

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1042.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1022x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1022x2048): 76.042
Elapsed time for attention_prob_times_values (48x2048x2048x1022): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1022): 84.883

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1040.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1023x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1023x2048): 78.361
Elapsed time for attention_prob_times_values (48x2048x2048x1023): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1023): 82.428

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1043.516
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1024x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1024x2048): 89.360
Elapsed time for attention_prob_times_values (48x2048x2048x1024): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1024): 94.085

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 1191.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1025x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1025x2048): 79.138
Elapsed time for attention_prob_times_values (48x2048x2048x1025): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1025): 74.512

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 998.723
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1026x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1026x2048): 80.808
Elapsed time for attention_prob_times_values (48x2048x2048x1026): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1026): 77.567

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1030.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1027x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1027x2048): 79.217
Elapsed time for attention_prob_times_values (48x2048x2048x1027): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1027): 75.378

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1006.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1028x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1028x2048): 86.011
Elapsed time for attention_prob_times_values (48x2048x2048x1028): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1028): 74.064

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1038.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1029x2048): 0.0053
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x209x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x209x2048): 66.482
Elapsed time for attention_prob_times_values (128x2048x2048x209): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x209): 52.228

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 440.572
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x210x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x210x2048): 68.109
Elapsed time for attention_prob_times_values (128x2048x2048x210): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x210): 57.552

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 471.802
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x211x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x211x2048): 70.390
Elapsed time for attention_prob_times_values (128x2048x2048x211): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x211): 55.027

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 469.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x212x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x212x2048): 69.640
Elapsed time for attention_prob_times_values (128x2048x2048x212): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x212): 58.189

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 483.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x213x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x213x2048): 70.727
Elapsed time for attention_prob_times_values (128x2048x2048x213): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x213): 55.149

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 474.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x214x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x214x2048): 72.160
Elapsed time for attention_prob_times_values (128x2048x2048x214): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x214): 58.510

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 496.780
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x215x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x215x2048): 68.270
Elapsed time for attention_prob_times_values (128x2048x2048x215): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x215): 55.744

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 473.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x216x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x216x2048): 68.879
Elapsed time for attention_prob_times_values (128x2048x2048x216): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x216): 56.324

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 480.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 6944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x217x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x217x2048): 70.951
Elapsed time for attention_prob_times_values (128x2048x2048x217): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x217): 52.852

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 471.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1029x2048): 78.305
Elapsed time for attention_prob_times_values (48x2048x2048x1029): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1029): 74.845

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 999.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1030x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1030x2048): 80.604
Elapsed time for attention_prob_times_values (48x2048x2048x1030): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1030): 77.932

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1035.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1031x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1031x2048): 75.616
Elapsed time for attention_prob_times_values (48x2048x2048x1031): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1031): 75.790

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 990.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1032x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1032x2048): 81.087
Elapsed time for attention_prob_times_values (48x2048x2048x1032): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1032): 80.712

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1059.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1033x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1033x2048): 78.844
Elapsed time for attention_prob_times_values (48x2048x2048x1033): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1033): 73.898

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 999.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1034x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1034x2048): 79.553
Elapsed time for attention_prob_times_values (48x2048x2048x1034): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1034): 78.081

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1033.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1035x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1035x2048): 79.097
Elapsed time for attention_prob_times_values (48x2048x2048x1035): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1035): 76.088

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1018.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1036x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1036x2048): 80.924
Elapsed time for attention_prob_times_values (48x2048x2048x1036): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1036): 78.446

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1046.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1037x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1037x2048): 79.182
Elapsed time for attention_prob_times_values (48x2048x2048x1037): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1037): 75.993

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1020.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1038x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1038x2048): 79.340
Elapsed time for attention_prob_times_values (48x2048x2048x1038): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1038): 78.477

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1038.730
MLP duration (in seconds): 0.0000
num_attention_heads: 32, hidden_size: 6976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x218x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x218x2048): 72.190
Elapsed time for attention_prob_times_values (128x2048x2048x218): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x218): 59.258

Attention duration (in seconds): 0.0072
Attention throughput (in TFLOP/s): 508.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0072
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x219x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x219x2048): 71.418
Elapsed time for attention_prob_times_values (128x2048x2048x219): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x219): 54.350

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 484.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x220x2048): 0.0032
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x220x2048): 72.954
Elapsed time for attention_prob_times_values (128x2048x2048x220): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x220): 58.142

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 509.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x221x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x221x2048): 71.702
Elapsed time for attention_prob_times_values (128x2048x2048x221): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x221): 57.021

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 502.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x222x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x222x2048): 73.133
Elapsed time for attention_prob_times_values (128x2048x2048x222): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x222): 58.138

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 514.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x223x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x223x2048): 72.294
Elapsed time for attention_prob_times_values (128x2048x2048x223): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x223): 57.366

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 509.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x224x2048): 0.0029
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x224x2048): 82.900
Elapsed time for attention_prob_times_values (128x2048x2048x224): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x224): 57.302

Attention duration (in seconds): 0.0071
Attention throughput (in TFLOP/s): 542.114
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0071
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x225x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x225x2048): 70.594
Elapsed time for attention_prob_times_values (128x2048x2048x225): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x225): 58.347

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 513.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x226x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x226x2048): 72.160
Elapsed time for attention_prob_times_values (128x2048x2048x226): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x226): 60.900

Attention duration (in seconds): 0.0073
Attention throughput (in TFLOP/s): 532.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0073
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x227x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x227x2048): 70.906
Elapsed time for attention_prob_times_values (128x2048x2048x227): 0.0042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1039x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1039x2048): 78.832
Elapsed time for attention_prob_times_values (48x2048x2048x1039): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1039): 76.080

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1020.221
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1040x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1040x2048): 77.573
Elapsed time for attention_prob_times_values (48x2048x2048x1040): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1040): 78.583

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1029.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1041x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1041x2048): 78.416
Elapsed time for attention_prob_times_values (48x2048x2048x1041): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1041): 76.469

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1022.019
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1042x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1042x2048): 76.506
Elapsed time for attention_prob_times_values (48x2048x2048x1042): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1042): 78.826

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1025.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1043x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1043x2048): 78.697
Elapsed time for attention_prob_times_values (48x2048x2048x1043): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1043): 76.642

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1026.818
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1044x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1044x2048): 80.326
Elapsed time for attention_prob_times_values (48x2048x2048x1044): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1044): 78.963

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1053.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1045x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1045x2048): 78.925
Elapsed time for attention_prob_times_values (48x2048x2048x1045): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1045): 76.833

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1031.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1046x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1046x2048): 79.610
Elapsed time for attention_prob_times_values (48x2048x2048x1046): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1046): 78.923

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1050.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1047x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1047x2048): 78.946
Elapsed time for attention_prob_times_values (48x2048x2048x1047): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1047): 76.616

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1031.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x227): 58.589

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 519.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x228x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x228x2048): 72.670
Elapsed time for attention_prob_times_values (128x2048x2048x228): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x228): 61.135

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 539.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x229x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x229x2048): 71.209
Elapsed time for attention_prob_times_values (128x2048x2048x229): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x229): 58.943

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 526.062
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x230x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x230x2048): 70.335
Elapsed time for attention_prob_times_values (128x2048x2048x230): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x230): 59.424

Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 527.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0077
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x231x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x231x2048): 71.836
Elapsed time for attention_prob_times_values (128x2048x2048x231): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x231): 59.376

Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 534.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0076
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x232x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x232x2048): 73.746
Elapsed time for attention_prob_times_values (128x2048x2048x232): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x232): 59.855

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 545.149
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x233x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x233x2048): 70.873
Elapsed time for attention_prob_times_values (128x2048x2048x233): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x233): 56.307

Attention duration (in seconds): 0.0080
Attention throughput (in TFLOP/s): 519.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0080
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x234x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x234x2048): 72.634
Elapsed time for attention_prob_times_values (128x2048x2048x234): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x234): 62.302

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 557.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x235x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x235x2048): 71.236
Elapsed time for attention_prob_times_values (128x2048x2048x235): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x235): 59.807

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 542.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x236x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x236x2048): 70.290
Elapsed time for attention_prob_times_values (128x2048x2048x236): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x236): 60.866

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 546.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 12, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1048x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1048x2048): 80.509
Elapsed time for attention_prob_times_values (48x2048x2048x1048): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1048): 83.573

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 1089.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1049x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1049x2048): 78.539
Elapsed time for attention_prob_times_values (48x2048x2048x1049): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1049): 77.130

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1034.568
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1050x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1050x2048): 79.218
Elapsed time for attention_prob_times_values (48x2048x2048x1050): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1050): 79.172

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1053.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1051x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1051x2048): 78.633
Elapsed time for attention_prob_times_values (48x2048x2048x1051): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1051): 77.277

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1038.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1052x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1052x2048): 80.052
Elapsed time for attention_prob_times_values (48x2048x2048x1052): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1052): 77.042

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1046.500
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1053x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1053x2048): 78.744
Elapsed time for attention_prob_times_values (48x2048x2048x1053): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1053): 77.367

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1041.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1054x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1054x2048): 79.449
Elapsed time for attention_prob_times_values (48x2048x2048x1054): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1054): 75.193

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1031.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1055x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1055x2048): 78.820
Elapsed time for attention_prob_times_values (48x2048x2048x1055): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1055): 77.165

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1042.120
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1056x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1056x2048): 88.905
Elapsed time for attention_prob_times_values (48x2048x2048x1056): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1056): 85.148

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1163.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1057x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1057x2048): 80.014
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x364x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x364x2048): 62.389
Elapsed time for attention_prob_times_values (320x2048x2048x364): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x364): 63.802

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1857.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x365x2048): 0.0154
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x365x2048): 63.438
Elapsed time for attention_prob_times_values (320x2048x2048x365): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x365): 62.446

Attention duration (in seconds): 0.0311
Attention throughput (in TFLOP/s): 1857.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0311
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x366x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x366x2048): 63.314
Elapsed time for attention_prob_times_values (320x2048x2048x366): 0.0151
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x366): 65.129

Attention duration (in seconds): 0.0306
Attention throughput (in TFLOP/s): 1900.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x367x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x367x2048): 61.372
Elapsed time for attention_prob_times_values (320x2048x2048x367): 0.0154
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x367): 63.821

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1856.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x368x2048): 0.0152
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x368x2048): 64.920
Elapsed time for attention_prob_times_values (320x2048x2048x368): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x368): 82.065

Attention duration (in seconds): 0.0273
Attention throughput (in TFLOP/s): 2156.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0273
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x369x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x369x2048): 62.226
Elapsed time for attention_prob_times_values (320x2048x2048x369): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x369): 62.997

Attention duration (in seconds): 0.0316
Attention throughput (in TFLOP/s): 1867.506
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0316
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x370x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x370x2048): 62.754
Elapsed time for attention_prob_times_values (320x2048x2048x370): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x370): 63.573

Attention duration (in seconds): 0.0315
Attention throughput (in TFLOP/s): 1888.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0315
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x371x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x371x2048): 62.812
Elapsed time for attention_prob_times_values (320x2048x2048x371): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x371): 64.109

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1902.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x372x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x372x2048): 63.402
Elapsed time for attention_prob_times_values (320x2048x2048x372): 0.0153
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x372): 65.359

Attention duration (in seconds): 0.0310
Attention throughput (in TFLOP/s): 1934.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 32, hidden_size: 7584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x237x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x237x2048): 71.943
Elapsed time for attention_prob_times_values (128x2048x2048x237): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x237): 59.799

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 549.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x238x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x238x2048): 73.405
Elapsed time for attention_prob_times_values (128x2048x2048x238): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x238): 62.950

Attention duration (in seconds): 0.0075
Attention throughput (in TFLOP/s): 571.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0075
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x239x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x239x2048): 72.210
Elapsed time for attention_prob_times_values (128x2048x2048x239): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x239): 60.419

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 557.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x240x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x240x2048): 56.625
Elapsed time for attention_prob_times_values (128x2048x2048x240): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x240): 62.987

Attention duration (in seconds): 0.0086
Attention throughput (in TFLOP/s): 506.916
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0086
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x241x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x241x2048): 54.093
Elapsed time for attention_prob_times_values (128x2048x2048x241): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x241): 58.275

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 478.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x242x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x242x2048): 55.155
Elapsed time for attention_prob_times_values (128x2048x2048x242): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x242): 63.713

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 506.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x243x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x243x2048): 54.392
Elapsed time for attention_prob_times_values (128x2048x2048x243): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x243): 61.258

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 495.184
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x244x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x244x2048): 55.544
Elapsed time for attention_prob_times_values (128x2048x2048x244): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x244): 64.484

Attention duration (in seconds): 0.0088
Attention throughput (in TFLOP/s): 514.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x245x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x245x2048): 54.242
Elapsed time for attention_prob_times_values (128x2048x2048x245): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x245): 61.766

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 499.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x246x2048): 0.0048
Elapsed time for attention_prob_times_values (48x2048x2048x1057): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1057): 77.610

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1054.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1058x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1058x2048): 78.703
Elapsed time for attention_prob_times_values (48x2048x2048x1058): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1058): 79.704

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1061.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1059x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1059x2048): 80.112
Elapsed time for attention_prob_times_values (48x2048x2048x1059): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1059): 75.505

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1042.509
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1060x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1060x2048): 81.796
Elapsed time for attention_prob_times_values (48x2048x2048x1060): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1060): 77.531

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1068.462
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1061x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1061x2048): 80.130
Elapsed time for attention_prob_times_values (48x2048x2048x1061): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1061): 77.725

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1060.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1062x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1062x2048): 78.541
Elapsed time for attention_prob_times_values (48x2048x2048x1062): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1062): 79.905

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1065.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1063x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1063x2048): 75.848
Elapsed time for attention_prob_times_values (48x2048x2048x1063): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1063): 77.848

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1033.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1064x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1064x2048): 81.225
Elapsed time for attention_prob_times_values (48x2048x2048x1064): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1064): 83.404

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1108.477
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1065x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1065x2048): 79.367
Elapsed time for attention_prob_times_values (48x2048x2048x1065): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1065): 77.967

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1060.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1066x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1066x2048): 80.074
Elapsed time for attention_prob_times_values (48x2048x2048x1066): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1066): 80.142

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 1080.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1067x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1067x2048): 77.114
Elapsed time for attention_prob_times_values (48x2048x2048x1067): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1067): 78.009

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1047.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1068x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1068x2048): 78.681
Elapsed time for attention_prob_times_values (48x2048x2048x1068): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1068): 80.407

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1074.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1069x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1069x2048): 75.469
Elapsed time for attention_prob_times_values (48x2048x2048x1069): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1069): 78.121

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1038.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1070x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1070x2048): 80.662
Elapsed time for attention_prob_times_values (48x2048x2048x1070): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1070): 79.060

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1081.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1071x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1071x2048): 79.709
Elapsed time for attention_prob_times_values (48x2048x2048x1071): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1071): 75.563

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1051.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1072x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1072x2048): 81.692
Elapsed time for attention_prob_times_values (48x2048x2048x1072): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1072): 81.754

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1108.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1073x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1073x2048): 76.182
Elapsed time for attention_prob_times_values (48x2048x2048x1073): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1073): 78.502

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1049.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1074x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1074x2048): 77.250
Elapsed time for attention_prob_times_values (48x2048x2048x1074): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1074): 80.816

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1073.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1075x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1075x2048): 79.356
Elapsed time for attention_prob_times_values (48x2048x2048x1075): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1075): 77.085

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1063.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x246x2048): 55.218
Elapsed time for attention_prob_times_values (128x2048x2048x246): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x246): 61.253

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 504.561
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x247x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x247x2048): 55.227
Elapsed time for attention_prob_times_values (128x2048x2048x247): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x247): 62.120

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 509.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x248x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x248x2048): 56.517
Elapsed time for attention_prob_times_values (128x2048x2048x248): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x248): 63.309

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 522.554
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 7968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x249x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x249x2048): 54.405
Elapsed time for attention_prob_times_values (128x2048x2048x249): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x249): 62.606

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 511.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x250x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x250x2048): 54.941
Elapsed time for attention_prob_times_values (128x2048x2048x250): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x250): 65.285

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 525.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x251x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x251x2048): 54.696
Elapsed time for attention_prob_times_values (128x2048x2048x251): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x251): 60.362

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 507.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x252x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x252x2048): 55.668
Elapsed time for attention_prob_times_values (128x2048x2048x252): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x252): 65.674

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 534.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x253x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x253x2048): 54.630
Elapsed time for attention_prob_times_values (128x2048x2048x253): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x253): 60.137

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 509.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x254x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x254x2048): 55.460
Elapsed time for attention_prob_times_values (128x2048x2048x254): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x254): 66.329

Attention duration (in seconds): 0.0090
Attention throughput (in TFLOP/s): 539.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0090
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x255x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x255x2048): 51.175
Elapsed time for attention_prob_times_values (128x2048x2048x255): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x255): 65.680

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 515.948
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 210.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0430
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x23x2048): 0.0226
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x23x2048): 17.492
Elapsed time for attention_prob_times_values (2048x2048x2048x23): 0.0215
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x23): 18.376

Attention duration (in seconds): 0.0441
Attention throughput (in TFLOP/s): 224.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0441
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x24x2048): 0.0231
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x24x2048): 17.849
Elapsed time for attention_prob_times_values (2048x2048x2048x24): 0.0200
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x24): 20.648

Attention duration (in seconds): 0.0431
Attention throughput (in TFLOP/s): 248.913
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0431
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x25x2048): 0.0251
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x25x2048): 17.078
Elapsed time for attention_prob_times_values (2048x2048x2048x25): 0.0206
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x25): 20.875

Attention duration (in seconds): 0.0457
Attention throughput (in TFLOP/s): 253.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0457
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x26x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x26x2048): 18.836
Elapsed time for attention_prob_times_values (2048x2048x2048x26): 0.0205
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x26): 21.839

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 283.174
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0442
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x27x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x27x2048): 19.608
Elapsed time for attention_prob_times_values (2048x2048x2048x27): 0.0210
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x27): 22.111

Attention duration (in seconds): 0.0446
Attention throughput (in TFLOP/s): 301.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x28x2048): 0.0235
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x28x2048): 20.489
Elapsed time for attention_prob_times_values (2048x2048x2048x28): 0.0204
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x28): 23.523

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 328.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x29x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x29x2048): 21.011
Elapsed time for attention_prob_times_values (2048x2048x2048x29): 0.0210
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x29): 23.720

Attention duration (in seconds): 0.0447
Attention throughput (in TFLOP/s): 345.393
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x30x2048): 0.0241
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x30x2048): 21.396
Elapsed time for attention_prob_times_values (2048x2048x2048x30): 0.0208
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x30): 24.827

Attention duration (in seconds): 0.0448
Attention throughput (in TFLOP/s): 367.740
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0448
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x31x2048): 0.0236
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x31x2048): 22.606
Elapsed time for attention_prob_times_values (2048x2048x2048x31): 0.0213
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x31): 25.032

Attention duration (in seconds): 0.0448
Attention throughput (in TFLOP/s): 391.996
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0448
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (48x2048x1076x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1076x2048): 77.183
Elapsed time for attention_prob_times_values (48x2048x2048x1076): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1076): 80.991

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1075.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1077x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1077x2048): 79.682
Elapsed time for attention_prob_times_values (48x2048x2048x1077): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1077): 76.782

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1065.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1078x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1078x2048): 78.680
Elapsed time for attention_prob_times_values (48x2048x2048x1078): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1078): 81.048

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1088.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1079x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1079x2048): 79.631
Elapsed time for attention_prob_times_values (48x2048x2048x1079): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1079): 78.780

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1080.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1080x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1080x2048): 80.910
Elapsed time for attention_prob_times_values (48x2048x2048x1080): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1080): 85.951

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1138.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1081x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1081x2048): 79.128
Elapsed time for attention_prob_times_values (48x2048x2048x1081): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1081): 78.995

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1080.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1082x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1082x2048): 79.985
Elapsed time for attention_prob_times_values (48x2048x2048x1082): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1082): 81.276

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1102.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 12996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1083x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1083x2048): 79.254
Elapsed time for attention_prob_times_values (48x2048x2048x1083): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1083): 79.141

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1084.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1084x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1084x2048): 80.894
Elapsed time for attention_prob_times_values (48x2048x2048x1084): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1084): 78.751

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1093.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1085x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1085x2048): 79.242
Elapsed time for attention_prob_times_values (48x2048x2048x1085): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1085): 79.212

Attention duration (in seconds): 0.0110
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x256x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x256x2048): 78.323
Elapsed time for attention_prob_times_values (128x2048x2048x256): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x256): 71.080

Attention duration (in seconds): 0.0074
Attention throughput (in TFLOP/s): 670.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x257x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x257x2048): 57.819
Elapsed time for attention_prob_times_values (128x2048x2048x257): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x257): 52.675

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 497.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x258x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x258x2048): 58.665
Elapsed time for attention_prob_times_values (128x2048x2048x258): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x258): 57.905

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 528.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x259x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x259x2048): 55.581
Elapsed time for attention_prob_times_values (128x2048x2048x259): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x259): 53.854

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 497.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x260x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x260x2048): 59.168
Elapsed time for attention_prob_times_values (128x2048x2048x260): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x260): 58.600

Attention duration (in seconds): 0.0095
Attention throughput (in TFLOP/s): 537.302
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0095
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x261x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x261x2048): 57.878
Elapsed time for attention_prob_times_values (128x2048x2048x261): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x261): 54.469

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 513.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x262x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x262x2048): 58.451
Elapsed time for attention_prob_times_values (128x2048x2048x262): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x262): 58.759

Attention duration (in seconds): 0.0096
Attention throughput (in TFLOP/s): 538.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0096
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x263x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x263x2048): 56.172
Elapsed time for attention_prob_times_values (128x2048x2048x263): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x263): 55.014

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 512.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x264x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x264x2048): 58.935
Elapsed time for attention_prob_times_values (128x2048x2048x264): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x264): 55.372

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 528.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Attention throughput (in TFLOP/s): 1086.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1086x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1086x2048): 80.123
Elapsed time for attention_prob_times_values (48x2048x2048x1086): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1086): 81.537

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1109.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1087x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1087x2048): 79.605
Elapsed time for attention_prob_times_values (48x2048x2048x1087): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1087): 79.221

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1090.992
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1088x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1088x2048): 91.507
Elapsed time for attention_prob_times_values (48x2048x2048x1088): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1088): 88.128

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 1234.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1089x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1089x2048): 80.860
Elapsed time for attention_prob_times_values (48x2048x2048x1089): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1089): 79.432

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1102.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1090x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1090x2048): 78.753
Elapsed time for attention_prob_times_values (48x2048x2048x1090): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1090): 81.252

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1101.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1091x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1091x2048): 78.539
Elapsed time for attention_prob_times_values (48x2048x2048x1091): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1091): 79.632

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1090.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1092x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1092x2048): 81.297
Elapsed time for attention_prob_times_values (48x2048x2048x1092): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1092): 82.057

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1126.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1093x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1093x2048): 80.562
Elapsed time for attention_prob_times_values (48x2048x2048x1093): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1093): 79.803

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1107.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1094x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1094x2048): 81.371
Elapsed time for attention_prob_times_values (48x2048x2048x1094): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1094): 82.127

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1129.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 80, hidden_size: 29840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x373x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x373x2048): 62.336
Elapsed time for attention_prob_times_values (320x2048x2048x373): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x373): 63.865

Attention duration (in seconds): 0.0317
Attention throughput (in TFLOP/s): 1901.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0317
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 29920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x374x2048): 0.0157
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x374x2048): 63.937
Elapsed time for attention_prob_times_values (320x2048x2048x374): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x374): 66.103

Attention duration (in seconds): 0.0309
Attention throughput (in TFLOP/s): 1964.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0309
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x375x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x375x2048): 62.524
Elapsed time for attention_prob_times_values (320x2048x2048x375): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x375): 63.953

Attention duration (in seconds): 0.0318
Attention throughput (in TFLOP/s): 1915.688
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x376x2048): 0.0155
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x376x2048): 65.255
Elapsed time for attention_prob_times_values (320x2048x2048x376): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x376): 83.556

Attention duration (in seconds): 0.0275
Attention throughput (in TFLOP/s): 2225.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x377x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x377x2048): 62.914
Elapsed time for attention_prob_times_values (320x2048x2048x377): 0.0164
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x377): 61.738

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 1897.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x378x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x378x2048): 62.525
Elapsed time for attention_prob_times_values (320x2048x2048x378): 0.0152
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x378): 66.826

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1972.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x379x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x379x2048): 62.102
Elapsed time for attention_prob_times_values (320x2048x2048x379): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x379): 63.662

Attention duration (in seconds): 0.0324
Attention throughput (in TFLOP/s): 1924.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0324
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x380x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x380x2048): 64.291
Elapsed time for attention_prob_times_values (320x2048x2048x380): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x380): 65.739

Attention duration (in seconds): 0.0314
Attention throughput (in TFLOP/s): 1994.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0314
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x381x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x381x2048): 61.257
Elapsed time for attention_prob_times_values (320x2048x2048x381): 0.0275
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x381): 37.221

Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 1424.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0442
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x382x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x382x2048): 62.766
num_attention_heads: 32, hidden_size: 8480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x265x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x265x2048): 50.729
Elapsed time for attention_prob_times_values (128x2048x2048x265): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x265): 54.211

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 486.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x266x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x266x2048): 55.104
Elapsed time for attention_prob_times_values (128x2048x2048x266): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x266): 56.499

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 519.571
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x267x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x267x2048): 55.664
Elapsed time for attention_prob_times_values (128x2048x2048x267): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x267): 57.189

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 527.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x268x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x268x2048): 58.131
Elapsed time for attention_prob_times_values (128x2048x2048x268): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x268): 60.384

Attention duration (in seconds): 0.0097
Attention throughput (in TFLOP/s): 555.338
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0097
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x269x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x269x2048): 57.304
Elapsed time for attention_prob_times_values (128x2048x2048x269): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x269): 57.642

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 540.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x270x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x270x2048): 56.113
Elapsed time for attention_prob_times_values (128x2048x2048x270): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x270): 59.154

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 543.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x271x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x271x2048): 57.548
Elapsed time for attention_prob_times_values (128x2048x2048x271): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x271): 55.722

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 536.122
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x272x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x272x2048): 56.820
Elapsed time for attention_prob_times_values (128x2048x2048x272): 0.0040
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x272): 73.273

Attention duration (in seconds): 0.0091
Attention throughput (in TFLOP/s): 608.057
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0091
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x273x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x273x2048): 54.731
Elapsed time for attention_prob_times_values (128x2048x2048x273): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x273): 58.112

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 537.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x274x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x274x2048): 57.795
Elapsed time for attention_prob_times_values (128x2048x2048x274): 0.0048
========================================================================================================================
num_attention_heads: 12, hidden_size: 13140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1095x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1095x2048): 80.355
Elapsed time for attention_prob_times_values (48x2048x2048x1095): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1095): 74.031

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1065.943
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1096x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1096x2048): 82.482
Elapsed time for attention_prob_times_values (48x2048x2048x1096): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1096): 87.347

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 1174.574
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1097x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1097x2048): 79.984
Elapsed time for attention_prob_times_values (48x2048x2048x1097): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1097): 80.143

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1109.317
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1098x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1098x2048): 80.740
Elapsed time for attention_prob_times_values (48x2048x2048x1098): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1098): 82.409

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1131.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1099x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1099x2048): 80.078
Elapsed time for attention_prob_times_values (48x2048x2048x1099): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1099): 80.348

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1113.267
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1100x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1100x2048): 81.623
Elapsed time for attention_prob_times_values (48x2048x2048x1100): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1100): 82.592

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1140.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1101x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1101x2048): 79.958
Elapsed time for attention_prob_times_values (48x2048x2048x1101): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1101): 80.439

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1114.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1102x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1102x2048): 80.971
Elapsed time for attention_prob_times_values (48x2048x2048x1102): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1102): 81.901

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1133.065
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1103x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1103x2048): 80.069
Elapsed time for attention_prob_times_values (48x2048x2048x1103): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1103): 77.701

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1098.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1104x2048): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x274): 61.137

Attention duration (in seconds): 0.0099
Attention throughput (in TFLOP/s): 568.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0099
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x275x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x275x2048): 56.989
Elapsed time for attention_prob_times_values (128x2048x2048x275): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x275): 57.806

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 550.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x276x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x276x2048): 58.310
Elapsed time for attention_prob_times_values (128x2048x2048x276): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x276): 59.900

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 568.784
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x277x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x277x2048): 57.685
Elapsed time for attention_prob_times_values (128x2048x2048x277): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x277): 58.072

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 558.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x278x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x278x2048): 56.554
Elapsed time for attention_prob_times_values (128x2048x2048x278): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x278): 61.079

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 568.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x279x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x279x2048): 57.965
Elapsed time for attention_prob_times_values (128x2048x2048x279): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x279): 59.488

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 570.651
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x280x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x280x2048): 57.352
Elapsed time for attention_prob_times_values (128x2048x2048x280): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x280): 76.268

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 638.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 8992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x281x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x281x2048): 57.216
Elapsed time for attention_prob_times_values (128x2048x2048x281): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x281): 59.844

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 572.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x282x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x282x2048): 57.527
Elapsed time for attention_prob_times_values (128x2048x2048x282): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x282): 62.884

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 589.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x283x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x283x2048): 57.470
Elapsed time for attention_prob_times_values (128x2048x2048x283): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x283): 60.401

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 579.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1104x2048): 81.833
Elapsed time for attention_prob_times_values (48x2048x2048x1104): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1104): 88.490

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1185.129
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1105x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1105x2048): 77.032
Elapsed time for attention_prob_times_values (48x2048x2048x1105): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1105): 80.773

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1100.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1106x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1106x2048): 80.518
Elapsed time for attention_prob_times_values (48x2048x2048x1106): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1106): 83.035

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1141.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1107x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1107x2048): 79.810
Elapsed time for attention_prob_times_values (48x2048x2048x1107): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1107): 80.961

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1123.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1108x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1108x2048): 81.468
Elapsed time for attention_prob_times_values (48x2048x2048x1108): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1108): 83.203

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1151.288
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1109x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1109x2048): 80.057
Elapsed time for attention_prob_times_values (48x2048x2048x1109): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1109): 80.494

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1123.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1110x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1110x2048): 80.674
Elapsed time for attention_prob_times_values (48x2048x2048x1110): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1110): 83.180

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1147.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1111x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1111x2048): 79.768
Elapsed time for attention_prob_times_values (48x2048x2048x1111): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1111): 81.131

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1127.785
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1112x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1112x2048): 82.228
Elapsed time for attention_prob_times_values (48x2048x2048x1112): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1112): 88.643

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 1197.079
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1113x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1113x2048): 79.651
Elapsed time for attention_prob_times_values (48x2048x2048x1113): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1113): 79.604

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1118.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1114x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1114x2048): 80.288
Elapsed time for attention_prob_times_values (48x2048x2048x1114): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1114): 83.561

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1150.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1115x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1115x2048): 79.632
Elapsed time for attention_prob_times_values (48x2048x2048x1115): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1115): 77.871

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1107.612
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1116x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1116x2048): 81.588
Elapsed time for attention_prob_times_values (48x2048x2048x1116): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1116): 81.362

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1147.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1117x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1117x2048): 77.288
Elapsed time for attention_prob_times_values (48x2048x2048x1117): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1117): 79.299

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1102.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1118x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1118x2048): 80.621
Elapsed time for attention_prob_times_values (48x2048x2048x1118): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1118): 83.783

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1158.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1119x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1119x2048): 77.439
Elapsed time for attention_prob_times_values (48x2048x2048x1119): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1119): 81.796

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1122.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1120x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1120x2048): 89.475
Elapsed time for attention_prob_times_values (48x2048x2048x1120): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1120): 90.300

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1269.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1121x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1121x2048): 81.232
Elapsed time for attention_prob_times_values (48x2048x2048x1121): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1121): 81.323

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1149.000
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1122x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1122x2048): 82.080
Elapsed time for attention_prob_times_values (48x2048x2048x1122): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1122): 82.833

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1166.604
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 32, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x284x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x284x2048): 58.655
Elapsed time for attention_prob_times_values (128x2048x2048x284): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x284): 63.554

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 602.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x285x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x285x2048): 57.801
Elapsed time for attention_prob_times_values (128x2048x2048x285): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x285): 60.736

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 586.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x286x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x286x2048): 58.321
Elapsed time for attention_prob_times_values (128x2048x2048x286): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x286): 63.049

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 602.145
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x287x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x287x2048): 55.314
Elapsed time for attention_prob_times_values (128x2048x2048x287): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x287): 61.234

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 579.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x288x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x288x2048): 79.612
Elapsed time for attention_prob_times_values (128x2048x2048x288): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x288): 78.565

Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 790.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0078
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x289x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x289x2048): 61.731
Elapsed time for attention_prob_times_values (128x2048x2048x289): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x289): 61.655

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 618.859
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x290x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x290x2048): 62.538
Elapsed time for attention_prob_times_values (128x2048x2048x290): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x290): 64.078

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 636.944
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x291x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x291x2048): 61.690
Elapsed time for attention_prob_times_values (128x2048x2048x291): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x291): 62.129

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 624.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x292x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x292x2048): 63.529
Elapsed time for attention_prob_times_values (128x2048x2048x292): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x292): 64.947

Attention duration (in seconds): 0.0098
Attention throughput (in TFLOP/s): 650.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0098
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x293x2048): 0.0053
Elapsed time for attention_prob_times_values (320x2048x2048x382): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x382): 65.896

Attention duration (in seconds): 0.0319
Attention throughput (in TFLOP/s): 1983.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x383x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x383x2048): 61.451
Elapsed time for attention_prob_times_values (320x2048x2048x383): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x383): 64.639

Attention duration (in seconds): 0.0326
Attention throughput (in TFLOP/s): 1948.222
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0326
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x384x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x384x2048): 77.547
Elapsed time for attention_prob_times_values (320x2048x2048x384): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x384): 81.595

Attention duration (in seconds): 0.0259
Attention throughput (in TFLOP/s): 2465.093
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x385x2048): 0.0158
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x385x2048): 65.250
Elapsed time for attention_prob_times_values (320x2048x2048x385): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x385): 57.579

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 1901.207
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x386x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x386x2048): 65.242
Elapsed time for attention_prob_times_values (320x2048x2048x386): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x386): 61.471

Attention duration (in seconds): 0.0327
Attention throughput (in TFLOP/s): 1972.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0327
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 30960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x387x2048): 0.0159
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x387x2048): 65.224
Elapsed time for attention_prob_times_values (320x2048x2048x387): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x387): 58.121

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 1919.920
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x388x2048): 0.0156
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x388x2048): 66.804
Elapsed time for attention_prob_times_values (320x2048x2048x388): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x388): 61.769

Attention duration (in seconds): 0.0325
Attention throughput (in TFLOP/s): 2009.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0325
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x389x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x389x2048): 65.416
Elapsed time for attention_prob_times_values (320x2048x2048x389): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x389): 58.173

Attention duration (in seconds): 0.0339
Attention throughput (in TFLOP/s): 1933.115
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x390x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x390x2048): 63.904
Elapsed time for attention_prob_times_values (320x2048x2048x390): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x390): 61.498

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 1972.388
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0334
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x391x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x391x2048): 64.353
Elapsed time for attention_prob_times_values (320x2048x2048x391): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x391): 58.827

Attention duration (in seconds): 0.0342
Attention throughput (in TFLOP/s): 1939.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0342
num_attention_heads: 12, hidden_size: 13476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1123x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1123x2048): 81.061
Elapsed time for attention_prob_times_values (48x2048x2048x1123): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1123): 81.876

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1153.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1124x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1124x2048): 82.194
Elapsed time for attention_prob_times_values (48x2048x2048x1124): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1124): 84.307

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1179.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1125x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1125x2048): 80.688
Elapsed time for attention_prob_times_values (48x2048x2048x1125): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1125): 82.043

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1153.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1126x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1126x2048): 81.679
Elapsed time for attention_prob_times_values (48x2048x2048x1126): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1126): 84.315

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1177.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1127x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1127x2048): 80.663
Elapsed time for attention_prob_times_values (48x2048x2048x1127): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1127): 82.129

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1156.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1128x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1128x2048): 81.803
Elapsed time for attention_prob_times_values (48x2048x2048x1128): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1128): 89.754

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1217.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1129x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1129x2048): 77.386
Elapsed time for attention_prob_times_values (48x2048x2048x1129): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1129): 82.246

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1134.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1130x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1130x2048): 79.434
Elapsed time for attention_prob_times_values (48x2048x2048x1130): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1130): 84.398

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1165.589
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1131x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1131x2048): 80.273
Elapsed time for attention_prob_times_values (48x2048x2048x1131): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1131): 81.719

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1154.412
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1132x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1132x2048): 81.547
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x293x2048): 59.514
Elapsed time for attention_prob_times_values (128x2048x2048x293): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x293): 62.571

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 619.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x294x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x294x2048): 62.288
Elapsed time for attention_prob_times_values (128x2048x2048x294): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x294): 62.523

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 635.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x295x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x295x2048): 58.350
Elapsed time for attention_prob_times_values (128x2048x2048x295): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x295): 63.061

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 619.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x296x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x296x2048): 63.163
Elapsed time for attention_prob_times_values (128x2048x2048x296): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x296): 82.090

Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 731.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x297x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x297x2048): 57.886
Elapsed time for attention_prob_times_values (128x2048x2048x297): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x297): 62.983

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 620.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x298x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x298x2048): 60.771
Elapsed time for attention_prob_times_values (128x2048x2048x298): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x298): 65.937

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 652.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x299x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x299x2048): 60.512
Elapsed time for attention_prob_times_values (128x2048x2048x299): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x299): 63.458

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 640.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x300x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x300x2048): 62.222
Elapsed time for attention_prob_times_values (128x2048x2048x300): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x300): 66.750

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 668.214
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x301x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x301x2048): 60.936
Elapsed time for attention_prob_times_values (128x2048x2048x301): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x301): 64.404

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 651.663
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x302x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x302x2048): 61.699
Elapsed time for attention_prob_times_values (128x2048x2048x302): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x302): 67.029

Attention duration (in seconds): 0.0101
Attention throughput (in TFLOP/s): 670.646
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (48x2048x2048x1132): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1132): 83.901

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1179.871
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1133x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1133x2048): 80.297
Elapsed time for attention_prob_times_values (48x2048x2048x1133): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1133): 82.431

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1161.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1134x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1134x2048): 81.240
Elapsed time for attention_prob_times_values (48x2048x2048x1134): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1134): 84.536

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1183.923
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1135x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1135x2048): 79.003
Elapsed time for attention_prob_times_values (48x2048x2048x1135): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1135): 82.353

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1153.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1136x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1136x2048): 82.596
Elapsed time for attention_prob_times_values (48x2048x2048x1136): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1136): 90.681

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 1237.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1137x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1137x2048): 79.883
Elapsed time for attention_prob_times_values (48x2048x2048x1137): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1137): 82.690

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1164.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1138x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1138x2048): 80.746
Elapsed time for attention_prob_times_values (48x2048x2048x1138): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1138): 84.714

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1185.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1139x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1139x2048): 79.616
Elapsed time for attention_prob_times_values (48x2048x2048x1139): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1139): 82.750

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1164.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1140x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1140x2048): 81.322
Elapsed time for attention_prob_times_values (48x2048x2048x1140): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1140): 84.923

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1193.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1141x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1141x2048): 80.061
Elapsed time for attention_prob_times_values (48x2048x2048x1141): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1141): 82.749

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1169.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x303x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x303x2048): 57.795
Elapsed time for attention_prob_times_values (128x2048x2048x303): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x303): 65.656

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 643.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x304x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x304x2048): 62.533
Elapsed time for attention_prob_times_values (128x2048x2048x304): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x304): 79.552

Attention duration (in seconds): 0.0093
Attention throughput (in TFLOP/s): 735.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x305x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x305x2048): 59.954
Elapsed time for attention_prob_times_values (128x2048x2048x305): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x305): 65.584

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 659.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x306x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x306x2048): 61.094
Elapsed time for attention_prob_times_values (128x2048x2048x306): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x306): 67.703

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 678.421
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x307x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x307x2048): 60.665
Elapsed time for attention_prob_times_values (128x2048x2048x307): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x307): 66.475

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 672.034
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x308x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x308x2048): 62.280
Elapsed time for attention_prob_times_values (128x2048x2048x308): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x308): 66.132

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 681.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x309x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x309x2048): 57.040
Elapsed time for attention_prob_times_values (128x2048x2048x309): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x309): 66.757

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 655.543
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x310x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x310x2048): 61.652
Elapsed time for attention_prob_times_values (128x2048x2048x310): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x310): 68.303

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 692.630
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 9952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x311x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x311x2048): 61.016
Elapsed time for attention_prob_times_values (128x2048x2048x311): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x311): 66.884

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 684.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0105
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1142x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1142x2048): 80.853
Elapsed time for attention_prob_times_values (48x2048x2048x1142): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1142): 84.945

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 1191.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1143x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1143x2048): 80.154
Elapsed time for attention_prob_times_values (48x2048x2048x1143): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1143): 82.511

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1170.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1144x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1144x2048): 78.673
Elapsed time for attention_prob_times_values (48x2048x2048x1144): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1144): 89.084

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1203.715
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1145x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1145x2048): 77.430
Elapsed time for attention_prob_times_values (48x2048x2048x1145): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1145): 82.886

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1154.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1146x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1146x2048): 77.232
Elapsed time for attention_prob_times_values (48x2048x2048x1146): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1146): 81.999

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1147.793
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1147x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1147x2048): 79.710
Elapsed time for attention_prob_times_values (48x2048x2048x1147): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1147): 81.391

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1163.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1148x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1148x2048): 77.314
Elapsed time for attention_prob_times_values (48x2048x2048x1148): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1148): 85.274

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1172.138
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1149x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1149x2048): 77.985
Elapsed time for attention_prob_times_values (48x2048x2048x1149): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1149): 80.288

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1144.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1150x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1150x2048): 77.886
Elapsed time for attention_prob_times_values (48x2048x2048x1150): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1150): 85.251

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1178.425
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x392x2048): 0.0160
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x392x2048): 65.573
Elapsed time for attention_prob_times_values (320x2048x2048x392): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x392): 74.994

Attention duration (in seconds): 0.0301
Attention throughput (in TFLOP/s): 2212.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0301
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x393x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x393x2048): 64.557
Elapsed time for attention_prob_times_values (320x2048x2048x393): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x393): 58.597

Attention duration (in seconds): 0.0343
Attention throughput (in TFLOP/s): 1947.610
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x394x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x394x2048): 65.146
Elapsed time for attention_prob_times_values (320x2048x2048x394): 0.0176
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x394): 60.035

Attention duration (in seconds): 0.0339
Attention throughput (in TFLOP/s): 1985.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0339
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x395x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x395x2048): 61.376
Elapsed time for attention_prob_times_values (320x2048x2048x395): 0.0178
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x395): 59.407

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 1923.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x396x2048): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x396x2048): 65.100
Elapsed time for attention_prob_times_values (320x2048x2048x396): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x396): 61.917

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 2027.024
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x397x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x397x2048): 63.823
Elapsed time for attention_prob_times_values (320x2048x2048x397): 0.0185
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x397): 57.715

Attention duration (in seconds): 0.0352
Attention throughput (in TFLOP/s): 1940.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0352
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x398x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x398x2048): 61.666
Elapsed time for attention_prob_times_values (320x2048x2048x398): 0.0180
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x398): 59.406

Attention duration (in seconds): 0.0353
Attention throughput (in TFLOP/s): 1942.143
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0353
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 31920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x399x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x399x2048): 65.104
Elapsed time for attention_prob_times_values (320x2048x2048x399): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x399): 59.904

Attention duration (in seconds): 0.0343
Attention throughput (in TFLOP/s): 2007.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0343
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x400x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x400x2048): 66.853
Elapsed time for attention_prob_times_values (320x2048x2048x400): 0.0140
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x400): 76.807

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 2305.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 32, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x312x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x312x2048): 62.999
Elapsed time for attention_prob_times_values (128x2048x2048x312): 0.0039
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x312): 86.501

Attention duration (in seconds): 0.0092
Attention throughput (in TFLOP/s): 783.702
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0092
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x313x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x313x2048): 26.234
Elapsed time for attention_prob_times_values (128x2048x2048x313): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x313): 66.966

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 406.444
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x314x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x314x2048): 61.093
Elapsed time for attention_prob_times_values (128x2048x2048x314): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x314): 69.337

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 702.323
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x315x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x315x2048): 60.893
Elapsed time for attention_prob_times_values (128x2048x2048x315): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x315): 65.958

Attention duration (in seconds): 0.0107
Attention throughput (in TFLOP/s): 686.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x316x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x316x2048): 59.595
Elapsed time for attention_prob_times_values (128x2048x2048x316): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x316): 69.472

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 697.695
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x317x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x317x2048): 61.125
Elapsed time for attention_prob_times_values (128x2048x2048x317): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x317): 67.625

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 700.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x318x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x318x2048): 61.571
Elapsed time for attention_prob_times_values (128x2048x2048x318): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x318): 70.121

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 717.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x319x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x319x2048): 61.059
Elapsed time for attention_prob_times_values (128x2048x2048x319): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x319): 68.139

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 706.441
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x320x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x320x2048): 80.381
Elapsed time for attention_prob_times_values (128x2048x2048x320): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x320): 84.421

Attention duration (in seconds): 0.0083
Attention throughput (in TFLOP/s): 905.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x321x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x321x2048): 64.146
--------
Elapsed time for attention_key_query_prob (48x2048x1151x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1151x2048): 79.608
Elapsed time for attention_prob_times_values (48x2048x2048x1151): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1151): 82.795

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1176.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1152x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1152x2048): 89.152
Elapsed time for attention_prob_times_values (48x2048x2048x1152): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1152): 92.338

Attention duration (in seconds): 0.0102
Attention throughput (in TFLOP/s): 1315.392
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0102
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1153x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1153x2048): 78.401
Elapsed time for attention_prob_times_values (48x2048x2048x1153): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1153): 76.440

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1123.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1154x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1154x2048): 82.064
Elapsed time for attention_prob_times_values (48x2048x2048x1154): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1154): 79.000

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1169.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1155x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1155x2048): 80.534
Elapsed time for attention_prob_times_values (48x2048x2048x1155): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1155): 71.076

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1097.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1156x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1156x2048): 82.658
Elapsed time for attention_prob_times_values (48x2048x2048x1156): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1156): 79.127

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1176.179
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1157x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1157x2048): 80.995
Elapsed time for attention_prob_times_values (48x2048x2048x1157): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1157): 76.911

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1148.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1158x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1158x2048): 81.790
Elapsed time for attention_prob_times_values (48x2048x2048x1158): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1158): 79.142

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1172.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1159x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1159x2048): 76.484
Elapsed time for attention_prob_times_values (48x2048x2048x1159): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1159): 77.105

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1119.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1160x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1160x2048): 78.561
Elapsed time for attention_prob_times_values (48x2048x2048x1160): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1160): 83.825

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1183.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1161x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1161x2048): 80.309
Elapsed time for attention_prob_times_values (48x2048x2048x1161): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1161): 77.201

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1149.810
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1162x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1162x2048): 79.823
Elapsed time for attention_prob_times_values (48x2048x2048x1162): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1162): 79.384

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1163.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1163x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1163x2048): 80.646
Elapsed time for attention_prob_times_values (48x2048x2048x1163): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1163): 76.718

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1150.314
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1164x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1164x2048): 82.012
Elapsed time for attention_prob_times_values (48x2048x2048x1164): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1164): 79.536

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1182.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1165x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1165x2048): 80.831
Elapsed time for attention_prob_times_values (48x2048x2048x1165): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1165): 77.361

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1158.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 13992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1166x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1166x2048): 79.974
Elapsed time for attention_prob_times_values (48x2048x2048x1166): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1166): 77.166

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1151.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1167x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1167x2048): 79.571
Elapsed time for attention_prob_times_values (48x2048x2048x1167): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1167): 77.554

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1152.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1168x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1168x2048): 82.881
Elapsed time for attention_prob_times_values (48x2048x2048x1168): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1168): 84.693

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1230.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1169x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1169x2048): 78.130
Elapsed time for attention_prob_times_values (48x2048x2048x1169): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1169): 75.446

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1128.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (128x2048x2048x321): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x321): 59.921

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 683.513
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x322x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x322x2048): 65.258
Elapsed time for attention_prob_times_values (128x2048x2048x322): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x322): 62.851

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 708.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x323x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x323x2048): 63.972
Elapsed time for attention_prob_times_values (128x2048x2048x323): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x323): 60.070

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 687.367
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x324x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x324x2048): 64.648
Elapsed time for attention_prob_times_values (128x2048x2048x324): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x324): 63.255

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 711.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x325x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x325x2048): 64.061
Elapsed time for attention_prob_times_values (128x2048x2048x325): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x325): 60.345

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 693.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x326x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x326x2048): 64.757
Elapsed time for attention_prob_times_values (128x2048x2048x326): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x326): 60.940

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 702.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x327x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x327x2048): 61.142
Elapsed time for attention_prob_times_values (128x2048x2048x327): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x327): 60.905

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 684.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x328x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x328x2048): 65.758
Elapsed time for attention_prob_times_values (128x2048x2048x328): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x328): 76.198

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 794.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x329x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x329x2048): 62.666
Elapsed time for attention_prob_times_values (128x2048x2048x329): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x329): 59.258

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 687.192
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x330x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x330x2048): 63.507
Elapsed time for attention_prob_times_values (128x2048x2048x330): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x330): 63.006

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 715.578
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
========================================================================================================================
num_attention_heads: 12, hidden_size: 14040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1170x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1170x2048): 81.022
Elapsed time for attention_prob_times_values (48x2048x2048x1170): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1170): 79.730

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1182.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1171x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1171x2048): 80.379
Elapsed time for attention_prob_times_values (48x2048x2048x1171): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1171): 77.619

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1162.716
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1172x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1172x2048): 81.862
Elapsed time for attention_prob_times_values (48x2048x2048x1172): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1172): 79.892

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1191.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1173x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1173x2048): 77.065
Elapsed time for attention_prob_times_values (48x2048x2048x1173): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1173): 77.834

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1142.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1174x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1174x2048): 81.362
Elapsed time for attention_prob_times_values (48x2048x2048x1174): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1174): 79.855

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1189.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1175x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1175x2048): 82.143
Elapsed time for attention_prob_times_values (48x2048x2048x1175): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1175): 74.498

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1153.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1176x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1176x2048): 83.748
Elapsed time for attention_prob_times_values (48x2048x2048x1176): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1176): 82.189

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1226.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1177x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1177x2048): 80.059
Elapsed time for attention_prob_times_values (48x2048x2048x1177): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1177): 77.873

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1167.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1178x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1178x2048): 80.464
Elapsed time for attention_prob_times_values (48x2048x2048x1178): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1178): 80.007

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1187.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1179x2048): 0.0059
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x331x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x331x2048): 61.477
Elapsed time for attention_prob_times_values (128x2048x2048x331): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x331): 59.190

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 684.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x332x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x332x2048): 64.755
Elapsed time for attention_prob_times_values (128x2048x2048x332): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x332): 64.626

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 735.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x333x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x333x2048): 63.324
Elapsed time for attention_prob_times_values (128x2048x2048x333): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x333): 59.446

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 699.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x334x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x334x2048): 64.126
Elapsed time for attention_prob_times_values (128x2048x2048x334): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x334): 64.750

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 736.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x335x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x335x2048): 61.447
Elapsed time for attention_prob_times_values (128x2048x2048x335): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x335): 59.924

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 695.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x336x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x336x2048): 64.092
Elapsed time for attention_prob_times_values (128x2048x2048x336): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x336): 77.916

Attention duration (in seconds): 0.0103
Attention throughput (in TFLOP/s): 808.808
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0103
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x337x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x337x2048): 62.553
Elapsed time for attention_prob_times_values (128x2048x2048x337): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x337): 60.191

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 707.431
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x338x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x338x2048): 62.948
Elapsed time for attention_prob_times_values (128x2048x2048x338): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x338): 63.333

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 730.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x339x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x339x2048): 63.067
Elapsed time for attention_prob_times_values (128x2048x2048x339): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x339): 61.223

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 720.334
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1179x2048): 79.998
Elapsed time for attention_prob_times_values (48x2048x2048x1179): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1179): 78.161

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1171.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1180x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1180x2048): 81.560
Elapsed time for attention_prob_times_values (48x2048x2048x1180): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1180): 80.076

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1198.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1181x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1181x2048): 79.781
Elapsed time for attention_prob_times_values (48x2048x2048x1181): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1181): 78.315

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1172.963
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1182x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1182x2048): 80.855
Elapsed time for attention_prob_times_values (48x2048x2048x1182): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1182): 80.399

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1197.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1183x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1183x2048): 80.234
Elapsed time for attention_prob_times_values (48x2048x2048x1183): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1183): 76.393

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1163.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1184x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1184x2048): 91.936
Elapsed time for attention_prob_times_values (48x2048x2048x1184): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1184): 83.143

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1298.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1185x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1185x2048): 81.742
Elapsed time for attention_prob_times_values (48x2048x2048x1185): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1185): 78.580

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1192.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1186x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1186x2048): 82.645
Elapsed time for attention_prob_times_values (48x2048x2048x1186): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1186): 80.707

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1216.668
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1187x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1187x2048): 78.682
Elapsed time for attention_prob_times_values (48x2048x2048x1187): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1187): 73.248

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1131.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1188x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1188x2048): 83.258
Elapsed time for attention_prob_times_values (48x2048x2048x1188): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1188): 80.888

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1224.431
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x32x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x32x2048): 41.244
Elapsed time for attention_prob_times_values (2048x2048x2048x32): 0.0205
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x32): 26.882

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 553.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0338
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x33x2048): 0.0209
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x33x2048): 27.077
Elapsed time for attention_prob_times_values (2048x2048x2048x33): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x33): 26.914

Attention duration (in seconds): 0.0420
Attention throughput (in TFLOP/s): 472.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0420
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x34x2048): 0.0220
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x34x2048): 26.551
Elapsed time for attention_prob_times_values (2048x2048x2048x34): 0.0219
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x34): 26.612

Attention duration (in seconds): 0.0439
Attention throughput (in TFLOP/s): 478.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x35x2048): 0.0237
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x35x2048): 25.369
Elapsed time for attention_prob_times_values (2048x2048x2048x35): 0.0215
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x35): 28.016

Attention duration (in seconds): 0.0452
Attention throughput (in TFLOP/s): 492.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0452
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x36x2048): 0.0230
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x36x2048): 26.870
Elapsed time for attention_prob_times_values (2048x2048x2048x36): 0.0210
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x36): 29.449

Attention duration (in seconds): 0.0440
Attention throughput (in TFLOP/s): 533.910
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0440
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x37x2048): 0.0238
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x37x2048): 26.705
Elapsed time for attention_prob_times_values (2048x2048x2048x37): 0.0217
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x37): 29.304

Attention duration (in seconds): 0.0455
Attention throughput (in TFLOP/s): 544.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0455
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x38x2048): 0.0240
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x38x2048): 27.201
Elapsed time for attention_prob_times_values (2048x2048x2048x38): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x38): 30.945

Attention duration (in seconds): 0.0451
Attention throughput (in TFLOP/s): 579.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0451
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x39x2048): 0.0244
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x39x2048): 27.438
Elapsed time for attention_prob_times_values (2048x2048x2048x39): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x39): 30.337

Attention duration (in seconds): 0.0465
Attention throughput (in TFLOP/s): 590.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x40x2048): 0.0239
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x40x2048): 28.772
Elapsed time for attention_prob_times_values (2048x2048x2048x40): 0.0208
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x40): 33.073

Attention duration (in seconds): 0.0447
Attention throughput (in TFLOP/s): 646.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x41x2048): 0.0260
--------
Elapsed time for attention_key_query_prob (320x2048x401x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x401x2048): 64.381
Elapsed time for attention_prob_times_values (320x2048x2048x401): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x401): 60.209

Attention duration (in seconds): 0.0346
Attention throughput (in TFLOP/s): 2011.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0346
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x402x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x402x2048): 63.490
Elapsed time for attention_prob_times_values (320x2048x2048x402): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x402): 62.116

Attention duration (in seconds): 0.0344
Attention throughput (in TFLOP/s): 2034.970
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0344
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x403x2048): 0.0169
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x403x2048): 63.969
Elapsed time for attention_prob_times_values (320x2048x2048x403): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x403): 60.439

Attention duration (in seconds): 0.0348
Attention throughput (in TFLOP/s): 2019.025
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0348
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x404x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x404x2048): 65.721
Elapsed time for attention_prob_times_values (320x2048x2048x404): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x404): 63.894

Attention duration (in seconds): 0.0335
Attention throughput (in TFLOP/s): 2109.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0335
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x405x2048): 0.0167
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x405x2048): 64.973
Elapsed time for attention_prob_times_values (320x2048x2048x405): 0.0183
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x405): 59.275

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 2023.504
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x406x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x406x2048): 64.269
Elapsed time for attention_prob_times_values (320x2048x2048x406): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x406): 63.961

Attention duration (in seconds): 0.0340
Attention throughput (in TFLOP/s): 2097.753
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x407x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x407x2048): 64.973
Elapsed time for attention_prob_times_values (320x2048x2048x407): 0.0179
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x407): 60.962

Attention duration (in seconds): 0.0347
Attention throughput (in TFLOP/s): 2063.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0347
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x408x2048): 0.0164
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x408x2048): 66.738
Elapsed time for attention_prob_times_values (320x2048x2048x408): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x408): 47.407

Attention duration (in seconds): 0.0395
Attention throughput (in TFLOP/s): 1822.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0395
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 80, hidden_size: 32720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (320x2048x409x2048): 0.0170
Throughput (in TFLOP/s) for attention_key_query_prob (320x2048x409x2048): 64.716
Elapsed time for attention_prob_times_values (320x2048x2048x409): 0.0181
Throughput (in TFLOP/s) for attention_prob_times_values (320x2048x2048x409): 60.576

Attention duration (in seconds): 0.0351
Attention throughput (in TFLOP/s): 2062.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0351
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_key_query_prob (128x2048x340x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x340x2048): 64.014
Elapsed time for attention_prob_times_values (128x2048x2048x340): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x340): 64.584

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 747.464
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x341x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x341x2048): 63.426
Elapsed time for attention_prob_times_values (128x2048x2048x341): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x341): 60.840

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 723.924
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x342x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x342x2048): 61.720
Elapsed time for attention_prob_times_values (128x2048x2048x342): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x342): 63.894

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 733.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 10976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x343x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x343x2048): 61.465
Elapsed time for attention_prob_times_values (128x2048x2048x343): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x343): 62.232

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 724.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x344x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x344x2048): 64.455
Elapsed time for attention_prob_times_values (128x2048x2048x344): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x344): 79.624

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 837.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x345x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x345x2048): 58.466
Elapsed time for attention_prob_times_values (128x2048x2048x345): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x345): 58.930

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 691.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x346x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x346x2048): 59.062
Elapsed time for attention_prob_times_values (128x2048x2048x346): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x346): 64.380

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 727.727
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x347x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x347x2048): 62.761
Elapsed time for attention_prob_times_values (128x2048x2048x347): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x347): 60.057

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 726.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x348x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x348x2048): 61.516
Elapsed time for attention_prob_times_values (128x2048x2048x348): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x348): 64.808

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 749.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x349x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x349x2048): 62.941
Elapsed time for attention_prob_times_values (128x2048x2048x349): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x349): 62.923

Attention duration (in seconds): 0.0119
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1189x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1189x2048): 80.761
Elapsed time for attention_prob_times_values (48x2048x2048x1189): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1189): 78.633

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1189.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1190x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1190x2048): 82.184
Elapsed time for attention_prob_times_values (48x2048x2048x1190): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1190): 80.855

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1218.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1191x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1191x2048): 75.134
Elapsed time for attention_prob_times_values (48x2048x2048x1191): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1191): 78.016

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1144.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1192x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1192x2048): 83.245
Elapsed time for attention_prob_times_values (48x2048x2048x1192): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1192): 85.975

Attention duration (in seconds): 0.0113
Attention throughput (in TFLOP/s): 1266.176
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0113
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1193x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1193x2048): 80.724
Elapsed time for attention_prob_times_values (48x2048x2048x1193): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1193): 78.891

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1195.399
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1194x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1194x2048): 79.131
Elapsed time for attention_prob_times_values (48x2048x2048x1194): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1194): 81.026

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1200.379
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1195x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1195x2048): 78.740
Elapsed time for attention_prob_times_values (48x2048x2048x1195): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1195): 78.940

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1182.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1196x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1196x2048): 82.582
Elapsed time for attention_prob_times_values (48x2048x2048x1196): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1196): 81.282

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1230.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1197x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1197x2048): 81.001
Elapsed time for attention_prob_times_values (48x2048x2048x1197): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1197): 77.220

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1188.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1198x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1198x2048): 81.530
Elapsed time for attention_prob_times_values (48x2048x2048x1198): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1198): 81.287

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1224.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1199x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1199x2048): 81.062
Elapsed time for attention_prob_times_values (48x2048x2048x1199): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1199): 79.124

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1205.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1200x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1200x2048): 83.218
Elapsed time for attention_prob_times_values (48x2048x2048x1200): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1200): 85.213

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1268.321
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1201x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1201x2048): 78.398
Elapsed time for attention_prob_times_values (48x2048x2048x1201): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1201): 76.557

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1167.749
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1202x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1202x2048): 81.451
Elapsed time for attention_prob_times_values (48x2048x2048x1202): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1202): 81.572

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1229.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1203x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1203x2048): 80.709
Elapsed time for attention_prob_times_values (48x2048x2048x1203): 0.1099
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1203): 4.408

Attention duration (in seconds): 0.1159
Attention throughput (in TFLOP/s): 126.204
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1204x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1204x2048): 79.837
Elapsed time for attention_prob_times_values (48x2048x2048x1204): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1204): 81.688

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1220.112
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1205x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1205x2048): 80.943
Elapsed time for attention_prob_times_values (48x2048x2048x1205): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1205): 77.308

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1195.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1206x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1206x2048): 81.578
Elapsed time for attention_prob_times_values (48x2048x2048x1206): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1206): 80.216

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1224.107
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1207x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1207x2048): 77.833
Attention throughput (in TFLOP/s): 749.283
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x350x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x350x2048): 61.136
Elapsed time for attention_prob_times_values (128x2048x2048x350): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x350): 62.829

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 739.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x351x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x351x2048): 63.172
Elapsed time for attention_prob_times_values (128x2048x2048x351): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x351): 62.105

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 749.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x352x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x352x2048): 82.756
Elapsed time for attention_prob_times_values (128x2048x2048x352): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x352): 78.127

Attention duration (in seconds): 0.0094
Attention throughput (in TFLOP/s): 964.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0094
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x353x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x353x2048): 62.691
Elapsed time for attention_prob_times_values (128x2048x2048x353): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x353): 64.822

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 766.856
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x354x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x354x2048): 66.936
Elapsed time for attention_prob_times_values (128x2048x2048x354): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x354): 65.430

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 798.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x355x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x355x2048): 65.945
Elapsed time for attention_prob_times_values (128x2048x2048x355): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x355): 65.033

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 791.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x356x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x356x2048): 67.535
Elapsed time for attention_prob_times_values (128x2048x2048x356): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x356): 62.453

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 786.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x357x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x357x2048): 63.159
Elapsed time for attention_prob_times_values (128x2048x2048x357): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x357): 62.024

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 760.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x358x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x358x2048): 66.396
Elapsed time for attention_prob_times_values (128x2048x2048x358): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x358): 65.983

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 806.674
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Elapsed time for attention_prob_times_values (48x2048x2048x1207): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1207): 79.648

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1192.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1208x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1208x2048): 82.925
Elapsed time for attention_prob_times_values (48x2048x2048x1208): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1208): 86.914

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1286.354
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1209x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1209x2048): 80.555
Elapsed time for attention_prob_times_values (48x2048x2048x1209): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1209): 79.737

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1215.616
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1210x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1210x2048): 81.311
Elapsed time for attention_prob_times_values (48x2048x2048x1210): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1210): 82.075

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1240.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1211x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1211x2048): 80.628
Elapsed time for attention_prob_times_values (48x2048x2048x1211): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1211): 79.947

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1219.652
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1212x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1212x2048): 82.188
Elapsed time for attention_prob_times_values (48x2048x2048x1212): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1212): 78.918

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1224.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1213x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1213x2048): 80.672
Elapsed time for attention_prob_times_values (48x2048x2048x1213): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1213): 79.982

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1222.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1214x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1214x2048): 81.450
Elapsed time for attention_prob_times_values (48x2048x2048x1214): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1214): 82.343

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1246.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1215x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1215x2048): 80.919
Elapsed time for attention_prob_times_values (48x2048x2048x1215): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1215): 80.123

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1226.975
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1216x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1216x2048): 92.245
Elapsed time for attention_prob_times_values (48x2048x2048x1216): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1216): 89.491

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 1385.416
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
num_attention_heads: 32, hidden_size: 11488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x359x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x359x2048): 65.354
Elapsed time for attention_prob_times_values (128x2048x2048x359): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x359): 64.690

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 794.466
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x360x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x360x2048): 67.140
Elapsed time for attention_prob_times_values (128x2048x2048x360): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x360): 82.990

Attention duration (in seconds): 0.0104
Attention throughput (in TFLOP/s): 909.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x361x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x361x2048): 64.472
Elapsed time for attention_prob_times_values (128x2048x2048x361): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x361): 63.500

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 785.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x362x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x362x2048): 63.357
Elapsed time for attention_prob_times_values (128x2048x2048x362): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x362): 66.565

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 799.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x363x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x363x2048): 64.755
Elapsed time for attention_prob_times_values (128x2048x2048x363): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x363): 64.007

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 794.678
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x364x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x364x2048): 65.979
Elapsed time for attention_prob_times_values (128x2048x2048x364): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x364): 66.706

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 820.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x365x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x365x2048): 64.996
Elapsed time for attention_prob_times_values (128x2048x2048x365): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x365): 64.471

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 803.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x366x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x366x2048): 65.746
Elapsed time for attention_prob_times_values (128x2048x2048x366): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x366): 67.449

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 828.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x367x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x367x2048): 65.141
Elapsed time for attention_prob_times_values (128x2048x2048x367): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x367): 65.113

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 812.050
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x368x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x368x2048): 66.995
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1217x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1217x2048): 82.036
Elapsed time for attention_prob_times_values (48x2048x2048x1217): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1217): 80.268

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1238.368
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1218x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1218x2048): 83.015
Elapsed time for attention_prob_times_values (48x2048x2048x1218): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1218): 82.628

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1264.958
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1219x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1219x2048): 81.971
Elapsed time for attention_prob_times_values (48x2048x2048x1219): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1219): 80.330

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1240.276
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1220x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1220x2048): 83.862
Elapsed time for attention_prob_times_values (48x2048x2048x1220): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1220): 82.803

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1274.676
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1221x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1221x2048): 81.865
Elapsed time for attention_prob_times_values (48x2048x2048x1221): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1221): 80.475

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1242.502
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1222x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1222x2048): 82.687
Elapsed time for attention_prob_times_values (48x2048x2048x1222): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1222): 82.796

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1267.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1223x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1223x2048): 81.700
Elapsed time for attention_prob_times_values (48x2048x2048x1223): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1223): 80.648

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1244.508
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1224x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1224x2048): 84.405
Elapsed time for attention_prob_times_values (48x2048x2048x1224): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1224): 87.859

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1321.055
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1225x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1225x2048): 81.281
Elapsed time for attention_prob_times_values (48x2048x2048x1225): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1225): 80.903

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1245.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (128x2048x2048x368): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x368): 84.917

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 936.236
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0106
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x369x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x369x2048): 64.424
Elapsed time for attention_prob_times_values (128x2048x2048x369): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x369): 65.142

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 811.788
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x370x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x370x2048): 65.341
Elapsed time for attention_prob_times_values (128x2048x2048x370): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x370): 67.944

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 836.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x371x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x371x2048): 63.887
Elapsed time for attention_prob_times_values (128x2048x2048x371): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x371): 65.356

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 813.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x372x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x372x2048): 65.594
Elapsed time for attention_prob_times_values (128x2048x2048x372): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x372): 68.566

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 846.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x373x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x373x2048): 64.382
Elapsed time for attention_prob_times_values (128x2048x2048x373): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x373): 65.817

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 823.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x374x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x374x2048): 65.467
Elapsed time for attention_prob_times_values (128x2048x2048x374): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x374): 68.455

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 849.150
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x375x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x375x2048): 64.921
Elapsed time for attention_prob_times_values (128x2048x2048x375): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x375): 66.038

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 832.754
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x376x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x376x2048): 66.602
Elapsed time for attention_prob_times_values (128x2048x2048x376): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x376): 84.820

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 951.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0108
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x377x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x377x2048): 62.844
Elapsed time for attention_prob_times_values (128x2048x2048x377): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x377): 66.307

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 824.762
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
--------
Elapsed time for attention_key_query_prob (48x2048x1226x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1226x2048): 82.435
Elapsed time for attention_prob_times_values (48x2048x2048x1226): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1226): 83.057

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1271.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1227x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1227x2048): 81.590
Elapsed time for attention_prob_times_values (48x2048x2048x1227): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1227): 80.962

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1249.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1228x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1228x2048): 83.143
Elapsed time for attention_prob_times_values (48x2048x2048x1228): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1228): 83.269

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1280.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1229x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1229x2048): 81.364
Elapsed time for attention_prob_times_values (48x2048x2048x1229): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1229): 80.882

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1249.474
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1230x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1230x2048): 82.393
Elapsed time for attention_prob_times_values (48x2048x2048x1230): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1230): 83.298

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1276.945
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1231x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1231x2048): 81.554
Elapsed time for attention_prob_times_values (48x2048x2048x1231): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1231): 81.251

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1255.694
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1232x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1232x2048): 83.935
Elapsed time for attention_prob_times_values (48x2048x2048x1232): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1232): 89.314

Attention duration (in seconds): 0.0115
Attention throughput (in TFLOP/s): 1335.977
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0115
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1233x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1233x2048): 78.322
Elapsed time for attention_prob_times_values (48x2048x2048x1233): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1233): 81.393

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1233.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1234x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1234x2048): 82.133
Elapsed time for attention_prob_times_values (48x2048x2048x1234): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1234): 81.173

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1262.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1235x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1235x2048): 80.996
Elapsed time for attention_prob_times_values (48x2048x2048x1235): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1235): 81.350

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1255.956
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1236x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1236x2048): 82.804
Elapsed time for attention_prob_times_values (48x2048x2048x1236): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1236): 83.554

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1287.949
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1237x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1237x2048): 81.033
Elapsed time for attention_prob_times_values (48x2048x2048x1237): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1237): 71.677

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1178.765
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1238x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1238x2048): 81.973
Elapsed time for attention_prob_times_values (48x2048x2048x1238): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1238): 77.009

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1231.528
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1239x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1239x2048): 81.104
Elapsed time for attention_prob_times_values (48x2048x2048x1239): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1239): 71.889

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1182.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1240x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1240x2048): 82.944
Elapsed time for attention_prob_times_values (48x2048x2048x1240): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1240): 89.152

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 1334.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1241x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1241x2048): 80.753
Elapsed time for attention_prob_times_values (48x2048x2048x1241): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1241): 72.152

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1184.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1242x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1242x2048): 81.484
Elapsed time for attention_prob_times_values (48x2048x2048x1242): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1242): 75.585

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1219.852
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1243x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1243x2048): 80.799
Elapsed time for attention_prob_times_values (48x2048x2048x1243): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1243): 72.573

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1190.290
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1244x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1244x2048): 81.892
Elapsed time for attention_prob_times_values (48x2048x2048x1244): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1244): 77.597

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1241.363
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x378x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x378x2048): 64.998
Elapsed time for attention_prob_times_values (128x2048x2048x378): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x378): 66.291

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 840.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x379x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x379x2048): 64.529
Elapsed time for attention_prob_times_values (128x2048x2048x379): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x379): 66.581

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 841.766
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x380x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x380x2048): 62.336
Elapsed time for attention_prob_times_values (128x2048x2048x380): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x380): 69.166

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 844.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x381x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x381x2048): 64.344
Elapsed time for attention_prob_times_values (128x2048x2048x381): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x381): 66.843

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 846.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x382x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x382x2048): 65.297
Elapsed time for attention_prob_times_values (128x2048x2048x382): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x382): 69.479

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 870.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x383x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x383x2048): 64.193
Elapsed time for attention_prob_times_values (128x2048x2048x383): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x383): 66.457

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 846.929
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x384x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x384x2048): 79.912
Elapsed time for attention_prob_times_values (128x2048x2048x384): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x384): 85.246

Attention duration (in seconds): 0.0100
Attention throughput (in TFLOP/s): 1072.408
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x385x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x385x2048): 66.798
Elapsed time for attention_prob_times_values (128x2048x2048x385): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x385): 59.639

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 821.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x386x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x386x2048): 68.191
Elapsed time for attention_prob_times_values (128x2048x2048x386): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x386): 63.498

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 859.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
========================================================================================================================
num_attention_heads: 12, hidden_size: 14940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1245x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1245x2048): 80.659
Elapsed time for attention_prob_times_values (48x2048x2048x1245): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1245): 72.682

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1192.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1246x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1246x2048): 81.683
Elapsed time for attention_prob_times_values (48x2048x2048x1246): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1246): 75.667

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1225.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1247x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1247x2048): 80.919
Elapsed time for attention_prob_times_values (48x2048x2048x1247): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1247): 73.166

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1199.836
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1248x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1248x2048): 91.607
Elapsed time for attention_prob_times_values (48x2048x2048x1248): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1248): 90.841

Attention duration (in seconds): 0.0110
Attention throughput (in TFLOP/s): 1425.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 14988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1249x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1249x2048): 81.851
Elapsed time for attention_prob_times_values (48x2048x2048x1249): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1249): 73.113

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1207.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1250x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1250x2048): 82.644
Elapsed time for attention_prob_times_values (48x2048x2048x1250): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1250): 77.793

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1254.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1251x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1251x2048): 81.719
Elapsed time for attention_prob_times_values (48x2048x2048x1251): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1251): 72.992

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1207.541
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1252x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1252x2048): 82.942
Elapsed time for attention_prob_times_values (48x2048x2048x1252): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1252): 77.980

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1259.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1253x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1253x2048): 81.597
Elapsed time for attention_prob_times_values (48x2048x2048x1253): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1253): 72.940

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1208.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1254x2048): 0.0061
Elapsed time for attention_key_query_prob (128x2048x387x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x387x2048): 67.019
Elapsed time for attention_prob_times_values (128x2048x2048x387): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x387): 60.235

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 830.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x388x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x388x2048): 66.199
Elapsed time for attention_prob_times_values (128x2048x2048x388): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x388): 61.822

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 839.157
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x389x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x389x2048): 67.373
Elapsed time for attention_prob_times_values (128x2048x2048x389): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x389): 60.653

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 839.851
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x390x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x390x2048): 68.054
Elapsed time for attention_prob_times_values (128x2048x2048x390): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x390): 64.135

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 870.855
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x391x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x391x2048): 67.151
Elapsed time for attention_prob_times_values (128x2048x2048x391): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x391): 60.986

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 844.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x392x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x392x2048): 68.714
Elapsed time for attention_prob_times_values (128x2048x2048x392): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x392): 77.553

Attention duration (in seconds): 0.0116
Attention throughput (in TFLOP/s): 965.482
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x393x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x393x2048): 66.200
Elapsed time for attention_prob_times_values (128x2048x2048x393): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x393): 60.914

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 842.654
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x394x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x394x2048): 66.894
Elapsed time for attention_prob_times_values (128x2048x2048x394): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x394): 64.588

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 874.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x395x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x395x2048): 66.365
Elapsed time for attention_prob_times_values (128x2048x2048x395): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x395): 58.965

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 833.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x396x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x396x2048): 67.682
Elapsed time for attention_prob_times_values (128x2048x2048x396): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x396): 63.352

Attention duration (in seconds): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1254x2048): 82.370
Elapsed time for attention_prob_times_values (48x2048x2048x1254): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1254): 77.935

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1257.060
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1255x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1255x2048): 81.313
Elapsed time for attention_prob_times_values (48x2048x2048x1255): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1255): 72.830

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1206.901
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1256x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1256x2048): 83.129
Elapsed time for attention_prob_times_values (48x2048x2048x1256): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1256): 90.387

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1361.337
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1257x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1257x2048): 81.131
Elapsed time for attention_prob_times_values (48x2048x2048x1257): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1257): 68.706

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1170.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1258x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1258x2048): 82.065
Elapsed time for attention_prob_times_values (48x2048x2048x1258): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1258): 78.132

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1260.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1259x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1259x2048): 78.174
Elapsed time for attention_prob_times_values (48x2048x2048x1259): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1259): 73.095

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1190.198
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1260x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1260x2048): 82.501
Elapsed time for attention_prob_times_values (48x2048x2048x1260): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1260): 78.346

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1267.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1261x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1261x2048): 81.303
Elapsed time for attention_prob_times_values (48x2048x2048x1261): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1261): 73.279

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1216.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1262x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1262x2048): 82.348
Elapsed time for attention_prob_times_values (48x2048x2048x1262): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1262): 78.343

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1267.794
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1263x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1263x2048): 78.138
Elapsed time for attention_prob_times_values (48x2048x2048x1263): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1263): 71.372

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1178.768
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 875.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x397x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x397x2048): 66.746
Elapsed time for attention_prob_times_values (128x2048x2048x397): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x397): 60.046

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 847.530
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x398x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x398x2048): 65.506
Elapsed time for attention_prob_times_values (128x2048x2048x398): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x398): 65.056

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 877.203
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x399x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x399x2048): 66.893
Elapsed time for attention_prob_times_values (128x2048x2048x399): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x399): 61.759

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 865.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x400x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x400x2048): 68.828
Elapsed time for attention_prob_times_values (128x2048x2048x400): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x400): 76.942

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 980.899
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x401x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x401x2048): 63.594
Elapsed time for attention_prob_times_values (128x2048x2048x401): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x401): 62.024

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 849.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x402x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x402x2048): 67.031
Elapsed time for attention_prob_times_values (128x2048x2048x402): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x402): 64.314

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 890.306
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x403x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x403x2048): 64.868
Elapsed time for attention_prob_times_values (128x2048x2048x403): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x403): 60.289

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 849.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x404x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x404x2048): 67.595
Elapsed time for attention_prob_times_values (128x2048x2048x404): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x404): 66.155

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 911.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 12960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x405x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x405x2048): 66.733
Elapsed time for attention_prob_times_values (128x2048x2048x405): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x405): 60.831

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 869.161
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1264x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1264x2048): 83.684
Elapsed time for attention_prob_times_values (48x2048x2048x1264): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1264): 91.137

Attention duration (in seconds): 0.0117
Attention throughput (in TFLOP/s): 1379.671
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0117
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1265x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1265x2048): 80.878
Elapsed time for attention_prob_times_values (48x2048x2048x1265): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1265): 73.094

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1215.132
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1266x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1266x2048): 80.884
Elapsed time for attention_prob_times_values (48x2048x2048x1266): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1266): 78.612

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1262.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1267x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1267x2048): 78.242
Elapsed time for attention_prob_times_values (48x2048x2048x1267): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1267): 71.641

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1185.346
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1268x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1268x2048): 82.481
Elapsed time for attention_prob_times_values (48x2048x2048x1268): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1268): 78.852

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1278.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1269x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1269x2048): 81.131
Elapsed time for attention_prob_times_values (48x2048x2048x1269): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1269): 73.093

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1220.523
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1270x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1270x2048): 82.285
Elapsed time for attention_prob_times_values (48x2048x2048x1270): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1270): 78.614

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1277.103
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1271x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1271x2048): 81.225
Elapsed time for attention_prob_times_values (48x2048x2048x1271): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1271): 73.006

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1222.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1272x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1272x2048): 83.074
Elapsed time for attention_prob_times_values (48x2048x2048x1272): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1272): 91.281

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1383.600
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1273x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1273x2048): 80.648
Elapsed time for attention_prob_times_values (48x2048x2048x1273): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1273): 72.888

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1218.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1274x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1274x2048): 81.744
Elapsed time for attention_prob_times_values (48x2048x2048x1274): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1274): 78.969

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1279.677
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1275x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1275x2048): 80.221
Elapsed time for attention_prob_times_values (48x2048x2048x1275): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1275): 72.936

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1218.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1276x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1276x2048): 82.355
Elapsed time for attention_prob_times_values (48x2048x2048x1276): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1276): 79.192

Attention duration (in seconds): 0.0127
Attention throughput (in TFLOP/s): 1288.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0127
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1277x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1277x2048): 80.370
Elapsed time for attention_prob_times_values (48x2048x2048x1277): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1277): 72.880

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1220.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1278x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1278x2048): 81.269
Elapsed time for attention_prob_times_values (48x2048x2048x1278): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1278): 79.055

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1280.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1279x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1279x2048): 79.898
Elapsed time for attention_prob_times_values (48x2048x2048x1279): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1279): 73.046

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1220.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1280x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1280x2048): 86.462
Elapsed time for attention_prob_times_values (48x2048x2048x1280): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1280): 94.013

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 1441.272
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1281x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1281x2048): 80.942
Elapsed time for attention_prob_times_values (48x2048x2048x1281): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1281): 65.228

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1156.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1282x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1282x2048): 82.273
num_attention_heads: 32, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x406x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x406x2048): 65.207
Elapsed time for attention_prob_times_values (128x2048x2048x406): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x406): 66.169

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 899.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x407x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x407x2048): 66.967
Elapsed time for attention_prob_times_values (128x2048x2048x407): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x407): 62.441

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 886.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x408x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x408x2048): 68.272
Elapsed time for attention_prob_times_values (128x2048x2048x408): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x408): 80.019

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1013.100
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x409x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x409x2048): 65.339
Elapsed time for attention_prob_times_values (128x2048x2048x409): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x409): 62.860

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 883.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x410x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x410x2048): 66.994
Elapsed time for attention_prob_times_values (128x2048x2048x410): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x410): 64.116

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 905.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x411x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x411x2048): 63.286
Elapsed time for attention_prob_times_values (128x2048x2048x411): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x411): 63.160

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 875.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x412x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x412x2048): 67.835
Elapsed time for attention_prob_times_values (128x2048x2048x412): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x412): 67.169

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 936.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x413x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x413x2048): 66.542
Elapsed time for attention_prob_times_values (128x2048x2048x413): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x413): 63.403

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 902.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x414x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x414x2048): 67.257
Elapsed time for attention_prob_times_values (128x2048x2048x414): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x414): 67.063

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 936.040
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x415x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x415x2048): 64.738
Elapsed time for attention_prob_times_values (48x2048x2048x1282): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1282): 73.511

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1244.148
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1283x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1283x2048): 81.175
Elapsed time for attention_prob_times_values (48x2048x2048x1283): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1283): 68.351

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1190.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1284x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1284x2048): 83.387
Elapsed time for attention_prob_times_values (48x2048x2048x1284): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1284): 73.688

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 1255.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1285x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1285x2048): 81.216
Elapsed time for attention_prob_times_values (48x2048x2048x1285): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1285): 68.337

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1191.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1286x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1286x2048): 82.775
Elapsed time for attention_prob_times_values (48x2048x2048x1286): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1286): 73.612

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1252.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1287x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1287x2048): 81.379
Elapsed time for attention_prob_times_values (48x2048x2048x1287): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1287): 68.361

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1194.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1288x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1288x2048): 83.650
Elapsed time for attention_prob_times_values (48x2048x2048x1288): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1288): 84.495

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1353.009
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1289x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1289x2048): 80.996
Elapsed time for attention_prob_times_values (48x2048x2048x1289): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1289): 68.673

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1197.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1290x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1290x2048): 82.184
Elapsed time for attention_prob_times_values (48x2048x2048x1290): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1290): 73.756

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1252.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1291x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1291x2048): 79.827
Elapsed time for attention_prob_times_values (48x2048x2048x1291): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1291): 68.997

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1193.826
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Elapsed time for attention_prob_times_values (128x2048x2048x415): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x415): 63.762

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 897.443
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x416x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x416x2048): 85.497
Elapsed time for attention_prob_times_values (128x2048x2048x416): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x416): 78.931

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 1149.158
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0109
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x417x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x417x2048): 68.473
Elapsed time for attention_prob_times_values (128x2048x2048x417): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x417): 64.320

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 930.709
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x418x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x418x2048): 70.742
Elapsed time for attention_prob_times_values (128x2048x2048x418): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x418): 67.792

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 973.620
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x419x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x419x2048): 69.827
Elapsed time for attention_prob_times_values (128x2048x2048x419): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x419): 64.522

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 945.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x420x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x420x2048): 71.443
Elapsed time for attention_prob_times_values (128x2048x2048x420): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x420): 68.293

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 986.381
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x421x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x421x2048): 69.640
Elapsed time for attention_prob_times_values (128x2048x2048x421): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x421): 64.532

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 948.309
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x422x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x422x2048): 70.353
Elapsed time for attention_prob_times_values (128x2048x2048x422): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x422): 68.222

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 982.782
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x423x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x423x2048): 69.276
Elapsed time for attention_prob_times_values (128x2048x2048x423): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x423): 65.263

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 955.637
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x424x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x424x2048): 71.318
Elapsed time for attention_prob_times_values (128x2048x2048x424): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x424): 83.502

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1096.257
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1292x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1292x2048): 82.745
Elapsed time for attention_prob_times_values (48x2048x2048x1292): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1292): 73.943

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1260.536
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1293x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1293x2048): 81.114
Elapsed time for attention_prob_times_values (48x2048x2048x1293): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1293): 69.104

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1205.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1294x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1294x2048): 82.588
Elapsed time for attention_prob_times_values (48x2048x2048x1294): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1294): 73.885

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1260.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1295x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1295x2048): 81.077
Elapsed time for attention_prob_times_values (48x2048x2048x1295): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1295): 69.411

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1209.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1296x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1296x2048): 81.719
Elapsed time for attention_prob_times_values (48x2048x2048x1296): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1296): 85.053

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1349.266
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1297x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1297x2048): 80.804
Elapsed time for attention_prob_times_values (48x2048x2048x1297): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1297): 69.353

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1209.139
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1298x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1298x2048): 81.884
Elapsed time for attention_prob_times_values (48x2048x2048x1298): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1298): 70.495

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1228.211
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1299x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1299x2048): 78.795
Elapsed time for attention_prob_times_values (48x2048x2048x1299): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1299): 69.394

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1197.172
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1300x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1300x2048): 82.759
Elapsed time for attention_prob_times_values (48x2048x2048x1300): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1300): 74.285

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1271.045
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x41x2048): 27.118
Elapsed time for attention_prob_times_values (2048x2048x2048x41): 0.0219
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x41): 32.149

Attention duration (in seconds): 0.0479
Attention throughput (in TFLOP/s): 632.525
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0479
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x42x2048): 0.0248
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x42x2048): 29.114
Elapsed time for attention_prob_times_values (2048x2048x2048x42): 0.0216
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x42): 33.474

Attention duration (in seconds): 0.0463
Attention throughput (in TFLOP/s): 685.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0463
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x43x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x43x2048): 29.320
Elapsed time for attention_prob_times_values (2048x2048x2048x43): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x43): 33.416

Attention duration (in seconds): 0.0473
Attention throughput (in TFLOP/s): 702.772
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0473
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x44x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x44x2048): 30.009
Elapsed time for attention_prob_times_values (2048x2048x2048x44): 0.0217
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x44): 34.849

Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 741.706
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x45x2048): 0.0255
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x45x2048): 30.338
Elapsed time for attention_prob_times_values (2048x2048x2048x45): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x45): 33.977

Attention duration (in seconds): 0.0482
Attention throughput (in TFLOP/s): 753.282
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x46x2048): 0.0251
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x46x2048): 31.495
Elapsed time for attention_prob_times_values (2048x2048x2048x46): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x46): 35.707

Attention duration (in seconds): 0.0472
Attention throughput (in TFLOP/s): 803.255
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0472
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x47x2048): 0.0257
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x47x2048): 31.443
Elapsed time for attention_prob_times_values (2048x2048x2048x47): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x47): 36.481

Attention duration (in seconds): 0.0478
Attention throughput (in TFLOP/s): 827.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0478
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x48x2048): 0.0250
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x48x2048): 32.979
Elapsed time for attention_prob_times_values (2048x2048x2048x48): 0.0211
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x48): 39.060

Attention duration (in seconds): 0.0461
Attention throughput (in TFLOP/s): 894.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0461
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x49x2048): 0.0262
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x49x2048): 32.077
Elapsed time for attention_prob_times_values (2048x2048x2048x49): 0.0222
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x49): 37.906

Attention duration (in seconds): 0.0485
Attention throughput (in TFLOP/s): 886.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x50x2048): 0.0251
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x50x2048): 34.286
Elapsed time for attention_prob_times_values (2048x2048x2048x50): 0.0219
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x50): 39.255

Attention duration (in seconds): 0.0469
Attention throughput (in TFLOP/s): 951.661
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x425x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x425x2048): 68.341
Elapsed time for attention_prob_times_values (128x2048x2048x425): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x425): 65.174

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 952.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x426x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x426x2048): 68.664
Elapsed time for attention_prob_times_values (128x2048x2048x426): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x426): 68.720

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 983.154
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x427x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x427x2048): 67.267
Elapsed time for attention_prob_times_values (128x2048x2048x427): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x427): 65.141

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 949.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x428x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x428x2048): 68.063
Elapsed time for attention_prob_times_values (128x2048x2048x428): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x428): 69.341

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 987.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x429x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x429x2048): 68.906
Elapsed time for attention_prob_times_values (128x2048x2048x429): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x429): 63.427

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 951.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x430x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x430x2048): 67.567
Elapsed time for attention_prob_times_values (128x2048x2048x430): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x430): 69.358

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 988.262
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x431x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x431x2048): 68.854
Elapsed time for attention_prob_times_values (128x2048x2048x431): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x431): 65.892

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 974.330
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x432x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x432x2048): 70.632
Elapsed time for attention_prob_times_values (128x2048x2048x432): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x432): 84.726

Attention duration (in seconds): 0.0120
Attention throughput (in TFLOP/s): 1117.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x433x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x433x2048): 68.122
Elapsed time for attention_prob_times_values (128x2048x2048x433): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x433): 66.365

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 976.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
--------
Elapsed time for attention_key_query_prob (48x2048x1301x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1301x2048): 79.027
Elapsed time for attention_prob_times_values (48x2048x2048x1301): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1301): 68.079

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1188.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1302x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1302x2048): 82.026
Elapsed time for attention_prob_times_values (48x2048x2048x1302): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1302): 71.367

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1240.898
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1303x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1303x2048): 79.377
Elapsed time for attention_prob_times_values (48x2048x2048x1303): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1303): 69.572

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1206.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1304x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1304x2048): 83.514
Elapsed time for attention_prob_times_values (48x2048x2048x1304): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1304): 85.349

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1374.487
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1305x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1305x2048): 80.793
Elapsed time for attention_prob_times_values (48x2048x2048x1305): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1305): 69.849

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1220.726
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1306x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1306x2048): 81.503
Elapsed time for attention_prob_times_values (48x2048x2048x1306): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1306): 74.324

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1267.660
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1307x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1307x2048): 80.689
Elapsed time for attention_prob_times_values (48x2048x2048x1307): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1307): 70.076

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1223.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1308x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1308x2048): 82.488
Elapsed time for attention_prob_times_values (48x2048x2048x1308): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1308): 74.601

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1279.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1309x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1309x2048): 80.996
Elapsed time for attention_prob_times_values (48x2048x2048x1309): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1309): 70.105

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1228.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1310x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1310x2048): 82.102
Elapsed time for attention_prob_times_values (48x2048x2048x1310): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1310): 74.516

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1277.472
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1311x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1311x2048): 81.063
Elapsed time for attention_prob_times_values (48x2048x2048x1311): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1311): 70.402

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1233.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1312x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1312x2048): 91.990
Elapsed time for attention_prob_times_values (48x2048x2048x1312): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1312): 86.726

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1461.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1313x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1313x2048): 82.281
Elapsed time for attention_prob_times_values (48x2048x2048x1313): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1313): 70.534

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1244.670
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1314x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1314x2048): 83.308
Elapsed time for attention_prob_times_values (48x2048x2048x1314): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1314): 74.772

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1292.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1315x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1315x2048): 81.082
Elapsed time for attention_prob_times_values (48x2048x2048x1315): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1315): 70.389

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1236.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1316x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1316x2048): 83.683
Elapsed time for attention_prob_times_values (48x2048x2048x1316): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1316): 74.984

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1298.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1317x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1317x2048): 81.866
Elapsed time for attention_prob_times_values (48x2048x2048x1317): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1317): 70.313

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1243.218
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1318x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1318x2048): 82.901
Elapsed time for attention_prob_times_values (48x2048x2048x1318): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1318): 74.876

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1293.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1319x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1319x2048): 81.834
Elapsed time for attention_prob_times_values (48x2048x2048x1319): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1319): 70.088

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1242.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_key_query_prob (128x2048x434x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x434x2048): 68.959
Elapsed time for attention_prob_times_values (128x2048x2048x434): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x434): 69.744

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1009.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x435x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x435x2048): 68.446
Elapsed time for attention_prob_times_values (128x2048x2048x435): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x435): 66.646

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 985.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x436x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x436x2048): 70.082
Elapsed time for attention_prob_times_values (128x2048x2048x436): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x436): 70.354

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1026.935
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 13984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x437x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x437x2048): 68.708
Elapsed time for attention_prob_times_values (128x2048x2048x437): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x437): 67.014

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 994.435
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x438x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x438x2048): 69.320
Elapsed time for attention_prob_times_values (128x2048x2048x438): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x438): 66.892

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 999.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x439x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x439x2048): 68.893
Elapsed time for attention_prob_times_values (128x2048x2048x439): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x439): 67.371

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1002.693
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x440x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x440x2048): 68.541
Elapsed time for attention_prob_times_values (128x2048x2048x440): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x440): 86.302

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 1126.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x441x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x441x2048): 68.007
Elapsed time for attention_prob_times_values (128x2048x2048x441): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x441): 67.465

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1001.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x442x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x442x2048): 68.660
Elapsed time for attention_prob_times_values (128x2048x2048x442): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x442): 70.920

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1033.496
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x443x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x443x2048): 68.271
Elapsed time for attention_prob_times_values (128x2048x2048x443): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x443): 67.793

Attention duration (in seconds): 0.0140
========================================================================================================================
num_attention_heads: 12, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1320x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1320x2048): 84.216
Elapsed time for attention_prob_times_values (48x2048x2048x1320): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1320): 86.219

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1403.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1321x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1321x2048): 81.494
Elapsed time for attention_prob_times_values (48x2048x2048x1321): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1321): 70.267

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1243.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1322x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1322x2048): 82.340
Elapsed time for attention_prob_times_values (48x2048x2048x1322): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1322): 74.989

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1294.524
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1323x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1323x2048): 81.397
Elapsed time for attention_prob_times_values (48x2048x2048x1323): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1323): 70.291

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1245.015
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1324x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1324x2048): 83.028
Elapsed time for attention_prob_times_values (48x2048x2048x1324): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1324): 75.282

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1304.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1325x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1325x2048): 81.737
Elapsed time for attention_prob_times_values (48x2048x2048x1325): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1325): 70.480

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1250.991
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1326x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1326x2048): 82.465
Elapsed time for attention_prob_times_values (48x2048x2048x1326): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1326): 75.201

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1301.056
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1327x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1327x2048): 81.747
Elapsed time for attention_prob_times_values (48x2048x2048x1327): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1327): 70.613

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1254.104
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1328x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1328x2048): 84.265
Elapsed time for attention_prob_times_values (48x2048x2048x1328): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1328): 87.133

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 1418.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1329x2048): 0.0066
Attention throughput (in TFLOP/s): 1009.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x444x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x444x2048): 69.733
Elapsed time for attention_prob_times_values (128x2048x2048x444): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x444): 71.051

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1046.993
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x445x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x445x2048): 68.509
Elapsed time for attention_prob_times_values (128x2048x2048x445): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x445): 68.106

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1018.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x446x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x446x2048): 69.188
Elapsed time for attention_prob_times_values (128x2048x2048x446): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x446): 71.519

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1050.618
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x447x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x447x2048): 65.986
Elapsed time for attention_prob_times_values (128x2048x2048x447): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x447): 68.361

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1005.195
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x448x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x448x2048): 85.977
Elapsed time for attention_prob_times_values (128x2048x2048x448): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x448): 85.128

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 1283.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x449x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x449x2048): 71.465
Elapsed time for attention_prob_times_values (128x2048x2048x449): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x449): 62.374

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1001.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x450x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x450x2048): 72.530
Elapsed time for attention_prob_times_values (128x2048x2048x450): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x450): 65.387

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1035.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x451x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x451x2048): 71.445
Elapsed time for attention_prob_times_values (128x2048x2048x451): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x451): 62.502

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1006.377
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x452x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x452x2048): 73.250
Elapsed time for attention_prob_times_values (128x2048x2048x452): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x452): 65.691

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1047.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1329x2048): 81.245
Elapsed time for attention_prob_times_values (48x2048x2048x1329): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1329): 70.577

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1251.953
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1330x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1330x2048): 81.569
Elapsed time for attention_prob_times_values (48x2048x2048x1330): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1330): 75.223

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1298.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1331x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1331x2048): 81.347
Elapsed time for attention_prob_times_values (48x2048x2048x1331): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1331): 70.442

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1253.164
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1332x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1332x2048): 82.814
Elapsed time for attention_prob_times_values (48x2048x2048x1332): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1332): 75.648

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1313.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 15996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1333x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1333x2048): 81.528
Elapsed time for attention_prob_times_values (48x2048x2048x1333): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1333): 70.422

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1256.039
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1334x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1334x2048): 82.038
Elapsed time for attention_prob_times_values (48x2048x2048x1334): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1334): 75.432

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1307.279
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1335x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1335x2048): 81.571
Elapsed time for attention_prob_times_values (48x2048x2048x1335): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1335): 70.510

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1258.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1336x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1336x2048): 83.493
Elapsed time for attention_prob_times_values (48x2048x2048x1336): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1336): 87.264

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1421.390
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1337x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1337x2048): 80.908
Elapsed time for attention_prob_times_values (48x2048x2048x1337): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1337): 70.443

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1255.333
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1338x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1338x2048): 78.033
Elapsed time for attention_prob_times_values (48x2048x2048x1338): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1338): 73.041

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1258.562
MLP duration (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x453x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x453x2048): 71.283
Elapsed time for attention_prob_times_values (128x2048x2048x453): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x453): 62.376

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1008.389
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x454x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x454x2048): 71.858
Elapsed time for attention_prob_times_values (128x2048x2048x454): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x454): 65.607

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1041.711
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x455x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x455x2048): 71.099
Elapsed time for attention_prob_times_values (128x2048x2048x455): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x455): 62.738

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1014.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x456x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x456x2048): 73.329
Elapsed time for attention_prob_times_values (128x2048x2048x456): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x456): 82.671

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1185.234
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x457x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x457x2048): 67.832
Elapsed time for attention_prob_times_values (128x2048x2048x457): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x457): 62.487

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 994.043
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x458x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x458x2048): 70.811
Elapsed time for attention_prob_times_values (128x2048x2048x458): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x458): 66.280

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1048.459
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x459x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x459x2048): 70.206
Elapsed time for attention_prob_times_values (128x2048x2048x459): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x459): 63.194

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1020.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x460x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x460x2048): 69.638
Elapsed time for attention_prob_times_values (128x2048x2048x460): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x460): 66.630

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1047.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x461x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x461x2048): 70.293
Elapsed time for attention_prob_times_values (128x2048x2048x461): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x461): 63.598

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1028.798
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x462x2048): 0.0070
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1339x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1339x2048): 81.207
Elapsed time for attention_prob_times_values (48x2048x2048x1339): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1339): 70.547

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1260.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1340x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1340x2048): 82.787
Elapsed time for attention_prob_times_values (48x2048x2048x1340): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1340): 76.008

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1323.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1341x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1341x2048): 81.439
Elapsed time for attention_prob_times_values (48x2048x2048x1341): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1341): 70.597

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1264.169
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1342x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1342x2048): 78.231
Elapsed time for attention_prob_times_values (48x2048x2048x1342): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1342): 73.043

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1263.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1343x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1343x2048): 81.353
Elapsed time for attention_prob_times_values (48x2048x2048x1343): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1343): 70.685

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1266.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1344x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1344x2048): 91.658
Elapsed time for attention_prob_times_values (48x2048x2048x1344): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1344): 89.720

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 1518.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1345x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1345x2048): 82.708
Elapsed time for attention_prob_times_values (48x2048x2048x1345): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1345): 70.535

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1276.205
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1346x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1346x2048): 84.292
Elapsed time for attention_prob_times_values (48x2048x2048x1346): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1346): 72.882

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1311.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1347x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1347x2048): 83.101
Elapsed time for attention_prob_times_values (48x2048x2048x1347): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1347): 68.274

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1258.233
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x462x2048): 71.161
Elapsed time for attention_prob_times_values (128x2048x2048x462): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x462): 66.926

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1064.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x463x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x463x2048): 68.112
Elapsed time for attention_prob_times_values (128x2048x2048x463): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x463): 61.726

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1001.783
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x464x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x464x2048): 69.600
Elapsed time for attention_prob_times_values (128x2048x2048x464): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x464): 86.907

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1198.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x465x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x465x2048): 69.558
Elapsed time for attention_prob_times_values (128x2048x2048x465): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x465): 63.909

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1034.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x466x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x466x2048): 70.501
Elapsed time for attention_prob_times_values (128x2048x2048x466): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x466): 67.440

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1072.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x467x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x467x2048): 69.859
Elapsed time for attention_prob_times_values (128x2048x2048x467): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x467): 64.213

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1043.494
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x468x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x468x2048): 71.428
Elapsed time for attention_prob_times_values (128x2048x2048x468): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x468): 67.915

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1087.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x469x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x469x2048): 69.887
Elapsed time for attention_prob_times_values (128x2048x2048x469): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x469): 64.423

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1049.658
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x470x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x470x2048): 68.243
Elapsed time for attention_prob_times_values (128x2048x2048x470): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x470): 67.928

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1068.082
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x471x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x471x2048): 70.341
Elapsed time for attention_prob_times_values (128x2048x2048x471): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x471): 64.501

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1057.788
MLP duration (in seconds): 0.0000
num_attention_heads: 12, hidden_size: 16176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1348x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1348x2048): 83.529
Elapsed time for attention_prob_times_values (48x2048x2048x1348): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1348): 76.485

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1341.258
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1349x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1349x2048): 82.259
Elapsed time for attention_prob_times_values (48x2048x2048x1349): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1349): 70.473

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1275.967
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1350x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1350x2048): 80.029
Elapsed time for attention_prob_times_values (48x2048x2048x1350): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1350): 73.847

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1292.031
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1351x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1351x2048): 82.248
Elapsed time for attention_prob_times_values (48x2048x2048x1351): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1351): 70.799

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1280.835
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1352x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1352x2048): 84.141
Elapsed time for attention_prob_times_values (48x2048x2048x1352): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1352): 88.573

Attention duration (in seconds): 0.0126
Attention throughput (in TFLOP/s): 1453.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0126
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1353x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1353x2048): 81.950
Elapsed time for attention_prob_times_values (48x2048x2048x1353): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1353): 70.913

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1281.569
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1354x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1354x2048): 79.632
Elapsed time for attention_prob_times_values (48x2048x2048x1354): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1354): 72.948

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1284.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1355x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1355x2048): 81.989
Elapsed time for attention_prob_times_values (48x2048x2048x1355): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1355): 71.261

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1287.003
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1356x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1356x2048): 85.276
Elapsed time for attention_prob_times_values (48x2048x2048x1356): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1356): 76.768

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1364.742
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1357x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1357x2048): 82.649
Elapsed time for attention_prob_times_values (48x2048x2048x1357): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1357): 71.228

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1293.278
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1358x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1358x2048): 79.976
Elapsed time for attention_prob_times_values (48x2048x2048x1358): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1358): 73.339

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1294.156
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1359x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1359x2048): 83.423
Elapsed time for attention_prob_times_values (48x2048x2048x1359): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1359): 71.130

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1299.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1360x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1360x2048): 88.987
Elapsed time for attention_prob_times_values (48x2048x2048x1360): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1360): 89.612

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1512.486
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1361x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1361x2048): 85.823
Elapsed time for attention_prob_times_values (48x2048x2048x1361): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1361): 71.383

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1321.020
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1362x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1362x2048): 83.263
Elapsed time for attention_prob_times_values (48x2048x2048x1362): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1362): 76.830

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1355.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1363x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1363x2048): 82.452
Elapsed time for attention_prob_times_values (48x2048x2048x1363): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1363): 71.421

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1299.109
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1364x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1364x2048): 84.374
Elapsed time for attention_prob_times_values (48x2048x2048x1364): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1364): 77.043

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1367.961
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1365x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1365x2048): 81.811
Elapsed time for attention_prob_times_values (48x2048x2048x1365): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1365): 71.664

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1298.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1366x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1366x2048): 82.507
Elapsed time for attention_prob_times_values (48x2048x2048x1366): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1366): 74.654

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1333.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x472x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x472x2048): 69.275
Elapsed time for attention_prob_times_values (128x2048x2048x472): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x472): 83.006

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1189.461
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x473x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x473x2048): 66.863
Elapsed time for attention_prob_times_values (128x2048x2048x473): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x473): 64.736

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1038.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x474x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x474x2048): 70.214
Elapsed time for attention_prob_times_values (128x2048x2048x474): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x474): 68.319

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1095.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x475x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x475x2048): 69.681
Elapsed time for attention_prob_times_values (128x2048x2048x475): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x475): 65.012

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1065.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x476x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x476x2048): 71.043
Elapsed time for attention_prob_times_values (128x2048x2048x476): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x476): 66.953

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1094.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x477x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x477x2048): 69.266
Elapsed time for attention_prob_times_values (128x2048x2048x477): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x477): 65.256

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1068.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x478x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x478x2048): 70.459
Elapsed time for attention_prob_times_values (128x2048x2048x478): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x478): 66.716

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1092.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x479x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x479x2048): 67.386
Elapsed time for attention_prob_times_values (128x2048x2048x479): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x479): 63.256

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1042.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x480x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x480x2048): 84.058
Elapsed time for attention_prob_times_values (128x2048x2048x480): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x480): 90.718

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 1396.183
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1367x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1367x2048): 84.693
Elapsed time for attention_prob_times_values (48x2048x2048x1367): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1367): 69.192

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1296.240
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1368x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1368x2048): 80.700
Elapsed time for attention_prob_times_values (48x2048x2048x1368): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1368): 89.743

Attention duration (in seconds): 0.0130
Attention throughput (in TFLOP/s): 1447.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1369x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1369x2048): 82.620
Elapsed time for attention_prob_times_values (48x2048x2048x1369): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1369): 68.103

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1272.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1370x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1370x2048): 86.417
Elapsed time for attention_prob_times_values (48x2048x2048x1370): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1370): 77.176

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1390.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1371x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1371x2048): 85.198
Elapsed time for attention_prob_times_values (48x2048x2048x1371): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1371): 72.497

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1336.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1372x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1372x2048): 84.047
Elapsed time for attention_prob_times_values (48x2048x2048x1372): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1372): 77.570

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1377.847
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1373x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1373x2048): 82.314
Elapsed time for attention_prob_times_values (48x2048x2048x1373): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1373): 72.501

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1317.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1374x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1374x2048): 85.937
Elapsed time for attention_prob_times_values (48x2048x2048x1374): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1374): 77.494

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1393.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1375x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1375x2048): 85.005
Elapsed time for attention_prob_times_values (48x2048x2048x1375): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1375): 73.056

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1344.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
num_attention_heads: 32, hidden_size: 15392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x481x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x481x2048): 72.205
Elapsed time for attention_prob_times_values (128x2048x2048x481): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x481): 64.700

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1094.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x482x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x482x2048): 73.544
Elapsed time for attention_prob_times_values (128x2048x2048x482): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x482): 68.331

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1137.897
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x483x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x483x2048): 72.426
Elapsed time for attention_prob_times_values (128x2048x2048x483): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x483): 66.020

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1111.675
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x484x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x484x2048): 74.050
Elapsed time for attention_prob_times_values (128x2048x2048x484): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x484): 68.504

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1147.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x485x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x485x2048): 72.087
Elapsed time for attention_prob_times_values (128x2048x2048x485): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x485): 66.149

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1114.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x486x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x486x2048): 72.810
Elapsed time for attention_prob_times_values (128x2048x2048x486): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x486): 68.356

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1141.424
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x487x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x487x2048): 69.784
Elapsed time for attention_prob_times_values (128x2048x2048x487): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x487): 66.519

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1104.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x488x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x488x2048): 73.595
Elapsed time for attention_prob_times_values (128x2048x2048x488): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x488): 91.038

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1322.626
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x489x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x489x2048): 68.566
Elapsed time for attention_prob_times_values (128x2048x2048x489): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x489): 64.008

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1077.957
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x490x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x490x2048): 69.885
--------
Elapsed time for attention_key_query_prob (48x2048x1376x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1376x2048): 91.877
Elapsed time for attention_prob_times_values (48x2048x2048x1376): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1376): 91.157

Attention duration (in seconds): 0.0121
Attention throughput (in TFLOP/s): 1567.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1377x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1377x2048): 83.073
Elapsed time for attention_prob_times_values (48x2048x2048x1377): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1377): 72.321

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1325.095
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1378x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1378x2048): 84.673
Elapsed time for attention_prob_times_values (48x2048x2048x1378): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1378): 77.798

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1390.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1379x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1379x2048): 82.686
Elapsed time for attention_prob_times_values (48x2048x2048x1379): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1379): 72.072

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1321.584
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1380x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1380x2048): 84.065
Elapsed time for attention_prob_times_values (48x2048x2048x1380): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1380): 78.038

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1389.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1381x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1381x2048): 82.460
Elapsed time for attention_prob_times_values (48x2048x2048x1381): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1381): 73.030

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1331.030
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1382x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1382x2048): 83.387
Elapsed time for attention_prob_times_values (48x2048x2048x1382): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1382): 78.013

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1386.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1383x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1383x2048): 82.245
Elapsed time for attention_prob_times_values (48x2048x2048x1383): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1383): 72.951

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1330.446
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1384x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1384x2048): 84.196
Elapsed time for attention_prob_times_values (48x2048x2048x1384): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1384): 90.603

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1502.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1385x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1385x2048): 81.911
Elapsed time for attention_prob_times_values (48x2048x2048x1385): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1385): 73.011

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1330.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1386x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1386x2048): 83.091
Elapsed time for attention_prob_times_values (48x2048x2048x1386): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1386): 78.134

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1388.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1387x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1387x2048): 81.775
Elapsed time for attention_prob_times_values (48x2048x2048x1387): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1387): 73.096

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1331.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1388x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1388x2048): 83.531
Elapsed time for attention_prob_times_values (48x2048x2048x1388): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1388): 78.428

Attention duration (in seconds): 0.0138
Attention throughput (in TFLOP/s): 1396.768
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0138
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1389x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1389x2048): 81.853
Elapsed time for attention_prob_times_values (48x2048x2048x1389): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1389): 73.231

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1335.577
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1390x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1390x2048): 83.195
Elapsed time for attention_prob_times_values (48x2048x2048x1390): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1390): 78.344

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1395.167
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1391x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1391x2048): 81.862
Elapsed time for attention_prob_times_values (48x2048x2048x1391): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1391): 73.211

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1337.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1392x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1392x2048): 84.267
Elapsed time for attention_prob_times_values (48x2048x2048x1392): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1392): 91.463

Attention duration (in seconds): 0.0128
Attention throughput (in TFLOP/s): 1518.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0128
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1393x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1393x2048): 81.783
Elapsed time for attention_prob_times_values (48x2048x2048x1393): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1393): 72.998

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1336.407
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1394x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1394x2048): 82.858
Elapsed time for attention_prob_times_values (48x2048x2048x1394): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1394): 78.696

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1399.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (128x2048x2048x490): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x490): 68.644

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1129.787
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x491x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x491x2048): 71.175
Elapsed time for attention_prob_times_values (128x2048x2048x491): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x491): 66.753

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1125.974
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x492x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x492x2048): 72.371
Elapsed time for attention_prob_times_values (128x2048x2048x492): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x492): 69.120

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1157.841
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x493x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x493x2048): 71.265
Elapsed time for attention_prob_times_values (128x2048x2048x493): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x493): 66.916

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1132.397
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x494x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x494x2048): 72.039
Elapsed time for attention_prob_times_values (128x2048x2048x494): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x494): 70.233

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1169.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x495x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x495x2048): 71.342
Elapsed time for attention_prob_times_values (128x2048x2048x495): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x495): 66.978

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1137.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x496x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x496x2048): 73.429
Elapsed time for attention_prob_times_values (128x2048x2048x496): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x496): 90.418

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 1337.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0131
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x497x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x497x2048): 70.604
Elapsed time for attention_prob_times_values (128x2048x2048x497): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x497): 67.325

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1139.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x498x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x498x2048): 71.675
Elapsed time for attention_prob_times_values (128x2048x2048x498): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x498): 69.669

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1170.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 15968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x499x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x499x2048): 70.797
Elapsed time for attention_prob_times_values (128x2048x2048x499): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x499): 67.521

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1146.959
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0469
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x51x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x51x2048): 34.800
Elapsed time for attention_prob_times_values (2048x2048x2048x51): 0.0225
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x51): 38.951

Attention duration (in seconds): 0.0477
Attention throughput (in TFLOP/s): 974.102
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0477
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x52x2048): 0.0253
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x52x2048): 35.267
Elapsed time for attention_prob_times_values (2048x2048x2048x52): 0.0221
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x52): 40.457

Attention duration (in seconds): 0.0474
Attention throughput (in TFLOP/s): 1017.468
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0474
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x53x2048): 0.0253
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x53x2048): 35.991
Elapsed time for attention_prob_times_values (2048x2048x2048x53): 0.0223
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x53): 40.842

Attention duration (in seconds): 0.0476
Attention throughput (in TFLOP/s): 1052.244
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0476
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x54x2048): 0.0254
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x54x2048): 36.511
Elapsed time for attention_prob_times_values (2048x2048x2048x54): 0.0224
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x54): 41.334

Attention duration (in seconds): 0.0479
Attention throughput (in TFLOP/s): 1085.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0479
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x55x2048): 0.0256
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x55x2048): 36.891
Elapsed time for attention_prob_times_values (2048x2048x2048x55): 0.0228
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x55): 41.451

Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 1112.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0484
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x56x2048): 0.0252
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x56x2048): 38.200
Elapsed time for attention_prob_times_values (2048x2048x2048x56): 0.0214
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x56): 45.048

Attention duration (in seconds): 0.0465
Attention throughput (in TFLOP/s): 1198.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x57x2048): 0.0260
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x57x2048): 37.667
Elapsed time for attention_prob_times_values (2048x2048x2048x57): 0.0224
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x57): 43.642

Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 1192.843
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0484
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x58x2048): 0.0259
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x58x2048): 38.428
Elapsed time for attention_prob_times_values (2048x2048x2048x58): 0.0229
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x58): 43.558

Attention duration (in seconds): 0.0488
Attention throughput (in TFLOP/s): 1224.972
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0488
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x59x2048): 0.0266
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x59x2048): 38.095
Elapsed time for attention_prob_times_values (2048x2048x2048x59): 0.0227
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x59): 44.567

Attention duration (in seconds): 0.0494
Attention throughput (in TFLOP/s): 1252.864
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0494
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 12, hidden_size: 16740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1395x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1395x2048): 81.704
Elapsed time for attention_prob_times_values (48x2048x2048x1395): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1395): 73.300

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1340.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1396x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1396x2048): 82.878
Elapsed time for attention_prob_times_values (48x2048x2048x1396): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1396): 78.627

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1400.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1397x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1397x2048): 81.739
Elapsed time for attention_prob_times_values (48x2048x2048x1397): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1397): 72.727

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1337.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1398x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1398x2048): 82.909
Elapsed time for attention_prob_times_values (48x2048x2048x1398): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1398): 78.898

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1405.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1399x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1399x2048): 81.902
Elapsed time for attention_prob_times_values (48x2048x2048x1399): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1399): 73.364

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1346.308
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1400x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1400x2048): 80.484
Elapsed time for attention_prob_times_values (48x2048x2048x1400): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1400): 89.567

Attention duration (in seconds): 0.0133
Attention throughput (in TFLOP/s): 1475.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0133
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1401x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1401x2048): 79.776
Elapsed time for attention_prob_times_values (48x2048x2048x1401): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1401): 73.163

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1329.450
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1402x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1402x2048): 81.456
Elapsed time for attention_prob_times_values (48x2048x2048x1402): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1402): 79.018

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1398.182
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1403x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1403x2048): 81.461
Elapsed time for attention_prob_times_values (48x2048x2048x1403): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1403): 73.378

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1346.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1404x2048): 0.0069
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x500x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x500x2048): 72.044
Elapsed time for attention_prob_times_values (128x2048x2048x500): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x500): 70.049

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1180.915
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x501x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x501x2048): 70.955
Elapsed time for attention_prob_times_values (128x2048x2048x501): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x501): 66.117

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1140.131
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x502x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x502x2048): 71.867
Elapsed time for attention_prob_times_values (128x2048x2048x502): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x502): 69.761

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1181.449
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x503x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x503x2048): 71.180
Elapsed time for attention_prob_times_values (128x2048x2048x503): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x503): 66.590

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1150.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x504x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x504x2048): 70.332
Elapsed time for attention_prob_times_values (128x2048x2048x504): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x504): 91.029

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1329.163
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x505x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x505x2048): 70.349
Elapsed time for attention_prob_times_values (128x2048x2048x505): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x505): 61.332

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1099.707
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x506x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x506x2048): 71.104
Elapsed time for attention_prob_times_values (128x2048x2048x506): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x506): 68.803

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1175.777
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x507x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x507x2048): 70.410
Elapsed time for attention_prob_times_values (128x2048x2048x507): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x507): 61.750

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1108.254
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x508x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x508x2048): 71.886
Elapsed time for attention_prob_times_values (128x2048x2048x508): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x508): 68.884

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1187.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1404x2048): 82.300
Elapsed time for attention_prob_times_values (48x2048x2048x1404): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1404): 78.676

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 1404.058
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1405x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1405x2048): 81.569
Elapsed time for attention_prob_times_values (48x2048x2048x1405): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1405): 73.410

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1349.590
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1406x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1406x2048): 82.030
Elapsed time for attention_prob_times_values (48x2048x2048x1406): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1406): 79.352

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1409.811
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1407x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1407x2048): 81.113
Elapsed time for attention_prob_times_values (48x2048x2048x1407): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1407): 73.492

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1348.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1408x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1408x2048): 90.487
Elapsed time for attention_prob_times_values (48x2048x2048x1408): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1408): 94.118

Attention duration (in seconds): 0.0123
Attention throughput (in TFLOP/s): 1614.669
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0123
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1409x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1409x2048): 82.111
Elapsed time for attention_prob_times_values (48x2048x2048x1409): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1409): 66.752

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1289.556
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1410x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1410x2048): 83.119
Elapsed time for attention_prob_times_values (48x2048x2048x1410): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1410): 72.925

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1361.375
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1411x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1411x2048): 82.527
Elapsed time for attention_prob_times_values (48x2048x2048x1411): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1411): 67.750

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1304.825
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1412x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1412x2048): 84.186
Elapsed time for attention_prob_times_values (48x2048x2048x1412): 0.0172
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1412): 33.122

Attention duration (in seconds): 0.0239
Attention throughput (in TFLOP/s): 834.171
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0239
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1413x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1413x2048): 82.424
Elapsed time for attention_prob_times_values (48x2048x2048x1413): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1413): 67.782

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1306.177
MLP duration (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (128x2048x509x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x509x2048): 70.300
Elapsed time for attention_prob_times_values (128x2048x2048x509): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x509): 61.486

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1109.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x510x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x510x2048): 70.395
Elapsed time for attention_prob_times_values (128x2048x2048x510): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x510): 69.301

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1182.973
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x511x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x511x2048): 68.378
Elapsed time for attention_prob_times_values (128x2048x2048x511): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x511): 65.418

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1134.622
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x512x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x512x2048): 84.346
Elapsed time for attention_prob_times_values (128x2048x2048x512): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x512): 96.209

Attention duration (in seconds): 0.0122
Attention throughput (in TFLOP/s): 1528.090
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x513x2048): 0.0185
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x513x2048): 29.705
Elapsed time for attention_prob_times_values (128x2048x2048x513): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x513): 61.080

Attention duration (in seconds): 0.0276
Attention throughput (in TFLOP/s): 680.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0276
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x514x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x514x2048): 74.769
Elapsed time for attention_prob_times_values (128x2048x2048x514): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x514): 66.634

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1202.348
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x515x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x515x2048): 72.582
Elapsed time for attention_prob_times_values (128x2048x2048x515): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x515): 63.144

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1154.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x516x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x516x2048): 73.361
Elapsed time for attention_prob_times_values (128x2048x2048x516): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x516): 65.962

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1189.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x517x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x517x2048): 71.819
Elapsed time for attention_prob_times_values (128x2048x2048x517): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x517): 63.756

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1158.861
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x518x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x518x2048): 73.734
Elapsed time for attention_prob_times_values (128x2048x2048x518): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x518): 66.839

Attention duration (in seconds): 0.0159
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1414x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1414x2048): 83.714
Elapsed time for attention_prob_times_values (48x2048x2048x1414): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1414): 72.977

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1370.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1415x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1415x2048): 82.441
Elapsed time for attention_prob_times_values (48x2048x2048x1415): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1415): 68.041

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1310.773
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1416x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1416x2048): 84.291
Elapsed time for attention_prob_times_values (48x2048x2048x1416): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1416): 84.742

Attention duration (in seconds): 0.0135
Attention throughput (in TFLOP/s): 1486.946
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1417x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1417x2048): 82.054
Elapsed time for attention_prob_times_values (48x2048x2048x1417): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1417): 67.362

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1302.551
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1418x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1418x2048): 82.287
Elapsed time for attention_prob_times_values (48x2048x2048x1418): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1418): 73.213

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1365.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1419x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1419x2048): 81.738
Elapsed time for attention_prob_times_values (48x2048x2048x1419): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1419): 68.375

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1312.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1420x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1420x2048): 83.592
Elapsed time for attention_prob_times_values (48x2048x2048x1420): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1420): 73.516

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 1380.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1421x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1421x2048): 80.046
Elapsed time for attention_prob_times_values (48x2048x2048x1421): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1421): 67.200

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1289.729
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1422x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1422x2048): 82.135
Elapsed time for attention_prob_times_values (48x2048x2048x1422): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1422): 71.890

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1354.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1423x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1423x2048): 81.582
Elapsed time for attention_prob_times_values (48x2048x2048x1423): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1423): 68.782

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1319.265
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1424x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1424x2048): 85.094
Elapsed time for attention_prob_times_values (48x2048x2048x1424): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1424): 85.484

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1508.540
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1425x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1425x2048): 82.081
Elapsed time for attention_prob_times_values (48x2048x2048x1425): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1425): 67.046

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1306.299
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1426x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1426x2048): 82.990
Elapsed time for attention_prob_times_values (48x2048x2048x1426): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1426): 73.529

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1380.990
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1427x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1427x2048): 82.109
Elapsed time for attention_prob_times_values (48x2048x2048x1427): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1427): 67.234

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1310.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1428x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1428x2048): 83.414
Elapsed time for attention_prob_times_values (48x2048x2048x1428): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1428): 73.765

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1388.488
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1429x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1429x2048): 82.177
Elapsed time for attention_prob_times_values (48x2048x2048x1429): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1429): 68.877

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1329.912
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1430x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1430x2048): 83.060
Elapsed time for attention_prob_times_values (48x2048x2048x1430): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1430): 73.587

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1385.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1431x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1431x2048): 79.739
Elapsed time for attention_prob_times_values (48x2048x2048x1431): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1431): 68.947

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1314.088
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1432x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1432x2048): 84.134
Attention throughput (in TFLOP/s): 1205.137
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x519x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x519x2048): 72.655
Elapsed time for attention_prob_times_values (128x2048x2048x519): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x519): 63.868

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1170.503
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x520x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x520x2048): 74.265
Elapsed time for attention_prob_times_values (128x2048x2048x520): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x520): 77.751

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1310.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x521x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x521x2048): 71.711
Elapsed time for attention_prob_times_values (128x2048x2048x521): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x521): 62.744

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1156.606
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x522x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x522x2048): 72.645
Elapsed time for attention_prob_times_values (128x2048x2048x522): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x522): 66.112

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1198.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x523x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x523x2048): 70.594
Elapsed time for attention_prob_times_values (128x2048x2048x523): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x523): 62.608

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1150.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x524x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x524x2048): 71.354
Elapsed time for attention_prob_times_values (128x2048x2048x524): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x524): 67.819

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1208.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x525x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x525x2048): 72.188
Elapsed time for attention_prob_times_values (128x2048x2048x525): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x525): 63.439

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1175.475
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x526x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x526x2048): 73.051
Elapsed time for attention_prob_times_values (128x2048x2048x526): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x526): 67.824

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1226.563
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x527x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x527x2048): 72.263
Elapsed time for attention_prob_times_values (128x2048x2048x527): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x527): 64.437

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1190.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Elapsed time for attention_prob_times_values (48x2048x2048x1432): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1432): 85.569

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1508.657
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1433x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1433x2048): 81.885
Elapsed time for attention_prob_times_values (48x2048x2048x1433): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1433): 69.221

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1334.878
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1434x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1434x2048): 82.661
Elapsed time for attention_prob_times_values (48x2048x2048x1434): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1434): 73.732

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1387.731
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1435x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1435x2048): 81.950
Elapsed time for attention_prob_times_values (48x2048x2048x1435): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1435): 69.463

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1339.644
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1436x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1436x2048): 83.411
Elapsed time for attention_prob_times_values (48x2048x2048x1436): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1436): 73.727

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1395.426
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1437x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1437x2048): 82.045
Elapsed time for attention_prob_times_values (48x2048x2048x1437): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1437): 69.355

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1340.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1438x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1438x2048): 82.997
Elapsed time for attention_prob_times_values (48x2048x2048x1438): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1438): 73.873

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1395.445
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1439x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1439x2048): 82.131
Elapsed time for attention_prob_times_values (48x2048x2048x1439): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1439): 69.703

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1347.038
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1440x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1440x2048): 92.858
Elapsed time for attention_prob_times_values (48x2048x2048x1440): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1440): 87.045

Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 1606.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0129
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1441x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1441x2048): 83.008
Elapsed time for attention_prob_times_values (48x2048x2048x1441): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1441): 69.736

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1355.728
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
num_attention_heads: 512, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x60x2048): 0.0261
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x60x2048): 39.535
Elapsed time for attention_prob_times_values (2048x2048x2048x60): 0.0220
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x60): 46.847

Attention duration (in seconds): 0.0481
Attention throughput (in TFLOP/s): 1329.331
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0481
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x61x2048): 0.0262
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x61x2048): 40.058
Elapsed time for attention_prob_times_values (2048x2048x2048x61): 0.0231
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x61): 45.421

Attention duration (in seconds): 0.0492
Attention throughput (in TFLOP/s): 1341.004
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0492
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x62x2048): 0.0265
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x62x2048): 40.264
Elapsed time for attention_prob_times_values (2048x2048x2048x62): 0.0227
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x62): 46.964

Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 1387.411
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0491
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 512, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (2048x2048x63x2048): 0.0265
Throughput (in TFLOP/s) for attention_key_query_prob (2048x2048x63x2048): 40.769
Elapsed time for attention_prob_times_values (2048x2048x2048x63): 0.0227
Throughput (in TFLOP/s) for attention_prob_times_values (2048x2048x2048x63): 47.643

Attention duration (in seconds): 0.0493
Attention throughput (in TFLOP/s): 1428.012
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0493
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 32, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x528x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x528x2048): 72.191
Elapsed time for attention_prob_times_values (128x2048x2048x528): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x528): 79.079

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1320.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x529x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x529x2048): 70.496
Elapsed time for attention_prob_times_values (128x2048x2048x529): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x529): 64.261

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1178.697
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x530x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x530x2048): 72.445
Elapsed time for attention_prob_times_values (128x2048x2048x530): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x530): 66.228

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1215.268
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 16992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x531x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x531x2048): 71.366
Elapsed time for attention_prob_times_values (128x2048x2048x531): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x531): 64.334

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1190.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x532x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x532x2048): 72.933
Elapsed time for attention_prob_times_values (128x2048x2048x532): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x532): 68.696

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1246.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x533x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x533x2048): 72.087
Elapsed time for attention_prob_times_values (128x2048x2048x533): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x533): 64.215

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1199.275
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x534x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x534x2048): 72.854
Elapsed time for attention_prob_times_values (128x2048x2048x534): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x534): 68.571

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1249.583
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x535x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x535x2048): 72.394
Elapsed time for attention_prob_times_values (128x2048x2048x535): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x535): 63.851

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1202.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x536x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x536x2048): 72.114
Elapsed time for attention_prob_times_values (128x2048x2048x536): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x536): 79.939

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1345.895
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x537x2048): 0.0082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1442x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1442x2048): 84.291
Elapsed time for attention_prob_times_values (48x2048x2048x1442): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1442): 72.618

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1396.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1443x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1443x2048): 83.334
Elapsed time for attention_prob_times_values (48x2048x2048x1443): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1443): 69.478

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1357.190
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1444x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1444x2048): 84.677
Elapsed time for attention_prob_times_values (48x2048x2048x1444): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1444): 74.410

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1419.627
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1445x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1445x2048): 83.091
Elapsed time for attention_prob_times_values (48x2048x2048x1445): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1445): 69.558

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1358.021
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1446x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1446x2048): 84.094
Elapsed time for attention_prob_times_values (48x2048x2048x1446): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1446): 74.184

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1414.608
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1447x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1447x2048): 83.022
Elapsed time for attention_prob_times_values (48x2048x2048x1447): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1447): 69.544

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1359.126
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1448x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1448x2048): 85.021
Elapsed time for attention_prob_times_values (48x2048x2048x1448): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1448): 86.522

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1541.086
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1449x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1449x2048): 82.637
Elapsed time for attention_prob_times_values (48x2048x2048x1449): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1449): 69.533

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1357.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1450x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1450x2048): 83.294
Elapsed time for attention_prob_times_values (48x2048x2048x1450): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1450): 74.424

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1414.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x537x2048): 69.929
Elapsed time for attention_prob_times_values (128x2048x2048x537): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x537): 64.961

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1197.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x538x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x538x2048): 72.251
Elapsed time for attention_prob_times_values (128x2048x2048x538): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x538): 67.501

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1243.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x539x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x539x2048): 69.180
Elapsed time for attention_prob_times_values (128x2048x2048x539): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x539): 63.366

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1180.284
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x540x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x540x2048): 71.301
Elapsed time for attention_prob_times_values (128x2048x2048x540): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x540): 69.282

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1256.206
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x541x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x541x2048): 71.838
Elapsed time for attention_prob_times_values (128x2048x2048x541): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x541): 64.607

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1218.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x542x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x542x2048): 72.643
Elapsed time for attention_prob_times_values (128x2048x2048x542): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x542): 67.454

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1254.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x543x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x543x2048): 72.012
Elapsed time for attention_prob_times_values (128x2048x2048x543): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x543): 64.458

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1222.336
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x544x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x544x2048): 89.232
Elapsed time for attention_prob_times_values (128x2048x2048x544): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x544): 82.180

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1540.101
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x545x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x545x2048): 74.792
Elapsed time for attention_prob_times_values (128x2048x2048x545): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x545): 65.909

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1263.457
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x546x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x546x2048): 74.654
Elapsed time for attention_prob_times_values (128x2048x2048x546): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x546): 67.768

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1283.246
MLP duration (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (48x2048x1451x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1451x2048): 82.246
Elapsed time for attention_prob_times_values (48x2048x2048x1451): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1451): 69.572

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1357.130
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1452x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1452x2048): 84.248
Elapsed time for attention_prob_times_values (48x2048x2048x1452): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1452): 74.790

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1427.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1453x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1453x2048): 82.776
Elapsed time for attention_prob_times_values (48x2048x2048x1453): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1453): 69.749

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1364.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1454x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1454x2048): 83.828
Elapsed time for attention_prob_times_values (48x2048x2048x1454): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1454): 74.597

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1424.073
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1455x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1455x2048): 83.019
Elapsed time for attention_prob_times_values (48x2048x2048x1455): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1455): 69.873

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1369.703
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1456x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1456x2048): 84.974
Elapsed time for attention_prob_times_values (48x2048x2048x1456): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1456): 87.225

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1554.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1457x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1457x2048): 82.625
Elapsed time for attention_prob_times_values (48x2048x2048x1457): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1457): 69.453

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1364.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1458x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1458x2048): 83.353
Elapsed time for attention_prob_times_values (48x2048x2048x1458): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1458): 73.858

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1416.469
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1459x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1459x2048): 81.337
Elapsed time for attention_prob_times_values (48x2048x2048x1459): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1459): 69.811

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1359.756
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1460x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1460x2048): 83.988
Elapsed time for attention_prob_times_values (48x2048x2048x1460): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1460): 75.097

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1435.962
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1461x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1461x2048): 82.791
Elapsed time for attention_prob_times_values (48x2048x2048x1461): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1461): 69.788

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1372.402
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1462x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1462x2048): 83.472
Elapsed time for attention_prob_times_values (48x2048x2048x1462): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1462): 73.400

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1416.400
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1463x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1463x2048): 81.701
Elapsed time for attention_prob_times_values (48x2048x2048x1463): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1463): 69.830

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1366.287
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1464x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1464x2048): 83.243
Elapsed time for attention_prob_times_values (48x2048x2048x1464): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1464): 86.420

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 1539.680
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0139
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1465x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1465x2048): 82.291
Elapsed time for attention_prob_times_values (48x2048x2048x1465): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1465): 69.817

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1372.452
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1466x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1466x2048): 83.080
Elapsed time for attention_prob_times_values (48x2048x2048x1466): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1466): 75.055

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1433.721
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1467x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1467x2048): 82.516
Elapsed time for attention_prob_times_values (48x2048x2048x1467): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1467): 69.888

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1376.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1468x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1468x2048): 83.885
Elapsed time for attention_prob_times_values (48x2048x2048x1468): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1468): 75.446

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1446.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1469x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1469x2048): 82.373
Elapsed time for attention_prob_times_values (48x2048x2048x1469): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1469): 69.836

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1376.829
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x547x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x547x2048): 73.211
Elapsed time for attention_prob_times_values (128x2048x2048x547): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x547): 65.624

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1252.273
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x548x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x548x2048): 76.261
Elapsed time for attention_prob_times_values (128x2048x2048x548): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x548): 68.408

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1307.208
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x549x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x549x2048): 74.546
Elapsed time for attention_prob_times_values (128x2048x2048x549): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x549): 66.744

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1278.743
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x550x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x550x2048): 75.135
Elapsed time for attention_prob_times_values (128x2048x2048x550): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x550): 69.122

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1309.557
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x551x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x551x2048): 74.200
Elapsed time for attention_prob_times_values (128x2048x2048x551): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x551): 66.467

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1277.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x552x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x552x2048): 76.045
Elapsed time for attention_prob_times_values (128x2048x2048x552): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x552): 82.209

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1441.873
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x553x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x553x2048): 72.356
Elapsed time for attention_prob_times_values (128x2048x2048x553): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x553): 65.782

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1259.803
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x554x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x554x2048): 72.834
Elapsed time for attention_prob_times_values (128x2048x2048x554): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x554): 69.739

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1304.820
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x555x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x555x2048): 73.299
Elapsed time for attention_prob_times_values (128x2048x2048x555): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x555): 65.762

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1271.708
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 12, hidden_size: 17640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1470x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1470x2048): 83.654
Elapsed time for attention_prob_times_values (48x2048x2048x1470): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1470): 75.221

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1443.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1471x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1471x2048): 82.680
Elapsed time for attention_prob_times_values (48x2048x2048x1471): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1471): 70.049

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1383.238
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1472x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1472x2048): 91.846
Elapsed time for attention_prob_times_values (48x2048x2048x1472): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1472): 85.209

Attention duration (in seconds): 0.0134
Attention throughput (in TFLOP/s): 1613.359
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0134
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1473x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1473x2048): 84.020
Elapsed time for attention_prob_times_values (48x2048x2048x1473): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1473): 69.957

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1394.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1474x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1474x2048): 85.092
Elapsed time for attention_prob_times_values (48x2048x2048x1474): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1474): 75.472

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1461.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1475x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1475x2048): 83.916
Elapsed time for attention_prob_times_values (48x2048x2048x1475): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1475): 70.198

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1397.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1476x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1476x2048): 85.342
Elapsed time for attention_prob_times_values (48x2048x2048x1476): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1476): 75.982

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1470.891
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1477x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1477x2048): 83.596
Elapsed time for attention_prob_times_values (48x2048x2048x1477): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1477): 70.210

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1397.327
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1478x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1478x2048): 84.680
Elapsed time for attention_prob_times_values (48x2048x2048x1478): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1478): 75.817

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1465.696
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1479x2048): 0.0071
num_attention_heads: 32, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x556x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x556x2048): 74.959
Elapsed time for attention_prob_times_values (128x2048x2048x556): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x556): 70.981

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1339.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x557x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x557x2048): 72.421
Elapsed time for attention_prob_times_values (128x2048x2048x557): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x557): 66.421

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1275.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x558x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x558x2048): 72.725
Elapsed time for attention_prob_times_values (128x2048x2048x558): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x558): 71.164

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1326.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x559x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x559x2048): 73.741
Elapsed time for attention_prob_times_values (128x2048x2048x559): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x559): 67.761

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1304.344
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x560x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x560x2048): 75.668
Elapsed time for attention_prob_times_values (128x2048x2048x560): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x560): 80.951

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1447.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x561x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x561x2048): 71.294
Elapsed time for attention_prob_times_values (128x2048x2048x561): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x561): 65.481

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1265.017
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x562x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x562x2048): 71.585
Elapsed time for attention_prob_times_values (128x2048x2048x562): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x562): 71.526

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1328.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x563x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x563x2048): 73.453
Elapsed time for attention_prob_times_values (128x2048x2048x563): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x563): 67.411

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1307.185
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x564x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x564x2048): 74.918
Elapsed time for attention_prob_times_values (128x2048x2048x564): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x564): 72.007

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1367.700
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x565x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x565x2048): 73.567
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1479x2048): 83.530
Elapsed time for attention_prob_times_values (48x2048x2048x1479): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1479): 70.446

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1401.160
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1480x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1480x2048): 85.470
Elapsed time for attention_prob_times_values (48x2048x2048x1480): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1480): 82.674

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1541.778
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1481x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1481x2048): 83.099
Elapsed time for attention_prob_times_values (48x2048x2048x1481): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1481): 70.645

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1401.760
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1482x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1482x2048): 84.043
Elapsed time for attention_prob_times_values (48x2048x2048x1482): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1482): 75.983

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1465.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1483x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1483x2048): 82.905
Elapsed time for attention_prob_times_values (48x2048x2048x1483): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1483): 70.818

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1403.902
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1484x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1484x2048): 84.562
Elapsed time for attention_prob_times_values (48x2048x2048x1484): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1484): 76.382

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1476.111
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1485x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1485x2048): 83.149
Elapsed time for attention_prob_times_values (48x2048x2048x1485): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1485): 70.726

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1406.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1486x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1486x2048): 84.219
Elapsed time for attention_prob_times_values (48x2048x2048x1486): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1486): 76.350

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1474.812
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1487x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1487x2048): 83.225
Elapsed time for attention_prob_times_values (48x2048x2048x1487): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1487): 71.243

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1414.537
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1488x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1488x2048): 85.506
Elapsed time for attention_prob_times_values (48x2048x2048x1488): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1488): 83.572

Attention duration (in seconds): 0.0142
Attention throughput (in TFLOP/s): 1558.480
MLP duration (in seconds): 0.0000
Elapsed time for attention_prob_times_values (128x2048x2048x565): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x565): 68.429

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1322.822
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x566x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x566x2048): 72.109
Elapsed time for attention_prob_times_values (128x2048x2048x566): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x566): 72.104

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1347.490
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x567x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x567x2048): 73.778
Elapsed time for attention_prob_times_values (128x2048x2048x567): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x567): 68.583

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1330.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x568x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x568x2048): 75.361
Elapsed time for attention_prob_times_values (128x2048x2048x568): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x568): 84.502

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1493.813
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x569x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x569x2048): 70.444
Elapsed time for attention_prob_times_values (128x2048x2048x569): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x569): 68.344

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1303.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x570x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x570x2048): 70.915
Elapsed time for attention_prob_times_values (128x2048x2048x570): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x570): 71.854

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1342.862
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x571x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x571x2048): 73.155
Elapsed time for attention_prob_times_values (128x2048x2048x571): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x571): 66.794

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1315.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x572x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x572x2048): 71.363
Elapsed time for attention_prob_times_values (128x2048x2048x572): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x572): 69.073

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1325.007
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x573x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x573x2048): 71.575
Elapsed time for attention_prob_times_values (128x2048x2048x573): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x573): 68.182

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1320.364
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x574x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x574x2048): 70.938
Elapsed time for attention_prob_times_values (128x2048x2048x574): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x574): 72.694

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1359.815
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0142
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1489x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1489x2048): 82.769
Elapsed time for attention_prob_times_values (48x2048x2048x1489): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1489): 71.353

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1413.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1490x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1490x2048): 83.871
Elapsed time for attention_prob_times_values (48x2048x2048x1490): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1490): 76.862

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1480.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1491x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1491x2048): 83.162
Elapsed time for attention_prob_times_values (48x2048x2048x1491): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1491): 71.380

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1419.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1492x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1492x2048): 84.496
Elapsed time for attention_prob_times_values (48x2048x2048x1492): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1492): 77.156

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1490.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1493x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1493x2048): 82.820
Elapsed time for attention_prob_times_values (48x2048x2048x1493): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1493): 71.508

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1419.567
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1494x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1494x2048): 83.915
Elapsed time for attention_prob_times_values (48x2048x2048x1494): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1494): 75.989

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1476.098
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1495x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1495x2048): 83.243
Elapsed time for attention_prob_times_values (48x2048x2048x1495): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1495): 71.645

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1426.178
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1496x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1496x2048): 83.768
Elapsed time for attention_prob_times_values (48x2048x2048x1496): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1496): 83.364

Attention duration (in seconds): 0.0144
Attention throughput (in TFLOP/s): 1548.573
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1497x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1497x2048): 82.675
Elapsed time for attention_prob_times_values (48x2048x2048x1497): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1497): 71.781

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1424.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1498x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1498x2048): 82.276
Elapsed time for attention_prob_times_values (48x2048x2048x1498): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1498): 75.259

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1458.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 17988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1499x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1499x2048): 82.570
Elapsed time for attention_prob_times_values (48x2048x2048x1499): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1499): 71.216

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1419.842
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1500x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1500x2048): 82.873
Elapsed time for attention_prob_times_values (48x2048x2048x1500): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1500): 77.596

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1488.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1501x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1501x2048): 82.723
Elapsed time for attention_prob_times_values (48x2048x2048x1501): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1501): 72.641

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1438.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1502x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1502x2048): 83.632
Elapsed time for attention_prob_times_values (48x2048x2048x1502): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1502): 77.566

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1497.151
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1503x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1503x2048): 82.990
Elapsed time for attention_prob_times_values (48x2048x2048x1503): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1503): 73.082

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1446.650
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1504x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1504x2048): 92.452
Elapsed time for attention_prob_times_values (48x2048x2048x1504): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1504): 85.494

Attention duration (in seconds): 0.0136
Attention throughput (in TFLOP/s): 1654.591
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1505x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1505x2048): 83.871
Elapsed time for attention_prob_times_values (48x2048x2048x1505): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1505): 72.852

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1453.180
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1506x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1506x2048): 84.939
Elapsed time for attention_prob_times_values (48x2048x2048x1506): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1506): 77.761

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1514.096
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1507x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1507x2048): 83.811
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x575x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x575x2048): 73.104
Elapsed time for attention_prob_times_values (128x2048x2048x575): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x575): 68.417

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1340.764
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x576x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x576x2048): 89.371
Elapsed time for attention_prob_times_values (128x2048x2048x576): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x576): 86.985

Attention duration (in seconds): 0.0140
Attention throughput (in TFLOP/s): 1675.075
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0140
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x577x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x577x2048): 75.558
Elapsed time for attention_prob_times_values (128x2048x2048x577): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x577): 63.558

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1313.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x578x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x578x2048): 76.794
Elapsed time for attention_prob_times_values (128x2048x2048x578): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x578): 66.480

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1358.499
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x579x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x579x2048): 75.822
Elapsed time for attention_prob_times_values (128x2048x2048x579): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x579): 63.017

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1314.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x580x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x580x2048): 77.515
Elapsed time for attention_prob_times_values (128x2048x2048x580): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x580): 65.173

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1354.248
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x581x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x581x2048): 73.512
Elapsed time for attention_prob_times_values (128x2048x2048x581): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x581): 62.077

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1289.448
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x582x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x582x2048): 76.223
Elapsed time for attention_prob_times_values (128x2048x2048x582): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x582): 66.906

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1367.325
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x583x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x583x2048): 77.540
Elapsed time for attention_prob_times_values (128x2048x2048x583): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x583): 64.070

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1348.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (48x2048x2048x1507): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1507): 72.694

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1452.840
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1508x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1508x2048): 85.440
Elapsed time for attention_prob_times_values (48x2048x2048x1508): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1508): 77.984

Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 1522.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1509x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1509x2048): 83.633
Elapsed time for attention_prob_times_values (48x2048x2048x1509): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1509): 72.870

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1455.105
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1510x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1510x2048): 84.616
Elapsed time for attention_prob_times_values (48x2048x2048x1510): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1510): 76.361

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1500.806
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18132, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1511x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1511x2048): 83.615
Elapsed time for attention_prob_times_values (48x2048x2048x1511): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1511): 72.675

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1454.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1512x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1512x2048): 85.534
Elapsed time for attention_prob_times_values (48x2048x2048x1512): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1512): 84.214

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1588.643
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18156, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1513x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1513x2048): 82.871
Elapsed time for attention_prob_times_values (48x2048x2048x1513): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1513): 72.923

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1453.097
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1514x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1514x2048): 82.960
Elapsed time for attention_prob_times_values (48x2048x2048x1514): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1514): 78.064

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1507.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18180, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1515x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1515x2048): 83.072
Elapsed time for attention_prob_times_values (48x2048x2048x1515): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1515): 73.034

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1457.746
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1516x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1516x2048): 84.747
Elapsed time for attention_prob_times_values (48x2048x2048x1516): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1516): 78.349

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1527.948
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
--------
Elapsed time for attention_key_query_prob (128x2048x584x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x584x2048): 77.660
Elapsed time for attention_prob_times_values (128x2048x2048x584): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x584): 83.445

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1548.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x585x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x585x2048): 71.891
Elapsed time for attention_prob_times_values (128x2048x2048x585): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x585): 60.524

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1267.159
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x586x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x586x2048): 72.396
Elapsed time for attention_prob_times_values (128x2048x2048x586): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x586): 65.258

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1325.647
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x587x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x587x2048): 71.662
Elapsed time for attention_prob_times_values (128x2048x2048x587): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x587): 64.176

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1309.821
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x588x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x588x2048): 77.920
Elapsed time for attention_prob_times_values (128x2048x2048x588): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x588): 67.630

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1402.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x589x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x589x2048): 74.827
Elapsed time for attention_prob_times_values (128x2048x2048x589): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x589): 63.521

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1333.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x590x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x590x2048): 74.064
Elapsed time for attention_prob_times_values (128x2048x2048x590): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x590): 64.197

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1336.884
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x591x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x591x2048): 71.578
Elapsed time for attention_prob_times_values (128x2048x2048x591): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x591): 64.596

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1322.087
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x592x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x592x2048): 73.676
Elapsed time for attention_prob_times_values (128x2048x2048x592): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x592): 88.776

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1570.231
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 18976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x593x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x593x2048): 74.049
Elapsed time for attention_prob_times_values (128x2048x2048x593): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x593): 64.796

Attention duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18204, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1517x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1517x2048): 83.100
Elapsed time for attention_prob_times_values (48x2048x2048x1517): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1517): 70.531

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1432.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1518x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1518x2048): 82.988
Elapsed time for attention_prob_times_values (48x2048x2048x1518): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1518): 78.366

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1514.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18228, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1519x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1519x2048): 82.057
Elapsed time for attention_prob_times_values (48x2048x2048x1519): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1519): 72.004

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1442.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1520x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1520x2048): 85.791
Elapsed time for attention_prob_times_values (48x2048x2048x1520): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1520): 85.301

Attention duration (in seconds): 0.0143
Attention throughput (in TFLOP/s): 1609.326
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18252, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1521x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1521x2048): 81.909
Elapsed time for attention_prob_times_values (48x2048x2048x1521): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1521): 73.088

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1454.119
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1522x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1522x2048): 84.122
Elapsed time for attention_prob_times_values (48x2048x2048x1522): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1522): 78.578

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1530.519
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18276, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1523x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1523x2048): 81.157
Elapsed time for attention_prob_times_values (48x2048x2048x1523): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1523): 71.623

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1434.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1524x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1524x2048): 84.365
Elapsed time for attention_prob_times_values (48x2048x2048x1524): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1524): 78.463

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1533.395
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18300, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1525x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1525x2048): 82.840
Elapsed time for attention_prob_times_values (48x2048x2048x1525): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1525): 72.561

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1459.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1526x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1526x2048): 84.021
Elapsed time for attention_prob_times_values (48x2048x2048x1526): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1526): 78.601

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1533.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18324, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1527x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1527x2048): 83.243
Elapsed time for attention_prob_times_values (48x2048x2048x1527): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1527): 72.444

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1463.745
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1528x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1528x2048): 85.097
Elapsed time for attention_prob_times_values (48x2048x2048x1528): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1528): 85.117

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1609.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18348, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1529x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1529x2048): 82.841
Elapsed time for attention_prob_times_values (48x2048x2048x1529): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1529): 72.320

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1460.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1530x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1530x2048): 83.399
Elapsed time for attention_prob_times_values (48x2048x2048x1530): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1530): 78.820

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1534.146
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18372, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1531x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1531x2048): 82.745
Elapsed time for attention_prob_times_values (48x2048x2048x1531): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1531): 72.283

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1461.539
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1532x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1532x2048): 84.050
Elapsed time for attention_prob_times_values (48x2048x2048x1532): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1532): 78.862

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1542.286
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18396, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1533x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1533x2048): 82.682
Elapsed time for attention_prob_times_values (48x2048x2048x1533): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1533): 72.270

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1462.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1534x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1534x2048): 83.490
Elapsed time for attention_prob_times_values (48x2048x2048x1534): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1534): 78.392

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1534.458
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18420, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1535x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1535x2048): 82.097
Elapsed time for attention_prob_times_values (48x2048x2048x1535): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1535): 72.075

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1349.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x594x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x594x2048): 72.564
Elapsed time for attention_prob_times_values (128x2048x2048x594): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x594): 67.813

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1371.484
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x595x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x595x2048): 73.995
Elapsed time for attention_prob_times_values (128x2048x2048x595): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x595): 65.132

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1357.470
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x596x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x596x2048): 71.723
Elapsed time for attention_prob_times_values (128x2048x2048x596): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x596): 68.515

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1375.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x597x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x597x2048): 74.424
Elapsed time for attention_prob_times_values (128x2048x2048x597): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x597): 65.319

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1367.580
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x598x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x598x2048): 75.100
Elapsed time for attention_prob_times_values (128x2048x2048x598): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x598): 68.425

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1409.770
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x599x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x599x2048): 72.772
Elapsed time for attention_prob_times_values (128x2048x2048x599): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x599): 65.183

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1356.032
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x600x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x600x2048): 76.416
Elapsed time for attention_prob_times_values (128x2048x2048x600): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x600): 89.499

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1628.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x601x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x601x2048): 73.997
Elapsed time for attention_prob_times_values (128x2048x2048x601): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x601): 65.513

Attention duration (in seconds): 0.0186
Attention throughput (in TFLOP/s): 1374.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0186
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x602x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x602x2048): 74.524
Elapsed time for attention_prob_times_values (128x2048x2048x602): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x602): 67.752

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1406.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Attention throughput (in TFLOP/s): 1457.545
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1536x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1536x2048): 91.693
Elapsed time for attention_prob_times_values (48x2048x2048x1536): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1536): 89.036

Attention duration (in seconds): 0.0137
Attention throughput (in TFLOP/s): 1716.553
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0137
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18444, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1537x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1537x2048): 83.457
Elapsed time for attention_prob_times_values (48x2048x2048x1537): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1537): 68.871

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1434.736
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1538x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1538x2048): 84.773
Elapsed time for attention_prob_times_values (48x2048x2048x1538): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1538): 74.385

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1507.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18468, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1539x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1539x2048): 83.716
Elapsed time for attention_prob_times_values (48x2048x2048x1539): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1539): 69.283

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1443.220
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1540x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1540x2048): 85.599
Elapsed time for attention_prob_times_values (48x2048x2048x1540): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1540): 74.614

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1518.605
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18492, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1541x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1541x2048): 83.901
Elapsed time for attention_prob_times_values (48x2048x2048x1541): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1541): 69.314

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1446.791
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1542x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1542x2048): 84.748
Elapsed time for attention_prob_times_values (48x2048x2048x1542): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1542): 71.375

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1477.737
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18516, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1543x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1543x2048): 81.034
Elapsed time for attention_prob_times_values (48x2048x2048x1543): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1543): 69.581

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1428.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1544x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1544x2048): 83.328
Elapsed time for attention_prob_times_values (48x2048x2048x1544): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1544): 77.494

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1533.324
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x603x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x603x2048): 70.585
Elapsed time for attention_prob_times_values (128x2048x2048x603): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x603): 61.441

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1303.653
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x604x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x604x2048): 73.546
Elapsed time for attention_prob_times_values (128x2048x2048x604): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x604): 66.818

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1391.667
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x605x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x605x2048): 71.480
Elapsed time for attention_prob_times_values (128x2048x2048x605): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x605): 66.054

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1366.767
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x606x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x606x2048): 75.089
Elapsed time for attention_prob_times_values (128x2048x2048x606): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x606): 69.267

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1436.705
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x607x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x607x2048): 74.449
Elapsed time for attention_prob_times_values (128x2048x2048x607): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x607): 64.118

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1375.817
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x608x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x608x2048): 87.912
Elapsed time for attention_prob_times_values (128x2048x2048x608): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x608): 92.024

Attention duration (in seconds): 0.0145
Attention throughput (in TFLOP/s): 1798.423
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x609x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x609x2048): 74.079
Elapsed time for attention_prob_times_values (128x2048x2048x609): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x609): 66.538

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1404.313
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x610x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x610x2048): 77.670
Elapsed time for attention_prob_times_values (128x2048x2048x610): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x610): 67.985

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1454.641
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x611x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x611x2048): 76.330
Elapsed time for attention_prob_times_values (128x2048x2048x611): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x611): 66.627

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1429.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x612x2048): 0.0084
========================================================================================================================
num_attention_heads: 12, hidden_size: 18540, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1545x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1545x2048): 83.315
Elapsed time for attention_prob_times_values (48x2048x2048x1545): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1545): 69.655

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1449.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1546x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1546x2048): 84.546
Elapsed time for attention_prob_times_values (48x2048x2048x1546): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1546): 71.205

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1477.846
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18564, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1547x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1547x2048): 83.461
Elapsed time for attention_prob_times_values (48x2048x2048x1547): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1547): 69.879

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1455.108
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1548x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1548x2048): 80.451
Elapsed time for attention_prob_times_values (48x2048x2048x1548): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1548): 72.036

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1454.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18588, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1549x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1549x2048): 83.675
Elapsed time for attention_prob_times_values (48x2048x2048x1549): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1549): 69.927

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1459.136
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1550x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1550x2048): 81.648
Elapsed time for attention_prob_times_values (48x2048x2048x1550): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1550): 74.800

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1496.223
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18612, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1551x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1551x2048): 83.631
Elapsed time for attention_prob_times_values (48x2048x2048x1551): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1551): 70.150

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1463.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1552x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1552x2048): 83.496
Elapsed time for attention_prob_times_values (48x2048x2048x1552): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1552): 77.264

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1539.968
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18636, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1553x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1553x2048): 83.189
Elapsed time for attention_prob_times_values (48x2048x2048x1553): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1553): 67.341

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1429.008
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1554x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x612x2048): 78.088
Elapsed time for attention_prob_times_values (128x2048x2048x612): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x612): 69.924

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1484.844
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x613x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x613x2048): 76.151
Elapsed time for attention_prob_times_values (128x2048x2048x613): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x613): 66.907

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1435.735
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x614x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x614x2048): 76.937
Elapsed time for attention_prob_times_values (128x2048x2048x614): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x614): 69.621

Attention duration (in seconds): 0.0180
Attention throughput (in TFLOP/s): 1475.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x615x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x615x2048): 75.878
Elapsed time for attention_prob_times_values (128x2048x2048x615): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x615): 67.251

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1441.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x616x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x616x2048): 77.827
Elapsed time for attention_prob_times_values (128x2048x2048x616): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x616): 91.737

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1705.280
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x617x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x617x2048): 74.989
Elapsed time for attention_prob_times_values (128x2048x2048x617): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x617): 67.116

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1436.607
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x618x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x618x2048): 75.798
Elapsed time for attention_prob_times_values (128x2048x2048x618): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x618): 69.228

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1469.892
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x619x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x619x2048): 73.777
Elapsed time for attention_prob_times_values (128x2048x2048x619): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x619): 67.359

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1432.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x620x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x620x2048): 76.506
Elapsed time for attention_prob_times_values (128x2048x2048x620): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x620): 70.623

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 1496.481
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x621x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x621x2048): 75.294
Elapsed time for attention_prob_times_values (128x2048x2048x621): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x621): 67.590

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1453.628
MLP duration (in seconds): 0.0000
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1554x2048): 81.724
Elapsed time for attention_prob_times_values (48x2048x2048x1554): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1554): 74.972

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1502.341
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18660, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1555x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1555x2048): 79.927
Elapsed time for attention_prob_times_values (48x2048x2048x1555): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1555): 70.322

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1438.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1556x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1556x2048): 84.904
Elapsed time for attention_prob_times_values (48x2048x2048x1556): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1556): 75.178

Attention duration (in seconds): 0.0157
Attention throughput (in TFLOP/s): 1533.853
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0157
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18684, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1557x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1557x2048): 83.313
Elapsed time for attention_prob_times_values (48x2048x2048x1557): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1557): 68.065

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1441.936
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1558x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1558x2048): 81.101
Elapsed time for attention_prob_times_values (48x2048x2048x1558): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1558): 75.054

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1501.339
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18708, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1559x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1559x2048): 81.308
Elapsed time for attention_prob_times_values (48x2048x2048x1559): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1559): 67.670

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1423.343
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1560x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1560x2048): 85.553
Elapsed time for attention_prob_times_values (48x2048x2048x1560): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1560): 80.564

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1600.028
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18732, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1561x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1561x2048): 80.821
Elapsed time for attention_prob_times_values (48x2048x2048x1561): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1561): 70.667

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1454.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1562x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1562x2048): 84.058
Elapsed time for attention_prob_times_values (48x2048x2048x1562): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1562): 75.122

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1531.615
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18756, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1563x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1563x2048): 79.448
Elapsed time for attention_prob_times_values (48x2048x2048x1563): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1563): 68.320

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1419.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1564x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1564x2048): 84.789
Elapsed time for attention_prob_times_values (48x2048x2048x1564): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1564): 71.948

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1504.555
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18780, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1565x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1565x2048): 80.939
Elapsed time for attention_prob_times_values (48x2048x2048x1565): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1565): 70.878

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1461.611
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1566x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1566x2048): 81.550
Elapsed time for attention_prob_times_values (48x2048x2048x1566): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1566): 75.338

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1515.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18804, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1567x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1567x2048): 83.166
Elapsed time for attention_prob_times_values (48x2048x2048x1567): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1567): 71.097

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1484.373
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1568x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1568x2048): 93.112
Elapsed time for attention_prob_times_values (48x2048x2048x1568): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1568): 79.349

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1660.080
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18828, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1569x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1569x2048): 85.003
Elapsed time for attention_prob_times_values (48x2048x2048x1569): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1569): 71.145

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1501.682
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1570x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1570x2048): 86.101
Elapsed time for attention_prob_times_values (48x2048x2048x1570): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1570): 72.124

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1522.687
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18852, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1571x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1571x2048): 84.542
Elapsed time for attention_prob_times_values (48x2048x2048x1571): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1571): 67.237

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1453.883
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1572x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1572x2048): 84.034
Elapsed time for attention_prob_times_values (48x2048x2048x1572): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1572): 75.661

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1546.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x622x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x622x2048): 76.243
Elapsed time for attention_prob_times_values (128x2048x2048x622): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x622): 70.401

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1496.134
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x623x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x623x2048): 74.792
Elapsed time for attention_prob_times_values (128x2048x2048x623): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x623): 66.613

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 1442.350
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0190
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x624x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x624x2048): 77.343
Elapsed time for attention_prob_times_values (128x2048x2048x624): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x624): 93.385

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1734.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x625x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x625x2048): 74.692
Elapsed time for attention_prob_times_values (128x2048x2048x625): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x625): 67.853

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1459.942
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x626x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x626x2048): 75.512
Elapsed time for attention_prob_times_values (128x2048x2048x626): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x626): 70.946

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1504.311
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x627x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x627x2048): 74.774
Elapsed time for attention_prob_times_values (128x2048x2048x627): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x627): 68.111

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1468.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x628x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x628x2048): 76.135
Elapsed time for attention_prob_times_values (128x2048x2048x628): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x628): 70.960

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1515.041
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x629x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x629x2048): 74.905
Elapsed time for attention_prob_times_values (128x2048x2048x629): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x629): 68.342

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1476.366
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x630x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x630x2048): 75.851
Elapsed time for attention_prob_times_values (128x2048x2048x630): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x630): 70.988

Attention duration (in seconds): 0.0184
Attention throughput (in TFLOP/s): 1517.202
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0184
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18876, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1573x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1573x2048): 84.154
Elapsed time for attention_prob_times_values (48x2048x2048x1573): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1573): 70.891

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1495.511
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1574x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1574x2048): 85.389
Elapsed time for attention_prob_times_values (48x2048x2048x1574): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1574): 75.523

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1558.609
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18900, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1575x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1575x2048): 84.159
Elapsed time for attention_prob_times_values (48x2048x2048x1575): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1575): 70.754

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1495.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1576x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1576x2048): 79.243
Elapsed time for attention_prob_times_values (48x2048x2048x1576): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1576): 81.249

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1562.047
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18924, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1577x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1577x2048): 76.215
Elapsed time for attention_prob_times_values (48x2048x2048x1577): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1577): 66.318

Attention duration (in seconds): 0.0179
Attention throughput (in TFLOP/s): 1381.603
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1578x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1578x2048): 84.429
Elapsed time for attention_prob_times_values (48x2048x2048x1578): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1578): 75.678

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1555.759
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18948, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1579x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1579x2048): 83.611
Elapsed time for attention_prob_times_values (48x2048x2048x1579): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1579): 63.355

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1405.986
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1580x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1580x2048): 85.380
Elapsed time for attention_prob_times_values (48x2048x2048x1580): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1580): 75.918

Attention duration (in seconds): 0.0158
Attention throughput (in TFLOP/s): 1568.505
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0158
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18972, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1581x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1581x2048): 83.581
Elapsed time for attention_prob_times_values (48x2048x2048x1581): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1581): 64.848

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1426.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1582x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1582x2048): 84.845
num_attention_heads: 32, hidden_size: 20192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x631x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x631x2048): 75.149
Elapsed time for attention_prob_times_values (128x2048x2048x631): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x631): 68.330

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 1482.997
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0189
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x632x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x632x2048): 76.435
Elapsed time for attention_prob_times_values (128x2048x2048x632): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x632): 94.061

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1749.984
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x633x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x633x2048): 74.377
Elapsed time for attention_prob_times_values (128x2048x2048x633): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x633): 68.394

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1480.865
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x634x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x634x2048): 75.175
Elapsed time for attention_prob_times_values (128x2048x2048x634): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x634): 70.485

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1514.201
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x635x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x635x2048): 74.439
Elapsed time for attention_prob_times_values (128x2048x2048x635): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x635): 68.459

Attention duration (in seconds): 0.0191
Attention throughput (in TFLOP/s): 1486.649
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x636x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x636x2048): 75.440
Elapsed time for attention_prob_times_values (128x2048x2048x636): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x636): 70.428

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1520.691
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x637x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x637x2048): 73.868
Elapsed time for attention_prob_times_values (128x2048x2048x637): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x637): 69.119

Attention duration (in seconds): 0.0192
Attention throughput (in TFLOP/s): 1493.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0192
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x638x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x638x2048): 75.152
Elapsed time for attention_prob_times_values (128x2048x2048x638): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x638): 71.511

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1534.430
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x639x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x639x2048): 74.233
Elapsed time for attention_prob_times_values (128x2048x2048x639): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x639): 67.587

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1483.635
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x640x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x640x2048): 86.473
Elapsed time for attention_prob_times_values (48x2048x2048x1582): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1582): 75.734

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1563.734
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 18996, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1583x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1583x2048): 74.975
Elapsed time for attention_prob_times_values (48x2048x2048x1583): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1583): 70.980

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1425.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1584x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1584x2048): 86.164
Elapsed time for attention_prob_times_values (48x2048x2048x1584): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1584): 82.170

Attention duration (in seconds): 0.0152
Attention throughput (in TFLOP/s): 1645.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0152
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19020, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1585x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1585x2048): 83.515
Elapsed time for attention_prob_times_values (48x2048x2048x1585): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1585): 70.949

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1501.750
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1586x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1586x2048): 84.304
Elapsed time for attention_prob_times_values (48x2048x2048x1586): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1586): 75.919

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1564.769
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19044, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1587x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1587x2048): 76.296
Elapsed time for attention_prob_times_values (48x2048x2048x1587): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1587): 70.942

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1440.857
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1588x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1588x2048): 78.381
Elapsed time for attention_prob_times_values (48x2048x2048x1588): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1588): 76.182

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1515.133
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19068, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1589x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1589x2048): 83.427
Elapsed time for attention_prob_times_values (48x2048x2048x1589): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1589): 70.899

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1504.037
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1590x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1590x2048): 84.658
Elapsed time for attention_prob_times_values (48x2048x2048x1590): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1590): 68.827

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1490.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19092, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1591x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1591x2048): 83.573
Elapsed time for attention_prob_times_values (48x2048x2048x1591): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1591): 70.941

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1507.535
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1592x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1592x2048): 82.766
Elapsed time for attention_prob_times_values (48x2048x2048x1592): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1592): 76.922

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1567.329
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19116, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1593x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1593x2048): 83.274
Elapsed time for attention_prob_times_values (48x2048x2048x1593): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1593): 63.490

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1417.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1594x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1594x2048): 75.836
Elapsed time for attention_prob_times_values (48x2048x2048x1594): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1594): 76.205

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1496.053
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19140, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1595x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1595x2048): 83.428
Elapsed time for attention_prob_times_values (48x2048x2048x1595): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1595): 70.908

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1509.542
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1596x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1596x2048): 84.857
Elapsed time for attention_prob_times_values (48x2048x2048x1596): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1596): 76.442

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1584.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19164, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1597x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1597x2048): 83.316
Elapsed time for attention_prob_times_values (48x2048x2048x1597): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1597): 66.339

Attention duration (in seconds): 0.0174
Attention throughput (in TFLOP/s): 1456.230
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0174
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1598x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1598x2048): 84.300
Elapsed time for attention_prob_times_values (48x2048x2048x1598): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1598): 76.312

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1580.246
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19188, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1599x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1599x2048): 83.533
Elapsed time for attention_prob_times_values (48x2048x2048x1599): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1599): 68.063

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1480.547
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1600x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1600x2048): 92.613
Elapsed time for attention_prob_times_values (48x2048x2048x1600): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1600): 81.658

Attention duration (in seconds): 0.0148
Attention throughput (in TFLOP/s): 1714.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0148
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19212, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Elapsed time for attention_prob_times_values (128x2048x2048x640): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x640): 97.385

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 1923.704
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0150
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x641x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x641x2048): 76.296
Elapsed time for attention_prob_times_values (128x2048x2048x641): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x641): 62.779

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1448.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x642x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x642x2048): 77.351
Elapsed time for attention_prob_times_values (128x2048x2048x642): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x642): 66.470

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1505.947
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x643x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x643x2048): 75.916
Elapsed time for attention_prob_times_values (128x2048x2048x643): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x643): 63.836

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1462.940
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x644x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x644x2048): 77.907
Elapsed time for attention_prob_times_values (128x2048x2048x644): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x644): 66.235

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 1512.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x645x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x645x2048): 74.422
Elapsed time for attention_prob_times_values (128x2048x2048x645): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x645): 64.090

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1457.042
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x646x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x646x2048): 77.183
Elapsed time for attention_prob_times_values (128x2048x2048x646): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x646): 66.722

Attention duration (in seconds): 0.0194
Attention throughput (in TFLOP/s): 1516.429
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0194
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x647x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x647x2048): 76.220
Elapsed time for attention_prob_times_values (128x2048x2048x647): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x647): 64.088

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1477.455
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x648x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x648x2048): 73.508
Elapsed time for attention_prob_times_values (128x2048x2048x648): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x648): 77.225

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 1600.562
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x649x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x649x2048): 71.754
Elapsed time for attention_prob_times_values (128x2048x2048x649): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x649): 62.436

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1420.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
--------
Elapsed time for attention_key_query_prob (48x2048x1601x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1601x2048): 84.813
Elapsed time for attention_prob_times_values (48x2048x2048x1601): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1601): 71.045

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1528.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1602x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1602x2048): 86.046
Elapsed time for attention_prob_times_values (48x2048x2048x1602): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1602): 71.185

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1540.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19236, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1603x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1603x2048): 84.717
Elapsed time for attention_prob_times_values (48x2048x2048x1603): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1603): 71.325

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1532.295
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1604x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1604x2048): 86.192
Elapsed time for attention_prob_times_values (48x2048x2048x1604): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1604): 72.840

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1563.067
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19260, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1605x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1605x2048): 84.529
Elapsed time for attention_prob_times_values (48x2048x2048x1605): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1605): 71.328

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1532.582
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1606x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1606x2048): 79.689
Elapsed time for attention_prob_times_values (48x2048x2048x1606): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1606): 71.073

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1489.196
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19284, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1607x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1607x2048): 84.393
Elapsed time for attention_prob_times_values (48x2048x2048x1607): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1607): 68.601

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1500.921
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1608x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1608x2048): 83.778
Elapsed time for attention_prob_times_values (48x2048x2048x1608): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1608): 82.568

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1650.382
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19308, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1609x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1609x2048): 76.792
Elapsed time for attention_prob_times_values (48x2048x2048x1609): 0.0094
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1609): 69.165

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1445.066
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1610x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1610x2048): 84.954
Elapsed time for attention_prob_times_values (48x2048x2048x1610): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1610): 76.866

Attention duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x650x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x650x2048): 75.404
Elapsed time for attention_prob_times_values (128x2048x2048x650): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x650): 67.198

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1514.570
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x651x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x651x2048): 75.303
Elapsed time for attention_prob_times_values (128x2048x2048x651): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x651): 64.460

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1482.558
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x652x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x652x2048): 76.781
Elapsed time for attention_prob_times_values (128x2048x2048x652): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x652): 67.247

Attention duration (in seconds): 0.0195
Attention throughput (in TFLOP/s): 1532.559
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x653x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x653x2048): 75.762
Elapsed time for attention_prob_times_values (128x2048x2048x653): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x653): 64.427

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1490.655
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x654x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x654x2048): 76.515
Elapsed time for attention_prob_times_values (128x2048x2048x654): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x654): 67.161

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1533.498
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x655x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x655x2048): 75.612
Elapsed time for attention_prob_times_values (128x2048x2048x655): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x655): 64.686

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1496.874
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x656x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x656x2048): 77.886
Elapsed time for attention_prob_times_values (128x2048x2048x656): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x656): 81.943

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 1717.048
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0176
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x657x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x657x2048): 75.239
Elapsed time for attention_prob_times_values (128x2048x2048x657): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x657): 64.984

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1501.522
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x658x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x658x2048): 76.158
Elapsed time for attention_prob_times_values (128x2048x2048x658): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x658): 67.925

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1548.320
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
Attention throughput (in TFLOP/s): 1603.438
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19332, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1611x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1611x2048): 83.958
Elapsed time for attention_prob_times_values (48x2048x2048x1611): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1611): 71.727

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1537.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1612x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1612x2048): 85.605
Elapsed time for attention_prob_times_values (48x2048x2048x1612): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1612): 77.077

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1613.478
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19356, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1613x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1613x2048): 83.798
Elapsed time for attention_prob_times_values (48x2048x2048x1613): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1613): 71.753

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1538.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1614x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1614x2048): 85.171
Elapsed time for attention_prob_times_values (48x2048x2048x1614): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1614): 76.959

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1610.187
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19380, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1615x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1615x2048): 84.011
Elapsed time for attention_prob_times_values (48x2048x2048x1615): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1615): 72.181

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1547.191
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1616x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1616x2048): 86.260
Elapsed time for attention_prob_times_values (48x2048x2048x1616): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1616): 83.674

Attention duration (in seconds): 0.0153
Attention throughput (in TFLOP/s): 1693.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0153
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19404, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1617x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1617x2048): 83.687
Elapsed time for attention_prob_times_values (48x2048x2048x1617): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1617): 72.385

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1548.593
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1618x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1618x2048): 84.665
Elapsed time for attention_prob_times_values (48x2048x2048x1618): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1618): 77.180

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1611.833
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19428, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1619x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1619x2048): 83.749
Elapsed time for attention_prob_times_values (48x2048x2048x1619): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1619): 72.325

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1550.259
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
--------
Elapsed time for attention_key_query_prob (128x2048x659x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x659x2048): 75.378
Elapsed time for attention_prob_times_values (128x2048x2048x659): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x659): 65.041

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1507.870
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x660x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x660x2048): 76.750
Elapsed time for attention_prob_times_values (128x2048x2048x660): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x660): 68.166

Attention duration (in seconds): 0.0196
Attention throughput (in TFLOP/s): 1561.406
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x661x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x661x2048): 75.473
Elapsed time for attention_prob_times_values (128x2048x2048x661): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x661): 65.294

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1516.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x662x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x662x2048): 76.352
Elapsed time for attention_prob_times_values (128x2048x2048x662): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x662): 67.936

Attention duration (in seconds): 0.0198
Attention throughput (in TFLOP/s): 1559.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x663x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x663x2048): 75.728
Elapsed time for attention_prob_times_values (128x2048x2048x663): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x663): 65.444

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1524.907
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x664x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x664x2048): 77.604
Elapsed time for attention_prob_times_values (128x2048x2048x664): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x664): 82.382

Attention duration (in seconds): 0.0178
Attention throughput (in TFLOP/s): 1738.297
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0178
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x665x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x665x2048): 74.820
Elapsed time for attention_prob_times_values (128x2048x2048x665): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x665): 64.538

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1509.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x666x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x666x2048): 75.096
Elapsed time for attention_prob_times_values (128x2048x2048x666): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x666): 68.199

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1559.189
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x667x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x667x2048): 75.265
Elapsed time for attention_prob_times_values (128x2048x2048x667): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x667): 65.564

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1530.824
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x668x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x668x2048): 75.101
Elapsed time for attention_prob_times_values (128x2048x2048x668): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x668): 68.555

Attention duration (in seconds): 0.0200
========================================================================================================================
num_attention_heads: 12, hidden_size: 19440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1620x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1620x2048): 85.402
Elapsed time for attention_prob_times_values (48x2048x2048x1620): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1620): 77.453

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1623.396
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19452, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1621x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1621x2048): 83.561
Elapsed time for attention_prob_times_values (48x2048x2048x1621): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1621): 72.380

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1551.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1622x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1622x2048): 84.735
Elapsed time for attention_prob_times_values (48x2048x2048x1622): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1622): 77.373

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1618.369
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19476, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1623x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1623x2048): 83.736
Elapsed time for attention_prob_times_values (48x2048x2048x1623): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1623): 72.486

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1555.639
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1624x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1624x2048): 86.105
Elapsed time for attention_prob_times_values (48x2048x2048x1624): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1624): 83.286

Attention duration (in seconds): 0.0154
Attention throughput (in TFLOP/s): 1696.084
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0154
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19500, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1625x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1625x2048): 83.762
Elapsed time for attention_prob_times_values (48x2048x2048x1625): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1625): 72.647

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1559.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1626x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1626x2048): 84.573
Elapsed time for attention_prob_times_values (48x2048x2048x1626): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1626): 77.520

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1622.281
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19524, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1627x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1627x2048): 83.870
Elapsed time for attention_prob_times_values (48x2048x2048x1627): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1627): 72.902

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1565.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1628x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1628x2048): 85.085
Elapsed time for attention_prob_times_values (48x2048x2048x1628): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1628): 77.811

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1632.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19548, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1629x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1629x2048): 83.677
Elapsed time for attention_prob_times_values (48x2048x2048x1629): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1629): 72.839

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1564.656
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1630x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1630x2048): 84.723
Elapsed time for attention_prob_times_values (48x2048x2048x1630): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1630): 77.709

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1629.521
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19572, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1631x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1631x2048): 83.665
Elapsed time for attention_prob_times_values (48x2048x2048x1631): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1631): 73.210

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1570.628
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1632x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1632x2048): 94.171
Elapsed time for attention_prob_times_values (48x2048x2048x1632): 0.0077
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1632): 85.437

Attention duration (in seconds): 0.0147
Attention throughput (in TFLOP/s): 1803.036
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19596, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1633x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1633x2048): 85.803
Elapsed time for attention_prob_times_values (48x2048x2048x1633): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1633): 72.274

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1579.918
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1634x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1634x2048): 86.779
Elapsed time for attention_prob_times_values (48x2048x2048x1634): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1634): 77.915

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1654.356
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19620, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1635x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1635x2048): 87.227
Elapsed time for attention_prob_times_values (48x2048x2048x1635): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1635): 73.123

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1603.838
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1636x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1636x2048): 86.141
Elapsed time for attention_prob_times_values (48x2048x2048x1636): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1636): 78.149

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1653.094
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19644, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1637x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1637x2048): 85.048
Elapsed time for attention_prob_times_values (48x2048x2048x1637): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1637): 72.957

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1585.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1638x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1638x2048): 85.529
Elapsed time for attention_prob_times_values (48x2048x2048x1638): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1638): 78.061

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1648.432
MLP duration (in seconds): 0.0000
Attention throughput (in TFLOP/s): 1567.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x669x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x669x2048): 75.334
Elapsed time for attention_prob_times_values (128x2048x2048x669): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x669): 65.895

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1539.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x670x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x670x2048): 76.224
Elapsed time for attention_prob_times_values (128x2048x2048x670): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x670): 68.584

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1583.941
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x671x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x671x2048): 75.652
Elapsed time for attention_prob_times_values (128x2048x2048x671): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x671): 65.992

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1548.636
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x672x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x672x2048): 91.807
Elapsed time for attention_prob_times_values (128x2048x2048x672): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x672): 84.990

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1941.879
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x673x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x673x2048): 75.639
Elapsed time for attention_prob_times_values (128x2048x2048x673): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x673): 64.291

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1531.274
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x674x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x674x2048): 76.734
Elapsed time for attention_prob_times_values (128x2048x2048x674): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x674): 68.697

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1599.376
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x675x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x675x2048): 74.724
Elapsed time for attention_prob_times_values (128x2048x2048x675): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x675): 66.175

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1550.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x676x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x676x2048): 79.499
Elapsed time for attention_prob_times_values (128x2048x2048x676): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x676): 68.975

Attention duration (in seconds): 0.0197
Attention throughput (in TFLOP/s): 1634.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0197
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x677x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x677x2048): 77.731
Elapsed time for attention_prob_times_values (128x2048x2048x677): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x677): 64.717

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1564.886
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19668, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1639x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1639x2048): 84.309
Elapsed time for attention_prob_times_values (48x2048x2048x1639): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1639): 72.908

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1580.089
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1640x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1640x2048): 86.241
Elapsed time for attention_prob_times_values (48x2048x2048x1640): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1640): 76.503

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1639.349
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19692, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1641x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1641x2048): 83.969
Elapsed time for attention_prob_times_values (48x2048x2048x1641): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1641): 72.926

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1579.168
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1642x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1642x2048): 86.632
Elapsed time for attention_prob_times_values (48x2048x2048x1642): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1642): 78.254

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1664.520
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19716, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1643x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1643x2048): 84.440
Elapsed time for attention_prob_times_values (48x2048x2048x1643): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1643): 72.946

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1585.335
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1644x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1644x2048): 85.617
Elapsed time for attention_prob_times_values (48x2048x2048x1644): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1644): 78.549

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1660.378
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19740, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1645x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1645x2048): 86.945
Elapsed time for attention_prob_times_values (48x2048x2048x1645): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1645): 73.115

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1610.684
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1646x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1646x2048): 85.162
Elapsed time for attention_prob_times_values (48x2048x2048x1646): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1646): 78.416

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1656.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19764, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1647x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1647x2048): 84.486
Elapsed time for attention_prob_times_values (48x2048x2048x1647): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1647): 72.263

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1581.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
========================================================================================================================
num_attention_heads: 32, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x678x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x678x2048): 78.509
Elapsed time for attention_prob_times_values (128x2048x2048x678): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x678): 68.424

Attention duration (in seconds): 0.0199
Attention throughput (in TFLOP/s): 1622.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0199
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x679x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x679x2048): 75.121
Elapsed time for attention_prob_times_values (128x2048x2048x679): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x679): 65.472

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1554.544
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x680x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x680x2048): 81.984
Elapsed time for attention_prob_times_values (128x2048x2048x680): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x680): 84.882

Attention duration (in seconds): 0.0175
Attention throughput (in TFLOP/s): 1855.827
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x681x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x681x2048): 75.604
Elapsed time for attention_prob_times_values (128x2048x2048x681): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x681): 66.194

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1572.757
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x682x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x682x2048): 77.733
Elapsed time for attention_prob_times_values (128x2048x2048x682): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x682): 68.986

Attention duration (in seconds): 0.0200
Attention throughput (in TFLOP/s): 1631.011
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x683x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x683x2048): 77.018
Elapsed time for attention_prob_times_values (128x2048x2048x683): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x683): 63.432

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1554.415
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x684x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x684x2048): 78.913
Elapsed time for attention_prob_times_values (128x2048x2048x684): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x684): 68.116

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 1636.016
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0201
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x685x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x685x2048): 73.764
Elapsed time for attention_prob_times_values (128x2048x2048x685): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x685): 62.775

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1519.761
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x686x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x686x2048): 76.026
Elapsed time for attention_prob_times_values (128x2048x2048x686): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x686): 64.694

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1568.467
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 21984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x687x2048): 0.0097
num_attention_heads: 12, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1648x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1648x2048): 86.781
Elapsed time for attention_prob_times_values (48x2048x2048x1648): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1648): 83.666

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1730.517
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19788, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1649x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1649x2048): 83.764
Elapsed time for attention_prob_times_values (48x2048x2048x1649): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1649): 73.129

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1587.046
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1650x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1650x2048): 83.757
Elapsed time for attention_prob_times_values (48x2048x2048x1650): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1650): 77.215

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1634.054
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19812, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1651x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1651x2048): 83.851
Elapsed time for attention_prob_times_values (48x2048x2048x1651): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1651): 72.629

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1583.809
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1652x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1652x2048): 84.895
Elapsed time for attention_prob_times_values (48x2048x2048x1652): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1652): 78.978

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1665.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19836, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1653x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1653x2048): 84.545
Elapsed time for attention_prob_times_values (48x2048x2048x1653): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1653): 73.218

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1598.621
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1654x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1654x2048): 84.902
Elapsed time for attention_prob_times_values (48x2048x2048x1654): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1654): 78.895

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1667.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19860, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1655x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1655x2048): 83.837
Elapsed time for attention_prob_times_values (48x2048x2048x1655): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1655): 82.816

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1699.345
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1656x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1656x2048): 85.695
Elapsed time for attention_prob_times_values (48x2048x2048x1656): 0.0079
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1656): 84.922

Attention duration (in seconds): 0.0156
Attention throughput (in TFLOP/s): 1740.789
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0156
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19884, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1657x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1657x2048): 83.570
Elapsed time for attention_prob_times_values (48x2048x2048x1657): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1657): 83.066

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1701.173
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1658x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1658x2048): 84.652
Elapsed time for attention_prob_times_values (48x2048x2048x1658): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1658): 82.925

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 1711.585
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19908, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1659x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1659x2048): 83.611
Elapsed time for attention_prob_times_values (48x2048x2048x1659): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1659): 83.107

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1703.964
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1660x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1660x2048): 82.731
Elapsed time for attention_prob_times_values (48x2048x2048x1660): 0.0080
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1660): 83.113

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1696.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19932, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1661x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1661x2048): 83.624
Elapsed time for attention_prob_times_values (48x2048x2048x1661): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1661): 80.604

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1679.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1662x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1662x2048): 81.800
Elapsed time for attention_prob_times_values (48x2048x2048x1662): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1662): 85.607

Attention duration (in seconds): 0.0160
Attention throughput (in TFLOP/s): 1713.064
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0160
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19956, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1663x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1663x2048): 80.987
Elapsed time for attention_prob_times_values (48x2048x2048x1663): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1663): 82.508

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1674.719
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1664x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1664x2048): 92.263
Elapsed time for attention_prob_times_values (48x2048x2048x1664): 0.0078
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1664): 85.903

Attention duration (in seconds): 0.0151
Attention throughput (in TFLOP/s): 1823.882
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0151
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19980, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1665x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1665x2048): 83.105
Elapsed time for attention_prob_times_values (48x2048x2048x1665): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1665): 78.329

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1654.194
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 19992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1666x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1666x2048): 85.523
Elapsed time for attention_prob_times_values (48x2048x2048x1666): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1666): 78.904

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1684.566
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x687x2048): 76.165
Elapsed time for attention_prob_times_values (128x2048x2048x687): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x687): 66.994

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1601.698
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x688x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x688x2048): 72.637
Elapsed time for attention_prob_times_values (128x2048x2048x688): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x688): 86.554

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1777.216
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x689x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x689x2048): 76.415
Elapsed time for attention_prob_times_values (128x2048x2048x689): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x689): 66.984

Attention duration (in seconds): 0.0207
Attention throughput (in TFLOP/s): 1608.495
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0207
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x690x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x690x2048): 77.385
Elapsed time for attention_prob_times_values (128x2048x2048x690): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x690): 70.037

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1658.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x691x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x691x2048): 76.769
Elapsed time for attention_prob_times_values (128x2048x2048x691): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x691): 66.619

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1611.724
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x692x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x692x2048): 76.861
Elapsed time for attention_prob_times_values (128x2048x2048x692): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x692): 70.063

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1658.527
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x693x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x693x2048): 75.796
Elapsed time for attention_prob_times_values (128x2048x2048x693): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x693): 67.305

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1615.362
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x694x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x694x2048): 77.583
Elapsed time for attention_prob_times_values (128x2048x2048x694): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x694): 70.312

Attention duration (in seconds): 0.0202
Attention throughput (in TFLOP/s): 1673.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0202
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x695x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x695x2048): 75.044
Elapsed time for attention_prob_times_values (128x2048x2048x695): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x695): 67.883

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1619.480
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x696x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x696x2048): 77.545
Elapsed time for attention_prob_times_values (128x2048x2048x696): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x696): 86.814

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1863.632
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20004, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1667x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1667x2048): 84.508
Elapsed time for attention_prob_times_values (48x2048x2048x1667): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1667): 78.592

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1672.439
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1668x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1668x2048): 86.268
Elapsed time for attention_prob_times_values (48x2048x2048x1668): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1668): 80.536

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1711.633
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20028, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1669x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1669x2048): 84.556
Elapsed time for attention_prob_times_values (48x2048x2048x1669): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1669): 78.600

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1674.903
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1670x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1670x2048): 83.258
Elapsed time for attention_prob_times_values (48x2048x2048x1670): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1670): 78.951

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1667.177
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20052, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1671x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1671x2048): 84.536
Elapsed time for attention_prob_times_values (48x2048x2048x1671): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1671): 78.714

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1677.877
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1672x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1672x2048): 83.995
Elapsed time for attention_prob_times_values (48x2048x2048x1672): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1672): 79.486

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1682.072
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20076, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1673x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1673x2048): 81.987
Elapsed time for attention_prob_times_values (48x2048x2048x1673): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1673): 76.962

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1635.980
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1674x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1674x2048): 85.103
Elapsed time for attention_prob_times_values (48x2048x2048x1674): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1674): 81.193

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1713.332
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20100, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1675x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1675x2048): 82.288
Elapsed time for attention_prob_times_values (48x2048x2048x1675): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1675): 78.936

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1662.219
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x697x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x697x2048): 76.354
Elapsed time for attention_prob_times_values (128x2048x2048x697): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x697): 68.087

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1639.887
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x698x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x698x2048): 77.045
Elapsed time for attention_prob_times_values (128x2048x2048x698): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x698): 70.790

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1683.227
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x699x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x699x2048): 76.490
Elapsed time for attention_prob_times_values (128x2048x2048x699): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x699): 68.081

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1645.689
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x700x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x700x2048): 78.207
Elapsed time for attention_prob_times_values (128x2048x2048x700): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x700): 70.504

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 1696.318
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0203
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x701x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x701x2048): 76.599
Elapsed time for attention_prob_times_values (128x2048x2048x701): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x701): 67.965

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1649.797
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x702x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x702x2048): 77.473
Elapsed time for attention_prob_times_values (128x2048x2048x702): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x702): 70.910

Attention duration (in seconds): 0.0204
Attention throughput (in TFLOP/s): 1698.442
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0204
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x703x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x703x2048): 76.519
Elapsed time for attention_prob_times_values (128x2048x2048x703): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x703): 68.323

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1658.092
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x704x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x704x2048): 92.279
Elapsed time for attention_prob_times_values (128x2048x2048x704): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x704): 86.648

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 2055.625
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x705x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x705x2048): 78.873
Elapsed time for attention_prob_times_values (128x2048x2048x705): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x705): 65.190

Attention duration (in seconds): 0.0212
Attention throughput (in TFLOP/s): 1644.018
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0212
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
--------
Elapsed time for attention_key_query_prob (48x2048x1676x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1676x2048): 85.599
Elapsed time for attention_prob_times_values (48x2048x2048x1676): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1676): 78.133

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1686.251
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20124, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1677x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1677x2048): 84.232
Elapsed time for attention_prob_times_values (48x2048x2048x1677): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1677): 79.021

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1684.059
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1678x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1678x2048): 85.381
Elapsed time for attention_prob_times_values (48x2048x2048x1678): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1678): 76.509

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1667.631
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20148, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1679x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1679x2048): 84.164
Elapsed time for attention_prob_times_values (48x2048x2048x1679): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1679): 79.257

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1687.908
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1680x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1680x2048): 83.565
Elapsed time for attention_prob_times_values (48x2048x2048x1680): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1680): 77.693

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1665.796
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20172, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1681x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1681x2048): 83.703
Elapsed time for attention_prob_times_values (48x2048x2048x1681): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1681): 77.440

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1665.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1682x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1682x2048): 82.582
Elapsed time for attention_prob_times_values (48x2048x2048x1682): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1682): 81.340

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1697.391
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20196, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1683x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1683x2048): 82.382
Elapsed time for attention_prob_times_values (48x2048x2048x1683): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1683): 77.372

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1653.632
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1684x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1684x2048): 85.411
Elapsed time for attention_prob_times_values (48x2048x2048x1684): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1684): 81.594

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1730.465
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20220, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1685x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1685x2048): 81.797
Elapsed time for attention_prob_times_values (48x2048x2048x1685): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1685): 79.294

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1670.602
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1686x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1686x2048): 84.982
Elapsed time for attention_prob_times_values (48x2048x2048x1686): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1686): 79.394

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1704.074
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20244, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1687x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1687x2048): 84.094
Elapsed time for attention_prob_times_values (48x2048x2048x1687): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1687): 79.464

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1697.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1688x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1688x2048): 86.009
Elapsed time for attention_prob_times_values (48x2048x2048x1688): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1688): 80.290

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1725.911
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20268, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1689x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1689x2048): 83.820
Elapsed time for attention_prob_times_values (48x2048x2048x1689): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1689): 79.611

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1697.979
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1690x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1690x2048): 81.139
Elapsed time for attention_prob_times_values (48x2048x2048x1690): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1690): 81.643

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1693.301
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20292, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1691x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1691x2048): 83.808
Elapsed time for attention_prob_times_values (48x2048x2048x1691): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1691): 79.528

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1698.866
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1692x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1692x2048): 81.892
Elapsed time for attention_prob_times_values (48x2048x2048x1692): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1692): 81.763

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1704.310
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20316, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1693x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1693x2048): 81.835
Elapsed time for attention_prob_times_values (48x2048x2048x1693): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1693): 76.671

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1649.867
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1694x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1694x2048): 84.911
Elapsed time for attention_prob_times_values (48x2048x2048x1694): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1694): 77.799

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1693.142
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
num_attention_heads: 32, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x706x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x706x2048): 79.882
Elapsed time for attention_prob_times_values (128x2048x2048x706): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x706): 68.121

Attention duration (in seconds): 0.0206
Attention throughput (in TFLOP/s): 1695.880
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0206
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x707x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x707x2048): 78.716
Elapsed time for attention_prob_times_values (128x2048x2048x707): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x707): 65.185

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1646.917
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x708x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x708x2048): 80.515
Elapsed time for attention_prob_times_values (128x2048x2048x708): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x708): 66.312

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1681.801
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x709x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x709x2048): 78.554
Elapsed time for attention_prob_times_values (128x2048x2048x709): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x709): 65.312

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1651.588
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x710x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x710x2048): 79.273
Elapsed time for attention_prob_times_values (128x2048x2048x710): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x710): 68.466

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1703.681
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x711x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x711x2048): 78.301
Elapsed time for attention_prob_times_values (128x2048x2048x711): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x711): 65.547

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1656.868
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x712x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x712x2048): 80.240
Elapsed time for attention_prob_times_values (128x2048x2048x712): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x712): 88.335

Attention duration (in seconds): 0.0182
Attention throughput (in TFLOP/s): 1955.170
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0182
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x713x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x713x2048): 77.532
Elapsed time for attention_prob_times_values (128x2048x2048x713): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x713): 64.690

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1642.051
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x714x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x714x2048): 78.306
Elapsed time for attention_prob_times_values (128x2048x2048x714): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x714): 68.757

Attention duration (in seconds): 0.0209
Attention throughput (in TFLOP/s): 1706.982
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0209
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x715x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x715x2048): 77.793
========================================================================================================================
num_attention_heads: 12, hidden_size: 20340, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1695x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1695x2048): 83.709
Elapsed time for attention_prob_times_values (48x2048x2048x1695): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1695): 79.809

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1704.795
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1696x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1696x2048): 91.190
Elapsed time for attention_prob_times_values (48x2048x2048x1696): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1696): 79.220

Attention duration (in seconds): 0.0161
Attention throughput (in TFLOP/s): 1769.885
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20364, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1697x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1697x2048): 85.239
Elapsed time for attention_prob_times_values (48x2048x2048x1697): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1697): 79.866

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1722.417
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1698x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1698x2048): 83.953
Elapsed time for attention_prob_times_values (48x2048x2048x1698): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1698): 81.954

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1733.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20388, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1699x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1699x2048): 85.167
Elapsed time for attention_prob_times_values (48x2048x2048x1699): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1699): 78.176

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1704.629
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1700x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1700x2048): 87.186
Elapsed time for attention_prob_times_values (48x2048x2048x1700): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1700): 82.044

Attention duration (in seconds): 0.0162
Attention throughput (in TFLOP/s): 1768.672
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0162
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20412, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1701x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1701x2048): 82.614
Elapsed time for attention_prob_times_values (48x2048x2048x1701): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1701): 79.736

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1698.751
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1702x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1702x2048): 85.999
Elapsed time for attention_prob_times_values (48x2048x2048x1702): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1702): 81.944

Attention duration (in seconds): 0.0163
Attention throughput (in TFLOP/s): 1757.779
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0163
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20436, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1703x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1703x2048): 82.531
Elapsed time for attention_prob_times_values (48x2048x2048x1703): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1703): 79.874

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1701.307
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1704x2048): 0.0079
Elapsed time for attention_prob_times_values (128x2048x2048x715): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x715): 65.593

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1661.460
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x716x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x716x2048): 79.232
Elapsed time for attention_prob_times_values (128x2048x2048x716): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x716): 69.111

Attention duration (in seconds): 0.0208
Attention throughput (in TFLOP/s): 1725.686
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x717x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x717x2048): 77.843
Elapsed time for attention_prob_times_values (128x2048x2048x717): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x717): 63.825

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1641.720
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x718x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x718x2048): 78.643
Elapsed time for attention_prob_times_values (128x2048x2048x718): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x718): 69.029

Attention duration (in seconds): 0.0210
Attention throughput (in TFLOP/s): 1723.199
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x719x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x719x2048): 77.779
Elapsed time for attention_prob_times_values (128x2048x2048x719): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x719): 65.196

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1664.725
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x720x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x720x2048): 79.822
Elapsed time for attention_prob_times_values (128x2048x2048x720): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x720): 89.708

Attention duration (in seconds): 0.0183
Attention throughput (in TFLOP/s): 1985.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x721x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x721x2048): 77.235
Elapsed time for attention_prob_times_values (128x2048x2048x721): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x721): 65.664

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1670.271
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x722x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x722x2048): 78.055
Elapsed time for attention_prob_times_values (128x2048x2048x722): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x722): 69.295

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 1729.828
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x723x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x723x2048): 77.357
Elapsed time for attention_prob_times_values (128x2048x2048x723): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x723): 65.882

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1678.931
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x724x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x724x2048): 78.879
Elapsed time for attention_prob_times_values (128x2048x2048x724): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x724): 67.968

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1725.044
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1704x2048): 86.809
Elapsed time for attention_prob_times_values (48x2048x2048x1704): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1704): 79.150

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1736.264
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20460, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1705x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1705x2048): 82.133
Elapsed time for attention_prob_times_values (48x2048x2048x1705): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1705): 79.975

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1700.243
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1706x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1706x2048): 85.250
Elapsed time for attention_prob_times_values (48x2048x2048x1706): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1706): 79.507

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1727.200
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20484, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1707x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1707x2048): 84.529
Elapsed time for attention_prob_times_values (48x2048x2048x1707): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1707): 77.888

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1702.845
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1708x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1708x2048): 83.758
Elapsed time for attention_prob_times_values (48x2048x2048x1708): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1708): 82.277

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1744.532
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20508, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1709x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1709x2048): 81.814
Elapsed time for attention_prob_times_values (48x2048x2048x1709): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1709): 77.598

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1674.832
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1710x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1710x2048): 85.657
Elapsed time for attention_prob_times_values (48x2048x2048x1710): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1710): 82.409

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1767.315
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20532, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1711x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1711x2048): 82.568
Elapsed time for attention_prob_times_values (48x2048x2048x1711): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1711): 80.134

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1712.113
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1712x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1712x2048): 86.881
Elapsed time for attention_prob_times_values (48x2048x2048x1712): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1712): 79.811

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1752.316
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20556, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1713x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1713x2048): 84.531
Elapsed time for attention_prob_times_values (48x2048x2048x1713): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1713): 80.227

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1734.896
MLP duration (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x725x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x725x2048): 77.400
Elapsed time for attention_prob_times_values (128x2048x2048x725): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x725): 65.168

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1673.896
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x726x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x726x2048): 78.168
Elapsed time for attention_prob_times_values (128x2048x2048x726): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x726): 68.754

Attention duration (in seconds): 0.0213
Attention throughput (in TFLOP/s): 1732.969
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0213
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x727x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x727x2048): 76.534
Elapsed time for attention_prob_times_values (128x2048x2048x727): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x727): 66.092

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1682.383
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x728x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x728x2048): 78.101
Elapsed time for attention_prob_times_values (128x2048x2048x728): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x728): 90.134

Attention duration (in seconds): 0.0187
Attention throughput (in TFLOP/s): 1987.575
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0187
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x729x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x729x2048): 76.818
Elapsed time for attention_prob_times_values (128x2048x2048x729): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x729): 66.203

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1691.237
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x730x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x730x2048): 77.641
Elapsed time for attention_prob_times_values (128x2048x2048x730): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x730): 68.080

Attention duration (in seconds): 0.0216
Attention throughput (in TFLOP/s): 1727.514
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x731x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x731x2048): 77.031
Elapsed time for attention_prob_times_values (128x2048x2048x731): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x731): 66.364

Attention duration (in seconds): 0.0220
Attention throughput (in TFLOP/s): 1700.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x732x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x732x2048): 78.555
Elapsed time for attention_prob_times_values (128x2048x2048x732): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x732): 68.435

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1746.370
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x733x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x733x2048): 77.101
Elapsed time for attention_prob_times_values (128x2048x2048x733): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x733): 66.308

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1704.471
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1714x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1714x2048): 85.228
Elapsed time for attention_prob_times_values (48x2048x2048x1714): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1714): 82.559

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1768.529
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20580, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1715x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1715x2048): 84.195
Elapsed time for attention_prob_times_values (48x2048x2048x1715): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1715): 80.368

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1735.002
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1716x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1716x2048): 85.521
Elapsed time for attention_prob_times_values (48x2048x2048x1716): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1716): 82.641

Attention duration (in seconds): 0.0164
Attention throughput (in TFLOP/s): 1774.385
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0164
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20604, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1717x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1717x2048): 84.536
Elapsed time for attention_prob_times_values (48x2048x2048x1717): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1717): 77.704

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1710.304
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1718x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1718x2048): 85.301
Elapsed time for attention_prob_times_values (48x2048x2048x1718): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1718): 82.716

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1774.925
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20628, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1719x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1719x2048): 82.130
Elapsed time for attention_prob_times_values (48x2048x2048x1719): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1719): 77.918

Attention duration (in seconds): 0.0173
Attention throughput (in TFLOP/s): 1690.894
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0173
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1720x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1720x2048): 86.486
Elapsed time for attention_prob_times_values (48x2048x2048x1720): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1720): 79.919

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1757.518
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20652, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1721x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1721x2048): 81.980
Elapsed time for attention_prob_times_values (48x2048x2048x1721): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1721): 80.499

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1719.534
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1722x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1722x2048): 82.890
Elapsed time for attention_prob_times_values (48x2048x2048x1722): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1722): 80.873

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1733.965
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20676, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1723x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1723x2048): 84.313
Elapsed time for attention_prob_times_values (48x2048x2048x1723): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1723): 78.383

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1721.586
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1724x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1724x2048): 85.766
Elapsed time for attention_prob_times_values (48x2048x2048x1724): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1724): 83.055

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1789.303
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20700, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1725x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1725x2048): 84.250
Elapsed time for attention_prob_times_values (48x2048x2048x1725): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1725): 80.686

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1748.730
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1726x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1726x2048): 85.347
Elapsed time for attention_prob_times_values (48x2048x2048x1726): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1726): 83.116

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1787.634
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20724, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1727x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1727x2048): 84.442
Elapsed time for attention_prob_times_values (48x2048x2048x1727): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1727): 80.798

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1753.858
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1728x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1728x2048): 94.240
Elapsed time for attention_prob_times_values (48x2048x2048x1728): 0.0081
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1728): 85.775

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 1908.434
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20748, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1729x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1729x2048): 83.525
Elapsed time for attention_prob_times_values (48x2048x2048x1729): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1729): 80.782

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1746.232
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1730x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1730x2048): 86.559
Elapsed time for attention_prob_times_values (48x2048x2048x1730): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1730): 81.163

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1782.166
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20772, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1731x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1731x2048): 85.460
Elapsed time for attention_prob_times_values (48x2048x2048x1731): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1731): 80.884

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1768.987
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1732x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1732x2048): 85.352
--------
Elapsed time for attention_key_query_prob (128x2048x734x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x734x2048): 77.923
Elapsed time for attention_prob_times_values (128x2048x2048x734): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x734): 68.148

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1740.453
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x735x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x735x2048): 77.188
Elapsed time for attention_prob_times_values (128x2048x2048x735): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x735): 66.042

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1706.128
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x736x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x736x2048): 92.907
Elapsed time for attention_prob_times_values (128x2048x2048x736): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x736): 92.547

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 2225.437
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x737x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x737x2048): 79.203
Elapsed time for attention_prob_times_values (128x2048x2048x737): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x737): 66.883

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1742.834
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x738x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x738x2048): 80.129
Elapsed time for attention_prob_times_values (128x2048x2048x738): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x738): 68.508

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1777.351
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x739x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x739x2048): 79.148
Elapsed time for attention_prob_times_values (128x2048x2048x739): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x739): 65.984

Attention duration (in seconds): 0.0221
Attention throughput (in TFLOP/s): 1733.998
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x740x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x740x2048): 80.717
Elapsed time for attention_prob_times_values (128x2048x2048x740): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x740): 68.727

Attention duration (in seconds): 0.0214
Attention throughput (in TFLOP/s): 1791.076
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0214
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x741x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x741x2048): 79.022
Elapsed time for attention_prob_times_values (128x2048x2048x741): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x741): 67.050

Attention duration (in seconds): 0.0219
Attention throughput (in TFLOP/s): 1752.428
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x742x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x742x2048): 79.870
Elapsed time for attention_prob_times_values (128x2048x2048x742): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x742): 68.876

Attention duration (in seconds): 0.0215
Attention throughput (in TFLOP/s): 1789.071
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0215
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x743x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x743x2048): 78.685
Elapsed time for attention_prob_times_values (128x2048x2048x743): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x743): 67.292

Attention duration (in seconds): 0.0220
Elapsed time for attention_prob_times_values (48x2048x2048x1732): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1732): 81.406

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1774.717
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20796, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1733x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1733x2048): 85.416
Elapsed time for attention_prob_times_values (48x2048x2048x1733): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1733): 81.038

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1772.225
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1734x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1734x2048): 84.207
Elapsed time for attention_prob_times_values (48x2048x2048x1734): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1734): 83.453

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1787.250
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20820, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1735x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1735x2048): 84.156
Elapsed time for attention_prob_times_values (48x2048x2048x1735): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1735): 80.135

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1751.289
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1736x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1736x2048): 87.400
Elapsed time for attention_prob_times_values (48x2048x2048x1736): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1736): 80.885

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1793.229
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20844, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1737x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1737x2048): 84.757
Elapsed time for attention_prob_times_values (48x2048x2048x1737): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1737): 81.266

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1771.966
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1738x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1738x2048): 85.617
Elapsed time for attention_prob_times_values (48x2048x2048x1738): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1738): 83.503

Attention duration (in seconds): 0.0166
Attention throughput (in TFLOP/s): 1806.533
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0166
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20868, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1739x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1739x2048): 84.997
Elapsed time for attention_prob_times_values (48x2048x2048x1739): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1739): 81.357

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1777.386
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1740x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1740x2048): 86.297
Elapsed time for attention_prob_times_values (48x2048x2048x1740): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1740): 83.656

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1817.270
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20892, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1741x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1741x2048): 85.014
Elapsed time for attention_prob_times_values (48x2048x2048x1741): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1741): 81.498

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1781.078
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1756.930
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x744x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x744x2048): 80.864
Elapsed time for attention_prob_times_values (128x2048x2048x744): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x744): 92.310

Attention duration (in seconds): 0.0185
Attention throughput (in TFLOP/s): 2090.564
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x745x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x745x2048): 77.879
Elapsed time for attention_prob_times_values (128x2048x2048x745): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x745): 67.029

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1749.404
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x746x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x746x2048): 78.689
Elapsed time for attention_prob_times_values (128x2048x2048x746): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x746): 69.006

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1787.692
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x747x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x747x2048): 77.949
Elapsed time for attention_prob_times_values (128x2048x2048x747): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x747): 67.272

Attention duration (in seconds): 0.0222
Attention throughput (in TFLOP/s): 1758.049
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x748x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x748x2048): 79.527
Elapsed time for attention_prob_times_values (128x2048x2048x748): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x748): 69.468

Attention duration (in seconds): 0.0217
Attention throughput (in TFLOP/s): 1807.597
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 23968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x749x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x749x2048): 78.011
Elapsed time for attention_prob_times_values (128x2048x2048x749): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x749): 67.262

Attention duration (in seconds): 0.0223
Attention throughput (in TFLOP/s): 1763.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x750x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x750x2048): 78.962
Elapsed time for attention_prob_times_values (128x2048x2048x750): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x750): 69.170

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 1802.081
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0218
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x751x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x751x2048): 78.087
Elapsed time for attention_prob_times_values (128x2048x2048x751): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x751): 67.061

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1765.548
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x752x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x752x2048): 80.507
Elapsed time for attention_prob_times_values (128x2048x2048x752): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x752): 92.336

Attention duration (in seconds): 0.0188
Attention throughput (in TFLOP/s): 2107.418
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0188
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1742x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1742x2048): 86.078
Elapsed time for attention_prob_times_values (48x2048x2048x1742): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1742): 83.712

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1817.595
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20916, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1743x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1743x2048): 84.908
Elapsed time for attention_prob_times_values (48x2048x2048x1743): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1743): 81.526

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1782.252
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1744x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1744x2048): 87.205
Elapsed time for attention_prob_times_values (48x2048x2048x1744): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1744): 83.367

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 1827.394
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0165
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20940, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1745x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1745x2048): 84.498
Elapsed time for attention_prob_times_values (48x2048x2048x1745): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1745): 81.722

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1782.144
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1746x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1746x2048): 85.721
Elapsed time for attention_prob_times_values (48x2048x2048x1746): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1746): 81.934

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1798.106
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20964, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1747x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1747x2048): 84.838
Elapsed time for attention_prob_times_values (48x2048x2048x1747): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1747): 81.722

Attention duration (in seconds): 0.0169
Attention throughput (in TFLOP/s): 1787.623
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1748x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1748x2048): 83.667
Elapsed time for attention_prob_times_values (48x2048x2048x1748): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1748): 83.981

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1800.904
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 20988, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1749x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1749x2048): 84.605
Elapsed time for attention_prob_times_values (48x2048x2048x1749): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1749): 79.667

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1763.999
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1750x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1750x2048): 84.945
Elapsed time for attention_prob_times_values (48x2048x2048x1750): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1750): 83.987

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1816.614
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21012, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1751x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1751x2048): 84.508
Elapsed time for attention_prob_times_values (48x2048x2048x1751): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1751): 80.634

Attention duration (in seconds): 0.0171
Attention throughput (in TFLOP/s): 1775.909
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1752x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1752x2048): 86.509
Elapsed time for attention_prob_times_values (48x2048x2048x1752): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1752): 82.404

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1817.374
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21036, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1753x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1753x2048): 84.181
Elapsed time for attention_prob_times_values (48x2048x2048x1753): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1753): 81.992

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1789.613
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1754x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1754x2048): 84.418
Elapsed time for attention_prob_times_values (48x2048x2048x1754): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1754): 83.743

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 1812.305
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0168
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21060, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1755x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1755x2048): 84.262
Elapsed time for attention_prob_times_values (48x2048x2048x1755): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1755): 81.947

Attention duration (in seconds): 0.0170
Attention throughput (in TFLOP/s): 1791.919
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0170
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1756x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1756x2048): 85.204
Elapsed time for attention_prob_times_values (48x2048x2048x1756): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1756): 84.272

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1828.436
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21084, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1757x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1757x2048): 84.342
Elapsed time for attention_prob_times_values (48x2048x2048x1757): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1757): 80.191

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1774.988
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1758x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1758x2048): 85.282
Elapsed time for attention_prob_times_values (48x2048x2048x1758): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1758): 84.332

Attention duration (in seconds): 0.0167
Attention throughput (in TFLOP/s): 1831.905
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21108, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1759x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1759x2048): 82.528
Elapsed time for attention_prob_times_values (48x2048x2048x1759): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1759): 82.213

Attention duration (in seconds): 0.0172
Attention throughput (in TFLOP/s): 1780.294
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0172
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 12, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (48x2048x1760x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (48x2048x1760x2048): 94.580
Elapsed time for attention_prob_times_values (48x2048x2048x1760): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (48x2048x2048x1760): 85.612

Attention duration (in seconds): 0.0158
========================================================================================================================
num_attention_heads: 32, hidden_size: 24096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x753x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x753x2048): 76.348
Elapsed time for attention_prob_times_values (128x2048x2048x753): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x753): 64.337

Attention duration (in seconds): 0.0232
Attention throughput (in TFLOP/s): 1713.010
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0232
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x754x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x754x2048): 75.183
Elapsed time for attention_prob_times_values (128x2048x2048x754): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x754): 67.610

Attention duration (in seconds): 0.0227
Attention throughput (in TFLOP/s): 1748.744
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0227
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x755x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x755x2048): 75.163
Elapsed time for attention_prob_times_values (128x2048x2048x755): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x755): 65.674

Attention duration (in seconds): 0.0231
Attention throughput (in TFLOP/s): 1723.995
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0231
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x756x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x756x2048): 73.761
Elapsed time for attention_prob_times_values (128x2048x2048x756): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x756): 67.490

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1735.718
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x757x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x757x2048): 74.532
Elapsed time for attention_prob_times_values (128x2048x2048x757): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x757): 64.333

Attention duration (in seconds): 0.0235
Attention throughput (in TFLOP/s): 1702.713
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x758x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x758x2048): 75.477
Elapsed time for attention_prob_times_values (128x2048x2048x758): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x758): 69.868

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 1791.419
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0224
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x759x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x759x2048): 74.935
Elapsed time for attention_prob_times_values (128x2048x2048x759): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x759): 67.112

Attention duration (in seconds): 0.0230
Attention throughput (in TFLOP/s): 1750.291
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0230
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x760x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x760x2048): 76.332
Elapsed time for attention_prob_times_values (128x2048x2048x760): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x760): 94.337

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 2088.526
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0193
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x761x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (128x2048x761x2048): 73.658
Elapsed time for attention_prob_times_values (128x2048x2048x761): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (128x2048x2048x761): 67.152

Attention duration (in seconds): 0.0233
Attention throughput (in TFLOP/s): 1741.001
MLP duration (in seconds): 0.0000
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0233
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 32, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_query_prob (128x2048x762x2048): 0.0108
slurmstepd: error: *** JOB 1508159 ON frontier10354 CANCELLED AT 2023-11-24T16:33:43 DUE TO TIME LIMIT ***
