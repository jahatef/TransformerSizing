bash: /fsx/home-quentin/jacob/miniconda3/lib/libtinfo.so.6: no version information available (required by bash)
1.13.1 

[2023-09-27 00:08:42,471] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-09-27 00:08:43,197] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.146.194, master_port=6000
[2023-09-27 00:08:43,197] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-09-27 00:08:46,312] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0388
Attention throughput (in TFLOP/s): 127.357
MLP duration (in seconds): 0.0367
MLP throughput (in TFLOP/s): 239.830
Transformer duration (in seconds): 0.0786
Transformer throughput (in TFLOP/s): 174.765
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0533
Attention throughput (in TFLOP/s): 95.514
MLP duration (in seconds): 0.0382
MLP throughput (in TFLOP/s): 237.644
Transformer duration (in seconds): 0.0956
Transformer throughput (in TFLOP/s): 148.248
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0546
Attention throughput (in TFLOP/s): 96.132
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 237.158
Transformer duration (in seconds): 0.0980
Transformer throughput (in TFLOP/s): 148.901
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0551
Attention throughput (in TFLOP/s): 97.836
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 235.913
Transformer duration (in seconds): 0.1007
Transformer throughput (in TFLOP/s): 149.357
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 97.406
MLP duration (in seconds): 0.0418
MLP throughput (in TFLOP/s): 237.724
Transformer duration (in seconds): 0.1027
Transformer throughput (in TFLOP/s): 150.684
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0576
Attention throughput (in TFLOP/s): 99.071
MLP duration (in seconds): 0.0431
MLP throughput (in TFLOP/s): 237.349
Transformer duration (in seconds): 0.1047
Transformer throughput (in TFLOP/s): 152.090
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0596
Attention throughput (in TFLOP/s): 98.347
MLP duration (in seconds): 0.0432
MLP throughput (in TFLOP/s): 243.422
Transformer duration (in seconds): 0.1057
Transformer throughput (in TFLOP/s): 155.036
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 99.073
MLP duration (in seconds): 0.0446
MLP throughput (in TFLOP/s): 242.653
Transformer duration (in seconds): 0.1075
Transformer throughput (in TFLOP/s): 156.691
Transformer - MLP - Attention (in seconds): 0.0021
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0454
Attention throughput (in TFLOP/s): 136.295
MLP duration (in seconds): 0.0468
MLP throughput (in TFLOP/s): 237.722
Transformer duration (in seconds): 0.0955
Transformer throughput (in TFLOP/s): 181.365
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0607
Attention throughput (in TFLOP/s): 104.572
MLP duration (in seconds): 0.0479
MLP throughput (in TFLOP/s): 239.074
Transformer duration (in seconds): 0.1128
Transformer throughput (in TFLOP/s): 157.748
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0490
Attention throughput (in TFLOP/s): 132.995
MLP duration (in seconds): 0.0496
MLP throughput (in TFLOP/s): 237.087
Transformer duration (in seconds): 0.1020
Transformer throughput (in TFLOP/s): 179.088
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0622
Attention throughput (in TFLOP/s): 107.531
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 237.037
Transformer duration (in seconds): 0.1179
Transformer throughput (in TFLOP/s): 159.144
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0502
Attention throughput (in TFLOP/s): 136.652
MLP duration (in seconds): 0.0510
MLP throughput (in TFLOP/s): 243.043
Transformer duration (in seconds): 0.1040
Transformer throughput (in TFLOP/s): 185.134
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0638
Attention throughput (in TFLOP/s): 110.152
MLP duration (in seconds): 0.0540
MLP throughput (in TFLOP/s): 235.675
Transformer duration (in seconds): 0.1218
Transformer throughput (in TFLOP/s): 162.273
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0518
Attention throughput (in TFLOP/s): 139.017
MLP duration (in seconds): 0.0540
MLP throughput (in TFLOP/s): 241.832
Transformer duration (in seconds): 0.1087
Transformer throughput (in TFLOP/s): 186.479
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0657
Attention throughput (in TFLOP/s): 112.254
MLP duration (in seconds): 0.0572
MLP throughput (in TFLOP/s): 234.205
Transformer duration (in seconds): 0.1274
Transformer throughput (in TFLOP/s): 163.074
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0512
Attention throughput (in TFLOP/s): 147.607
MLP duration (in seconds): 0.0570
MLP throughput (in TFLOP/s): 241.095
Transformer duration (in seconds): 0.1107
Transformer throughput (in TFLOP/s): 192.473
Transformer - MLP - Attention (in seconds): 0.0025
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0672
Attention throughput (in TFLOP/s): 115.255
MLP duration (in seconds): 0.0600
MLP throughput (in TFLOP/s): 234.702
Transformer duration (in seconds): 0.1317
Transformer throughput (in TFLOP/s): 165.816
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0551
Attention throughput (in TFLOP/s): 143.942
MLP duration (in seconds): 0.0616
MLP throughput (in TFLOP/s): 234.476
Transformer duration (in seconds): 0.1204
Transformer throughput (in TFLOP/s): 185.774
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 117.979
MLP duration (in seconds): 0.0626
MLP throughput (in TFLOP/s): 236.146
Transformer duration (in seconds): 0.1352
Transformer throughput (in TFLOP/s): 169.412
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0561
Attention throughput (in TFLOP/s): 147.970
MLP duration (in seconds): 0.0621
MLP throughput (in TFLOP/s): 243.954
Transformer duration (in seconds): 0.1211
Transformer throughput (in TFLOP/s): 193.589
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0704
Attention throughput (in TFLOP/s): 120.629
MLP duration (in seconds): 0.0656
MLP throughput (in TFLOP/s): 236.520
Transformer duration (in seconds): 0.1403
Transformer throughput (in TFLOP/s): 171.062
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0575
Attention throughput (in TFLOP/s): 150.828
MLP duration (in seconds): 0.0654
MLP throughput (in TFLOP/s): 242.785
Transformer duration (in seconds): 0.1262
Transformer throughput (in TFLOP/s): 194.678
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0717
Attention throughput (in TFLOP/s): 123.848
MLP duration (in seconds): 0.0686
MLP throughput (in TFLOP/s): 236.772
Transformer duration (in seconds): 0.1443
Transformer throughput (in TFLOP/s): 174.166
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0572
Attention throughput (in TFLOP/s): 158.612
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 243.462
Transformer duration (in seconds): 0.1282
Transformer throughput (in TFLOP/s): 200.524
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0734
Attention throughput (in TFLOP/s): 126.223
MLP duration (in seconds): 0.0702
MLP throughput (in TFLOP/s): 242.374
Transformer duration (in seconds): 0.1467
Transformer throughput (in TFLOP/s): 179.173
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 155.829
MLP duration (in seconds): 0.0735
MLP throughput (in TFLOP/s): 236.613
Transformer duration (in seconds): 0.1387
Transformer throughput (in TFLOP/s): 193.708
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0750
Attention throughput (in TFLOP/s): 129.048
MLP duration (in seconds): 0.0753
MLP throughput (in TFLOP/s): 236.284
Transformer duration (in seconds): 0.1549
Transformer throughput (in TFLOP/s): 177.239
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0626
Attention throughput (in TFLOP/s): 157.708
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 240.541
Transformer duration (in seconds): 0.1418
Transformer throughput (in TFLOP/s): 197.871
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 130.544
MLP duration (in seconds): 0.0791
MLP throughput (in TFLOP/s): 234.716
Transformer duration (in seconds): 0.1609
Transformer throughput (in TFLOP/s): 178.148
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0641
Attention throughput (in TFLOP/s): 160.654
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 244.472
Transformer duration (in seconds): 0.1452
Transformer throughput (in TFLOP/s): 201.538
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 132.946
MLP duration (in seconds): 0.0810
MLP throughput (in TFLOP/s): 239.125
Transformer duration (in seconds): 0.1645
Transformer throughput (in TFLOP/s): 181.708
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0633
Attention throughput (in TFLOP/s): 169.467
MLP duration (in seconds): 0.0808
MLP throughput (in TFLOP/s): 244.825
Transformer duration (in seconds): 0.1481
Transformer throughput (in TFLOP/s): 206.023
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0824
Attention throughput (in TFLOP/s): 132.739
MLP duration (in seconds): 0.0847
MLP throughput (in TFLOP/s): 238.532
Transformer duration (in seconds): 0.1715
Transformer throughput (in TFLOP/s): 181.628
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 162.283
MLP duration (in seconds): 0.0839
MLP throughput (in TFLOP/s): 245.734
Transformer duration (in seconds): 0.1564
Transformer throughput (in TFLOP/s): 203.142
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 134.102
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 238.595
Transformer duration (in seconds): 0.1780
Transformer throughput (in TFLOP/s): 182.167
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 165.655
MLP duration (in seconds): 0.0873
MLP throughput (in TFLOP/s): 245.929
Transformer duration (in seconds): 0.1613
Transformer throughput (in TFLOP/s): 205.024
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0870
Attention throughput (in TFLOP/s): 135.932
MLP duration (in seconds): 0.0891
MLP throughput (in TFLOP/s): 245.806
Transformer duration (in seconds): 0.1812
Transformer throughput (in TFLOP/s): 186.180
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0719
Attention throughput (in TFLOP/s): 167.568
MLP duration (in seconds): 0.0907
MLP throughput (in TFLOP/s): 246.287
Transformer duration (in seconds): 0.1667
Transformer throughput (in TFLOP/s): 206.316
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0896
Attention throughput (in TFLOP/s): 137.001
MLP duration (in seconds): 0.0931
MLP throughput (in TFLOP/s): 244.659
Transformer duration (in seconds): 0.1870
Transformer throughput (in TFLOP/s): 187.486
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0722
Attention throughput (in TFLOP/s): 173.249
MLP duration (in seconds): 0.0949
MLP throughput (in TFLOP/s): 244.724
Transformer duration (in seconds): 0.1712
Transformer throughput (in TFLOP/s): 208.674
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0927
Attention throughput (in TFLOP/s): 137.444
MLP duration (in seconds): 0.0968
MLP throughput (in TFLOP/s): 244.646
Transformer duration (in seconds): 0.1936
Transformer throughput (in TFLOP/s): 188.118
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 169.916
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 245.188
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 207.048
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 139.983
MLP duration (in seconds): 0.1002
MLP throughput (in TFLOP/s): 245.272
Transformer duration (in seconds): 0.1977
Transformer throughput (in TFLOP/s): 191.199
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 174.113
MLP duration (in seconds): 0.1008
MLP throughput (in TFLOP/s): 248.455
Transformer duration (in seconds): 0.1830
Transformer throughput (in TFLOP/s): 210.333
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0956
Attention throughput (in TFLOP/s): 143.175
MLP duration (in seconds): 0.1037
MLP throughput (in TFLOP/s): 246.051
Transformer duration (in seconds): 0.2037
Transformer throughput (in TFLOP/s): 192.493
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0794
Attention throughput (in TFLOP/s): 175.533
MLP duration (in seconds): 0.1056
MLP throughput (in TFLOP/s): 246.141
Transformer duration (in seconds): 0.1900
Transformer throughput (in TFLOP/s): 210.076
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0982
Attention throughput (in TFLOP/s): 144.412
MLP duration (in seconds): 0.1078
MLP throughput (in TFLOP/s): 245.528
Transformer duration (in seconds): 0.2092
Transformer throughput (in TFLOP/s): 194.305
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 181.512
MLP duration (in seconds): 0.1090
MLP throughput (in TFLOP/s): 247.227
Transformer duration (in seconds): 0.1931
Transformer throughput (in TFLOP/s): 214.213
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1005
Attention throughput (in TFLOP/s): 146.064
MLP duration (in seconds): 0.1115
MLP throughput (in TFLOP/s): 245.822
Transformer duration (in seconds): 0.2153
Transformer throughput (in TFLOP/s): 195.556
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0836
Attention throughput (in TFLOP/s): 178.662
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 245.279
Transformer duration (in seconds): 0.2021
Transformer throughput (in TFLOP/s): 211.950
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1024
Attention throughput (in TFLOP/s): 148.315
MLP duration (in seconds): 0.1164
MLP throughput (in TFLOP/s): 243.930
Transformer duration (in seconds): 0.2230
Transformer throughput (in TFLOP/s): 195.432
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0859
Attention throughput (in TFLOP/s): 179.725
MLP duration (in seconds): 0.1180
MLP throughput (in TFLOP/s): 244.886
Transformer duration (in seconds): 0.2080
Transformer throughput (in TFLOP/s): 213.201
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1041
Attention throughput (in TFLOP/s): 150.889
MLP duration (in seconds): 0.1191
MLP throughput (in TFLOP/s): 246.722
Transformer duration (in seconds): 0.2271
Transformer throughput (in TFLOP/s): 198.611
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 182.682
MLP duration (in seconds): 0.1212
MLP throughput (in TFLOP/s): 246.748
Transformer duration (in seconds): 0.2129
Transformer throughput (in TFLOP/s): 215.456
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1061
Attention throughput (in TFLOP/s): 152.933
MLP duration (in seconds): 0.1231
MLP throughput (in TFLOP/s): 246.971
Transformer duration (in seconds): 0.2341
Transformer throughput (in TFLOP/s): 199.214
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0872
Attention throughput (in TFLOP/s): 189.179
MLP duration (in seconds): 0.1256
MLP throughput (in TFLOP/s): 246.166
Transformer duration (in seconds): 0.2180
Transformer throughput (in TFLOP/s): 217.504
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1082
Attention throughput (in TFLOP/s): 154.934
MLP duration (in seconds): 0.1285
MLP throughput (in TFLOP/s): 244.753
Transformer duration (in seconds): 0.2415
Transformer throughput (in TFLOP/s): 199.607
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 183.976
MLP duration (in seconds): 0.1304
MLP throughput (in TFLOP/s): 245.157
Transformer duration (in seconds): 0.2276
Transformer throughput (in TFLOP/s): 215.218
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1106
Attention throughput (in TFLOP/s): 156.446
MLP duration (in seconds): 0.1326
MLP throughput (in TFLOP/s): 244.972
Transformer duration (in seconds): 0.2484
Transformer throughput (in TFLOP/s): 200.431
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0941
Attention throughput (in TFLOP/s): 186.791
MLP duration (in seconds): 0.1346
MLP throughput (in TFLOP/s): 245.404
Transformer duration (in seconds): 0.2331
Transformer throughput (in TFLOP/s): 217.014
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1137
Attention throughput (in TFLOP/s): 157.037
MLP duration (in seconds): 0.1359
MLP throughput (in TFLOP/s): 246.986
Transformer duration (in seconds): 0.2526
Transformer throughput (in TFLOP/s): 203.508
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0947
Attention throughput (in TFLOP/s): 191.522
MLP duration (in seconds): 0.1384
MLP throughput (in TFLOP/s): 246.295
Transformer duration (in seconds): 0.2386
Transformer throughput (in TFLOP/s): 218.837
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1155
Attention throughput (in TFLOP/s): 159.414
MLP duration (in seconds): 0.1409
MLP throughput (in TFLOP/s): 245.765
Transformer duration (in seconds): 0.2606
Transformer throughput (in TFLOP/s): 203.556
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0958
Attention throughput (in TFLOP/s): 195.086
MLP duration (in seconds): 0.1435
MLP throughput (in TFLOP/s): 245.133
Transformer duration (in seconds): 0.2459
Transformer throughput (in TFLOP/s): 219.072
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1229
Attention throughput (in TFLOP/s): 154.415
MLP duration (in seconds): 0.1455
MLP throughput (in TFLOP/s): 245.565
Transformer duration (in seconds): 0.2722
Transformer throughput (in TFLOP/s): 201.003
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1031
Attention throughput (in TFLOP/s): 186.821
MLP duration (in seconds): 0.1480
MLP throughput (in TFLOP/s): 245.232
Transformer duration (in seconds): 0.2564
Transformer throughput (in TFLOP/s): 216.699
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1259
Attention throughput (in TFLOP/s): 155.348
MLP duration (in seconds): 0.1503
MLP throughput (in TFLOP/s): 245.159
Transformer duration (in seconds): 0.2799
Transformer throughput (in TFLOP/s): 201.540
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1055
Attention throughput (in TFLOP/s): 188.149
MLP duration (in seconds): 0.1532
MLP throughput (in TFLOP/s): 244.254
Transformer duration (in seconds): 0.2632
Transformer throughput (in TFLOP/s): 217.592
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1272
Attention throughput (in TFLOP/s): 158.327
MLP duration (in seconds): 0.1539
MLP throughput (in TFLOP/s): 246.759
Transformer duration (in seconds): 0.2864
Transformer throughput (in TFLOP/s): 202.954
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1075
Attention throughput (in TFLOP/s): 190.007
MLP duration (in seconds): 0.1567
MLP throughput (in TFLOP/s): 246.135
Transformer duration (in seconds): 0.2690
Transformer throughput (in TFLOP/s): 219.299
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1312
Attention throughput (in TFLOP/s): 158.007
MLP duration (in seconds): 0.1596
MLP throughput (in TFLOP/s): 245.172
Transformer duration (in seconds): 0.2970
Transformer throughput (in TFLOP/s): 201.578
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1083
Attention throughput (in TFLOP/s): 194.115
MLP duration (in seconds): 0.1618
MLP throughput (in TFLOP/s): 245.473
Transformer duration (in seconds): 0.2744
Transformer throughput (in TFLOP/s): 221.380
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1346
Attention throughput (in TFLOP/s): 158.467
MLP duration (in seconds): 0.1647
MLP throughput (in TFLOP/s): 244.704
Transformer duration (in seconds): 0.3047
Transformer throughput (in TFLOP/s): 202.271
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1139
Attention throughput (in TFLOP/s): 189.962
MLP duration (in seconds): 0.1673
MLP throughput (in TFLOP/s): 244.392
Transformer duration (in seconds): 0.2860
Transformer throughput (in TFLOP/s): 218.657
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1379
Attention throughput (in TFLOP/s): 159.139
MLP duration (in seconds): 0.1691
MLP throughput (in TFLOP/s): 245.315
Transformer duration (in seconds): 0.3118
Transformer throughput (in TFLOP/s): 203.405
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1157
Attention throughput (in TFLOP/s): 192.285
MLP duration (in seconds): 0.1723
MLP throughput (in TFLOP/s): 244.292
Transformer duration (in seconds): 0.2923
Transformer throughput (in TFLOP/s): 220.143
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1400
Attention throughput (in TFLOP/s): 161.118
MLP duration (in seconds): 0.1742
MLP throughput (in TFLOP/s): 245.037
Transformer duration (in seconds): 0.3196
Transformer throughput (in TFLOP/s): 204.182
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1184
Attention throughput (in TFLOP/s): 193.169
MLP duration (in seconds): 0.1766
MLP throughput (in TFLOP/s): 245.139
Transformer duration (in seconds): 0.3003
Transformer throughput (in TFLOP/s): 220.378
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1420
Attention throughput (in TFLOP/s): 163.254
MLP duration (in seconds): 0.1799
MLP throughput (in TFLOP/s): 244.069
Transformer duration (in seconds): 0.3253
Transformer throughput (in TFLOP/s): 206.282
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1181
Attention throughput (in TFLOP/s): 198.990
MLP duration (in seconds): 0.1816
MLP throughput (in TFLOP/s): 245.224
Transformer duration (in seconds): 0.3030
Transformer throughput (in TFLOP/s): 224.538
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 180.836
MLP duration (in seconds): 0.1834
MLP throughput (in TFLOP/s): 246.215
Transformer duration (in seconds): 0.3198
Transformer throughput (in TFLOP/s): 215.657
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1224
Attention throughput (in TFLOP/s): 197.243
MLP duration (in seconds): 0.1870
MLP throughput (in TFLOP/s): 244.775
Transformer duration (in seconds): 0.3124
Transformer throughput (in TFLOP/s): 223.796
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1343
Attention throughput (in TFLOP/s): 182.147
MLP duration (in seconds): 0.1892
MLP throughput (in TFLOP/s): 245.207
Transformer duration (in seconds): 0.3277
Transformer throughput (in TFLOP/s): 216.262
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1239
Attention throughput (in TFLOP/s): 200.075
MLP duration (in seconds): 0.1913
MLP throughput (in TFLOP/s): 245.838
Transformer duration (in seconds): 0.3190
Transformer throughput (in TFLOP/s): 225.153
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1366
Attention throughput (in TFLOP/s): 183.829
MLP duration (in seconds): 0.1936
MLP throughput (in TFLOP/s): 246.250
Transformer duration (in seconds): 0.3341
Transformer throughput (in TFLOP/s): 217.907
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1270
Attention throughput (in TFLOP/s): 200.435
MLP duration (in seconds): 0.1959
MLP throughput (in TFLOP/s): 246.613
Transformer duration (in seconds): 0.3278
Transformer throughput (in TFLOP/s): 225.033
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1405
Attention throughput (in TFLOP/s): 183.449
MLP duration (in seconds): 0.1995
MLP throughput (in TFLOP/s): 245.459
Transformer duration (in seconds): 0.3442
Transformer throughput (in TFLOP/s): 217.136
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1288
Attention throughput (in TFLOP/s): 202.790
MLP duration (in seconds): 0.2026
MLP throughput (in TFLOP/s): 244.929
Transformer duration (in seconds): 0.3348
Transformer throughput (in TFLOP/s): 226.191
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1419
Attention throughput (in TFLOP/s): 186.335
MLP duration (in seconds): 0.2045
MLP throughput (in TFLOP/s): 245.784
Transformer duration (in seconds): 0.3498
Transformer throughput (in TFLOP/s): 219.303
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 203.822
MLP duration (in seconds): 0.2065
MLP throughput (in TFLOP/s): 246.623
Transformer duration (in seconds): 0.3421
Transformer throughput (in TFLOP/s): 227.181
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1433
Attention throughput (in TFLOP/s): 189.322
MLP duration (in seconds): 0.2092
MLP throughput (in TFLOP/s): 246.579
Transformer duration (in seconds): 0.3565
Transformer throughput (in TFLOP/s): 220.825
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1339
Attention throughput (in TFLOP/s): 205.178
MLP duration (in seconds): 0.2127
MLP throughput (in TFLOP/s): 245.761
Transformer duration (in seconds): 0.3514
Transformer throughput (in TFLOP/s): 226.908
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1464
Attention throughput (in TFLOP/s): 189.968
MLP duration (in seconds): 0.2159
MLP throughput (in TFLOP/s): 245.197
Transformer duration (in seconds): 0.3672
Transformer throughput (in TFLOP/s): 219.901
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1380
Attention throughput (in TFLOP/s): 204.014
MLP duration (in seconds): 0.2187
MLP throughput (in TFLOP/s): 245.133
Transformer duration (in seconds): 0.3613
Transformer throughput (in TFLOP/s): 226.308
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1487
Attention throughput (in TFLOP/s): 191.798
MLP duration (in seconds): 0.2206
MLP throughput (in TFLOP/s): 246.067
Transformer duration (in seconds): 0.3740
Transformer throughput (in TFLOP/s): 221.411
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 209.973
MLP duration (in seconds): 0.2237
MLP throughput (in TFLOP/s): 245.802
Transformer duration (in seconds): 0.3667
Transformer throughput (in TFLOP/s): 228.612
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1537
Attention throughput (in TFLOP/s): 190.132
MLP duration (in seconds): 0.2879
MLP throughput (in TFLOP/s): 193.350
Transformer duration (in seconds): 0.4482
Transformer throughput (in TFLOP/s): 189.392
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1429
Attention throughput (in TFLOP/s): 206.978
MLP duration (in seconds): 0.2824
MLP throughput (in TFLOP/s): 199.582
Transformer duration (in seconds): 0.4349
Transformer throughput (in TFLOP/s): 197.584
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1555
Attention throughput (in TFLOP/s): 192.418
MLP duration (in seconds): 0.2948
MLP throughput (in TFLOP/s): 193.537
Transformer duration (in seconds): 0.4580
Transformer throughput (in TFLOP/s): 189.903
Transformer - MLP - Attention (in seconds): 0.0077
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1467
Attention throughput (in TFLOP/s): 206.463
MLP duration (in seconds): 0.2995
MLP throughput (in TFLOP/s): 192.878
Transformer duration (in seconds): 0.4559
Transformer throughput (in TFLOP/s): 193.132
Transformer - MLP - Attention (in seconds): 0.0097
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1599
Attention throughput (in TFLOP/s): 191.721
MLP duration (in seconds): 0.3106
MLP throughput (in TFLOP/s): 188.258
Transformer duration (in seconds): 0.4745
Transformer throughput (in TFLOP/s): 187.793
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1495
Attention throughput (in TFLOP/s): 207.388
MLP duration (in seconds): 0.3089
MLP throughput (in TFLOP/s): 191.548
Transformer duration (in seconds): 0.4625
Transformer throughput (in TFLOP/s): 195.013
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1619
Attention throughput (in TFLOP/s): 193.771
MLP duration (in seconds): 0.3092
MLP throughput (in TFLOP/s): 193.698
Transformer duration (in seconds): 0.4817
Transformer throughput (in TFLOP/s): 189.493
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1516
Attention throughput (in TFLOP/s): 209.400
MLP duration (in seconds): 0.3135
MLP throughput (in TFLOP/s): 193.322
Transformer duration (in seconds): 0.4717
Transformer throughput (in TFLOP/s): 195.800
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1654
Attention throughput (in TFLOP/s): 194.223
MLP duration (in seconds): 0.3189
MLP throughput (in TFLOP/s): 192.312
Transformer duration (in seconds): 0.4878
Transformer throughput (in TFLOP/s): 191.561
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1551
Attention throughput (in TFLOP/s): 209.437
MLP duration (in seconds): 0.3173
MLP throughput (in TFLOP/s): 195.597
Transformer duration (in seconds): 0.4772
Transformer throughput (in TFLOP/s): 198.143
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1680
Attention throughput (in TFLOP/s): 195.652
MLP duration (in seconds): 0.3265
MLP throughput (in TFLOP/s): 192.329
Transformer duration (in seconds): 0.5023
Transformer throughput (in TFLOP/s): 190.435
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1578
Attention throughput (in TFLOP/s): 210.696
MLP duration (in seconds): 0.3340
MLP throughput (in TFLOP/s): 190.210
Transformer duration (in seconds): 0.4954
Transformer throughput (in TFLOP/s): 195.351
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1706
Attention throughput (in TFLOP/s): 197.076
MLP duration (in seconds): 0.3341
MLP throughput (in TFLOP/s): 192.345
Transformer duration (in seconds): 0.5143
Transformer throughput (in TFLOP/s): 190.329
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1617
Attention throughput (in TFLOP/s): 210.233
MLP duration (in seconds): 0.3372
MLP throughput (in TFLOP/s): 192.796
Transformer duration (in seconds): 0.5053
Transformer throughput (in TFLOP/s): 195.960
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1747
Attention throughput (in TFLOP/s): 196.820
MLP duration (in seconds): 0.2740
MLP throughput (in TFLOP/s): 240.002
Transformer duration (in seconds): 0.4528
Transformer throughput (in TFLOP/s): 221.173
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1632
Attention throughput (in TFLOP/s): 213.074
MLP duration (in seconds): 0.2768
MLP throughput (in TFLOP/s): 240.297
Transformer duration (in seconds): 0.4441
Transformer throughput (in TFLOP/s): 228.060
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1778
Attention throughput (in TFLOP/s): 197.776
MLP duration (in seconds): 0.2791
MLP throughput (in TFLOP/s): 241.094
Transformer duration (in seconds): 0.4619
Transformer throughput (in TFLOP/s): 221.752
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1682
Attention throughput (in TFLOP/s): 211.377
MLP duration (in seconds): 0.2826
MLP throughput (in TFLOP/s): 240.763
Transformer duration (in seconds): 0.4544
Transformer throughput (in TFLOP/s): 227.972
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.1807
Attention throughput (in TFLOP/s): 198.922
MLP duration (in seconds): 0.2867
MLP throughput (in TFLOP/s): 239.957
Traceback (most recent call last):
  File "/fsx/home-quentin/jacob/TransformerSizing/torch_transformer_flops.py", line 478, in <module>
    benchmark_transformer(configuration, seq_length, train_batch_size)
  File "/fsx/home-quentin/jacob/TransformerSizing/torch_transformer_flops.py", line 423, in benchmark_transformer
    out = layer(inp, attention_mask)
  File "/fsx/home-quentin/jacob/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/home-quentin/jacob/TransformerSizing/megatron/model/transformer.py", line 856, in forward
    attention_output, attention_bias = self.attention(
  File "/fsx/home-quentin/jacob/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/home-quentin/jacob/TransformerSizing/megatron/model/transformer.py", line 685, in forward
    context_layer = self.attention(
  File "/fsx/home-quentin/jacob/TransformerSizing/megatron/model/transformer.py", line 415, in attention
    matmul_result = torch.baddbmm(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB (GPU 0; 39.56 GiB total capacity; 29.92 GiB already allocated; 1.97 GiB free; 36.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
