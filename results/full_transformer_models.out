1.13.1 

[2023-11-03 01:19:02,507] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-03 01:19:02,968] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.147.120, master_port=6000
[2023-11-03 01:19:02,968] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-03 01:19:04,184] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 32, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 131.035
MLP duration (in seconds): 0.0096
MLP throughput (in TFLOP/s): 229.866
Transformer duration (in seconds): 0.0211
Transformer throughput (in TFLOP/s): 169.016
Transformer - MLP - Attention (in seconds): 0.0011
========================================================================================================================
num_attention_heads: 80, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
Attention duration (in seconds): 0.0149
Attention throughput (in TFLOP/s): 40.346
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 214.751
Transformer duration (in seconds): 0.0195
Transformer throughput (in TFLOP/s): 74.960
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 32, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 78.754
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 214.905
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 119.370
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
1.13.1 

[2023-11-03 01:48:52,094] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-03 01:48:52,663] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.147.120, master_port=6000
[2023-11-03 01:48:52,663] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-03 01:48:55,723] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
Attention duration (in seconds): 0.0129
Attention throughput (in TFLOP/s): 46.522
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 214.854
Transformer duration (in seconds): 0.0175
Transformer throughput (in TFLOP/s): 83.392
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 40, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
Attention duration (in seconds): 0.0089
Attention throughput (in TFLOP/s): 67.939
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 212.961
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 108.505
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
1.13.1 

[2023-11-03 02:24:49,713] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-03 02:24:50,299] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.147.120, master_port=6000
[2023-11-03 02:24:50,299] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-03 02:24:53,208] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 32, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
Attention duration (in seconds): 0.0076
Attention throughput (in TFLOP/s): 78.791
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 214.892
Transformer duration (in seconds): 0.0122
Transformer throughput (in TFLOP/s): 119.466
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
num_attention_heads: 20, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
Attention duration (in seconds): 0.0058
Attention throughput (in TFLOP/s): 104.483
MLP duration (in seconds): 0.0040
MLP throughput (in TFLOP/s): 214.815
Transformer duration (in seconds): 0.0104
Transformer throughput (in TFLOP/s): 140.695
Transformer - MLP - Attention (in seconds): 0.0006
========================================================================================================================
