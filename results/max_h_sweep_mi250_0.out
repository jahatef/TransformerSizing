num_attention_heads: 128, hidden_size: 128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x128x384, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x128x384, b=2048): 13.730
Elapsed time for attention_key_query_prob (512x2048x1x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x1x2048): 1.056
Elapsed time for attention_prob_times_values (512x2048x2048x1): 0.0195
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x1): 0.221
Elapsed time for attention_linear_projection (4x128x128, b=2048): 0.0000
Throughput (in TFLOP/s) for attention_linear_projection (4x128x128, b=2048): 6.623
Elapsed time for mlp_h_to_4h (4x128x512, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x128x512, b=2048): 20.105
Elapsed time for mlp_4h_to_h (4x512x128, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x512x128, b=2048): 19.083

Attention duration (in seconds): 0.0236
Attention throughput (in TFLOP/s): 0.409
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 0.033
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 12.562
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 0.041
Transformer - MLP - Attention (in seconds): -0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x256x768, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x256x768, b=2048): 42.354
Elapsed time for attention_key_query_prob (512x2048x2x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x2x2048): 1.985
Elapsed time for attention_prob_times_values (512x2048x2048x2): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x2): 1.428
Elapsed time for attention_linear_projection (4x256x256, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x256x256, b=2048): 19.666
Elapsed time for mlp_h_to_4h (4x256x1024, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x256x1024, b=2048): 46.310
Elapsed time for mlp_4h_to_h (4x1024x256, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x1024x256, b=2048): 47.784

Attention duration (in seconds): 0.0105
Attention throughput (in TFLOP/s): 2.050
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0107
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 0.074
MLP duration (in seconds): 0.0002
MLP throughput (in TFLOP/s): 39.333
Transformer duration (in seconds): 0.2892
Transformer throughput (in TFLOP/s): 0.104
Transformer - MLP - Attention (in seconds): 0.0002
========================================================================================================================
num_attention_heads: 128, hidden_size: 384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x384x1152, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x384x1152, b=2048): 60.436
Elapsed time for attention_key_query_prob (512x2048x3x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x3x2048): 2.680
Elapsed time for attention_prob_times_values (512x2048x2048x3): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x3): 2.118
Elapsed time for attention_linear_projection (4x384x384, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x384x384, b=2048): 34.118
Elapsed time for mlp_h_to_4h (4x384x1536, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x384x1536, b=2048): 68.583
Elapsed time for mlp_4h_to_h (4x1536x384, b=2048): 0.0001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x1536x384, b=2048): 65.906

Attention duration (in seconds): 0.0111
Attention throughput (in TFLOP/s): 3.197
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0114
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2895
Attention throughput (in TFLOP/s): 0.122
MLP duration (in seconds): 0.0003
MLP throughput (in TFLOP/s): 55.907
Transformer duration (in seconds): 0.2891
Transformer throughput (in TFLOP/s): 0.189
Transformer - MLP - Attention (in seconds): -0.0007
========================================================================================================================
num_attention_heads: 128, hidden_size: 512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x512x1536, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x512x1536, b=2048): 74.645
Elapsed time for attention_key_query_prob (512x2048x4x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x4x2048): 3.484
Elapsed time for attention_prob_times_values (512x2048x2048x4): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x4): 3.083
Elapsed time for attention_linear_projection (4x512x512, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x512x512, b=2048): 47.282
Elapsed time for mlp_h_to_4h (4x512x2048, b=2048): 0.0002
Throughput (in TFLOP/s) for mlp_h_to_4h (4x512x2048, b=2048): 79.184
Elapsed time for mlp_4h_to_h (4x2048x512, b=2048): 0.0002
Throughput (in TFLOP/s) for mlp_4h_to_h (4x2048x512, b=2048): 79.446

Attention duration (in seconds): 0.0108
Attention throughput (in TFLOP/s): 4.787
MLP duration (in seconds): 0.0004
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0112
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2897
Attention throughput (in TFLOP/s): 0.178
MLP duration (in seconds): 0.0005
MLP throughput (in TFLOP/s): 67.218
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 0.297
Transformer - MLP - Attention (in seconds): -0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x640x1920, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x640x1920, b=2048): 81.508
Elapsed time for attention_key_query_prob (512x2048x5x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x5x2048): 4.351
Elapsed time for attention_prob_times_values (512x2048x2048x5): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x5): 3.817
Elapsed time for attention_linear_projection (4x640x640, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x640x640, b=2048): 58.397
Elapsed time for mlp_h_to_4h (4x640x2560, b=2048): 0.0003
Throughput (in TFLOP/s) for mlp_h_to_4h (4x640x2560, b=2048): 84.846
Elapsed time for mlp_4h_to_h (4x2560x640, b=2048): 0.0003
Throughput (in TFLOP/s) for mlp_4h_to_h (4x2560x640, b=2048): 84.782

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 6.389
MLP duration (in seconds): 0.0006
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 0.242
MLP duration (in seconds): 0.0007
MLP throughput (in TFLOP/s): 73.757
Transformer duration (in seconds): 0.2887
Transformer throughput (in TFLOP/s): 0.428
Transformer - MLP - Attention (in seconds): -0.0008
========================================================================================================================
num_attention_heads: 128, hidden_size: 768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x768x2304, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x768x2304, b=2048): 85.632
Elapsed time for attention_key_query_prob (512x2048x6x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x6x2048): 5.305
Elapsed time for attention_prob_times_values (512x2048x2048x6): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x6): 4.601
Elapsed time for attention_linear_projection (4x768x768, b=2048): 0.0001
Throughput (in TFLOP/s) for attention_linear_projection (4x768x768, b=2048): 70.369
Elapsed time for mlp_h_to_4h (4x768x3072, b=2048): 0.0004
Throughput (in TFLOP/s) for mlp_h_to_4h (4x768x3072, b=2048): 90.222
Elapsed time for mlp_4h_to_h (4x3072x768, b=2048): 0.0004
Throughput (in TFLOP/s) for mlp_4h_to_h (4x3072x768, b=2048): 92.329

Attention duration (in seconds): 0.0109
Attention throughput (in TFLOP/s): 8.249
MLP duration (in seconds): 0.0008
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0118
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 0.312
MLP duration (in seconds): 0.0010
MLP throughput (in TFLOP/s): 80.401
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 0.580
Transformer - MLP - Attention (in seconds): -0.0010
========================================================================================================================
num_attention_heads: 128, hidden_size: 896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x896x2688, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x896x2688, b=2048): 90.540
Elapsed time for attention_key_query_prob (512x2048x7x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x7x2048): 6.084
Elapsed time for attention_prob_times_values (512x2048x2048x7): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x7): 5.315
Elapsed time for attention_linear_projection (4x896x896, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_linear_projection (4x896x896, b=2048): 76.730
Elapsed time for mlp_h_to_4h (4x896x3584, b=2048): 0.0006
Throughput (in TFLOP/s) for mlp_h_to_4h (4x896x3584, b=2048): 82.930
Elapsed time for mlp_4h_to_h (4x3584x896, b=2048): 0.0006
Throughput (in TFLOP/s) for mlp_4h_to_h (4x3584x896, b=2048): 88.911

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 10.062
MLP duration (in seconds): 0.0012
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0124
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 0.390
MLP duration (in seconds): 0.0013
MLP throughput (in TFLOP/s): 77.991
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 0.755
Transformer - MLP - Attention (in seconds): -0.0014
========================================================================================================================
num_attention_heads: 128, hidden_size: 1024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1024x3072, b=2048): 0.0005
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1024x3072, b=2048): 94.152
Elapsed time for attention_key_query_prob (512x2048x8x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x8x2048): 6.697
Elapsed time for attention_prob_times_values (512x2048x2048x8): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x8): 7.277
Elapsed time for attention_linear_projection (4x1024x1024, b=2048): 0.0002
Throughput (in TFLOP/s) for attention_linear_projection (4x1024x1024, b=2048): 79.798
Elapsed time for mlp_h_to_4h (4x1024x4096, b=2048): 0.0007
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1024x4096, b=2048): 95.377
Elapsed time for mlp_4h_to_h (4x4096x1024, b=2048): 0.0007
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4096x1024, b=2048): 100.780

Attention duration (in seconds): 0.0106
Attention throughput (in TFLOP/s): 12.947
MLP duration (in seconds): 0.0014
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0120
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 0.476
MLP duration (in seconds): 0.0016
MLP throughput (in TFLOP/s): 88.632
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 0.952
Transformer - MLP - Attention (in seconds): -0.0016
========================================================================================================================
num_attention_heads: 128, hidden_size: 1152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1152x3456, b=2048): 0.0007
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1152x3456, b=2048): 95.395
Elapsed time for attention_key_query_prob (512x2048x9x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x9x2048): 7.218
Elapsed time for attention_prob_times_values (512x2048x2048x9): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x9): 7.855
Elapsed time for attention_linear_projection (4x1152x1152, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_linear_projection (4x1152x1152, b=2048): 81.499
Elapsed time for mlp_h_to_4h (4x1152x4608, b=2048): 0.0009
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1152x4608, b=2048): 97.071
Elapsed time for mlp_4h_to_h (4x4608x1152, b=2048): 0.0009
Throughput (in TFLOP/s) for mlp_4h_to_h (4x4608x1152, b=2048): 96.736

Attention duration (in seconds): 0.0112
Attention throughput (in TFLOP/s): 14.633
MLP duration (in seconds): 0.0018
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2895
Attention throughput (in TFLOP/s): 0.567
MLP duration (in seconds): 0.0020
MLP throughput (in TFLOP/s): 88.231
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 1.171
Transformer - MLP - Attention (in seconds): -0.0027
========================================================================================================================
num_attention_heads: 128, hidden_size: 1280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1280x3840, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1280x3840, b=2048): 93.513
Elapsed time for attention_key_query_prob (512x2048x10x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x10x2048): 7.938
Elapsed time for attention_prob_times_values (512x2048x2048x10): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x10): 8.938
Elapsed time for attention_linear_projection (4x1280x1280, b=2048): 0.0003
Throughput (in TFLOP/s) for attention_linear_projection (4x1280x1280, b=2048): 83.400
Elapsed time for mlp_h_to_4h (4x1280x5120, b=2048): 0.0011
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1280x5120, b=2048): 98.182
Elapsed time for mlp_4h_to_h (4x5120x1280, b=2048): 0.0010
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5120x1280, b=2048): 104.735

Attention duration (in seconds): 0.0114
Attention throughput (in TFLOP/s): 16.955
MLP duration (in seconds): 0.0021
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0135
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 0.669
MLP duration (in seconds): 0.0023
MLP throughput (in TFLOP/s): 93.107
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 1.413
Transformer - MLP - Attention (in seconds): -0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 1408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1408x4224, b=2048): 0.0010
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1408x4224, b=2048): 98.459
Elapsed time for attention_key_query_prob (512x2048x11x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x11x2048): 8.695
Elapsed time for attention_prob_times_values (512x2048x2048x11): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x11): 9.416
Elapsed time for attention_linear_projection (4x1408x1408, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_linear_projection (4x1408x1408, b=2048): 85.682
Elapsed time for mlp_h_to_4h (4x1408x5632, b=2048): 0.0013
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1408x5632, b=2048): 99.133
Elapsed time for mlp_4h_to_h (4x5632x1408, b=2048): 0.0013
Throughput (in TFLOP/s) for mlp_4h_to_h (4x5632x1408, b=2048): 101.743

Attention duration (in seconds): 0.0118
Attention throughput (in TFLOP/s): 18.986
MLP duration (in seconds): 0.0026
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 0.777
MLP duration (in seconds): 0.0028
MLP throughput (in TFLOP/s): 93.056
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 1.677
Transformer - MLP - Attention (in seconds): -0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 1536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1536x4608, b=2048): 0.0012
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1536x4608, b=2048): 98.459
Elapsed time for attention_key_query_prob (512x2048x12x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x12x2048): 9.453
Elapsed time for attention_prob_times_values (512x2048x2048x12): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x12): 10.691
Elapsed time for attention_linear_projection (4x1536x1536, b=2048): 0.0004
Throughput (in TFLOP/s) for attention_linear_projection (4x1536x1536, b=2048): 86.700
Elapsed time for mlp_h_to_4h (4x1536x6144, b=2048): 0.0016
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1536x6144, b=2048): 99.726
Elapsed time for mlp_4h_to_h (4x6144x1536, b=2048): 0.0014
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6144x1536, b=2048): 107.016

Attention duration (in seconds): 0.0119
Attention throughput (in TFLOP/s): 21.661
MLP duration (in seconds): 0.0030
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 0.892
MLP duration (in seconds): 0.0032
MLP throughput (in TFLOP/s): 95.864
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 1.963
Transformer - MLP - Attention (in seconds): -0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 1664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1664x4992, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1664x4992, b=2048): 99.674
Elapsed time for attention_key_query_prob (512x2048x13x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x13x2048): 10.222
Elapsed time for attention_prob_times_values (512x2048x2048x13): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x13): 11.126
Elapsed time for attention_linear_projection (4x1664x1664, b=2048): 0.0005
Throughput (in TFLOP/s) for attention_linear_projection (4x1664x1664, b=2048): 84.945
Elapsed time for mlp_h_to_4h (4x1664x6656, b=2048): 0.0018
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1664x6656, b=2048): 100.331
Elapsed time for mlp_4h_to_h (4x6656x1664, b=2048): 0.0017
Throughput (in TFLOP/s) for mlp_4h_to_h (4x6656x1664, b=2048): 105.651

Attention duration (in seconds): 0.0124
Attention throughput (in TFLOP/s): 23.677
MLP duration (in seconds): 0.0035
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0159
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 1.015
MLP duration (in seconds): 0.0038
MLP throughput (in TFLOP/s): 96.331
Transformer duration (in seconds): 0.2887
Transformer throughput (in TFLOP/s): 2.272
Transformer - MLP - Attention (in seconds): -0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 1792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1792x5376, b=2048): 0.0016
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1792x5376, b=2048): 100.460
Elapsed time for attention_key_query_prob (512x2048x14x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x14x2048): 10.989
Elapsed time for attention_prob_times_values (512x2048x2048x14): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x14): 12.414
Elapsed time for attention_linear_projection (4x1792x1792, b=2048): 0.0006
Throughput (in TFLOP/s) for attention_linear_projection (4x1792x1792, b=2048): 86.134
Elapsed time for mlp_h_to_4h (4x1792x7168, b=2048): 0.0021
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1792x7168, b=2048): 100.524
Elapsed time for mlp_4h_to_h (4x7168x1792, b=2048): 0.0021
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7168x1792, b=2048): 101.636

Attention duration (in seconds): 0.0125
Attention throughput (in TFLOP/s): 26.462
MLP duration (in seconds): 0.0042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0167
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 1.145
MLP duration (in seconds): 0.0044
MLP throughput (in TFLOP/s): 95.258
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 2.602
Transformer - MLP - Attention (in seconds): -0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 1920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x1920x5760, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x1920x5760, b=2048): 92.805
Elapsed time for attention_key_query_prob (512x2048x15x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x15x2048): 11.763
Elapsed time for attention_prob_times_values (512x2048x2048x15): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x15): 12.794
Elapsed time for attention_linear_projection (4x1920x1920, b=2048): 0.0007
Throughput (in TFLOP/s) for attention_linear_projection (4x1920x1920, b=2048): 86.994
Elapsed time for mlp_h_to_4h (4x1920x7680, b=2048): 0.0024
Throughput (in TFLOP/s) for mlp_h_to_4h (4x1920x7680, b=2048): 101.260
Elapsed time for mlp_4h_to_h (4x7680x1920, b=2048): 0.0023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x7680x1920, b=2048): 103.325

Attention duration (in seconds): 0.0132
Attention throughput (in TFLOP/s): 28.151
MLP duration (in seconds): 0.0047
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0179
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 1.283
MLP duration (in seconds): 0.0050
MLP throughput (in TFLOP/s): 96.625
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 2.956
Transformer - MLP - Attention (in seconds): -0.0050
========================================================================================================================
num_attention_heads: 128, hidden_size: 2048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2048x6144, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2048x6144, b=2048): 101.454
Elapsed time for attention_key_query_prob (512x2048x16x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x16x2048): 12.601
Elapsed time for attention_prob_times_values (512x2048x2048x16): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x16): 14.141
Elapsed time for attention_linear_projection (4x2048x2048, b=2048): 0.0008
Throughput (in TFLOP/s) for attention_linear_projection (4x2048x2048, b=2048): 90.383
Elapsed time for mlp_h_to_4h (4x2048x8192, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2048x8192, b=2048): 101.285
Elapsed time for mlp_4h_to_h (4x8192x2048, b=2048): 0.0027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8192x2048, b=2048): 102.701

Attention duration (in seconds): 0.0131
Attention throughput (in TFLOP/s): 31.462
MLP duration (in seconds): 0.0054
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0185
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 1.428
MLP duration (in seconds): 0.0057
MLP throughput (in TFLOP/s): 96.547
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 3.332
Transformer - MLP - Attention (in seconds): -0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 2176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2176x6528, b=2048): 0.0023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2176x6528, b=2048): 102.087
Elapsed time for attention_key_query_prob (512x2048x17x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x17x2048): 12.903
Elapsed time for attention_prob_times_values (512x2048x2048x17): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x17): 14.428
Elapsed time for attention_linear_projection (4x2176x2176, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x2176x2176, b=2048): 88.492
Elapsed time for mlp_h_to_4h (4x2176x8704, b=2048): 0.0031
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2176x8704, b=2048): 101.185
Elapsed time for mlp_4h_to_h (4x8704x2176, b=2048): 0.0031
Throughput (in TFLOP/s) for mlp_4h_to_h (4x8704x2176, b=2048): 101.098

Attention duration (in seconds): 0.0139
Attention throughput (in TFLOP/s): 32.888
MLP duration (in seconds): 0.0061
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0200
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 1.580
MLP duration (in seconds): 0.0064
MLP throughput (in TFLOP/s): 96.350
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 3.729
Transformer - MLP - Attention (in seconds): -0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 2304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2304x6912, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2304x6912, b=2048): 101.181
Elapsed time for attention_key_query_prob (512x2048x18x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x18x2048): 13.633
Elapsed time for attention_prob_times_values (512x2048x2048x18): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x18): 15.677
Elapsed time for attention_linear_projection (4x2304x2304, b=2048): 0.0009
Throughput (in TFLOP/s) for attention_linear_projection (4x2304x2304, b=2048): 93.536
Elapsed time for mlp_h_to_4h (4x2304x9216, b=2048): 0.0034
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2304x9216, b=2048): 101.359
Elapsed time for mlp_4h_to_h (4x9216x2304, b=2048): 0.0035
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9216x2304, b=2048): 100.231

Attention duration (in seconds): 0.0141
Attention throughput (in TFLOP/s): 35.612
MLP duration (in seconds): 0.0069
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0210
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 1.740
MLP duration (in seconds): 0.0072
MLP throughput (in TFLOP/s): 96.538
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 4.149
Transformer - MLP - Attention (in seconds): -0.0072
========================================================================================================================
num_attention_heads: 128, hidden_size: 2432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2432x7296, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2432x7296, b=2048): 100.856
Elapsed time for attention_key_query_prob (512x2048x19x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x19x2048): 14.469
Elapsed time for attention_prob_times_values (512x2048x2048x19): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x19): 16.008
Elapsed time for attention_linear_projection (4x2432x2432, b=2048): 0.0010
Throughput (in TFLOP/s) for attention_linear_projection (4x2432x2432, b=2048): 96.705
Elapsed time for mlp_h_to_4h (4x2432x9728, b=2048): 0.0039
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2432x9728, b=2048): 100.625
Elapsed time for mlp_4h_to_h (4x9728x2432, b=2048): 0.0038
Throughput (in TFLOP/s) for mlp_4h_to_h (4x9728x2432, b=2048): 102.684

Attention duration (in seconds): 0.0146
Attention throughput (in TFLOP/s): 37.671
MLP duration (in seconds): 0.0076
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0222
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 1.907
MLP duration (in seconds): 0.0080
MLP throughput (in TFLOP/s): 97.164
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 4.591
Transformer - MLP - Attention (in seconds): -0.0080
========================================================================================================================
num_attention_heads: 128, hidden_size: 2560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2560x7680, b=2048): 0.0032
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2560x7680, b=2048): 101.204
Elapsed time for attention_key_query_prob (512x2048x20x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x20x2048): 15.015
Elapsed time for attention_prob_times_values (512x2048x2048x20): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x20): 17.265
Elapsed time for attention_linear_projection (4x2560x2560, b=2048): 0.0011
Throughput (in TFLOP/s) for attention_linear_projection (4x2560x2560, b=2048): 98.011
Elapsed time for mlp_h_to_4h (4x2560x10240, b=2048): 0.0043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2560x10240, b=2048): 100.735
Elapsed time for mlp_4h_to_h (4x10240x2560, b=2048): 0.0042
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10240x2560, b=2048): 102.442

Attention duration (in seconds): 0.0150
Attention throughput (in TFLOP/s): 40.154
MLP duration (in seconds): 0.0085
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0234
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 2.083
MLP duration (in seconds): 0.0088
MLP throughput (in TFLOP/s): 97.425
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 5.056
Transformer - MLP - Attention (in seconds): -0.0087
========================================================================================================================
num_attention_heads: 128, hidden_size: 2688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2688x8064, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2688x8064, b=2048): 100.796
Elapsed time for attention_key_query_prob (512x2048x21x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x21x2048): 15.974
Elapsed time for attention_prob_times_values (512x2048x2048x21): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x21): 17.583
Elapsed time for attention_linear_projection (4x2688x2688, b=2048): 0.0012
Throughput (in TFLOP/s) for attention_linear_projection (4x2688x2688, b=2048): 97.721
Elapsed time for mlp_h_to_4h (4x2688x10752, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2688x10752, b=2048): 100.267
Elapsed time for mlp_4h_to_h (4x10752x2688, b=2048): 0.0047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x10752x2688, b=2048): 101.006

Attention duration (in seconds): 0.0155
Attention throughput (in TFLOP/s): 42.159
MLP duration (in seconds): 0.0094
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0249
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 2.265
MLP duration (in seconds): 0.0098
MLP throughput (in TFLOP/s): 97.053
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 5.543
Transformer - MLP - Attention (in seconds): -0.0097
========================================================================================================================
num_attention_heads: 128, hidden_size: 2816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2816x8448, b=2048): 0.0039
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2816x8448, b=2048): 101.045
Elapsed time for attention_key_query_prob (512x2048x22x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x22x2048): 16.731
Elapsed time for attention_prob_times_values (512x2048x2048x22): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x22): 18.830
Elapsed time for attention_linear_projection (4x2816x2816, b=2048): 0.0013
Throughput (in TFLOP/s) for attention_linear_projection (4x2816x2816, b=2048): 97.432
Elapsed time for mlp_h_to_4h (4x2816x11264, b=2048): 0.0049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2816x11264, b=2048): 105.485
Elapsed time for mlp_4h_to_h (4x11264x2816, b=2048): 0.0052
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11264x2816, b=2048): 99.933

Attention duration (in seconds): 0.0159
Attention throughput (in TFLOP/s): 44.693
MLP duration (in seconds): 0.0101
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0260
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 2.454
MLP duration (in seconds): 0.0105
MLP throughput (in TFLOP/s): 98.886
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 6.053
Transformer - MLP - Attention (in seconds): -0.0105
========================================================================================================================
num_attention_heads: 128, hidden_size: 2944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x2944x8832, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x2944x8832, b=2048): 101.379
Elapsed time for attention_key_query_prob (512x2048x23x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x23x2048): 17.466
Elapsed time for attention_prob_times_values (512x2048x2048x23): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x23): 18.992
Elapsed time for attention_linear_projection (4x2944x2944, b=2048): 0.0014
Throughput (in TFLOP/s) for attention_linear_projection (4x2944x2944, b=2048): 99.383
Elapsed time for mlp_h_to_4h (4x2944x11776, b=2048): 0.0053
Throughput (in TFLOP/s) for mlp_h_to_4h (4x2944x11776, b=2048): 106.357
Elapsed time for mlp_4h_to_h (4x11776x2944, b=2048): 0.0055
Throughput (in TFLOP/s) for mlp_4h_to_h (4x11776x2944, b=2048): 102.663

Attention duration (in seconds): 0.0165
Attention throughput (in TFLOP/s): 46.432
MLP duration (in seconds): 0.0109
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 2.650
MLP duration (in seconds): 0.0113
MLP throughput (in TFLOP/s): 100.832
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 6.583
Transformer - MLP - Attention (in seconds): -0.0113
========================================================================================================================
num_attention_heads: 128, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3072x9216, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3072x9216, b=2048): 101.336
Elapsed time for attention_key_query_prob (512x2048x24x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x24x2048): 18.216
Elapsed time for attention_prob_times_values (512x2048x2048x24): 0.0050
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x24): 20.620
Elapsed time for attention_linear_projection (4x3072x3072, b=2048): 0.0015
Throughput (in TFLOP/s) for attention_linear_projection (4x3072x3072, b=2048): 100.018
Elapsed time for mlp_h_to_4h (4x3072x12288, b=2048): 0.0059
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3072x12288, b=2048): 105.177
Elapsed time for mlp_4h_to_h (4x12288x3072, b=2048): 0.0060
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12288x3072, b=2048): 102.964

Attention duration (in seconds): 0.0168
Attention throughput (in TFLOP/s): 49.141
MLP duration (in seconds): 0.0119
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0287
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 2.855
MLP duration (in seconds): 0.0122
MLP throughput (in TFLOP/s): 101.446
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 7.140
Transformer - MLP - Attention (in seconds): -0.0122
========================================================================================================================
num_attention_heads: 128, hidden_size: 3200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3200x9600, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3200x9600, b=2048): 101.998
Elapsed time for attention_key_query_prob (512x2048x25x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x25x2048): 18.451
Elapsed time for attention_prob_times_values (512x2048x2048x25): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x25): 20.600
Elapsed time for attention_linear_projection (4x3200x3200, b=2048): 0.0017
Throughput (in TFLOP/s) for attention_linear_projection (4x3200x3200, b=2048): 99.828
Elapsed time for mlp_h_to_4h (4x3200x12800, b=2048): 0.0064
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3200x12800, b=2048): 105.504
Elapsed time for mlp_4h_to_h (4x12800x3200, b=2048): 0.0066
Throughput (in TFLOP/s) for mlp_4h_to_h (4x12800x3200, b=2048): 101.851

Attention duration (in seconds): 0.0176
Attention throughput (in TFLOP/s): 50.198
MLP duration (in seconds): 0.0129
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0306
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 3.068
MLP duration (in seconds): 0.0134
MLP throughput (in TFLOP/s): 100.182
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 7.715
Transformer - MLP - Attention (in seconds): -0.0134
========================================================================================================================
num_attention_heads: 128, hidden_size: 3328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3328x9984, b=2048): 0.0053
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3328x9984, b=2048): 101.939
Elapsed time for attention_key_query_prob (512x2048x26x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x26x2048): 19.164
Elapsed time for attention_prob_times_values (512x2048x2048x26): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x26): 21.689
Elapsed time for attention_linear_projection (4x3328x3328, b=2048): 0.0018
Throughput (in TFLOP/s) for attention_linear_projection (4x3328x3328, b=2048): 99.116
Elapsed time for mlp_h_to_4h (4x3328x13312, b=2048): 0.0068
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3328x13312, b=2048): 106.430
Elapsed time for mlp_4h_to_h (4x13312x3328, b=2048): 0.0072
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13312x3328, b=2048): 100.331

Attention duration (in seconds): 0.0181
Attention throughput (in TFLOP/s): 52.306
MLP duration (in seconds): 0.0141
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0322
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 3.286
MLP duration (in seconds): 0.0145
MLP throughput (in TFLOP/s): 100.133
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 8.313
Transformer - MLP - Attention (in seconds): -0.0145
========================================================================================================================
num_attention_heads: 128, hidden_size: 3456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3456x10368, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3456x10368, b=2048): 101.649
Elapsed time for attention_key_query_prob (512x2048x27x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x27x2048): 19.815
Elapsed time for attention_prob_times_values (512x2048x2048x27): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x27): 22.116
Elapsed time for attention_linear_projection (4x3456x3456, b=2048): 0.0020
Throughput (in TFLOP/s) for attention_linear_projection (4x3456x3456, b=2048): 98.806
Elapsed time for mlp_h_to_4h (4x3456x13824, b=2048): 0.0073
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3456x13824, b=2048): 106.685
Elapsed time for mlp_4h_to_h (4x13824x3456, b=2048): 0.0078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x13824x3456, b=2048): 100.114

Attention duration (in seconds): 0.0189
Attention throughput (in TFLOP/s): 53.824
MLP duration (in seconds): 0.0152
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0340
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 3.513
MLP duration (in seconds): 0.0156
MLP throughput (in TFLOP/s): 100.112
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 8.934
Transformer - MLP - Attention (in seconds): -0.0157
========================================================================================================================
num_attention_heads: 128, hidden_size: 3584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3584x10752, b=2048): 0.0062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3584x10752, b=2048): 101.941
Elapsed time for attention_key_query_prob (512x2048x28x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x28x2048): 20.476
Elapsed time for attention_prob_times_values (512x2048x2048x28): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x28): 23.300
Elapsed time for attention_linear_projection (4x3584x3584, b=2048): 0.0021
Throughput (in TFLOP/s) for attention_linear_projection (4x3584x3584, b=2048): 100.674
Elapsed time for mlp_h_to_4h (4x3584x14336, b=2048): 0.0078
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3584x14336, b=2048): 107.808
Elapsed time for mlp_4h_to_h (4x14336x3584, b=2048): 0.0083
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14336x3584, b=2048): 101.457

Attention duration (in seconds): 0.0193
Attention throughput (in TFLOP/s): 56.026
MLP duration (in seconds): 0.0161
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 3.748
MLP duration (in seconds): 0.0166
MLP throughput (in TFLOP/s): 101.526
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 9.578
Transformer - MLP - Attention (in seconds): -0.0166
========================================================================================================================
num_attention_heads: 128, hidden_size: 3712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3712x11136, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3712x11136, b=2048): 101.405
Elapsed time for attention_key_query_prob (512x2048x29x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x29x2048): 21.147
Elapsed time for attention_prob_times_values (512x2048x2048x29): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x29): 23.531
Elapsed time for attention_linear_projection (4x3712x3712, b=2048): 0.0022
Throughput (in TFLOP/s) for attention_linear_projection (4x3712x3712, b=2048): 100.829
Elapsed time for mlp_h_to_4h (4x3712x14848, b=2048): 0.0089
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3712x14848, b=2048): 101.925
Elapsed time for mlp_4h_to_h (4x14848x3712, b=2048): 0.0089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x14848x3712, b=2048): 101.341

Attention duration (in seconds): 0.0201
Attention throughput (in TFLOP/s): 57.317
MLP duration (in seconds): 0.0178
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0379
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 3.990
MLP duration (in seconds): 0.0183
MLP throughput (in TFLOP/s): 98.742
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 10.241
Transformer - MLP - Attention (in seconds): -0.0182
========================================================================================================================
num_attention_heads: 128, hidden_size: 3840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3840x11520, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3840x11520, b=2048): 106.172
Elapsed time for attention_key_query_prob (512x2048x30x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x30x2048): 21.924
Elapsed time for attention_prob_times_values (512x2048x2048x30): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x30): 24.841
Elapsed time for attention_linear_projection (4x3840x3840, b=2048): 0.0024
Throughput (in TFLOP/s) for attention_linear_projection (4x3840x3840, b=2048): 100.527
Elapsed time for mlp_h_to_4h (4x3840x15360, b=2048): 0.0091
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3840x15360, b=2048): 106.485
Elapsed time for mlp_4h_to_h (4x15360x3840, b=2048): 0.0096
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15360x3840, b=2048): 100.784

Attention duration (in seconds): 0.0203
Attention throughput (in TFLOP/s): 60.317
MLP duration (in seconds): 0.0187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0390
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 4.238
MLP duration (in seconds): 0.0191
MLP throughput (in TFLOP/s): 101.355
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 10.928
Transformer - MLP - Attention (in seconds): -0.0191
========================================================================================================================
num_attention_heads: 128, hidden_size: 3968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x3968x11904, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x3968x11904, b=2048): 105.763
Elapsed time for attention_key_query_prob (512x2048x31x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x31x2048): 22.574
Elapsed time for attention_prob_times_values (512x2048x2048x31): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x31): 25.003
Elapsed time for attention_linear_projection (4x3968x3968, b=2048): 0.0026
Throughput (in TFLOP/s) for attention_linear_projection (4x3968x3968, b=2048): 100.119
Elapsed time for mlp_h_to_4h (4x3968x15872, b=2048): 0.0101
Throughput (in TFLOP/s) for mlp_h_to_4h (4x3968x15872, b=2048): 102.328
Elapsed time for mlp_4h_to_h (4x15872x3968, b=2048): 0.0101
Throughput (in TFLOP/s) for mlp_4h_to_h (4x15872x3968, b=2048): 101.966

Attention duration (in seconds): 0.0211
Attention throughput (in TFLOP/s): 61.474
MLP duration (in seconds): 0.0202
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0413
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 4.495
MLP duration (in seconds): 0.0208
MLP throughput (in TFLOP/s): 99.031
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 11.641
Transformer - MLP - Attention (in seconds): -0.0208
========================================================================================================================
num_attention_heads: 128, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4096x12288, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4096x12288, b=2048): 106.019
Elapsed time for attention_key_query_prob (512x2048x32x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x32x2048): 41.138
Elapsed time for attention_prob_times_values (512x2048x2048x32): 0.0051
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x32): 26.816
Elapsed time for attention_linear_projection (4x4096x4096, b=2048): 0.0028
Throughput (in TFLOP/s) for attention_linear_projection (4x4096x4096, b=2048): 99.872
Elapsed time for mlp_h_to_4h (4x4096x16384, b=2048): 0.0117
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4096x16384, b=2048): 93.682
Elapsed time for mlp_4h_to_h (4x16384x4096, b=2048): 0.0139
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16384x4096, b=2048): 78.888

Attention duration (in seconds): 0.0190
Attention throughput (in TFLOP/s): 72.349
MLP duration (in seconds): 0.0257
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0447
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 4.757
MLP duration (in seconds): 0.0230
MLP throughput (in TFLOP/s): 95.781
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 12.368
Transformer - MLP - Attention (in seconds): -0.0229
========================================================================================================================
num_attention_heads: 128, hidden_size: 4224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4224x12672, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4224x12672, b=2048): 105.260
Elapsed time for attention_key_query_prob (512x2048x33x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x33x2048): 27.004
Elapsed time for attention_prob_times_values (512x2048x2048x33): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x33): 26.731
Elapsed time for attention_linear_projection (4x4224x4224, b=2048): 0.0029
Throughput (in TFLOP/s) for attention_linear_projection (4x4224x4224, b=2048): 101.381
Elapsed time for mlp_h_to_4h (4x4224x16896, b=2048): 0.0115
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4224x16896, b=2048): 101.381
Elapsed time for mlp_4h_to_h (4x16896x4224, b=2048): 0.0113
Throughput (in TFLOP/s) for mlp_4h_to_h (4x16896x4224, b=2048): 103.080

Attention duration (in seconds): 0.0218
Attention throughput (in TFLOP/s): 66.746
MLP duration (in seconds): 0.0229
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0446
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 5.031
MLP duration (in seconds): 0.0229
MLP throughput (in TFLOP/s): 102.278
Transformer duration (in seconds): 0.2887
Transformer throughput (in TFLOP/s): 13.131
Transformer - MLP - Attention (in seconds): -0.0229
========================================================================================================================
num_attention_heads: 128, hidden_size: 4352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4352x13056, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4352x13056, b=2048): 105.271
Elapsed time for attention_key_query_prob (512x2048x34x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x34x2048): 27.204
Elapsed time for attention_prob_times_values (512x2048x2048x34): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x34): 27.631
Elapsed time for attention_linear_projection (4x4352x4352, b=2048): 0.0031
Throughput (in TFLOP/s) for attention_linear_projection (4x4352x4352, b=2048): 100.989
Elapsed time for mlp_h_to_4h (4x4352x17408, b=2048): 0.0119
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4352x17408, b=2048): 104.739
Elapsed time for mlp_4h_to_h (4x17408x4352, b=2048): 0.0121
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17408x4352, b=2048): 102.866

Attention duration (in seconds): 0.0226
Attention throughput (in TFLOP/s): 67.939
MLP duration (in seconds): 0.0239
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0465
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 5.311
MLP duration (in seconds): 0.0242
MLP throughput (in TFLOP/s): 102.514
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 13.906
Transformer - MLP - Attention (in seconds): -0.0242
========================================================================================================================
num_attention_heads: 128, hidden_size: 4480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4480x13440, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4480x13440, b=2048): 97.568
Elapsed time for attention_key_query_prob (512x2048x35x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x35x2048): 26.136
Elapsed time for attention_prob_times_values (512x2048x2048x35): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x35): 28.137
Elapsed time for attention_linear_projection (4x4480x4480, b=2048): 0.0033
Throughput (in TFLOP/s) for attention_linear_projection (4x4480x4480, b=2048): 100.220
Elapsed time for mlp_h_to_4h (4x4480x17920, b=2048): 0.0130
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4480x17920, b=2048): 100.959
Elapsed time for mlp_4h_to_h (4x17920x4480, b=2048): 0.0129
Throughput (in TFLOP/s) for mlp_4h_to_h (4x17920x4480, b=2048): 102.091

Attention duration (in seconds): 0.0245
Attention throughput (in TFLOP/s): 65.996
MLP duration (in seconds): 0.0259
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0504
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 5.596
MLP duration (in seconds): 0.0258
MLP throughput (in TFLOP/s): 101.842
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 14.704
Transformer - MLP - Attention (in seconds): -0.0258
========================================================================================================================
num_attention_heads: 128, hidden_size: 4608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4608x13824, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4608x13824, b=2048): 103.167
Elapsed time for attention_key_query_prob (512x2048x36x2048): 0.0058
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x36x2048): 26.728
Elapsed time for attention_prob_times_values (512x2048x2048x36): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x36): 29.298
Elapsed time for attention_linear_projection (4x4608x4608, b=2048): 0.0035
Throughput (in TFLOP/s) for attention_linear_projection (4x4608x4608, b=2048): 99.970
Elapsed time for mlp_h_to_4h (4x4608x18432, b=2048): 0.0136
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4608x18432, b=2048): 102.118
Elapsed time for mlp_4h_to_h (4x18432x4608, b=2048): 0.0137
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18432x4608, b=2048): 101.613

Attention duration (in seconds): 0.0247
Attention throughput (in TFLOP/s): 68.974
MLP duration (in seconds): 0.0273
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0520
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 5.889
MLP duration (in seconds): 0.0281
MLP throughput (in TFLOP/s): 99.129
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 15.526
Transformer - MLP - Attention (in seconds): -0.0281
========================================================================================================================
num_attention_heads: 128, hidden_size: 4736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4736x14208, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4736x14208, b=2048): 100.105
Elapsed time for attention_key_query_prob (512x2048x37x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x37x2048): 26.609
Elapsed time for attention_prob_times_values (512x2048x2048x37): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x37): 29.542
Elapsed time for attention_linear_projection (4x4736x4736, b=2048): 0.0036
Throughput (in TFLOP/s) for attention_linear_projection (4x4736x4736, b=2048): 101.646
Elapsed time for mlp_h_to_4h (4x4736x18944, b=2048): 0.0148
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4736x18944, b=2048): 99.611
Elapsed time for mlp_4h_to_h (4x18944x4736, b=2048): 0.0144
Throughput (in TFLOP/s) for mlp_4h_to_h (4x18944x4736, b=2048): 102.426

Attention duration (in seconds): 0.0260
Attention throughput (in TFLOP/s): 68.814
MLP duration (in seconds): 0.0291
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 6.191
MLP duration (in seconds): 0.0286
MLP throughput (in TFLOP/s): 102.695
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 16.368
Transformer - MLP - Attention (in seconds): -0.0286
========================================================================================================================
num_attention_heads: 128, hidden_size: 4864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4864x14592, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4864x14592, b=2048): 102.065
Elapsed time for attention_key_query_prob (512x2048x38x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x38x2048): 27.438
Elapsed time for attention_prob_times_values (512x2048x2048x38): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x38): 30.734
Elapsed time for attention_linear_projection (4x4864x4864, b=2048): 0.0038
Throughput (in TFLOP/s) for attention_linear_projection (4x4864x4864, b=2048): 101.574
Elapsed time for mlp_h_to_4h (4x4864x19456, b=2048): 0.0158
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4864x19456, b=2048): 98.429
Elapsed time for mlp_4h_to_h (4x19456x4864, b=2048): 0.0151
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19456x4864, b=2048): 102.961

Attention duration (in seconds): 0.0265
Attention throughput (in TFLOP/s): 70.912
MLP duration (in seconds): 0.0308
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0573
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 6.499
MLP duration (in seconds): 0.0302
MLP throughput (in TFLOP/s): 102.816
Transformer duration (in seconds): 0.2887
Transformer throughput (in TFLOP/s): 17.242
Transformer - MLP - Attention (in seconds): -0.0302
========================================================================================================================
num_attention_heads: 128, hidden_size: 4992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x4992x14976, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x4992x14976, b=2048): 102.463
Elapsed time for attention_key_query_prob (512x2048x39x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x39x2048): 27.637
Elapsed time for attention_prob_times_values (512x2048x2048x39): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x39): 31.073
Elapsed time for attention_linear_projection (4x4992x4992, b=2048): 0.0040
Throughput (in TFLOP/s) for attention_linear_projection (4x4992x4992, b=2048): 101.541
Elapsed time for mlp_h_to_4h (4x4992x19968, b=2048): 0.0168
Throughput (in TFLOP/s) for mlp_h_to_4h (4x4992x19968, b=2048): 97.301
Elapsed time for mlp_4h_to_h (4x19968x4992, b=2048): 0.0159
Throughput (in TFLOP/s) for mlp_4h_to_h (4x19968x4992, b=2048): 102.413

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 71.761
MLP duration (in seconds): 0.0327
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 6.814
MLP duration (in seconds): 0.0317
MLP throughput (in TFLOP/s): 102.929
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 18.126
Transformer - MLP - Attention (in seconds): -0.0318
========================================================================================================================
num_attention_heads: 128, hidden_size: 5120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5120x15360, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5120x15360, b=2048): 101.524
Elapsed time for attention_key_query_prob (512x2048x40x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x40x2048): 28.607
Elapsed time for attention_prob_times_values (512x2048x2048x40): 0.0052
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x40): 32.902
Elapsed time for attention_linear_projection (4x5120x5120, b=2048): 0.0042
Throughput (in TFLOP/s) for attention_linear_projection (4x5120x5120, b=2048): 101.102
Elapsed time for mlp_h_to_4h (4x5120x20480, b=2048): 0.0172
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5120x20480, b=2048): 99.648
Elapsed time for mlp_4h_to_h (4x20480x5120, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20480x5120, b=2048): 69.278

Attention duration (in seconds): 0.0282
Attention throughput (in TFLOP/s): 73.192
MLP duration (in seconds): 0.0420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0702
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 7.137
MLP duration (in seconds): 0.0404
MLP throughput (in TFLOP/s): 84.980
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 19.030
Transformer - MLP - Attention (in seconds): -0.0404
========================================================================================================================
num_attention_heads: 128, hidden_size: 5248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5248x15744, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5248x15744, b=2048): 102.101
Elapsed time for attention_key_query_prob (512x2048x41x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x41x2048): 28.015
Elapsed time for attention_prob_times_values (512x2048x2048x41): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x41): 32.294
Elapsed time for attention_linear_projection (4x5248x5248, b=2048): 0.0045
Throughput (in TFLOP/s) for attention_linear_projection (4x5248x5248, b=2048): 100.389
Elapsed time for mlp_h_to_4h (4x5248x20992, b=2048): 0.0185
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5248x20992, b=2048): 97.594
Elapsed time for mlp_4h_to_h (4x20992x5248, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_4h_to_h (4x20992x5248, b=2048): 68.235

Attention duration (in seconds): 0.0295
Attention throughput (in TFLOP/s): 73.143
MLP duration (in seconds): 0.0449
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0744
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 7.470
MLP duration (in seconds): 0.0433
MLP throughput (in TFLOP/s): 83.417
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 19.963
Transformer - MLP - Attention (in seconds): -0.0432
========================================================================================================================
num_attention_heads: 128, hidden_size: 5376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5376x16128, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5376x16128, b=2048): 102.342
Elapsed time for attention_key_query_prob (512x2048x42x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x42x2048): 28.899
Elapsed time for attention_prob_times_values (512x2048x2048x42): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x42): 33.334
Elapsed time for attention_linear_projection (4x5376x5376, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x5376x5376, b=2048): 101.877
Elapsed time for mlp_h_to_4h (4x5376x21504, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5376x21504, b=2048): 95.440
Elapsed time for mlp_4h_to_h (4x21504x5376, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_4h_to_h (4x21504x5376, b=2048): 67.528

Attention duration (in seconds): 0.0302
Attention throughput (in TFLOP/s): 74.708
MLP duration (in seconds): 0.0479
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0781
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 7.808
MLP duration (in seconds): 0.0455
MLP throughput (in TFLOP/s): 83.244
Transformer duration (in seconds): 0.2891
Transformer throughput (in TFLOP/s): 20.906
Transformer - MLP - Attention (in seconds): -0.0452
========================================================================================================================
num_attention_heads: 128, hidden_size: 5504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5504x16512, b=2048): 0.0145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5504x16512, b=2048): 102.477
Elapsed time for attention_key_query_prob (512x2048x43x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x43x2048): 29.261
Elapsed time for attention_prob_times_values (512x2048x2048x43): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x43): 33.451
Elapsed time for attention_linear_projection (4x5504x5504, b=2048): 0.0049
Throughput (in TFLOP/s) for attention_linear_projection (4x5504x5504, b=2048): 101.978
Elapsed time for mlp_h_to_4h (4x5504x22016, b=2048): 0.0216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5504x22016, b=2048): 92.108
Elapsed time for mlp_4h_to_h (4x22016x5504, b=2048): 0.0288
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22016x5504, b=2048): 68.870

Attention duration (in seconds): 0.0312
Attention throughput (in TFLOP/s): 75.399
MLP duration (in seconds): 0.0504
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0816
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 8.156
MLP duration (in seconds): 0.0474
MLP throughput (in TFLOP/s): 83.687
Transformer duration (in seconds): 0.2886
Transformer throughput (in TFLOP/s): 21.915
Transformer - MLP - Attention (in seconds): -0.0475
========================================================================================================================
num_attention_heads: 128, hidden_size: 5632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5632x16896, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5632x16896, b=2048): 102.247
Elapsed time for attention_key_query_prob (512x2048x44x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x44x2048): 30.200
Elapsed time for attention_prob_times_values (512x2048x2048x44): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x44): 34.969
Elapsed time for attention_linear_projection (4x5632x5632, b=2048): 0.0051
Throughput (in TFLOP/s) for attention_linear_projection (4x5632x5632, b=2048): 101.677
Elapsed time for mlp_h_to_4h (4x5632x22528, b=2048): 0.0231
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5632x22528, b=2048): 89.836
Elapsed time for mlp_4h_to_h (4x22528x5632, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_4h_to_h (4x22528x5632, b=2048): 66.604

Attention duration (in seconds): 0.0320
Attention throughput (in TFLOP/s): 76.722
MLP duration (in seconds): 0.0544
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0864
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 8.507
MLP duration (in seconds): 0.0511
MLP throughput (in TFLOP/s): 81.381
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 22.902
Transformer - MLP - Attention (in seconds): -0.0511
========================================================================================================================
num_attention_heads: 128, hidden_size: 5760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5760x17280, b=2048): 0.0159
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5760x17280, b=2048): 102.663
Elapsed time for attention_key_query_prob (512x2048x45x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x45x2048): 30.617
Elapsed time for attention_prob_times_values (512x2048x2048x45): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x45): 34.906
Elapsed time for attention_linear_projection (4x5760x5760, b=2048): 0.0054
Throughput (in TFLOP/s) for attention_linear_projection (4x5760x5760, b=2048): 101.250
Elapsed time for mlp_h_to_4h (4x5760x23040, b=2048): 0.0240
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5760x23040, b=2048): 90.642
Elapsed time for mlp_4h_to_h (4x23040x5760, b=2048): 0.0326
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23040x5760, b=2048): 66.688

Attention duration (in seconds): 0.0331
Attention throughput (in TFLOP/s): 77.361
MLP duration (in seconds): 0.0566
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0897
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 8.867
MLP duration (in seconds): 0.0533
MLP throughput (in TFLOP/s): 81.545
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 23.928
Transformer - MLP - Attention (in seconds): -0.0534
========================================================================================================================
num_attention_heads: 128, hidden_size: 5888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x5888x17664, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x5888x17664, b=2048): 102.744
Elapsed time for attention_key_query_prob (512x2048x46x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x46x2048): 31.625
Elapsed time for attention_prob_times_values (512x2048x2048x46): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x46): 36.329
Elapsed time for attention_linear_projection (4x5888x5888, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x5888x5888, b=2048): 102.553
Elapsed time for mlp_h_to_4h (4x5888x23552, b=2048): 0.0248
Throughput (in TFLOP/s) for mlp_h_to_4h (4x5888x23552, b=2048): 91.630
Elapsed time for mlp_4h_to_h (4x23552x5888, b=2048): 0.0342
Throughput (in TFLOP/s) for mlp_4h_to_h (4x23552x5888, b=2048): 66.450

Attention duration (in seconds): 0.0338
Attention throughput (in TFLOP/s): 78.888
MLP duration (in seconds): 0.0590
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0928
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 9.236
MLP duration (in seconds): 0.0557
MLP throughput (in TFLOP/s): 81.602
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 24.972
Transformer - MLP - Attention (in seconds): -0.0557
========================================================================================================================
num_attention_heads: 128, hidden_size: 6016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6016x18048, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6016x18048, b=2048): 102.471
Elapsed time for attention_key_query_prob (512x2048x47x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x47x2048): 32.066
Elapsed time for attention_prob_times_values (512x2048x2048x47): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x47): 36.357
Elapsed time for attention_linear_projection (4x6016x6016, b=2048): 0.0058
Throughput (in TFLOP/s) for attention_linear_projection (4x6016x6016, b=2048): 102.405
Elapsed time for mlp_h_to_4h (4x6016x24064, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6016x24064, b=2048): 91.586
Elapsed time for mlp_4h_to_h (4x24064x6016, b=2048): 0.0354
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24064x6016, b=2048): 66.958

Attention duration (in seconds): 0.0350
Attention throughput (in TFLOP/s): 79.308
MLP duration (in seconds): 0.0613
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0963
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 9.608
MLP duration (in seconds): 0.0581
MLP throughput (in TFLOP/s): 81.622
Transformer duration (in seconds): 0.2886
Transformer throughput (in TFLOP/s): 26.055
Transformer - MLP - Attention (in seconds): -0.0584
========================================================================================================================
num_attention_heads: 128, hidden_size: 6144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6144x18432, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6144x18432, b=2048): 102.937
Elapsed time for attention_key_query_prob (512x2048x48x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x48x2048): 33.210
Elapsed time for attention_prob_times_values (512x2048x2048x48): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x48): 38.904
Elapsed time for attention_linear_projection (4x6144x6144, b=2048): 0.0060
Throughput (in TFLOP/s) for attention_linear_projection (4x6144x6144, b=2048): 102.541
Elapsed time for mlp_h_to_4h (4x6144x24576, b=2048): 0.0281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6144x24576, b=2048): 87.937
Elapsed time for mlp_4h_to_h (4x24576x6144, b=2048): 0.0370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x24576x6144, b=2048): 66.936

Attention duration (in seconds): 0.0356
Attention throughput (in TFLOP/s): 81.157
MLP duration (in seconds): 0.0651
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 9.994
MLP duration (in seconds): 0.0618
MLP throughput (in TFLOP/s): 80.067
Transformer duration (in seconds): 0.2890
Transformer throughput (in TFLOP/s): 27.111
Transformer - MLP - Attention (in seconds): -0.0616
========================================================================================================================
num_attention_heads: 128, hidden_size: 6272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6272x18816, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6272x18816, b=2048): 102.818
Elapsed time for attention_key_query_prob (512x2048x49x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x49x2048): 33.417
Elapsed time for attention_prob_times_values (512x2048x2048x49): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x49): 37.936
Elapsed time for attention_linear_projection (4x6272x6272, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x6272x6272, b=2048): 101.799
Elapsed time for mlp_h_to_4h (4x6272x25088, b=2048): 0.0290
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6272x25088, b=2048): 88.878
Elapsed time for mlp_4h_to_h (4x25088x6272, b=2048): 0.0387
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25088x6272, b=2048): 66.558

Attention duration (in seconds): 0.0370
Attention throughput (in TFLOP/s): 81.092
MLP duration (in seconds): 0.0677
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1047
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 10.384
MLP duration (in seconds): 0.0642
MLP throughput (in TFLOP/s): 80.289
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 28.242
Transformer - MLP - Attention (in seconds): -0.0643
========================================================================================================================
num_attention_heads: 128, hidden_size: 6400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6400x19200, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6400x19200, b=2048): 102.386
Elapsed time for attention_key_query_prob (512x2048x50x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x50x2048): 34.183
Elapsed time for attention_prob_times_values (512x2048x2048x50): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x50): 38.831
Elapsed time for attention_linear_projection (4x6400x6400, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x6400x6400, b=2048): 101.418
Elapsed time for mlp_h_to_4h (4x6400x25600, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6400x25600, b=2048): 87.015
Elapsed time for mlp_4h_to_h (4x25600x6400, b=2048): 0.0404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x25600x6400, b=2048): 66.526

Attention duration (in seconds): 0.0381
Attention throughput (in TFLOP/s): 81.743
MLP duration (in seconds): 0.0712
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1093
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 10.784
MLP duration (in seconds): 0.0667
MLP throughput (in TFLOP/s): 80.500
Transformer duration (in seconds): 0.2884
Transformer throughput (in TFLOP/s): 29.416
Transformer - MLP - Attention (in seconds): -0.0671
========================================================================================================================
num_attention_heads: 128, hidden_size: 6528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6528x19584, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6528x19584, b=2048): 90.895
Elapsed time for attention_key_query_prob (512x2048x51x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x51x2048): 34.760
Elapsed time for attention_prob_times_values (512x2048x2048x51): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x51): 38.790
Elapsed time for attention_linear_projection (4x6528x6528, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x6528x6528, b=2048): 102.804
Elapsed time for mlp_h_to_4h (4x6528x26112, b=2048): 0.0321
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6528x26112, b=2048): 87.078
Elapsed time for mlp_4h_to_h (4x26112x6528, b=2048): 0.0403
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26112x6528, b=2048): 69.321

Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 77.323
MLP duration (in seconds): 0.0724
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1141
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 11.189
MLP duration (in seconds): 0.0695
MLP throughput (in TFLOP/s): 80.343
Transformer duration (in seconds): 0.2886
Transformer throughput (in TFLOP/s): 30.552
Transformer - MLP - Attention (in seconds): -0.0697
========================================================================================================================
num_attention_heads: 128, hidden_size: 6656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6656x19968, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6656x19968, b=2048): 92.324
Elapsed time for attention_key_query_prob (512x2048x52x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x52x2048): 35.549
Elapsed time for attention_prob_times_values (512x2048x2048x52): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x52): 40.702
Elapsed time for attention_linear_projection (4x6656x6656, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x6656x6656, b=2048): 102.700
Elapsed time for mlp_h_to_4h (4x6656x26624, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6656x26624, b=2048): 88.885
Elapsed time for mlp_4h_to_h (4x26624x6656, b=2048): 0.0440
Throughput (in TFLOP/s) for mlp_4h_to_h (4x26624x6656, b=2048): 66.026

Attention duration (in seconds): 0.0424
Attention throughput (in TFLOP/s): 78.967
MLP duration (in seconds): 0.0766
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1191
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 11.600
MLP duration (in seconds): 0.0739
MLP throughput (in TFLOP/s): 78.533
Transformer duration (in seconds): 0.2884
Transformer throughput (in TFLOP/s): 31.748
Transformer - MLP - Attention (in seconds): -0.0743
========================================================================================================================
num_attention_heads: 128, hidden_size: 6784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6784x20352, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6784x20352, b=2048): 85.578
Elapsed time for attention_key_query_prob (512x2048x53x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x53x2048): 36.167
Elapsed time for attention_prob_times_values (512x2048x2048x53): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x53): 40.422
Elapsed time for attention_linear_projection (4x6784x6784, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x6784x6784, b=2048): 102.391
Elapsed time for mlp_h_to_4h (4x6784x27136, b=2048): 0.0293
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6784x27136, b=2048): 103.004
Elapsed time for mlp_4h_to_h (4x27136x6784, b=2048): 0.0467
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27136x6784, b=2048): 64.632

Attention duration (in seconds): 0.0457
Attention throughput (in TFLOP/s): 75.923
MLP duration (in seconds): 0.0759
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1217
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 12.018
MLP duration (in seconds): 0.0765
MLP throughput (in TFLOP/s): 78.890
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 32.899
Transformer - MLP - Attention (in seconds): -0.0764
========================================================================================================================
num_attention_heads: 128, hidden_size: 6912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x6912x20736, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x6912x20736, b=2048): 88.367
Elapsed time for attention_key_query_prob (512x2048x54x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x54x2048): 36.950
Elapsed time for attention_prob_times_values (512x2048x2048x54): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x54): 42.039
Elapsed time for attention_linear_projection (4x6912x6912, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x6912x6912, b=2048): 101.945
Elapsed time for mlp_h_to_4h (4x6912x27648, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x6912x27648, b=2048): 102.979
Elapsed time for mlp_4h_to_h (4x27648x6912, b=2048): 0.1049
Throughput (in TFLOP/s) for mlp_4h_to_h (4x27648x6912, b=2048): 29.854

Attention duration (in seconds): 0.0460
Attention throughput (in TFLOP/s): 78.071
MLP duration (in seconds): 0.1353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1813
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 12.447
MLP duration (in seconds): 0.0792
MLP throughput (in TFLOP/s): 79.114
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 34.123
Transformer - MLP - Attention (in seconds): -0.0791
========================================================================================================================
num_attention_heads: 128, hidden_size: 7040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7040x21120, b=2048): 0.0274
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7040x21120, b=2048): 88.857
Elapsed time for attention_key_query_prob (512x2048x55x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x55x2048): 37.512
Elapsed time for attention_prob_times_values (512x2048x2048x55): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x55): 41.639
Elapsed time for attention_linear_projection (4x7040x7040, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x7040x7040, b=2048): 102.917
Elapsed time for mlp_h_to_4h (4x7040x28160, b=2048): 0.0315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7040x28160, b=2048): 103.023
Elapsed time for mlp_4h_to_h (4x28160x7040, b=2048): 0.0506
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28160x7040, b=2048): 64.216

Attention duration (in seconds): 0.0473
Attention throughput (in TFLOP/s): 78.698
MLP duration (in seconds): 0.0821
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1294
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 12.884
MLP duration (in seconds): 0.0816
MLP throughput (in TFLOP/s): 79.646
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 35.363
Transformer - MLP - Attention (in seconds): -0.0814
========================================================================================================================
num_attention_heads: 128, hidden_size: 7168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7168x21504, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7168x21504, b=2048): 88.265
Elapsed time for attention_key_query_prob (512x2048x56x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x56x2048): 38.331
Elapsed time for attention_prob_times_values (512x2048x2048x56): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x56): 44.878
Elapsed time for attention_linear_projection (4x7168x7168, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x7168x7168, b=2048): 102.861
Elapsed time for mlp_h_to_4h (4x7168x28672, b=2048): 0.0328
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7168x28672, b=2048): 102.659
Elapsed time for mlp_4h_to_h (4x28672x7168, b=2048): 0.0518
Throughput (in TFLOP/s) for mlp_4h_to_h (4x28672x7168, b=2048): 64.955

Attention duration (in seconds): 0.0484
Attention throughput (in TFLOP/s): 79.461
MLP duration (in seconds): 0.0846
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1331
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 13.325
MLP duration (in seconds): 0.0846
MLP throughput (in TFLOP/s): 79.574
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 36.649
Transformer - MLP - Attention (in seconds): -0.0847
========================================================================================================================
num_attention_heads: 128, hidden_size: 7296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7296x21888, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7296x21888, b=2048): 88.682
Elapsed time for attention_key_query_prob (512x2048x57x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x57x2048): 37.676
Elapsed time for attention_prob_times_values (512x2048x2048x57): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x57): 43.465
Elapsed time for attention_linear_projection (4x7296x7296, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x7296x7296, b=2048): 102.665
Elapsed time for mlp_h_to_4h (4x7296x29184, b=2048): 0.0339
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7296x29184, b=2048): 102.769
Elapsed time for mlp_4h_to_h (4x29184x7296, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29184x7296, b=2048): 63.297

Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 79.360
MLP duration (in seconds): 0.0891
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1392
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 13.774
MLP duration (in seconds): 0.0889
MLP throughput (in TFLOP/s): 78.479
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 37.917
Transformer - MLP - Attention (in seconds): -0.0888
========================================================================================================================
num_attention_heads: 128, hidden_size: 7424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7424x22272, b=2048): 0.0316
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7424x22272, b=2048): 85.749
Elapsed time for attention_key_query_prob (512x2048x58x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x58x2048): 38.414
Elapsed time for attention_prob_times_values (512x2048x2048x58): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x58): 44.762
Elapsed time for attention_linear_projection (4x7424x7424, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x7424x7424, b=2048): 102.438
Elapsed time for mlp_h_to_4h (4x7424x29696, b=2048): 0.0351
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7424x29696, b=2048): 102.900
Elapsed time for mlp_4h_to_h (4x29696x7424, b=2048): 0.0573
Throughput (in TFLOP/s) for mlp_4h_to_h (4x29696x7424, b=2048): 62.987

Attention duration (in seconds): 0.0525
Attention throughput (in TFLOP/s): 78.354
MLP duration (in seconds): 0.0924
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1449
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 14.235
MLP duration (in seconds): 0.0928
MLP throughput (in TFLOP/s): 77.855
Transformer duration (in seconds): 0.2890
Transformer throughput (in TFLOP/s): 39.215
Transformer - MLP - Attention (in seconds): -0.0925
========================================================================================================================
num_attention_heads: 128, hidden_size: 7552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7552x22656, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7552x22656, b=2048): 85.713
Elapsed time for attention_key_query_prob (512x2048x59x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x59x2048): 38.840
Elapsed time for attention_prob_times_values (512x2048x2048x59): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x59): 44.421
Elapsed time for attention_linear_projection (4x7552x7552, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x7552x7552, b=2048): 102.059
Elapsed time for mlp_h_to_4h (4x7552x30208, b=2048): 0.0363
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7552x30208, b=2048): 103.071
Elapsed time for mlp_4h_to_h (4x30208x7552, b=2048): 0.0599
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30208x7552, b=2048): 62.352

Attention duration (in seconds): 0.0541
Attention throughput (in TFLOP/s): 78.471
MLP duration (in seconds): 0.0962
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 14.697
MLP duration (in seconds): 0.0966
MLP throughput (in TFLOP/s): 77.382
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 40.575
Transformer - MLP - Attention (in seconds): -0.0966
========================================================================================================================
num_attention_heads: 128, hidden_size: 7680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7680x23040, b=2048): 0.0343
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7680x23040, b=2048): 84.514
Elapsed time for attention_key_query_prob (512x2048x60x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x60x2048): 39.553
Elapsed time for attention_prob_times_values (512x2048x2048x60): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x60): 46.427
Elapsed time for attention_linear_projection (4x7680x7680, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x7680x7680, b=2048): 103.055
Elapsed time for mlp_h_to_4h (4x7680x30720, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7680x30720, b=2048): 103.106
Elapsed time for mlp_4h_to_h (4x30720x7680, b=2048): 0.0618
Throughput (in TFLOP/s) for mlp_4h_to_h (4x30720x7680, b=2048): 62.514

Attention duration (in seconds): 0.0557
Attention throughput (in TFLOP/s): 78.585
MLP duration (in seconds): 0.0993
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2887
Attention throughput (in TFLOP/s): 15.172
MLP duration (in seconds): 0.1010
MLP throughput (in TFLOP/s): 76.536
Transformer duration (in seconds): 0.2885
Transformer throughput (in TFLOP/s): 41.982
Transformer - MLP - Attention (in seconds): -0.1012
========================================================================================================================
num_attention_heads: 128, hidden_size: 7808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7808x23424, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7808x23424, b=2048): 87.747
Elapsed time for attention_key_query_prob (512x2048x61x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x61x2048): 39.956
Elapsed time for attention_prob_times_values (512x2048x2048x61): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x61): 45.848
Elapsed time for attention_linear_projection (4x7808x7808, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x7808x7808, b=2048): 102.885
Elapsed time for mlp_h_to_4h (4x7808x31232, b=2048): 0.0388
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7808x31232, b=2048): 102.994
Elapsed time for mlp_4h_to_h (4x31232x7808, b=2048): 0.0645
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31232x7808, b=2048): 61.966

Attention duration (in seconds): 0.0561
Attention throughput (in TFLOP/s): 80.517
MLP duration (in seconds): 0.1033
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1594
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 15.649
MLP duration (in seconds): 0.1032
MLP throughput (in TFLOP/s): 77.453
Transformer duration (in seconds): 0.2890
Transformer throughput (in TFLOP/s): 43.283
Transformer - MLP - Attention (in seconds): -0.1029
========================================================================================================================
num_attention_heads: 128, hidden_size: 7936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x7936x23808, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x7936x23808, b=2048): 83.396
Elapsed time for attention_key_query_prob (512x2048x62x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x62x2048): 40.676
Elapsed time for attention_prob_times_values (512x2048x2048x62): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x62): 47.710
Elapsed time for attention_linear_projection (4x7936x7936, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x7936x7936, b=2048): 102.490
Elapsed time for mlp_h_to_4h (4x7936x31744, b=2048): 0.0401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x7936x31744, b=2048): 103.010
Elapsed time for mlp_4h_to_h (4x31744x7936, b=2048): 0.0682
Throughput (in TFLOP/s) for mlp_4h_to_h (4x31744x7936, b=2048): 60.501

Attention duration (in seconds): 0.0593
Attention throughput (in TFLOP/s): 78.564
MLP duration (in seconds): 0.1083
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1676
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 16.134
MLP duration (in seconds): 0.1082
MLP throughput (in TFLOP/s): 76.321
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 44.724
Transformer - MLP - Attention (in seconds): -0.1082
========================================================================================================================
num_attention_heads: 128, hidden_size: 8064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8064x24192, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8064x24192, b=2048): 84.594
Elapsed time for attention_key_query_prob (512x2048x63x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x63x2048): 41.045
Elapsed time for attention_prob_times_values (512x2048x2048x63): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x63): 47.294
Elapsed time for attention_linear_projection (4x8064x8064, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x8064x8064, b=2048): 102.216
Elapsed time for mlp_h_to_4h (4x8064x32256, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8064x32256, b=2048): 102.769
Elapsed time for mlp_4h_to_h (4x32256x8064, b=2048): 0.0695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32256x8064, b=2048): 61.356

Attention duration (in seconds): 0.0605
Attention throughput (in TFLOP/s): 79.359
MLP duration (in seconds): 0.1109
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1714
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 16.629
MLP duration (in seconds): 0.1110
MLP throughput (in TFLOP/s): 76.809
Transformer duration (in seconds): 0.2889
Transformer throughput (in TFLOP/s): 46.133
Transformer - MLP - Attention (in seconds): -0.1109
========================================================================================================================
num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0391
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 84.332
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 56.870
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 51.161
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 100.729
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0512
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 85.849
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0814
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 54.022

Attention duration (in seconds): 0.0602
Attention throughput (in TFLOP/s): 82.141
MLP duration (in seconds): 0.1326
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1929
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2892
Attention throughput (in TFLOP/s): 17.108
MLP duration (in seconds): 0.1240
MLP throughput (in TFLOP/s): 70.911
Transformer duration (in seconds): 0.2897
Transformer throughput (in TFLOP/s): 47.448
Transformer - MLP - Attention (in seconds): -0.1236
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 87.463
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 43.000
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 33.313
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 101.574
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 101.774
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 81.131

Attention duration (in seconds): 0.0649
Attention throughput (in TFLOP/s): 78.456
MLP duration (in seconds): 0.1005
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1654
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 17.642
MLP duration (in seconds): 0.0925
MLP throughput (in TFLOP/s): 98.072
Transformer duration (in seconds): 0.2892
Transformer throughput (in TFLOP/s): 48.996
Transformer - MLP - Attention (in seconds): -0.0921
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 83.292
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 43.905
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 34.692
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 101.583
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0459
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 101.902
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0684
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 68.391

Attention duration (in seconds): 0.0683
Attention throughput (in TFLOP/s): 76.833
MLP duration (in seconds): 0.1143
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1825
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 18.160
MLP duration (in seconds): 0.1089
MLP throughput (in TFLOP/s): 85.876
Transformer duration (in seconds): 0.2887
Transformer throughput (in TFLOP/s): 50.558
Transformer - MLP - Attention (in seconds): -0.1090
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 85.149
Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 43.518
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 34.169
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 100.966
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0474
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 101.745
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0634
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 76.001

Attention duration (in seconds): 0.0694
Attention throughput (in TFLOP/s): 77.718
MLP duration (in seconds): 0.1108
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 18.683
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 97.939
Transformer duration (in seconds): 0.2890
Transformer throughput (in TFLOP/s): 52.024
Transformer - MLP - Attention (in seconds): -0.0982
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 97.982
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 44.396
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 34.119
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 97.609
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0489
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 101.553
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0698
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 71.110

Attention duration (in seconds): 0.0659
Attention throughput (in TFLOP/s): 84.257
MLP duration (in seconds): 0.1187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1846
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 19.212
MLP duration (in seconds): 0.1055
MLP throughput (in TFLOP/s): 94.143
Transformer duration (in seconds): 0.2903
Transformer throughput (in TFLOP/s): 53.319
Transformer - MLP - Attention (in seconds): -0.1040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0377
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 101.598
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 44.184
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 35.166
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 101.523
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 101.773
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0686
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 74.564

Attention duration (in seconds): 0.0655
Attention throughput (in TFLOP/s): 87.149
MLP duration (in seconds): 0.1188
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1843
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2890
Attention throughput (in TFLOP/s): 19.738
MLP duration (in seconds): 0.1040
MLP throughput (in TFLOP/s): 98.298
Transformer duration (in seconds): 0.2969
Transformer throughput (in TFLOP/s): 53.649
Transformer - MLP - Attention (in seconds): -0.0961
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 101.576
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 45.256
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 36.548
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 101.634
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 101.838
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0729
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 72.142

Attention duration (in seconds): 0.0667
Attention throughput (in TFLOP/s): 87.949
MLP duration (in seconds): 0.1246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1913
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 20.303
MLP duration (in seconds): 0.1228
MLP throughput (in TFLOP/s): 85.688
Transformer duration (in seconds): 0.2942
Transformer throughput (in TFLOP/s): 55.704
Transformer - MLP - Attention (in seconds): -0.1174
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 101.718
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 44.935
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 36.109
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 101.416
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 101.603
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0735
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 73.608

Attention duration (in seconds): 0.0685
Attention throughput (in TFLOP/s): 87.942
MLP duration (in seconds): 0.1268
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1953
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 20.854
MLP duration (in seconds): 0.1245
MLP throughput (in TFLOP/s): 86.958
Transformer duration (in seconds): 0.2981
Transformer throughput (in TFLOP/s): 56.515
Transformer - MLP - Attention (in seconds): -0.1152
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 101.824
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 46.192
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 36.067
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 101.162
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 101.693
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 72.603

Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 88.324
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 21.411
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 97.797
Transformer duration (in seconds): 0.2891
Transformer throughput (in TFLOP/s): 59.903
Transformer - MLP - Attention (in seconds): -0.1136
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 101.754
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 44.859
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 36.832
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 101.492
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 102.287
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0777
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 73.617

Attention duration (in seconds): 0.0718
Attention throughput (in TFLOP/s): 88.462
MLP duration (in seconds): 0.1337
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 102.213
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 45.851
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 38.401
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 102.202
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 102.209
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0827
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 71.073

Attention duration (in seconds): 0.0727
Attention throughput (in TFLOP/s): 89.578
MLP duration (in seconds): 0.1403
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 102.150
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 45.699
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 37.714
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 102.135
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 102.136
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 69.445

Attention duration (in seconds): 0.0747
Attention throughput (in TFLOP/s): 89.456
MLP duration (in seconds): 0.1461
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0459
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 101.252
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 46.730
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 39.460
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 101.698
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0607
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 102.176
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0853
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 72.670

Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 89.672
MLP duration (in seconds): 0.1460
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 102.022
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 46.524
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 38.489
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 101.466
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 100.472
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 72.291

Attention duration (in seconds): 0.0782
Attention throughput (in TFLOP/s): 89.883
MLP duration (in seconds): 0.1514
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 101.852
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 47.604
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 40.146
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 102.168
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0639
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 102.215
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 73.764

Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 90.632
MLP duration (in seconds): 0.1525
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 101.303
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 47.512
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 39.386
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 101.751
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0657
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 102.028
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0924
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 72.526

Attention duration (in seconds): 0.0818
Attention throughput (in TFLOP/s): 90.181
MLP duration (in seconds): 0.1581
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2399
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 102.181
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 49.285
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 39.652
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 102.148
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0684
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 100.434
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0980
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 70.155

Attention duration (in seconds): 0.0829
Attention throughput (in TFLOP/s): 91.190
MLP duration (in seconds): 0.1664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2493
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 101.709
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 47.371
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 40.238
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 101.691
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 102.139
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0975
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 72.243

Attention duration (in seconds): 0.0853
Attention throughput (in TFLOP/s): 90.791
MLP duration (in seconds): 0.1665
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2517
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 101.947
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 48.245
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 41.903
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 101.587
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0706
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 102.308
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.1010
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 71.466

Attention duration (in seconds): 0.0866
Attention throughput (in TFLOP/s): 91.517
MLP duration (in seconds): 0.1716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 102.173
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 45.653
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 40.986
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 102.260
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 100.973
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 72.320

Attention duration (in seconds): 0.0889
Attention throughput (in TFLOP/s): 91.238
MLP duration (in seconds): 0.1755
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2644
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
[2023-11-22 14:50:03,989] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0550
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 100.807
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 48.299
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 41.054
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 102.673
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0721
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 102.611
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1020
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 72.525

Attention duration (in seconds): 0.0891
Attention throughput (in TFLOP/s): 91.012
MLP duration (in seconds): 0.1741
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2632
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 99.667
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 48.820
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 43.000
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 99.841
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0739
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 102.497
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.1031
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 73.518

Attention duration (in seconds): 0.0918
Attention throughput (in TFLOP/s): 90.427
MLP duration (in seconds): 0.1770
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 100.488
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 48.954
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 42.055
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 102.751
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 100.986
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0900
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 86.240

Attention duration (in seconds): 0.0929
Attention throughput (in TFLOP/s): 91.352
MLP duration (in seconds): 0.1668
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2597
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 102.511
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 49.598
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 43.798
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 102.248
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0774
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 102.578
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.1073
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 74.003

Attention duration (in seconds): 0.0934
Attention throughput (in TFLOP/s): 92.936
MLP duration (in seconds): 0.1847
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2781
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 102.492
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 49.838
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 42.820
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 101.791
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0792
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 102.557
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.1113
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 73.051

Attention duration (in seconds): 0.0957
Attention throughput (in TFLOP/s): 92.775
MLP duration (in seconds): 0.1905
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2862
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0611
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 102.064
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 50.694
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 42.950
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 101.653
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0811
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 102.476
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.1191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 69.794

Attention duration (in seconds): 0.0978
Attention throughput (in TFLOP/s): 92.744
MLP duration (in seconds): 0.2003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2981
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0622
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 102.497
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 48.863
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 43.745
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 102.462
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0831
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 102.407
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.1175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 72.393

Attention duration (in seconds): 0.0995
Attention throughput (in TFLOP/s): 93.117
MLP duration (in seconds): 0.2005
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3001
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0638
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 102.262
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 49.596
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 45.477
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 102.240
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0848
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 102.538
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.1212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 71.756

Attention duration (in seconds): 0.1013
Attention throughput (in TFLOP/s): 93.445
MLP duration (in seconds): 0.2060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0651
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 102.455
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 49.604
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 44.447
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 102.043
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0866
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 102.668
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.3526
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 25.220

Attention duration (in seconds): 0.1035
Attention throughput (in TFLOP/s): 93.420
MLP duration (in seconds): 0.4392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5427
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0663
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 102.805
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 50.482
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 46.553
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 102.848
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 101.813
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.1504
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 60.414

Attention duration (in seconds): 0.1047
Attention throughput (in TFLOP/s): 94.343
MLP duration (in seconds): 0.2397
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3444
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0679
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 102.508
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 50.395
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 45.324
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 102.792
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.1015
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 91.461
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.1649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 56.321

Attention duration (in seconds): 0.1073
Attention throughput (in TFLOP/s): 94.019
MLP duration (in seconds): 0.2664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3737
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0720
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 98.897
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 51.210
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 47.207
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 102.604
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.1146
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 82.799
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.1669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 56.850

Attention duration (in seconds): 0.1115
Attention throughput (in TFLOP/s): 92.329
MLP duration (in seconds): 0.2815
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0725
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 100.314
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 51.167
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 46.068
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 102.417
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.1075
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 90.124
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.1711
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 56.645

Attention duration (in seconds): 0.1129
Attention throughput (in TFLOP/s): 93.030
MLP duration (in seconds): 0.2786
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3915
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0724
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 102.512
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 66.166
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 47.805
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 102.434
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.1328
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 74.526
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.2092
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 47.306

Attention duration (in seconds): 0.1114
Attention throughput (in TFLOP/s): 96.226
MLP duration (in seconds): 0.3420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4534
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0894
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 84.747
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 51.856
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 46.989
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 102.902
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.1184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 85.325
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.1879
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 53.776

Attention duration (in seconds): 0.1309
Attention throughput (in TFLOP/s): 83.575
MLP duration (in seconds): 0.3063
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0796
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 97.125
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 52.884
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 48.874
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 102.781
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.1294
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 79.667
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.2036
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 50.638

Attention duration (in seconds): 0.1213
Attention throughput (in TFLOP/s): 91.965
MLP duration (in seconds): 0.3331
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4544
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0931
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 84.801
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 52.443
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 47.963
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 102.601
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.1241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 84.789
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.2112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 49.825

Attention duration (in seconds): 0.1357
Attention throughput (in TFLOP/s): 83.824
MLP duration (in seconds): 0.3353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4710
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0911
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 88.400
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 53.737
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 49.802
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 102.450
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.1369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 78.428
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 56.680

Attention duration (in seconds): 0.1339
Attention throughput (in TFLOP/s): 86.594
MLP duration (in seconds): 0.3263
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4603
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.1009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 81.456
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 53.031
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 48.736
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 100.613
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.1418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 77.239
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.2119
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 51.702

Attention duration (in seconds): 0.1451
Attention throughput (in TFLOP/s): 81.440
MLP duration (in seconds): 0.3537
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4988
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 92.726
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 54.038
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 50.590
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 103.036
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.1302
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 85.800
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.1938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 57.658

Attention duration (in seconds): 0.1342
Attention throughput (in TFLOP/s): 89.753
MLP duration (in seconds): 0.3240
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.1011
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 84.533
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 53.602
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 49.569
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 102.799
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.1398
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 81.469
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.1693
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 67.285

Attention duration (in seconds): 0.1459
Attention throughput (in TFLOP/s): 84.113
MLP duration (in seconds): 0.3091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.1023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 85.153
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 55.121
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 50.108
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 102.593
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.1429
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 81.268
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.1680
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 69.134

Attention duration (in seconds): 0.1476
Attention throughput (in TFLOP/s): 84.732
MLP duration (in seconds): 0.3109
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.1074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 82.665
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 53.439
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 50.104
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 102.295
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.1511
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 78.330
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.1806
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 65.536

Attention duration (in seconds): 0.1538
Attention throughput (in TFLOP/s): 82.848
MLP duration (in seconds): 0.3318
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4855
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.1152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 78.557
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 54.429
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 52.157
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0293
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 102.892
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.1636
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 73.729
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.1857
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 64.954

Attention duration (in seconds): 0.1616
Attention throughput (in TFLOP/s): 80.296
MLP duration (in seconds): 0.3494
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.1154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 79.906
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 54.076
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 50.662
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 102.954
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.1603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 76.677
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.1788
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 68.759

Attention duration (in seconds): 0.1628
Attention throughput (in TFLOP/s): 81.154
MLP duration (in seconds): 0.3391
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.1139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 82.490
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 55.418
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 52.936
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0304
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 102.853
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.1661
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 75.395
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.1907
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 65.674

Attention duration (in seconds): 0.1614
Attention throughput (in TFLOP/s): 83.322
MLP duration (in seconds): 0.3568
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.1265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 75.629
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 54.766
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 51.508
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 102.663
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.1727
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 73.857
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.1977
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 64.524

Attention duration (in seconds): 0.1752
Attention throughput (in TFLOP/s): 78.153
MLP duration (in seconds): 0.3704
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5457
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.1229
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 79.306
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 55.945
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 53.649
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 103.291
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.1720
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 75.524
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.1823
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 71.256

Attention duration (in seconds): 0.1716
Attention throughput (in TFLOP/s): 81.235
MLP duration (in seconds): 0.3544
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.1305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 76.011
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 55.490
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 52.191
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 102.998
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.1753
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 75.450
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.1991
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 66.450

Attention duration (in seconds): 0.1804
Attention throughput (in TFLOP/s): 78.632
MLP duration (in seconds): 0.3744
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5548
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.1315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 76.800
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 57.551
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 54.139
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 102.994
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.1851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 72.759
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.2273
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 59.245

Attention duration (in seconds): 0.1815
Attention throughput (in TFLOP/s): 79.523
MLP duration (in seconds): 0.4125
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.1366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 75.278
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 55.193
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 52.392
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 102.533
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.1858
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 73.798
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.2759
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 49.689

Attention duration (in seconds): 0.1881
Attention throughput (in TFLOP/s): 78.056
MLP duration (in seconds): 0.4617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6498
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.1380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 75.834
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 56.476
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 55.128
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 101.552
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.1918
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 72.737
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.2572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 54.259

Attention duration (in seconds): 0.1899
Attention throughput (in TFLOP/s): 78.634
MLP duration (in seconds): 0.4490
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.1382
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 77.045
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 55.933
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 53.476
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 103.137
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.1939
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 73.251
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.2841
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 49.981

Attention duration (in seconds): 0.1907
Attention throughput (in TFLOP/s): 79.635
MLP duration (in seconds): 0.4780
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.1434
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 75.578
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 57.217
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 55.771
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 103.203
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.1987
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 72.698
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.3005
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 48.084

Attention duration (in seconds): 0.1960
Attention throughput (in TFLOP/s): 78.793
MLP duration (in seconds): 0.4992
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6952
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.1481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 74.426
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 56.588
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 54.297
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 103.053
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.2014
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 72.986
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.3086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 47.635

Attention duration (in seconds): 0.2019
Attention throughput (in TFLOP/s): 77.774
MLP duration (in seconds): 0.5100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.1485
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 75.526
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 57.641
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 56.530
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 101.689
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.2049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 72.972
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.3284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 45.526

Attention duration (in seconds): 0.2030
Attention throughput (in TFLOP/s): 78.650
MLP duration (in seconds): 0.5333
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7363
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.1581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 72.138
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 57.270
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 54.804
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 102.563
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.2099
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 72.448
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.3196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 47.576

Attention duration (in seconds): 0.2134
Attention throughput (in TFLOP/s): 76.043
MLP duration (in seconds): 0.5295
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7429
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.1621
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 71.536
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 58.961
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 56.728
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 103.298
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.2213
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 69.879
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.3144
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 49.176

Attention duration (in seconds): 0.2174
Attention throughput (in TFLOP/s): 75.880
MLP duration (in seconds): 0.5357
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7530
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.1606
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 73.411
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 56.498
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 43.503
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 99.950
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.2180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 72.110
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.3333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 47.161

Attention duration (in seconds): 0.2211
Attention throughput (in TFLOP/s): 75.812
MLP duration (in seconds): 0.5513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7724
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.1571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 76.309
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 57.731
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 57.910
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 102.917
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.2253
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 70.925
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.3589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 44.532

Attention duration (in seconds): 0.2140
Attention throughput (in TFLOP/s): 79.570
MLP duration (in seconds): 0.5842
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7982
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.1676
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 72.687
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 56.707
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 44.173
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 102.788
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.3463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 46.903
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.3528
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 46.047

Attention duration (in seconds): 0.2284
Attention throughput (in TFLOP/s): 75.749
MLP duration (in seconds): 0.6991
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.1729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 71.620
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 58.809
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 58.494
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 103.269
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.2409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 68.521
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.3768
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 43.814

Attention duration (in seconds): 0.2310
Attention throughput (in TFLOP/s): 76.076
MLP duration (in seconds): 0.6178
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8488
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.1728
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 72.798
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 57.634
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 43.751
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 103.077
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.2355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 71.256
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.3684
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 45.538

Attention duration (in seconds): 0.2351
Attention throughput (in TFLOP/s): 75.922
MLP duration (in seconds): 0.6039
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8390
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.1786
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 71.569
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 58.135
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 59.346
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 99.970
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.2452
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 69.530
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.3847
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 44.309

Attention duration (in seconds): 0.2397
Attention throughput (in TFLOP/s): 75.633
MLP duration (in seconds): 0.6299
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8696
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.1753
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 74.094
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 57.825
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 43.881
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 103.043
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.2439
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 70.993
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.3864
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 44.819

Attention duration (in seconds): 0.2392
Attention throughput (in TFLOP/s): 76.967
MLP duration (in seconds): 0.6303
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8695
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.2030
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 65.012
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 68.975
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 62.770
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0441
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 99.768
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.2746
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 64.072
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.6499
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 27.070

Attention duration (in seconds): 0.2638
Attention throughput (in TFLOP/s): 70.866
MLP duration (in seconds): 0.9244
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1882
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3195
Attention throughput (in TFLOP/s): 58.497
MLP duration (in seconds): 0.7622
MLP throughput (in TFLOP/s): 46.159
Transformer duration (in seconds): 1.1701
Transformer throughput (in TFLOP/s): 46.045
Transformer - MLP - Attention (in seconds): 0.0883
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.1844
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 72.687
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 56.853
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 44.147
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0434
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 102.982
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.2501
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 71.435
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.3998
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 44.692

Attention duration (in seconds): 0.2500
Attention throughput (in TFLOP/s): 75.893
MLP duration (in seconds): 0.6499
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3194
Attention throughput (in TFLOP/s): 59.408
MLP duration (in seconds): 0.6208
MLP throughput (in TFLOP/s): 57.567
Transformer duration (in seconds): 0.9373
Transformer throughput (in TFLOP/s): 58.373
Transformer - MLP - Attention (in seconds): -0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.1925
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 70.690
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 58.038
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 46.668
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 102.494
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.2632
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 68.936
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.4149
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 43.737

Attention duration (in seconds): 0.2584
Attention throughput (in TFLOP/s): 74.555
MLP duration (in seconds): 0.6781
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9365
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3151
Attention throughput (in TFLOP/s): 61.138
MLP duration (in seconds): 0.6483
MLP throughput (in TFLOP/s): 55.979
Transformer duration (in seconds): 0.9551
Transformer throughput (in TFLOP/s): 58.169
Transformer - MLP - Attention (in seconds): -0.0083
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.1938
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 71.326
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 57.736
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 45.107
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0449
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 102.515
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.2609
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 70.624
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.4017
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 45.873

Attention duration (in seconds): 0.2609
Attention throughput (in TFLOP/s): 74.937
MLP duration (in seconds): 0.6626
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9235
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3316
Attention throughput (in TFLOP/s): 58.954
MLP duration (in seconds): 0.6402
MLP throughput (in TFLOP/s): 57.561
Transformer duration (in seconds): 0.9587
Transformer throughput (in TFLOP/s): 58.832
Transformer - MLP - Attention (in seconds): -0.0131
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.2013
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 69.710
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 59.245
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 47.558
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0456
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 102.643
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.2742
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 68.227
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.4352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 42.986

Attention duration (in seconds): 0.2683
Attention throughput (in TFLOP/s): 73.945
MLP duration (in seconds): 0.7094
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9778
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3238
Attention throughput (in TFLOP/s): 61.286
MLP duration (in seconds): 0.7014
MLP throughput (in TFLOP/s): 53.349
Transformer duration (in seconds): 1.0062
Transformer throughput (in TFLOP/s): 56.906
Transformer - MLP - Attention (in seconds): -0.0189
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.2014
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 70.735
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 57.962
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 45.922
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0465
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 102.155
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.2757
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 68.894
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.4116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 46.146

Attention duration (in seconds): 0.2702
Attention throughput (in TFLOP/s): 74.532
MLP duration (in seconds): 0.6873
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9574
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3427
Attention throughput (in TFLOP/s): 58.758
MLP duration (in seconds): 0.6674
MLP throughput (in TFLOP/s): 56.921
Transformer duration (in seconds): 0.9980
Transformer throughput (in TFLOP/s): 58.240
Transformer - MLP - Attention (in seconds): -0.0121
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.2034
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 71.083
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 59.475
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 48.087
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 102.962
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.2819
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 68.404
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.4379
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 44.026

Attention duration (in seconds): 0.2719
Attention throughput (in TFLOP/s): 75.146
MLP duration (in seconds): 0.7198
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9917
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3328
Attention throughput (in TFLOP/s): 61.396
MLP duration (in seconds): 0.6951
MLP throughput (in TFLOP/s): 55.475
Transformer duration (in seconds): 1.0042
Transformer throughput (in TFLOP/s): 58.742
Transformer - MLP - Attention (in seconds): -0.0236
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.2110
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 69.571
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 58.933
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 46.627
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 102.862
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.2873
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 68.103
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.4343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 45.059

Attention duration (in seconds): 0.2808
Attention throughput (in TFLOP/s): 73.821
MLP duration (in seconds): 0.7216
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0024
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3534
Attention throughput (in TFLOP/s): 58.648
MLP duration (in seconds): 0.7006
MLP throughput (in TFLOP/s): 55.866
Transformer duration (in seconds): 1.0091
Transformer throughput (in TFLOP/s): 59.325
Transformer - MLP - Attention (in seconds): -0.0449
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.2171
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 68.617
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 61.005
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 45.737
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 72.086
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.2586
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 76.797
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.4614
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 43.039

Attention duration (in seconds): 0.3083
Attention throughput (in TFLOP/s): 68.208
MLP duration (in seconds): 0.7200
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0283
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3543
Attention throughput (in TFLOP/s): 59.350
MLP duration (in seconds): 0.6913
MLP throughput (in TFLOP/s): 57.458
Transformer duration (in seconds): 1.0565
Transformer throughput (in TFLOP/s): 57.498
Transformer - MLP - Attention (in seconds): 0.0109
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.2189
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 69.054
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 59.103
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 46.591
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0492
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 102.482
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.2616
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 77.026
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.4489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 44.898

Attention duration (in seconds): 0.2906
Attention throughput (in TFLOP/s): 73.392
MLP duration (in seconds): 0.7105
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3528
Attention throughput (in TFLOP/s): 60.457
MLP duration (in seconds): 0.6777
MLP throughput (in TFLOP/s): 59.473
Transformer duration (in seconds): 1.0765
Transformer throughput (in TFLOP/s): 57.254
Transformer - MLP - Attention (in seconds): 0.0460
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.2213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 69.295
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 60.145
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 49.263
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 102.947
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.2633
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 77.674
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.4789
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 42.697

Attention duration (in seconds): 0.2929
Attention throughput (in TFLOP/s): 73.870
MLP duration (in seconds): 0.7422
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0350
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3417
Attention throughput (in TFLOP/s): 63.307
MLP duration (in seconds): 0.6911
MLP throughput (in TFLOP/s): 59.179
Transformer duration (in seconds): 1.0993
Transformer throughput (in TFLOP/s): 56.882
Transformer - MLP - Attention (in seconds): 0.0665
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.2235
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 69.622
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 59.582
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 47.296
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 102.911
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.2722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 76.217
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.4598
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 45.120

Attention duration (in seconds): 0.2965
Attention throughput (in TFLOP/s): 73.990
MLP duration (in seconds): 0.7320
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0285
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3668
Attention throughput (in TFLOP/s): 59.810
MLP duration (in seconds): 0.6801
MLP throughput (in TFLOP/s): 61.010
Transformer duration (in seconds): 1.1039
Transformer throughput (in TFLOP/s): 57.459
Transformer - MLP - Attention (in seconds): 0.0570
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.2286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 69.034
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 60.987
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 50.169
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0512
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 102.860
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.2735
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 76.937
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.4702
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 44.760

Attention duration (in seconds): 0.3016
Attention throughput (in TFLOP/s): 73.757
MLP duration (in seconds): 0.7437
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0454
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3529
Attention throughput (in TFLOP/s): 63.049
MLP duration (in seconds): 0.6887
MLP throughput (in TFLOP/s): 61.119
Transformer duration (in seconds): 1.0720
Transformer throughput (in TFLOP/s): 60.018
Transformer - MLP - Attention (in seconds): 0.0305
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.2368
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 67.605
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 61.441
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 49.353
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 106.203
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.3154
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 67.678
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.4725
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 45.182

Attention duration (in seconds): 0.3092
Attention throughput (in TFLOP/s): 72.957
MLP duration (in seconds): 0.7879
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0971
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3917
Attention throughput (in TFLOP/s): 57.587
MLP duration (in seconds): 0.7667
MLP throughput (in TFLOP/s): 55.689
Transformer duration (in seconds): 1.1601
Transformer throughput (in TFLOP/s): 56.245
Transformer - MLP - Attention (in seconds): 0.0018
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.2381
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 68.188
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 62.414
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 51.782
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0511
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 105.835
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.3237
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 66.886
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.4652
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 46.545

Attention duration (in seconds): 0.3108
Attention throughput (in TFLOP/s): 73.579
MLP duration (in seconds): 0.7889
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.0997
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3804
Attention throughput (in TFLOP/s): 60.128
MLP duration (in seconds): 0.7580
MLP throughput (in TFLOP/s): 57.127
Transformer duration (in seconds): 1.1262
Transformer throughput (in TFLOP/s): 58.756
Transformer - MLP - Attention (in seconds): -0.0121
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.2424
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 67.931
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 61.841
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 49.830
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0520
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 105.657
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.3282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 66.906
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.4911
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 44.713

Attention duration (in seconds): 0.3166
Attention throughput (in TFLOP/s): 73.225
MLP duration (in seconds): 0.8192
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1359
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3986
Attention throughput (in TFLOP/s): 58.167
MLP duration (in seconds): 0.8052
MLP throughput (in TFLOP/s): 54.538
Transformer duration (in seconds): 1.1837
Transformer throughput (in TFLOP/s): 56.688
Transformer - MLP - Attention (in seconds): -0.0201
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.2427
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 68.817
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 64.194
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 50.176
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0523
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 106.441
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.3324
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 66.974
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.4923
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 45.226

Attention duration (in seconds): 0.3169
Attention throughput (in TFLOP/s): 74.160
MLP duration (in seconds): 0.8248
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1417
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3944
Attention throughput (in TFLOP/s): 59.594
MLP duration (in seconds): 0.8101
MLP throughput (in TFLOP/s): 54.967
Transformer duration (in seconds): 1.1899
Transformer throughput (in TFLOP/s): 57.174
Transformer - MLP - Attention (in seconds): -0.0146
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.2488
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 68.055
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 61.596
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 50.519
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0532
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 106.030
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.3400
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 66.397
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.5089
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 44.366

Attention duration (in seconds): 0.3245
Attention throughput (in TFLOP/s): 73.418
MLP duration (in seconds): 0.8489
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1733
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4073
Attention throughput (in TFLOP/s): 58.486
MLP duration (in seconds): 0.8310
MLP throughput (in TFLOP/s): 54.332
Transformer duration (in seconds): 1.2250
Transformer throughput (in TFLOP/s): 56.304
Transformer - MLP - Attention (in seconds): -0.0133
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.2498
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 68.724
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 62.887
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 53.064
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 106.132
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.3414
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 67.041
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.5108
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 44.804

Attention duration (in seconds): 0.3255
Attention throughput (in TFLOP/s): 74.173
MLP duration (in seconds): 0.8522
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.1777
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4064
Attention throughput (in TFLOP/s): 59.397
MLP duration (in seconds): 0.8383
MLP throughput (in TFLOP/s): 54.607
Transformer duration (in seconds): 1.2362
Transformer throughput (in TFLOP/s): 56.561
Transformer - MLP - Attention (in seconds): -0.0086
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.2610
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 66.664
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 62.225
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 51.087
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 105.645
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.3534
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 65.650
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.5605
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 41.398

Attention duration (in seconds): 0.3384
Attention throughput (in TFLOP/s): 72.286
MLP duration (in seconds): 0.9139
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2524
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4246
Attention throughput (in TFLOP/s): 57.626
MLP duration (in seconds): 0.8990
MLP throughput (in TFLOP/s): 51.620
Transformer duration (in seconds): 1.2877
Transformer throughput (in TFLOP/s): 55.036
Transformer - MLP - Attention (in seconds): -0.0358
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.2594
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 68.008
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 63.556
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 53.784
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0552
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 106.422
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.3504
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 67.117
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.5434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 43.280

Attention duration (in seconds): 0.3364
Attention throughput (in TFLOP/s): 73.684
MLP duration (in seconds): 0.8938
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2303
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4181
Attention throughput (in TFLOP/s): 59.296
MLP duration (in seconds): 0.8856
MLP throughput (in TFLOP/s): 53.116
Transformer duration (in seconds): 1.2680
Transformer throughput (in TFLOP/s): 56.646
Transformer - MLP - Attention (in seconds): -0.0356
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.2704
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 66.118
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 62.818
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 51.709
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0560
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 106.401
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.3608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 66.067
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.5664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 42.088

Attention duration (in seconds): 0.3490
Attention throughput (in TFLOP/s): 71.976
MLP duration (in seconds): 0.9272
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2762
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4319
Attention throughput (in TFLOP/s): 58.154
MLP duration (in seconds): 0.9131
MLP throughput (in TFLOP/s): 52.216
Transformer duration (in seconds): 1.3197
Transformer throughput (in TFLOP/s): 55.158
Transformer - MLP - Attention (in seconds): -0.0252
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.2687
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 67.423
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 62.734
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 53.133
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0591
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 102.269
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.3626
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 66.627
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.5703
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 42.365

Attention duration (in seconds): 0.3502
Attention throughput (in TFLOP/s): 72.667
MLP duration (in seconds): 0.9329
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2831
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4321
Attention throughput (in TFLOP/s): 58.888
MLP duration (in seconds): 0.8717
MLP throughput (in TFLOP/s): 55.433
Transformer duration (in seconds): 1.2704
Transformer throughput (in TFLOP/s): 58.066
Transformer - MLP - Attention (in seconds): -0.0334
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.2740
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 67.009
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 62.386
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 51.093
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0596
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 102.685
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.3243
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 75.492
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.5785
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 42.320

Attention duration (in seconds): 0.3567
Attention throughput (in TFLOP/s): 72.269
MLP duration (in seconds): 0.9028
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2595
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4111
Attention throughput (in TFLOP/s): 62.710
MLP duration (in seconds): 0.9404
MLP throughput (in TFLOP/s): 52.067
Transformer duration (in seconds): 1.3818
Transformer throughput (in TFLOP/s): 54.092
Transformer - MLP - Attention (in seconds): 0.0303
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.2751
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 67.633
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 64.330
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 51.420
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0602
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 103.052
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.3286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 75.499
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.5502
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 45.089

Attention duration (in seconds): 0.3581
Attention throughput (in TFLOP/s): 72.916
MLP duration (in seconds): 0.8788
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2369
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.3935
Attention throughput (in TFLOP/s): 66.366
MLP duration (in seconds): 0.9639
MLP throughput (in TFLOP/s): 51.476
Transformer duration (in seconds): 1.3698
Transformer throughput (in TFLOP/s): 55.283
Transformer - MLP - Attention (in seconds): 0.0125
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.2862
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 65.875
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 61.966
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 51.695
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 103.096
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.3364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 74.720
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.5529
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 45.459

Attention duration (in seconds): 0.3704
Attention throughput (in TFLOP/s): 71.401
MLP duration (in seconds): 0.8893
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2597
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4199
Attention throughput (in TFLOP/s): 62.985
MLP duration (in seconds): 0.9827
MLP throughput (in TFLOP/s): 51.155
Transformer duration (in seconds): 1.4298
Transformer throughput (in TFLOP/s): 53.656
Transformer - MLP - Attention (in seconds): 0.0272
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.2858
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 66.834
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 63.083
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 54.276
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0618
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 103.010
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.3414
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 74.588
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.5726
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 44.472

Attention duration (in seconds): 0.3702
Attention throughput (in TFLOP/s): 72.354
MLP duration (in seconds): 0.9140
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.2843
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4114
Attention throughput (in TFLOP/s): 65.106
MLP duration (in seconds): 1.0035
MLP throughput (in TFLOP/s): 50.753
Transformer duration (in seconds): 1.4130
Transformer throughput (in TFLOP/s): 55.002
Transformer - MLP - Attention (in seconds): -0.0019
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.2923
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 66.184
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 62.613
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 52.248
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0626
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 102.942
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.3502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 73.666
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.5831
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 44.244

Attention duration (in seconds): 0.3783
Attention throughput (in TFLOP/s): 71.701
MLP duration (in seconds): 0.9332
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3116
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4270
Attention throughput (in TFLOP/s): 63.526
MLP duration (in seconds): 1.0208
MLP throughput (in TFLOP/s): 50.542
Transformer duration (in seconds): 1.4435
Transformer throughput (in TFLOP/s): 54.537
Transformer - MLP - Attention (in seconds): -0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.3166
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 61.898
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 63.906
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0121
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 55.227
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0636
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 102.758
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.3635
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 71.893
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.6058
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 43.135

Attention duration (in seconds): 0.4028
Attention throughput (in TFLOP/s): 68.198
MLP duration (in seconds): 0.9693
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3721
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4533
Attention throughput (in TFLOP/s): 60.603
MLP duration (in seconds): 0.9480
MLP throughput (in TFLOP/s): 55.131
Transformer duration (in seconds): 1.4676
Transformer throughput (in TFLOP/s): 54.328
Transformer - MLP - Attention (in seconds): 0.0664
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.3006
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 66.045
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 63.252
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 52.967
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0644
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 102.680
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.3761
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 70.366
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.6134
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 43.148

Attention duration (in seconds): 0.3884
Attention throughput (in TFLOP/s): 71.618
MLP duration (in seconds): 0.9895
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3779
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4663
Attention throughput (in TFLOP/s): 59.646
MLP duration (in seconds): 0.9716
MLP throughput (in TFLOP/s): 54.480
Transformer duration (in seconds): 1.5090
Transformer throughput (in TFLOP/s): 53.512
Transformer - MLP - Attention (in seconds): 0.0710
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.3004
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 66.914
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 64.404
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0122
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 55.580
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0650
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 103.057
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.3782
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 70.876
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.6185
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 43.336

Attention duration (in seconds): 0.3882
Attention throughput (in TFLOP/s): 72.544
MLP duration (in seconds): 0.9967
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3849
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.4646
Attention throughput (in TFLOP/s): 60.613
MLP duration (in seconds): 0.9817
MLP throughput (in TFLOP/s): 54.608
Transformer duration (in seconds): 1.4728
Transformer throughput (in TFLOP/s): 55.520
Transformer - MLP - Attention (in seconds): 0.0265
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.3049
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 66.766
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 62.562
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 52.649
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0678
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 100.043
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.4120
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 65.886
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.6696
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 40.539

Attention duration (in seconds): 0.3966
Attention throughput (in TFLOP/s): 71.880
MLP duration (in seconds): 1.0816
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.4783
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.3012
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 68.443
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 75.187
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 54.681
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0687
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 100.050
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.4191
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 65.583
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.5886
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 46.702

Attention duration (in seconds): 0.3916
Attention throughput (in TFLOP/s): 73.703
MLP duration (in seconds): 1.0077
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.3993
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.3144
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 66.390
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 57.578
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 53.366
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0695
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 100.137
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.4281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 65.010
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.6904
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 40.311

Attention duration (in seconds): 0.4089
Attention throughput (in TFLOP/s): 71.454
MLP duration (in seconds): 1.1186
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.3223
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 65.567
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 62.663
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0135
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 51.642
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0704
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 100.012
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.4355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 64.710
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.7153
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 39.395

Attention duration (in seconds): 0.4174
Attention throughput (in TFLOP/s): 70.854
MLP duration (in seconds): 1.1508
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5681
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.3223
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 66.386
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 61.850
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 54.034
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0712
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 100.138
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.4328
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 65.921
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.7236
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 39.425

Attention duration (in seconds): 0.4178
Attention throughput (in TFLOP/s): 71.634
MLP duration (in seconds): 1.1564
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5742
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.3226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 67.141
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 63.470
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 56.320
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0723
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 99.918
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.4406
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 65.539
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.7371
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 39.178

Attention duration (in seconds): 0.4185
Attention throughput (in TFLOP/s): 72.380
MLP duration (in seconds): 1.1778
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5962
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.3317
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 66.107
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 62.348
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 54.108
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0735
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 99.446
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.4486
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 65.164
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.7486
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 39.050

Attention duration (in seconds): 0.4296
Attention throughput (in TFLOP/s): 71.345
MLP duration (in seconds): 1.1972
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.6268
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.3455
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 64.233
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 63.458
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 56.545
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0739
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 100.081
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.4602
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 64.298
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.7611
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 38.875

Attention duration (in seconds): 0.4432
Attention throughput (in TFLOP/s): 69.973
MLP duration (in seconds): 1.2213
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.6645
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.3378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 66.490
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 62.941
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0130
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 55.216
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0749
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 99.980
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.4562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 65.645
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.7608
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 39.360

Attention duration (in seconds): 0.4370
Attention throughput (in TFLOP/s): 71.800
MLP duration (in seconds): 1.2170
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.6540
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.3467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 65.561
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 61.329
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 55.176
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.1016
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 74.579
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.4662
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 65.003
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.7805
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 38.829

Attention duration (in seconds): 0.4731
Attention throughput (in TFLOP/s): 67.105
MLP duration (in seconds): 1.2467
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7198
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.3458
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 66.521
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 62.444
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 55.482
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.1038
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 73.837
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.4733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 64.798
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.8132
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 37.714

Attention duration (in seconds): 0.4743
Attention throughput (in TFLOP/s): 67.718
MLP duration (in seconds): 1.2864
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7607
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.3003
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 77.488
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 63.882
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 57.792
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0840
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 92.407
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.4234
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 73.287
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.7495
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 41.401

Attention duration (in seconds): 0.4084
Attention throughput (in TFLOP/s): 79.565
MLP duration (in seconds): 1.1729
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.5813
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.3542
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 66.482
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 63.157
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 55.986
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.1032
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 76.048
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.4785
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 65.623
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.8245
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 38.078

Attention duration (in seconds): 0.4822
Attention throughput (in TFLOP/s): 68.164
MLP duration (in seconds): 1.3030
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.7852
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.3549
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 67.121
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 64.581
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 58.600
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.1060
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 74.887
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.4812
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 66.012
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.8344
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 38.069

Attention duration (in seconds): 0.4850
Attention throughput (in TFLOP/s): 68.537
MLP duration (in seconds): 1.3156
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.8007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.3676
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 65.563
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 63.514
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 56.648
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.1093
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 73.479
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.4948
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 64.946
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.8573
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 37.484

Attention duration (in seconds): 0.5018
Attention throughput (in TFLOP/s): 67.007
MLP duration (in seconds): 1.3521
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.8539
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.3745
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 65.106
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 64.829
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0126
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 59.083
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.1108
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 73.348
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.5042
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 64.472
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.8615
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 37.735

Attention duration (in seconds): 0.5095
Attention throughput (in TFLOP/s): 66.742
MLP duration (in seconds): 1.3657
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.8752
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.3752
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 65.733
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 64.390
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 57.566
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.1146
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 71.745
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.5056
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 65.039
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.8755
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 37.557

Attention duration (in seconds): 0.5145
Attention throughput (in TFLOP/s): 66.834
MLP duration (in seconds): 1.3811
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.8956
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.3943
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 63.266
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 67.319
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 58.672
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.1214
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 68.506
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.5322
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 62.499
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.8868
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 37.508

Attention duration (in seconds): 0.5398
Attention throughput (in TFLOP/s): 64.419
MLP duration (in seconds): 1.4189
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9587
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.3917
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 64.407
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 63.996
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 57.989
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.1187
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 70.874
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.5252
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 64.046
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.9047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 37.183

Attention duration (in seconds): 0.5354
Attention throughput (in TFLOP/s): 65.674
MLP duration (in seconds): 1.4299
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9653
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.3988
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 63.979
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 65.369
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0127
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 60.366
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.1200
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 70.870
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.5360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 63.472
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.9175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 37.081

Attention duration (in seconds): 0.5432
Attention throughput (in TFLOP/s): 65.447
MLP duration (in seconds): 1.4535
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 1.9966
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.4009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 64.356
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 64.278
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0131
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 58.564
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.1236
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 69.612
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.5347
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 64.338
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.9412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 36.554

Attention duration (in seconds): 0.5496
Attention throughput (in TFLOP/s): 65.397
MLP duration (in seconds): 1.4759
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0255
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.4064
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 64.195
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 67.973
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 62.638
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.1312
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 66.276
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.5498
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 63.282
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.9435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 36.873

Attention duration (in seconds): 0.5614
Attention throughput (in TFLOP/s): 64.724
MLP duration (in seconds): 1.4932
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0546
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.4108
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 64.218
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 66.964
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 60.540
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.1318
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 66.700
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.5530
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 63.612
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.9627
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 36.541

Attention duration (in seconds): 0.5671
Attention throughput (in TFLOP/s): 64.768
MLP duration (in seconds): 1.5157
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.0828
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.4272
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 62.445
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 68.352
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 63.006
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.1361
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 65.331
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.5707
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 62.323
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.9925
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 35.837

Attention duration (in seconds): 0.5871
Attention throughput (in TFLOP/s): 63.241
MLP duration (in seconds): 1.5631
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1503
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.4201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 64.204
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 67.477
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0128
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 61.198
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.1348
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 66.713
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.5646
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 63.685
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.9909
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 36.288

Attention duration (in seconds): 0.5793
Attention throughput (in TFLOP/s): 64.787
MLP duration (in seconds): 1.5556
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1349
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.4265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 63.930
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 69.530
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 61.259
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.1371
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 66.309
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.5731
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 63.435
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.9977
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 36.435

Attention duration (in seconds): 0.5878
Attention throughput (in TFLOP/s): 64.534
MLP duration (in seconds): 1.5708
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1586
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.4298
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 64.126
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 67.102
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 61.780
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.1374
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 66.866
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.5776
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 63.628
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 1.0164
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 36.157

Attention duration (in seconds): 0.5919
Attention throughput (in TFLOP/s): 64.770
MLP duration (in seconds): 1.5939
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1858
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.4366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 63.814
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 68.388
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 64.156
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.1396
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 66.537
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.5847
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 63.531
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 1.0150
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 36.598

Attention duration (in seconds): 0.6003
Attention throughput (in TFLOP/s): 64.543
MLP duration (in seconds): 1.5997
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.2000
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.4453
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 63.244
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 67.503
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 62.340
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.1421
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 66.064
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.5689
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 66.004
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.9862
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 38.071

Attention duration (in seconds): 0.6121
Attention throughput (in TFLOP/s): 63.962
MLP duration (in seconds): 1.5551
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1673
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.4477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 63.577
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 68.997
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 62.626
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.1425
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 66.591
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.5354
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 70.884
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.9603
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 39.520

Attention duration (in seconds): 0.6148
Attention throughput (in TFLOP/s): 64.359
MLP duration (in seconds): 1.4957
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1104
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.4544
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 63.300
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 68.082
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 62.869
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.1443
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 66.464
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.5408
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 70.923
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.9817
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 39.072

Attention duration (in seconds): 0.6235
Attention throughput (in TFLOP/s): 64.115
MLP duration (in seconds): 1.5225
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1460
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.4551
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 63.878
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 69.233
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0125
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 65.195
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.1458
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 66.448
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.5472
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 70.838
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.9810
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 39.513

Attention duration (in seconds): 0.6252
Attention throughput (in TFLOP/s): 64.605
MLP duration (in seconds): 1.5282
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1534
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.4606
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 63.780
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 68.669
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0129
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 63.367
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.1463
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 66.955
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.5552
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 70.554
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 1.0032
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 39.045

Attention duration (in seconds): 0.6318
Attention throughput (in TFLOP/s): 64.599
MLP duration (in seconds): 1.5584
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.1902
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.4767
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 62.277
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 78.512
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 66.461
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.1502
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 65.899
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.6491
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 60.976
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 1.2074
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 32.783

Attention duration (in seconds): 0.6498
Attention throughput (in TFLOP/s): 63.456
MLP duration (in seconds): 1.8565
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5063
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.4728
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 63.446
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 66.721
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 53.109
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.1507
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 66.338
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.6345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 63.031
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 1.1017
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 36.302

Attention duration (in seconds): 0.6516
Attention throughput (in TFLOP/s): 63.930
MLP duration (in seconds): 1.7363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3878
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.4768
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 63.563
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 68.200
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0155
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 53.887
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.1535
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 65.818
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.6429
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 62.861
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 1.1117
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 36.353

Attention duration (in seconds): 0.6580
Attention throughput (in TFLOP/s): 63.948
MLP duration (in seconds): 1.7545
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4125
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.4831
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 63.385
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 67.255
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0156
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 53.526
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.1542
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 66.177
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.6485
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 62.958
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 1.1234
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 36.343

Attention duration (in seconds): 0.6655
Attention throughput (in TFLOP/s): 63.873
MLP duration (in seconds): 1.7720
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4374
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.4934
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 62.702
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 68.741
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 53.638
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.1569
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 65.739
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.6544
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 63.036
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 1.1305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 36.488

Attention duration (in seconds): 0.6782
Attention throughput (in TFLOP/s): 63.304
MLP duration (in seconds): 1.7849
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4631
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.5026
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 62.189
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 67.762
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 53.463
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.1601
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 65.090
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.5988
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 69.595
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 1.0929
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 38.127

Attention duration (in seconds): 0.6909
Attention throughput (in TFLOP/s): 62.762
MLP duration (in seconds): 1.6917
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3826
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.5032
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 62.736
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 68.958
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 53.997
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.1599
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 65.796
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.5975
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 70.448
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 1.1046
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 38.110

Attention duration (in seconds): 0.6913
Attention throughput (in TFLOP/s): 63.356
MLP duration (in seconds): 1.7021
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3934
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.5029
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 63.411
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 68.024
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0157
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 54.494
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.1608
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 66.115
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.6045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 70.336
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 1.1389
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 37.334

Attention duration (in seconds): 0.6920
Attention throughput (in TFLOP/s): 63.921
MLP duration (in seconds): 1.7435
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.4354
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.5078
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 63.430
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 70.477
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 51.824
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.1621
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 66.257
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.6121
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 70.170
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 1.0813
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 39.721

Attention duration (in seconds): 0.6987
Attention throughput (in TFLOP/s): 63.933
MLP duration (in seconds): 1.6934
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.3920
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.5212
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 62.419
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 67.829
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 52.174
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.1649
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 65.757
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.6195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 70.030
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 1.1718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 37.020

Attention duration (in seconds): 0.7154
Attention throughput (in TFLOP/s): 63.048
MLP duration (in seconds): 1.7913
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5067
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.5294
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 62.071
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 68.928
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 54.881
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.1679
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 65.245
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.6306
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 69.475
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 1.1569
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 37.870

Attention duration (in seconds): 0.7257
Attention throughput (in TFLOP/s): 62.768
MLP duration (in seconds): 1.7875
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.5132
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.5329
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 62.275
Elapsed time for attention_key_query_prob (512x2048x203x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x203x2048): 68.335
Elapsed time for attention_prob_times_values (512x2048x2048x203): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x203): 52.557
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.1696
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 65.211
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.7146
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 61.917
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 1.2262
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 36.086

Attention duration (in seconds): 0.7319
Attention throughput (in TFLOP/s): 62.841
MLP duration (in seconds): 1.9408
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6727
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.5152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 65.047
Elapsed time for attention_key_query_prob (512x2048x204x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x204x2048): 69.732
Elapsed time for attention_prob_times_values (512x2048x2048x204): 0.0158
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x204): 55.378
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.1628
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 68.634
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.6635
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 67.346
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 1.2470
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 35.834

Attention duration (in seconds): 0.7064
Attention throughput (in TFLOP/s): 65.740
MLP duration (in seconds): 1.9105
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.5458
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 62.008
Elapsed time for attention_key_query_prob (512x2048x205x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x205x2048): 68.778
Elapsed time for attention_prob_times_values (512x2048x2048x205): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x205): 53.109
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.1735
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 65.004
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.6830
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 66.066
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 1.1992
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 37.627

Attention duration (in seconds): 0.7487
Attention throughput (in TFLOP/s): 62.621
MLP duration (in seconds): 1.8823
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6310
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.5496
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 62.179
Elapsed time for attention_key_query_prob (512x2048x206x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x206x2048): 69.911
Elapsed time for attention_prob_times_values (512x2048x2048x206): 0.0159
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x206): 55.602
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.1746
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 65.253
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.6938
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 65.673
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 1.2259
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 37.170

Attention duration (in seconds): 0.7527
Attention throughput (in TFLOP/s): 62.883
MLP duration (in seconds): 1.9197
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6725
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.5580
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 61.845
Elapsed time for attention_key_query_prob (512x2048x207x2048): 0.0129
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x207x2048): 69.174
Elapsed time for attention_prob_times_values (512x2048x2048x207): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x207): 53.370
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.1757
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 65.474
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.6683
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 68.844
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 1.2234
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 37.606

Attention duration (in seconds): 0.7631
Attention throughput (in TFLOP/s): 62.619
MLP duration (in seconds): 1.8918
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.6549
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================

num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.5574
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 62.508
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 70.917
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 53.943
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.1773
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 65.520
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.7542
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 61.596
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 1.3110
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 35.435

Attention duration (in seconds): 0.7638
Attention throughput (in TFLOP/s): 63.160
MLP duration (in seconds): 2.0652
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.5724
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 61.459
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 67.660
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 53.108
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.1800
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 65.148
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.7673
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 61.123
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 1.3113
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 35.769

Attention duration (in seconds): 0.7825
Attention throughput (in TFLOP/s): 62.232
MLP duration (in seconds): 2.0786
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8611
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.5802
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 61.213
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 68.916
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 55.770
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.1818
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 65.122
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.7812
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 60.616
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 1.3229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 35.793

Attention duration (in seconds): 0.7912
Attention throughput (in TFLOP/s): 62.127
MLP duration (in seconds): 2.1041
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8953
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.5831
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 61.486
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 67.996
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 53.454
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.1832
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 65.217
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.7817
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 61.151
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 1.3282
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 35.992

Attention duration (in seconds): 0.7966
Attention throughput (in TFLOP/s): 62.283
MLP duration (in seconds): 2.1099
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.9066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.5965
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 60.673
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 69.565
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 56.349
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.1871
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 64.476
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.7286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 66.232
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 1.2903
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 37.402

Attention duration (in seconds): 0.8129
Attention throughput (in TFLOP/s): 61.606
MLP duration (in seconds): 2.0189
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.5960
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 61.307
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 68.386
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 53.854
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.1865
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 65.291
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.7131
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 68.309
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 1.3323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 36.565

Attention duration (in seconds): 0.8128
Attention throughput (in TFLOP/s): 62.182
MLP duration (in seconds): 2.0454
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8583
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.6097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 60.491
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 69.768
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 56.510
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.1906
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 64.486
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.7252
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 67.808
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 1.3336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 36.874

Attention duration (in seconds): 0.8297
Attention throughput (in TFLOP/s): 61.478
MLP duration (in seconds): 2.0587
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8885
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.6135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 60.681
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 68.866
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 54.317
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.1922
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 64.571
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.7360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 67.440
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 1.3598
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 36.501

Attention duration (in seconds): 0.8360
Attention throughput (in TFLOP/s): 61.577
MLP duration (in seconds): 2.0957
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.9318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.6145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 61.144
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 71.026
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 55.110
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.1923
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 65.122
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.7401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 67.690
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 1.3810
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 36.276

Attention duration (in seconds): 0.8367
Attention throughput (in TFLOP/s): 62.091
MLP duration (in seconds): 2.1211
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.9578
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.6279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 60.391
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 68.519
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 54.753
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.1955
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 64.665
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.8396
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 60.224
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 1.4290
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 35.383

Attention duration (in seconds): 0.8540
Attention throughput (in TFLOP/s): 61.387
MLP duration (in seconds): 2.2685
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.1225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.6320
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 60.555
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 71.508
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 58.467
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.1970
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 64.747
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.8464
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 60.291
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 1.4397
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 35.443

Attention duration (in seconds): 0.8581
Attention throughput (in TFLOP/s): 61.646
MLP duration (in seconds): 2.2861
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.1443
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.6396
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 60.389
Elapsed time for attention_key_query_prob (512x2048x219x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x219x2048): 70.815
Elapsed time for attention_prob_times_values (512x2048x2048x219): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x219): 56.239
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.1978
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 65.072
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.8569
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 60.098
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 1.4583
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 35.314

Attention duration (in seconds): 0.8674
Attention throughput (in TFLOP/s): 61.537
MLP duration (in seconds): 2.3152
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.1826
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.6428
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 60.636
Elapsed time for attention_key_query_prob (512x2048x220x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x220x2048): 72.260
Elapsed time for attention_prob_times_values (512x2048x2048x220): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x220): 59.216
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.2001
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 64.936
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.8612
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 60.346
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 1.4751
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 35.232

Attention duration (in seconds): 0.8719
Attention throughput (in TFLOP/s): 61.771
MLP duration (in seconds): 2.3363
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2082
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.6564
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 59.920
Elapsed time for attention_key_query_prob (512x2048x221x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x221x2048): 71.254
Elapsed time for attention_prob_times_values (512x2048x2048x221): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x221): 56.660
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.2028
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 64.656
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.8782
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 59.717
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 1.5023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 34.907

Attention duration (in seconds): 0.8893
Attention throughput (in TFLOP/s): 61.109
MLP duration (in seconds): 2.3805
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2698
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.6617
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 59.981
Elapsed time for attention_key_query_prob (512x2048x222x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x222x2048): 72.552
Elapsed time for attention_prob_times_values (512x2048x2048x222): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x222): 59.386
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.2053
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 64.456
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.8278
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 63.928
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 1.4754
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 35.867

Attention duration (in seconds): 0.8961
Attention throughput (in TFLOP/s): 61.179
MLP duration (in seconds): 2.3032
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.1993
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.6759
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 59.247
Elapsed time for attention_key_query_prob (512x2048x223x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x223x2048): 71.672
Elapsed time for attention_prob_times_values (512x2048x2048x223): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x223): 57.032
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.2099
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 63.585
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.8222
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 64.939
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 1.4870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 35.908

Attention duration (in seconds): 0.9160
Attention throughput (in TFLOP/s): 60.382
MLP duration (in seconds): 2.3093
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.6521
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 61.966
Elapsed time for attention_key_query_prob (512x2048x224x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x224x2048): 82.155
Elapsed time for attention_prob_times_values (512x2048x2048x224): 0.0160
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x224): 60.182
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.2033
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 66.255
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.7920
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 68.023
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 1.4008
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 38.461

Attention duration (in seconds): 0.8831
Attention throughput (in TFLOP/s): 63.189
MLP duration (in seconds): 2.1928
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.0759
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.7097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 57.448
Elapsed time for attention_key_query_prob (512x2048x225x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x225x2048): 70.082
Elapsed time for attention_prob_times_values (512x2048x2048x225): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x225): 58.034
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.2150
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 63.201
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.8431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 64.473
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 1.5178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 35.813

Attention duration (in seconds): 0.9551
Attention throughput (in TFLOP/s): 58.936
MLP duration (in seconds): 2.3609
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.3161
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.6921
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 59.427
Elapsed time for attention_key_query_prob (512x2048x226x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x226x2048): 71.698
Elapsed time for attention_prob_times_values (512x2048x2048x226): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x226): 60.186
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.2122
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 64.616
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.8317
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 65.944
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 1.5314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 35.812

Attention duration (in seconds): 0.9340
Attention throughput (in TFLOP/s): 60.797
MLP duration (in seconds): 2.3630
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.2970
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.6955
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 59.667
Elapsed time for attention_key_query_prob (512x2048x227x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x227x2048): 70.501
Elapsed time for attention_prob_times_values (512x2048x2048x227): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x227): 58.297
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.2140
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 64.650
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.8960
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 61.749
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 1.5789
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 35.042

Attention duration (in seconds): 0.9400
Attention throughput (in TFLOP/s): 60.936
MLP duration (in seconds): 2.4750
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.4149
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.6457
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 64.830
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 72.239
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 60.790
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.2158
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 64.664
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.9081
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 61.467
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 1.5775
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 35.383

Attention duration (in seconds): 0.8912
Attention throughput (in TFLOP/s): 64.830
MLP duration (in seconds): 2.4856
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.3768
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.6999
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 59.816
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 72.060
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0161
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 60.924
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.2160
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 64.607
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.9347
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 59.717
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 1.6027
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 34.828

Attention duration (in seconds): 0.9455
Attention throughput (in TFLOP/s): 61.105
MLP duration (in seconds): 2.5374
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.4829
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.7120
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 59.316
Elapsed time for attention_key_query_prob (512x2048x229x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x229x2048): 70.725
Elapsed time for attention_prob_times_values (512x2048x2048x229): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x229): 58.707
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.2182
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 64.510
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.9517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 59.163
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 1.6141
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 34.886

Attention duration (in seconds): 0.9608
Attention throughput (in TFLOP/s): 60.650
MLP duration (in seconds): 2.5658
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5266
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.7183
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 59.306
Elapsed time for attention_key_query_prob (512x2048x230x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x230x2048): 72.187
Elapsed time for attention_prob_times_values (512x2048x2048x230): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x230): 61.059
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.2202
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 64.493
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.9601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 59.160
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 1.6296
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 34.855

Attention duration (in seconds): 0.9684
Attention throughput (in TFLOP/s): 60.697
MLP duration (in seconds): 2.5898
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.7308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 58.800
Elapsed time for attention_key_query_prob (512x2048x231x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x231x2048): 71.069
Elapsed time for attention_prob_times_values (512x2048x2048x231): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x231): 58.992
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.2241
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 63.927
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.9455
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 60.598
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 1.6343
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 35.057

Attention duration (in seconds): 0.9857
Attention throughput (in TFLOP/s): 60.143
MLP duration (in seconds): 2.5799
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5655
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.7303
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 59.354
Elapsed time for attention_key_query_prob (512x2048x232x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x232x2048): 73.668
Elapsed time for attention_prob_times_values (512x2048x2048x232): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x232): 59.588
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.2249
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 64.245
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.8893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 64.988
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 1.6297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 35.463

Attention duration (in seconds): 0.9854
Attention throughput (in TFLOP/s): 60.670
MLP duration (in seconds): 2.5190
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5044
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.7440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 58.766
Elapsed time for attention_key_query_prob (512x2048x233x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x233x2048): 70.697
Elapsed time for attention_prob_times_values (512x2048x2048x233): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x233): 59.170
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.2278
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 63.986
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.9043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 64.461
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 1.6419
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 35.503

Attention duration (in seconds): 1.0028
Attention throughput (in TFLOP/s): 60.127
MLP duration (in seconds): 2.5462
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5490
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.7469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 59.035
Elapsed time for attention_key_query_prob (512x2048x234x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x234x2048): 72.075
Elapsed time for attention_prob_times_values (512x2048x2048x234): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x234): 61.902
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.2286
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 64.298
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.9155
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 64.221
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 1.6669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 35.271

Attention duration (in seconds): 1.0057
Attention throughput (in TFLOP/s): 60.459
MLP duration (in seconds): 2.5824
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.5881
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.7596
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 58.549
Elapsed time for attention_key_query_prob (512x2048x235x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x235x2048): 71.031
Elapsed time for attention_prob_times_values (512x2048x2048x235): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x235): 59.468
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.2326
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 63.735
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.9139
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 64.883
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 1.6847
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 35.197

Attention duration (in seconds): 1.0234
Attention throughput (in TFLOP/s): 59.916
MLP duration (in seconds): 2.5986
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.6220
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.7674
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 58.445
Elapsed time for attention_key_query_prob (512x2048x236x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x236x2048): 72.818
Elapsed time for attention_prob_times_values (512x2048x2048x236): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x236): 62.607
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.2337
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 63.976
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 1.0246
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 58.366
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 1.7361
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 34.447

Attention duration (in seconds): 1.0312
Attention throughput (in TFLOP/s): 59.958
MLP duration (in seconds): 2.7607
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.7920
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.7768
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 58.232
Elapsed time for attention_key_query_prob (512x2048x237x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x237x2048): 71.324
Elapsed time for attention_prob_times_values (512x2048x2048x237): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x237): 59.846
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.2366
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 63.718
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 1.0328
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 58.398
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 1.7505
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 34.454

Attention duration (in seconds): 1.0447
Attention throughput (in TFLOP/s): 59.680
MLP duration (in seconds): 2.7833
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.8280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.7819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 58.339
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 72.851
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 62.728
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.2385
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 63.756
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 1.0468
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 58.101
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 1.7718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 34.327

Attention duration (in seconds): 1.0507
Attention throughput (in TFLOP/s): 59.830
MLP duration (in seconds): 2.8186
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.8694
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
[2023-11-24 17:23:22,977] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.7860
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 58.038
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 70.881
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 61.598
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.2385
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 63.740
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 1.0477
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 58.054
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 1.7764
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 34.239

Attention duration (in seconds): 1.0555
Attention throughput (in TFLOP/s): 59.558
MLP duration (in seconds): 2.8240
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.8796
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.8053
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 57.121
Elapsed time for attention_key_query_prob (512x2048x239x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x239x2048): 69.784
Elapsed time for attention_prob_times_values (512x2048x2048x239): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x239): 59.366
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.2437
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 62.919
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 1.0747
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 57.071
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 1.8232
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 33.641

Attention duration (in seconds): 1.0810
Attention throughput (in TFLOP/s): 58.637
MLP duration (in seconds): 2.8979
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.9789
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.7979
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 58.137
Elapsed time for attention_key_query_prob (512x2048x240x2048): 0.0184
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x240x2048): 55.967
Elapsed time for attention_prob_times_values (512x2048x2048x240): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x240): 61.718
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.2432
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 63.567
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 1.0644
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 58.106
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 1.8000
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 34.360

Attention duration (in seconds): 1.0762
Attention throughput (in TFLOP/s): 59.383
MLP duration (in seconds): 2.8644
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.9406
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.8157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 57.344
Elapsed time for attention_key_query_prob (512x2048x241x2048): 0.0193
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x241x2048): 53.586
Elapsed time for attention_prob_times_values (512x2048x2048x241): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x241): 59.886
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.2503
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 62.288
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 1.0310
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 60.488
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 1.8914
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 32.973

Attention duration (in seconds): 1.1026
Attention throughput (in TFLOP/s): 58.440
MLP duration (in seconds): 2.9224
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.0250
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.8051
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 58.577
Elapsed time for attention_key_query_prob (512x2048x242x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x242x2048): 54.413
Elapsed time for attention_prob_times_values (512x2048x2048x242): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x242): 62.494
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.2451
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 64.129
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.9772
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 64.353
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 1.8647
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 33.723

Attention duration (in seconds): 1.0860
Attention throughput (in TFLOP/s): 59.817
MLP duration (in seconds): 2.8418
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.9278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.8187
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 58.082
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0194
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 53.878
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 60.205
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.2489
[2023-11-25 00:31:32,045] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.8214
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 57.893
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0193
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 54.019
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 60.254
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.2498
Throughput (in TFLOP/s) for attention_linear_projection (4x31104x31104, b=2048): 63.459
Elapsed time for mlp_h_to_4h (4x31104x124416, b=2048): 1.0953
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x124416, b=2048): 57.889
Elapsed time for mlp_4h_to_h (4x124416x31104, b=2048): 1.8748
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124416x31104, b=2048): 33.819

Attention duration (in seconds): 1.1078
Attention throughput (in TFLOP/s): 59.117
MLP duration (in seconds): 2.9700
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.0779
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x93696, b=2048): 0.8246
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x93696, b=2048): 58.144
Elapsed time for attention_key_query_prob (512x2048x244x2048): 0.0190
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x244x2048): 55.114
Elapsed time for attention_prob_times_values (512x2048x2048x244): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x244): 62.917
Elapsed time for attention_linear_projection (4x31232x31232, b=2048): 0.2504
Throughput (in TFLOP/s) for attention_linear_projection (4x31232x31232, b=2048): 63.817
Elapsed time for mlp_h_to_4h (4x31232x124928, b=2048): 1.1046
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x124928, b=2048): 57.874
Elapsed time for mlp_4h_to_h (4x124928x31232, b=2048): 1.9116
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124928x31232, b=2048): 33.441

Attention duration (in seconds): 1.1107
Attention throughput (in TFLOP/s): 59.443
MLP duration (in seconds): 3.0162
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.1269
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x94080, b=2048): 0.8423
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x94080, b=2048): 57.392
Elapsed time for attention_key_query_prob (512x2048x245x2048): 0.0194
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x245x2048): 54.378
Elapsed time for attention_prob_times_values (512x2048x2048x245): 0.0173
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x245): 60.700
Elapsed time for attention_linear_projection (4x31360x31360, b=2048): 0.2559
Throughput (in TFLOP/s) for attention_linear_projection (4x31360x31360, b=2048): 62.977
Elapsed time for mlp_h_to_4h (4x31360x125440, b=2048): 1.1277
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x125440, b=2048): 57.155
Elapsed time for mlp_4h_to_h (4x125440x31360, b=2048): 1.9313
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125440x31360, b=2048): 33.372

Attention duration (in seconds): 1.1348
Attention throughput (in TFLOP/s): 58.650
MLP duration (in seconds): 3.0590
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.1938
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x94464, b=2048): 0.8383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x94464, b=2048): 58.135
Elapsed time for attention_key_query_prob (512x2048x246x2048): 0.0192
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x246x2048): 55.042
Elapsed time for attention_prob_times_values (512x2048x2048x246): 0.0167
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x246): 63.223
Elapsed time for attention_linear_projection (4x31488x31488, b=2048): 0.2533
Throughput (in TFLOP/s) for attention_linear_projection (4x31488x31488, b=2048): 64.126
Elapsed time for mlp_h_to_4h (4x31488x125952, b=2048): 1.0561
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x125952, b=2048): 61.530
Elapsed time for mlp_4h_to_h (4x125952x31488, b=2048): 1.9112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125952x31488, b=2048): 34.000

Attention duration (in seconds): 1.1275
Attention throughput (in TFLOP/s): 59.504
MLP duration (in seconds): 2.9672
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.0947
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x94848, b=2048): 0.8781
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x94848, b=2048): 55.949
Elapsed time for attention_key_query_prob (512x2048x247x2048): 0.0194
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x247x2048): 54.734
Elapsed time for attention_prob_times_values (512x2048x2048x247): 0.0174
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x247): 60.995
Elapsed time for attention_linear_projection (4x31616x31616, b=2048): 0.2661
Throughput (in TFLOP/s) for attention_linear_projection (4x31616x31616, b=2048): 61.534
Elapsed time for mlp_h_to_4h (4x31616x126464, b=2048): 1.0925
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x126464, b=2048): 59.962
Elapsed time for mlp_4h_to_h (4x126464x31616, b=2048): 1.9433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126464x31616, b=2048): 33.709

Attention duration (in seconds): 1.1811
Attention throughput (in TFLOP/s): 57.262
MLP duration (in seconds): 3.0358
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.2169
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.8714
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 56.836
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0190
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 56.127
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 62.282
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.2666
[2023-11-26 15:57:25,618] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.8730
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 56.733
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0191
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 55.888
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0171
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 62.119
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.2661
Throughput (in TFLOP/s) for attention_linear_projection (4x31744x31744, b=2048): 62.039
Elapsed time for mlp_h_to_4h (4x31744x126976, b=2048): 1.1817
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x126976, b=2048): 55.885
Elapsed time for mlp_4h_to_h (4x126976x31744, b=2048): 2.0709
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126976x31744, b=2048): 31.889

Attention duration (in seconds): 1.1754
Attention throughput (in TFLOP/s): 57.999
MLP duration (in seconds): 3.2526
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.4279
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x95616, b=2048): 0.8805
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x95616, b=2048): 56.704
Elapsed time for attention_key_query_prob (512x2048x249x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x249x2048): 53.760
Elapsed time for attention_prob_times_values (512x2048x2048x249): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x249): 64.602
Elapsed time for attention_linear_projection (4x31872x31872, b=2048): 0.2690
Throughput (in TFLOP/s) for attention_linear_projection (4x31872x31872, b=2048): 61.882
Elapsed time for mlp_h_to_4h (4x31872x127488, b=2048): 1.1991
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x127488, b=2048): 55.517
Elapsed time for mlp_4h_to_h (4x127488x31872, b=2048): 2.0653
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127488x31872, b=2048): 32.234

Attention duration (in seconds): 1.1859
Attention throughput (in TFLOP/s): 57.939
MLP duration (in seconds): 3.2644
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.4504
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x96000, b=2048): 0.8659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x96000, b=2048): 58.125
Elapsed time for attention_key_query_prob (512x2048x250x2048): 0.0198
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x250x2048): 54.298
Elapsed time for attention_prob_times_values (512x2048x2048x250): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x250): 63.982
Elapsed time for attention_linear_projection (4x32000x32000, b=2048): 0.2616
Throughput (in TFLOP/s) for attention_linear_projection (4x32000x32000, b=2048): 64.135
Elapsed time for mlp_h_to_4h (4x32000x128000, b=2048): 1.1563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x128000, b=2048): 58.038
Elapsed time for mlp_4h_to_h (4x128000x32000, b=2048): 2.0451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128000x32000, b=2048): 32.814

Attention duration (in seconds): 1.1641
Attention throughput (in TFLOP/s): 59.495
MLP duration (in seconds): 3.2014
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.3655
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x96384, b=2048): 0.8751
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x96384, b=2048): 57.977
Elapsed time for attention_key_query_prob (512x2048x251x2048): 0.0200
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x251x2048): 53.963
Elapsed time for attention_prob_times_values (512x2048x2048x251): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x251): 63.397
Elapsed time for attention_linear_projection (4x32128x32128, b=2048): 0.2663
Throughput (in TFLOP/s) for attention_linear_projection (4x32128x32128, b=2048): 63.511
Elapsed time for mlp_h_to_4h (4x32128x128512, b=2048): 1.0855
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x128512, b=2048): 62.318
Elapsed time for mlp_4h_to_h (4x128512x32128, b=2048): 2.0261
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128512x32128, b=2048): 33.388

Attention duration (in seconds): 1.1784
Attention throughput (in TFLOP/s): 59.238
MLP duration (in seconds): 3.1116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.2899
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x96768, b=2048): 0.9008
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x96768, b=2048): 56.769
Elapsed time for attention_key_query_prob (512x2048x252x2048): 0.0197
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x252x2048): 55.033
Elapsed time for attention_prob_times_values (512x2048x2048x252): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x252): 64.337
Elapsed time for attention_linear_projection (4x32256x32256, b=2048): 0.2701
Throughput (in TFLOP/s) for attention_linear_projection (4x32256x32256, b=2048): 63.113
Elapsed time for mlp_h_to_4h (4x32256x129024, b=2048): 1.0890
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x129024, b=2048): 62.616
Elapsed time for mlp_4h_to_h (4x129024x32256, b=2048): 2.0310
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129024x32256, b=2048): 33.573

Attention duration (in seconds): 1.2074
Attention throughput (in TFLOP/s): 58.265
MLP duration (in seconds): 3.1200
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.3274
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x97152, b=2048): 0.8622
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x97152, b=2048): 59.784
Elapsed time for attention_key_query_prob (512x2048x253x2048): 0.0202
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x253x2048): 53.895
Elapsed time for attention_prob_times_values (512x2048x2048x253): 0.0175
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x253): 62.186
Elapsed time for attention_linear_projection (4x32384x32384, b=2048): 0.2656
Throughput (in TFLOP/s) for attention_linear_projection (4x32384x32384, b=2048): 64.702
Elapsed time for mlp_h_to_4h (4x32384x129536, b=2048): 1.0776
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x129536, b=2048): 63.781
Elapsed time for mlp_4h_to_h (4x129536x32384, b=2048): 2.0581
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129536x32384, b=2048): 33.394

Attention duration (in seconds): 1.1654
Attention throughput (in TFLOP/s): 60.839
MLP duration (in seconds): 3.1357
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.3011
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x97536, b=2048): 0.8931
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x97536, b=2048): 58.176
Elapsed time for attention_key_query_prob (512x2048x254x2048): 0.0199
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x254x2048): 54.794
Elapsed time for attention_prob_times_values (512x2048x2048x254): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x254): 64.851
Elapsed time for attention_linear_projection (4x32512x32512, b=2048): 0.2688
Throughput (in TFLOP/s) for attention_linear_projection (4x32512x32512, b=2048): 64.428
Elapsed time for mlp_h_to_4h (4x32512x130048, b=2048): 1.0962
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x130048, b=2048): 63.193
Elapsed time for mlp_4h_to_h (4x130048x32512, b=2048): 2.0900
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130048x32512, b=2048): 33.145

Attention duration (in seconds): 1.1986
Attention throughput (in TFLOP/s): 59.615
MLP duration (in seconds): 3.1862
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.3848
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x97920, b=2048): 0.7878
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x97920, b=2048): 66.471
Elapsed time for attention_key_query_prob (512x2048x255x2048): 0.0203
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x255x2048): 53.848
Elapsed time for attention_prob_times_values (512x2048x2048x255): 0.0165
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x255): 66.256
Elapsed time for attention_linear_projection (4x32640x32640, b=2048): 0.2343
Throughput (in TFLOP/s) for attention_linear_projection (4x32640x32640, b=2048): 74.494
Elapsed time for mlp_h_to_4h (4x32640x130560, b=2048): 1.0458
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x130560, b=2048): 66.761
Elapsed time for mlp_4h_to_h (4x130560x32640, b=2048): 2.0733
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130560x32640, b=2048): 33.675

Attention duration (in seconds): 1.0590
Attention throughput (in TFLOP/s): 68.000
MLP duration (in seconds): 3.1191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 4.1781
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
