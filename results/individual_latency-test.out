1.13.1 

[2023-11-02 19:44:14,278] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-02 19:44:14,876] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.156.2, master_port=6000
[2023-11-02 19:44:14,876] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-02 19:44:18,003] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 24, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
QKV Transform: 2.107232093811035
Attention Score: 0.0057239532470703125
fused
Attention Softmax: 0.010228395462036133
Attention Dropout: 0.00128006935119628906
Attention Over Value: 0.005192756652832031
Attention linproj: 0.00853729248046875
QKV Transform: 0.0022232532501220703
Attention Score: 0.0011432170867919922
fused
Attention Softmax: 0.0026705265045166016
Attention Dropout: 0.00007987022399902344
Attention Over Value: 0.0009834766387939453
Attention linproj: 0.0007698535919189453
QKV Transform: 0.0020933151245117188
Attention Score: 0.0011374950408935547
fused
Attention Softmax: 0.0026564598083496094
Attention Dropout: 0.00005674362182617188
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007452964782714844
QKV Transform: 0.0020787715911865234
Attention Score: 0.0011286735534667969
fused
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006604194641113281
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007450580596923828
QKV Transform: 0.002080202102661133
Attention Score: 0.0011336803436279297
fused
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007460117340087891
QKV Transform: 0.002079010009765625
Attention Score: 0.00113677978515625
fused
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007662773132324219
Attention linproj: 0.0007474422454833984
QKV Transform: 0.002077341079711914
Attention Score: 0.0011303424835205078
fused
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007462501525878906
QKV Transform: 0.0020821094512939453
Attention Score: 0.001135110855102539
fused
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005626678466796875
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007481575012207031
QKV Transform: 0.0020825862884521484
Attention Score: 0.0011463165283203125
fused
Attention Softmax: 0.0026564598083496094
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007462501525878906
QKV Transform: 0.0020842552185058594
Attention Score: 0.0011303424835205078
fused
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007495880126953125
QKV Transform: 0.002080202102661133
Attention Score: 0.001123189926147461
fused
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007510185241699219
QKV Transform: 0.0020837783813476562
Attention Score: 0.0011298656463623047
fused
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007550716400146484
QKV Transform: 0.002085447311401367
Attention Score: 0.0011265277862548828
fused
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007464885711669922
QKV Transform: 0.0020873546600341797
Attention Score: 0.001129150390625
fused
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007512569427490234
QKV Transform: 0.0020890235900878906
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007436275482177734
QKV Transform: 0.0020797252655029297
Attention Score: 0.001131296157836914
fused
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007443428039550781
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011219978332519531
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007643699645996094
Attention linproj: 0.0007443428039550781
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011458396911621094
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007457733154296875
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011353492736816406
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007483959197998047
QKV Transform: 0.002079010009765625
Attention Score: 0.0011303424835205078
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007479190826416016
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011343955993652344
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007436275482177734
QKV Transform: 0.0020842552185058594
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.0026602745056152344
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007486343383789062
QKV Transform: 0.0020864009857177734
Attention Score: 0.001129150390625
fused
Attention Softmax: 0.0026540756225585938
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007472038269042969
QKV Transform: 0.0020852088928222656
Attention Score: 0.0011196136474609375
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007505416870117188
QKV Transform: 0.002081632614135742
Attention Score: 0.0011301040649414062
fused
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007517337799072266
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011315345764160156
fused
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007622241973876953
QKV Transform: 0.002085447311401367
Attention Score: 0.0011303424835205078
fused
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007445812225341797
QKV Transform: 0.0020914077758789062
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007443428039550781
QKV Transform: 0.002089977264404297
Attention Score: 0.0011222362518310547
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007464885711669922
QKV Transform: 0.002081155776977539
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007457733154296875
QKV Transform: 0.002081632614135742
Attention Score: 0.001131296157836914
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007462501525878906
QKV Transform: 0.0020818710327148438
Attention Score: 0.0011196136474609375
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00006484985351562500
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007481575012207031
QKV Transform: 0.002080678939819336
Attention Score: 0.0011332035064697266
fused
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007479190826416016
QKV Transform: 0.002080678939819336
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007457733154296875
QKV Transform: 0.00208282470703125
Attention Score: 0.001131296157836914
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007452964782714844
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005602836608886719
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007531642913818359
QKV Transform: 0.0020842552185058594
Attention Score: 0.0011472702026367188
fused
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007481575012207031
QKV Transform: 0.002084970474243164
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.002646207809448242
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007522106170654297
QKV Transform: 0.0020847320556640625
Attention Score: 0.0011305809020996094
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007519721984863281
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011246204376220703
fused
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007560253143310547
QKV Transform: 0.002088308334350586
Attention Score: 0.0011286735534667969
fused
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007481575012207031
QKV Transform: 0.002093076705932617
Attention Score: 0.0011284351348876953
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007581710815429688
QKV Transform: 0.0020911693572998047
Attention Score: 0.0011248588562011719
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007460117340087891
QKV Transform: 0.002081155776977539
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.0026428699493408203
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007448196411132812
QKV Transform: 0.0020818710327148438
Attention Score: 0.0011255741119384766
fused
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007460117340087891
QKV Transform: 0.002073526382446289
Attention Score: 0.0011255741119384766
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00006532669067382812
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007464885711669922
QKV Transform: 0.002081155776977539
Attention Score: 0.0011284351348876953
fused
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007479190826416016
QKV Transform: 0.002082347869873047
Attention Score: 0.0011372566223144531
fused
Attention Softmax: 0.0026602745056152344
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007498264312744141
QKV Transform: 0.002087831497192383
Attention Score: 0.0011415481567382812
fused
Attention Softmax: 0.0026633739471435547
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007464885711669922
QKV Transform: 0.0020906925201416016
Attention Score: 0.0011551380157470703
fused
Attention Softmax: 0.0026640892028808594
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007402896881103516
QKV Transform: 0.0020928382873535156
Attention Score: 0.0011546611785888672
fused
Attention Softmax: 0.0026619434356689453
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007724761962890625
Attention linproj: 0.0007517337799072266
QKV Transform: 0.002089977264404297
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007522106170654297
QKV Transform: 0.0021131038665771484
Attention Score: 0.0011315345764160156
fused
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007524490356445312
QKV Transform: 0.002089262008666992
Attention Score: 0.0011289119720458984
fused
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007500648498535156
QKV Transform: 0.002092123031616211
Attention Score: 0.0011289119720458984
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007483959197998047
QKV Transform: 0.002096891403198242
Attention Score: 0.0011332035064697266
fused
Attention Softmax: 0.0026509761810302734
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007472038269042969
QKV Transform: 0.002095460891723633
Attention Score: 0.0011289119720458984
fused
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007464885711669922
QKV Transform: 0.002096891403198242
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007448196411132812
QKV Transform: 0.002084970474243164
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007464885711669922
QKV Transform: 0.0020856857299804688
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00006365776062011719
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007460117340087891
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011243820190429688
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007643699645996094
Attention linproj: 0.0007464885711669922
QKV Transform: 0.002088785171508789
Attention Score: 0.0011301040649414062
fused
Attention Softmax: 0.002646207809448242
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007479190826416016
QKV Transform: 0.0020782947540283203
Attention Score: 0.0011301040649414062
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007476806640625
QKV Transform: 0.002084493637084961
Attention Score: 0.001127004623413086
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007474422454833984
QKV Transform: 0.002093076705932617
Attention Score: 0.0011241436004638672
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00007224082946777344
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007517337799072266
QKV Transform: 0.00209808349609375
Attention Score: 0.0011525154113769531
fused
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007660388946533203
QKV Transform: 0.0020966529846191406
Attention Score: 0.0011303424835205078
fused
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007531642913818359
QKV Transform: 0.0020859241485595703
Attention Score: 0.0011327266693115234
fused
Attention Softmax: 0.00264739990234375
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007536411285400391
QKV Transform: 0.0020880699157714844
Attention Score: 0.0011250972747802734
fused
Attention Softmax: 0.00264739990234375
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007522106170654297
Attention linproj: 0.0007555484771728516
QKV Transform: 0.0020914077758789062
Attention Score: 0.0011453628540039062
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007474422454833984
QKV Transform: 0.0020966529846191406
Attention Score: 0.0011284351348876953
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007495880126953125
QKV Transform: 0.002093791961669922
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007467269897460938
QKV Transform: 0.002086162567138672
Attention Score: 0.0011234283447265625
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007457733154296875
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007457733154296875
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00006484985351562500
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007493495941162109
QKV Transform: 0.002087831497192383
Attention Score: 0.0011322498321533203
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007493495941162109
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011258125305175781
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007479190826416016
QKV Transform: 0.002086162567138672
Attention Score: 0.0011279582977294922
fused
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.00077056884765625
Attention linproj: 0.0007455348968505859
QKV Transform: 0.002093791961669922
Attention Score: 0.0011327266693115234
fused
Attention Softmax: 0.00266265869140625
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007541179656982422
QKV Transform: 0.002093791961669922
Attention Score: 0.0011365413665771484
fused
Attention Softmax: 0.002658367156982422
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.000762939453125
Attention linproj: 0.0007519721984863281
QKV Transform: 0.0020923614501953125
Attention Score: 0.0011165142059326172
fused
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007531642913818359
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007536411285400391
QKV Transform: 0.0020914077758789062
Attention Score: 0.001127004623413086
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007526874542236328
Attention linproj: 0.0007584095001220703
QKV Transform: 0.0020918846130371094
Attention Score: 0.001127481460571289
fused
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007474422454833984
QKV Transform: 0.0020961761474609375
Attention Score: 0.0011298656463623047
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007486343383789062
QKV Transform: 0.00209808349609375
Attention Score: 0.0011315345764160156
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007479190826416016
QKV Transform: 0.0020956993103027344
Attention Score: 0.0011239051818847656
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007452964782714844
QKV Transform: 0.0057277679443359375
Attention Score: 0.001155853271484375
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007517337799072266
QKV Transform: 0.002097606658935547
Attention Score: 0.0011408329010009766
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007476806640625
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011262893676757812
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007488727569580078
QKV Transform: 0.002088308334350586
Attention Score: 0.0011317729949951172
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007648468017578125
QKV Transform: 0.0020923614501953125
Attention Score: 0.001131296157836914
fused
Attention Softmax: 0.002646207809448242
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007441043853759766
QKV Transform: 0.0020928382873535156
Attention Score: 0.0011429786682128906
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007696151733398438
Attention linproj: 0.0007529258728027344
QKV Transform: 0.002093791961669922
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007488727569580078
QKV Transform: 0.0021033287048339844
Attention Score: 0.0011234283447265625
fused
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005006790161132812
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007543563842773438
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011188983917236328
fused
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007417201995849609
QKV Transform: 0.002091646194458008
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007469654083251953
QKV Transform: 0.0020961761474609375
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007464885711669922
QKV Transform: 0.002099752426147461
Attention Score: 0.0011262893676757812
fused
Attention Softmax: 0.0026428699493408203
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007464885711669922
QKV Transform: 0.002100229263305664
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007479190826416016
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011286735534667969
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007462501525878906
QKV Transform: 0.0021016597747802734
Attention Score: 0.0011286735534667969
fused
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00004982948303222656
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007507801055908203
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011413097381591797
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007469654083251953
QKV Transform: 0.002087831497192383
Attention Score: 0.0011241436004638672
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005578994750976562
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007495880126953125
QKV Transform: 0.002088308334350586
Attention Score: 0.0011343955993652344
fused
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007474422454833984
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005292892456054688
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007600784301757812
QKV Transform: 0.002091646194458008
Attention Score: 0.0011322498321533203
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007522106170654297
QKV Transform: 0.002092123031616211
Attention Score: 0.0011250972747802734
fused
Attention Softmax: 0.002646207809448242
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007503032684326172
QKV Transform: 0.002107381820678711
Attention Score: 0.001123666763305664
fused
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005340576171875000
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007524490356445312
QKV Transform: 0.0020906925201416016
Attention Score: 0.0011305809020996094
fused
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007550716400146484
QKV Transform: 0.0020906925201416016
Attention Score: 0.0011341571807861328
fused
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007479190826416016
QKV Transform: 0.0020978450775146484
Attention Score: 0.0011303424835205078
fused
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007562637329101562
QKV Transform: 0.002098560333251953
Attention Score: 0.0011298656463623047
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007479190826416016
QKV Transform: 0.002095937728881836
Attention Score: 0.0011355876922607422
fused
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007462501525878906
QKV Transform: 0.002087116241455078
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007469654083251953
QKV Transform: 0.0020890235900878906
Attention Score: 0.0011289119720458984
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00006508827209472656
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007488727569580078
QKV Transform: 0.0020895004272460938
Attention Score: 0.0011227130889892578
fused
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005698204040527344
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007481575012207031
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011355876922607422
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007507801055908203
QKV Transform: 0.0021004676818847656
Attention Score: 0.0011298656463623047
fused
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007517337799072266
QKV Transform: 0.002089262008666992
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.0026450157165527344
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007483959197998047
QKV Transform: 0.0020906925201416016
Attention Score: 0.0011324882507324219
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005578994750976562
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007512569427490234
QKV Transform: 0.0020918846130371094
Attention Score: 0.0011472702026367188
fused
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007641315460205078
QKV Transform: 0.0020914077758789062
Attention Score: 0.0011246204376220703
fused
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007505416870117188
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011363029479980469
fused
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00004959106445312500
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007534027099609375
QKV Transform: 0.002090930938720703
Attention Score: 0.0011315345764160156
fused
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.000762939453125
QKV Transform: 0.0020935535430908203
Attention Score: 0.0011348724365234375
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007474422454833984
QKV Transform: 0.0020995140075683594
Attention Score: 0.0011296272277832031
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007498264312744141
QKV Transform: 0.0020971298217773438
Attention Score: 0.0011227130889892578
fused
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007472038269042969
QKV Transform: 0.002087831497192383
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007457733154296875
QKV Transform: 0.0020880699157714844
Attention Score: 0.0011305809020996094
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007476806640625
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011451244354248047
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007519721984863281
Attention linproj: 0.0007524490356445312
QKV Transform: 0.002087116241455078
Attention Score: 0.0011332035064697266
fused
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007481575012207031
QKV Transform: 0.002089262008666992
Attention Score: 0.0011239051818847656
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007472038269042969
QKV Transform: 0.0020885467529296875
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.000762939453125
Attention linproj: 0.0007634162902832031
QKV Transform: 0.0020897388458251953
Attention Score: 0.0011298656463623047
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007407665252685547
QKV Transform: 0.0021047592163085938
Attention Score: 0.0011305809020996094
fused
Attention Softmax: 0.0026547908782958984
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007414817810058594
QKV Transform: 0.0020918846130371094
Attention Score: 0.0011258125305175781
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007541179656982422
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011365413665771484
fused
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007526874542236328
QKV Transform: 0.0020945072174072266
Attention Score: 0.0011258125305175781
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007474422454833984
QKV Transform: 0.0020923614501953125
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007472038269042969
QKV Transform: 0.002095937728881836
Attention Score: 0.001123666763305664
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007486343383789062
QKV Transform: 0.002096891403198242
Attention Score: 0.0011265277862548828
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007493495941162109
QKV Transform: 0.0020885467529296875
Attention Score: 0.0011296272277832031
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005340576171875000
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007464885711669922
QKV Transform: 0.0020890235900878906
Attention Score: 0.0011379718780517578
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007472038269042969
QKV Transform: 0.002089262008666992
Attention Score: 0.001127004623413086
fused
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007522106170654297
Attention linproj: 0.0007491111755371094
QKV Transform: 0.0020880699157714844
Attention Score: 0.0011334419250488281
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007483959197998047
QKV Transform: 0.002088308334350586
Attention Score: 0.001130819320678711
fused
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007505416870117188
QKV Transform: 0.002078533172607422
Attention Score: 0.0011305809020996094
fused
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007729530334472656
QKV Transform: 0.0020897388458251953
Attention Score: 0.0011265277862548828
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007393360137939453
QKV Transform: 0.002100706100463867
Attention Score: 0.0011341571807861328
fused
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007739067077636719
Attention linproj: 0.0007519721984863281
Attention duration (in seconds): 0.0077
Attention throughput (in TFLOP/s): 106.858
MLP_h_4h: 2.3157691955566406
MLP_4h_h: 0.0028116703033447266
MLP_h_4h: 0.003111600875854492
MLP_4h_h: 0.0026314258575439453
MLP_h_4h: 0.003040313720703125
MLP_4h_h: 0.0026030540466308594
MLP_h_4h: 0.0030307769775390625
MLP_4h_h: 0.002603769302368164
MLP_h_4h: 0.003027677536010742
MLP_4h_h: 0.002602100372314453
MLP_h_4h: 0.0030324459075927734
MLP_4h_h: 0.002604246139526367
MLP_h_4h: 0.0030295848846435547
MLP_4h_h: 0.002604961395263672
MLP_h_4h: 0.0030312538146972656
MLP_4h_h: 0.002619028091430664
MLP_h_4h: 0.0030786991119384766
MLP_4h_h: 0.0026330947875976562
MLP_h_4h: 0.0030760765075683594
MLP_4h_h: 0.002630472183227539
MLP_h_4h: 0.003083944320678711
MLP_4h_h: 0.002634763717651367
MLP_h_4h: 0.0030908584594726562
MLP_4h_h: 0.0026426315307617188
MLP_h_4h: 0.00313568115234375
MLP_4h_h: 0.0026361942291259766
MLP_h_4h: 0.003114461898803711
MLP_4h_h: 0.0026407241821289062
MLP_h_4h: 0.0031194686889648438
MLP_4h_h: 0.0026407241821289062
MLP_h_4h: 0.0031213760375976562
MLP_4h_h: 0.0026438236236572266
MLP_h_4h: 0.003114938735961914
MLP_4h_h: 0.002640962600708008
MLP_h_4h: 0.0031201839447021484
MLP_4h_h: 0.002644062042236328
MLP_h_4h: 0.0031137466430664062
MLP_4h_h: 0.002645730972290039
MLP_h_4h: 0.0031256675720214844
MLP_4h_h: 0.002638101577758789
MLP_h_4h: 0.0031194686889648438
MLP_4h_h: 0.0026443004608154297
MLP_h_4h: 0.003123760223388672
MLP_4h_h: 0.0026443004608154297
MLP_h_4h: 0.00312042236328125
MLP_4h_h: 0.002640247344970703
MLP_h_4h: 0.0031325817108154297
MLP_4h_h: 0.0026454925537109375
MLP_h_4h: 0.003117799758911133
MLP_4h_h: 0.0026428699493408203
MLP_h_4h: 0.003112316131591797
MLP_4h_h: 0.0026388168334960938
MLP_h_4h: 0.003084897994995117
MLP_4h_h: 0.002641916275024414
MLP_h_4h: 0.0030837059020996094
MLP_4h_h: 0.002642393112182617
MLP_h_4h: 0.003104686737060547
MLP_4h_h: 0.0026404857635498047
MLP_h_4h: 0.0030868053436279297
MLP_4h_h: 0.002638578414916992
MLP_h_4h: 0.0030922889709472656
MLP_4h_h: 0.002638101577758789
MLP_h_4h: 0.0030889511108398438
MLP_4h_h: 0.0026390552520751953
MLP_h_4h: 0.0031015872955322266
MLP_4h_h: 0.002646207809448242
MLP_h_4h: 0.003116130828857422
MLP_4h_h: 0.0026438236236572266
MLP_h_4h: 0.003117084503173828
MLP_4h_h: 0.0026466846466064453
MLP_h_4h: 0.0031311511993408203
MLP_4h_h: 0.0026471614837646484
MLP_h_4h: 0.003119945526123047
MLP_4h_h: 0.002649545669555664
MLP_h_4h: 0.0031251907348632812
MLP_4h_h: 0.002649068832397461
MLP_h_4h: 0.003119230270385742
MLP_4h_h: 0.0026481151580810547
MLP_h_4h: 0.0031213760375976562
MLP_4h_h: 0.0026569366455078125
MLP_h_4h: 0.0031588077545166016
MLP_4h_h: 0.002656221389770508
MLP_h_4h: 0.0031588077545166016
MLP_4h_h: 0.0026557445526123047
MLP_h_4h: 0.0031523704528808594
MLP_4h_h: 0.002673625946044922
MLP_h_4h: 0.0031228065490722656
MLP_4h_h: 0.0026483535766601562
MLP_h_4h: 0.0031213760375976562
MLP_4h_h: 0.002651214599609375
MLP_h_4h: 0.0031147003173828125
MLP_4h_h: 0.002651214599609375
MLP_h_4h: 0.0031189918518066406
MLP_4h_h: 0.0026514530181884766
MLP_h_4h: 0.003117084503173828
MLP_4h_h: 0.0026497840881347656
MLP_h_4h: 0.0031270980834960938
MLP_4h_h: 0.0026535987854003906
MLP_h_4h: 0.0031206607818603516
MLP_4h_h: 0.0026471614837646484
MLP_h_4h: 0.0031228065490722656
MLP_4h_h: 0.002665996551513672
MLP_h_4h: 0.0031180381774902344
MLP_4h_h: 0.002649068832397461
MLP_h_4h: 0.003122091293334961
MLP_4h_h: 0.002652883529663086
MLP_h_4h: 0.0031321048736572266
MLP_4h_h: 0.00264739990234375
MLP_h_4h: 0.0031228065490722656
MLP_4h_h: 0.002620220184326172
MLP_h_4h: 0.0030908584594726562
MLP_4h_h: 0.0026197433471679688
MLP_h_4h: 0.0030851364135742188
MLP_4h_h: 0.0026235580444335938
MLP_h_4h: 0.0031120777130126953
MLP_4h_h: 0.002620697021484375
MLP_h_4h: 0.0030863285064697266
MLP_4h_h: 0.0026187896728515625
MLP_h_4h: 0.003091096878051758
MLP_4h_h: 0.002628803253173828
MLP_h_4h: 0.0031180381774902344
MLP_4h_h: 0.0026319026947021484
MLP_h_4h: 0.003114461898803711
MLP_4h_h: 0.002622842788696289
MLP_h_4h: 0.003092050552368164
MLP_4h_h: 0.0026330947875976562
MLP_h_4h: 0.0030977725982666016
MLP_4h_h: 0.002622842788696289
MLP_h_4h: 0.0030884742736816406
MLP_4h_h: 0.0026192665100097656
MLP_h_4h: 0.0031032562255859375
MLP_4h_h: 0.002621889114379883
MLP_h_4h: 0.003104686737060547
MLP_4h_h: 0.0026230812072753906
MLP_h_4h: 0.003091096878051758
MLP_4h_h: 0.0026214122772216797
MLP_h_4h: 0.0030951499938964844
MLP_4h_h: 0.002626180648803711
MLP_h_4h: 0.003088712692260742
MLP_4h_h: 0.0026237964630126953
MLP_h_4h: 0.003098011016845703
MLP_4h_h: 0.0026192665100097656
MLP_h_4h: 0.0030918121337890625
MLP_4h_h: 0.0026214122772216797
MLP_h_4h: 0.005360126495361328
MLP_4h_h: 0.0026535987854003906
MLP_h_4h: 0.0031168460845947266
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.0030808448791503906
MLP_4h_h: 0.0026187896728515625
MLP_h_4h: 0.0030641555786132812
MLP_4h_h: 0.0026192665100097656
MLP_h_4h: 0.0030608177185058594
MLP_4h_h: 0.0026178359985351562
MLP_h_4h: 0.0030624866485595703
MLP_4h_h: 0.0026197433471679688
MLP_h_4h: 0.003067493438720703
MLP_4h_h: 0.0026183128356933594
MLP_h_4h: 0.0030617713928222656
MLP_4h_h: 0.002618551254272461
MLP_h_4h: 0.0030641555786132812
MLP_4h_h: 0.0026144981384277344
MLP_h_4h: 0.003058910369873047
MLP_4h_h: 0.0026178359985351562
MLP_h_4h: 0.0030622482299804688
MLP_4h_h: 0.002619028091430664
MLP_h_4h: 0.0030651092529296875
MLP_4h_h: 0.0026183128356933594
MLP_h_4h: 0.003060579299926758
MLP_4h_h: 0.0026216506958007812
MLP_h_4h: 0.0030982494354248047
MLP_4h_h: 0.0026273727416992188
MLP_h_4h: 0.003091573715209961
MLP_4h_h: 0.0026268959045410156
MLP_h_4h: 0.0030944347381591797
MLP_4h_h: 0.0026242733001708984
MLP_h_4h: 0.003088712692260742
MLP_4h_h: 0.0026247501373291016
MLP_h_4h: 0.0031037330627441406
MLP_4h_h: 0.002624988555908203
MLP_h_4h: 0.003089427947998047
MLP_4h_h: 0.002624988555908203
MLP_h_4h: 0.003117084503173828
MLP_4h_h: 0.0026373863220214844
MLP_h_4h: 0.0030901432037353516
MLP_4h_h: 0.002628803253173828
MLP_h_4h: 0.0030891895294189453
MLP_4h_h: 0.0026314258575439453
MLP_h_4h: 0.0031032562255859375
MLP_4h_h: 0.002647876739501953
MLP_h_4h: 0.0031027793884277344
MLP_4h_h: 0.0026273727416992188
MLP_h_4h: 0.0030968189239501953
MLP_4h_h: 0.0026230812072753906
MLP_h_4h: 0.0030922889709472656
MLP_4h_h: 0.002622365951538086
MLP_h_4h: 0.0031015872955322266
MLP_4h_h: 0.0026242733001708984
MLP_h_4h: 0.003092050552368164
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.0030906200408935547
MLP_4h_h: 0.0026276111602783203
MLP_h_4h: 0.0030875205993652344
MLP_4h_h: 0.0026276111602783203
MLP_h_4h: 0.0030875205993652344
MLP_4h_h: 0.0026273727416992188
MLP_h_4h: 0.0030918121337890625
MLP_4h_h: 0.0026221275329589844
MLP_h_4h: 0.0030891895294189453
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.0030908584594726562
MLP_4h_h: 0.0026285648345947266
MLP_h_4h: 0.0031180381774902344
MLP_4h_h: 0.0026416778564453125
MLP_h_4h: 0.00312042236328125
MLP_4h_h: 0.0026314258575439453
MLP_h_4h: 0.0031256675720214844
MLP_4h_h: 0.002633810043334961
MLP_h_4h: 0.003101348876953125
MLP_4h_h: 0.002634763717651367
MLP_h_4h: 0.003087759017944336
MLP_4h_h: 0.002628326416015625
MLP_h_4h: 0.003083944320678711
MLP_4h_h: 0.0026290416717529297
MLP_h_4h: 0.003091096878051758
MLP_4h_h: 0.00262451171875
MLP_h_4h: 0.0030965805053710938
MLP_4h_h: 0.0026268959045410156
MLP_h_4h: 0.0030922889709472656
MLP_4h_h: 0.00262451171875
MLP_h_4h: 0.003088235855102539
MLP_4h_h: 0.002625703811645508
MLP_h_4h: 0.003088712692260742
MLP_4h_h: 0.002626180648803711
MLP_h_4h: 0.0030879974365234375
MLP_4h_h: 0.0026276111602783203
MLP_h_4h: 0.003093242645263672
MLP_4h_h: 0.0026268959045410156
MLP_h_4h: 0.003080606460571289
MLP_4h_h: 0.0026307106018066406
MLP_h_4h: 0.0031201839447021484
MLP_4h_h: 0.002634286880493164
MLP_h_4h: 0.003126859664916992
MLP_4h_h: 0.0026323795318603516
MLP_h_4h: 0.003121614456176758
MLP_4h_h: 0.0026330947875976562
MLP_h_4h: 0.003126382827758789
MLP_4h_h: 0.00263214111328125
MLP_h_4h: 0.0031294822692871094
MLP_4h_h: 0.002632617950439453
MLP_h_4h: 0.003125905990600586
MLP_4h_h: 0.00263214111328125
MLP_h_4h: 0.003122568130493164
MLP_4h_h: 0.002633333206176758
MLP_h_4h: 0.003119230270385742
MLP_4h_h: 0.0026340484619140625
MLP_h_4h: 0.0031185150146484375
MLP_4h_h: 0.0026383399963378906
MLP_h_4h: 0.0031251907348632812
MLP_4h_h: 0.0026357173919677734
MLP_h_4h: 0.0031287670135498047
MLP_4h_h: 0.0026366710662841797
MLP_h_4h: 0.0031194686889648438
MLP_4h_h: 0.002636432647705078
MLP_h_4h: 0.003125905990600586
MLP_4h_h: 0.0026323795318603516
MLP_h_4h: 0.003126382827758789
MLP_4h_h: 0.002629995346069336
MLP_h_4h: 0.0030946731567382812
MLP_4h_h: 0.0026242733001708984
MLP_h_4h: 0.003090381622314453
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.0030875205993652344
MLP_4h_h: 0.002627134323120117
MLP_h_4h: 0.003100156784057617
MLP_4h_h: 0.0026285648345947266
MLP_h_4h: 0.003088235855102539
MLP_4h_h: 0.0026268959045410156
MLP_h_4h: 0.0030908584594726562
MLP_4h_h: 0.002630949020385742
MLP_h_4h: 0.003086566925048828
MLP_4h_h: 0.0026254653930664062
MLP_h_4h: 0.003093719482421875
MLP_4h_h: 0.0026276111602783203
MLP_h_4h: 0.003093242645263672
MLP_4h_h: 0.0026433467864990234
MLP_h_4h: 0.003085613250732422
MLP_4h_h: 0.002625703811645508
MLP_h_4h: 0.0030908584594726562
MLP_4h_h: 0.0026292800903320312
MLP_h_4h: 0.0030884742736816406
MLP_4h_h: 0.0026273727416992188
MLP_h_4h: 0.003088712692260742
MLP_4h_h: 0.002630949020385742
MLP_h_4h: 0.0030889511108398438
MLP_4h_h: 0.0026280879974365234
MLP_h_4h: 0.0031015872955322266
MLP_4h_h: 0.0026433467864990234
MLP_h_4h: 0.003093719482421875
MLP_4h_h: 0.0026276111602783203
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 214.555
LN1: 0.004517793655395508
QKV Transform: 0.0021963119506835938
Attention Score: 0.0012006759643554688
fused
Attention Softmax: 0.0027947425842285156
Attention Dropout: 0.00008893013000488281
Attention Over Value: 0.0012366771697998047
Attention linproj: 0.0007700920104980469
Post-attention Dropout: 0.07114624977111816
Post-attention residual: 0.003099203109741211
LN2: 0.00017213821411132812
MLP_h_4h: 0.003063201904296875
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.0019068717956542969
Attention layer time: 0.09533929824829102
LN1: 0.0001342296600341797
QKV Transform: 0.0020978450775146484
Attention Score: 0.0022203922271728516
fused
Attention Softmax: 0.0026874542236328125
Attention Dropout: 0.00006699562072753906
Attention Over Value: 0.0007646083831787109
Attention linproj: 0.0007505416870117188
Post-attention Dropout: 0.00039505958557128906
Post-attention residual: 0.00012922286987304688
LN2: 0.00012922286987304688
MLP_h_4h: 0.003023386001586914
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.015781402587890625
LN1: 0.0001342296600341797
QKV Transform: 0.002077341079711914
Attention Score: 0.0011394023895263672
fused
Attention Softmax: 0.0026743412017822266
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007491111755371094
Post-attention Dropout: 0.0003936290740966797
Post-attention residual: 0.0001285076141357422
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030241012573242188
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014619588851928711
LN1: 0.0001277923583984375
QKV Transform: 0.0020792484283447266
Attention Score: 0.0011324882507324219
fused
Attention Softmax: 0.0026695728302001953
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.0001304149627685547
LN2: 0.00013017654418945312
MLP_h_4h: 0.003034830093383789
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.0003829002380371094
Attention layer time: 0.014604330062866211
LN1: 0.0001366138458251953
QKV Transform: 0.0020749568939208984
Attention Score: 0.0011458396911621094
fused
Attention Softmax: 0.002669811248779297
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007445812225341797
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.00012731552124023438
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030248165130615234
MLP_4h_h: 0.00260162353515625
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.01459813117980957
LN1: 0.000125885009765625
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011436939239501953
fused
Attention Softmax: 0.002672910690307617
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007483959197998047
Post-attention Dropout: 0.0003848075866699219
Post-attention residual: 0.0001308917999267578
LN2: 0.00012803077697753906
MLP_h_4h: 0.003027677536010742
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014594078063964844
LN1: 0.00012445449829101562
QKV Transform: 0.002077341079711914
Attention Score: 0.0011248588562011719
fused
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.0003781318664550781
Post-attention residual: 0.00012969970703125
LN2: 0.00012874603271484375
MLP_h_4h: 0.00302886962890625
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014527559280395508
LN1: 0.0001246929168701172
QKV Transform: 0.002070903778076172
Attention Score: 0.0011262893676757812
fused
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007450580596923828
Post-attention Dropout: 0.0003781318664550781
Post-attention residual: 0.00012731552124023438
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030181407928466797
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.00038123130798339844
Attention layer time: 0.014533042907714844
LN1: 0.0001251697540283203
QKV Transform: 0.0020813941955566406
Attention Score: 0.0011301040649414062
fused
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007462501525878906
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.0001270771026611328
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030219554901123047
MLP_4h_h: 0.0025985240936279297
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014554738998413086
LN1: 0.0001239776611328125
QKV Transform: 0.0020689964294433594
Attention Score: 0.00112152099609375
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.00013518333435058594
LN2: 0.00014472007751464844
MLP_h_4h: 0.0030248165130615234
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003769397735595703
Attention layer time: 0.014526844024658203
LN1: 0.00012373924255371094
QKV Transform: 0.002068042755126953
Attention Score: 0.0011265277862548828
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.00074005126953125
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012803077697753906
LN2: 0.00012540817260742188
MLP_h_4h: 0.0030205249786376953
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.014510393142700195
LN1: 0.00012564659118652344
QKV Transform: 0.0020728111267089844
Attention Score: 0.0011255741119384766
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.00012922286987304688
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030202865600585938
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.014525175094604492
LN1: 0.00012373924255371094
QKV Transform: 0.002067089080810547
Attention Score: 0.0011310577392578125
fused
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007901191711425781
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.00037860870361328125
Post-attention residual: 0.00012803077697753906
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.002596139907836914
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.01456141471862793
LN1: 0.00012421607971191406
QKV Transform: 0.0020673274993896484
Attention Score: 0.001125335693359375
fused
Attention Softmax: 0.0026390552520751953
Attention Dropout: 0.00006914138793945312
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007383823394775391
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.0001277923583984375
LN2: 0.0001266002655029297
MLP_h_4h: 0.003016948699951172
MLP_4h_h: 0.0025997161865234375
Post-MLP residual: 0.0003826618194580078
Attention layer time: 0.01452183723449707
LN1: 0.00012540817260742188
QKV Transform: 0.002073049545288086
Attention Score: 0.0011250972747802734
fused
Attention Softmax: 0.0026412010192871094
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007448196411132812
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.0001289844512939453
LN2: 0.00012755393981933594
MLP_h_4h: 0.003025531768798828
MLP_4h_h: 0.00261688232421875
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014548540115356445
LN1: 0.00012493133544921875
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.00012755393981933594
LN2: 0.00012493133544921875
MLP_h_4h: 0.0030167102813720703
MLP_4h_h: 0.0025985240936279297
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014498710632324219
LN1: 0.0001266002655029297
QKV Transform: 0.002069234848022461
Attention Score: 0.0011210441589355469
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007464885711669922
Post-attention Dropout: 0.000377655029296875
Post-attention residual: 0.00012874603271484375
LN2: 0.00012636184692382812
MLP_h_4h: 0.003019094467163086
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014507770538330078
LN1: 0.00012731552124023438
QKV Transform: 0.0020728111267089844
Attention Score: 0.0011205673217773438
fused
Attention Softmax: 0.0026397705078125
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.00039005279541015625
Post-attention residual: 0.00012826919555664062
LN2: 0.00012755393981933594
MLP_h_4h: 0.003022909164428711
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014518260955810547
LN1: 0.0001239776611328125
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011258125305175781
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0003783702850341797
Post-attention residual: 0.0001285076141357422
LN2: 0.00012683868408203125
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.002597332000732422
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014525413513183594
LN1: 0.0001246929168701172
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011146068572998047
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007462501525878906
Post-attention Dropout: 0.00038051605224609375
Post-attention residual: 0.00012946128845214844
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030176639556884766
MLP_4h_h: 0.0025975704193115234
Post-MLP residual: 0.0003771781921386719
Attention layer time: 0.014528512954711914
LN1: 0.00012421607971191406
QKV Transform: 0.002079010009765625
Attention Score: 0.001125335693359375
fused
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007405281066894531
Post-attention Dropout: 0.00038504600524902344
Post-attention residual: 0.0001289844512939453
LN2: 0.0001270771026611328
MLP_h_4h: 0.003023386001586914
MLP_4h_h: 0.0025963783264160156
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014524221420288086
LN1: 0.00012421607971191406
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011255741119384766
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007574558258056641
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.00012755393981933594
LN2: 0.00012564659118652344
MLP_h_4h: 0.003020048141479492
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014521598815917969
LN1: 0.00012540817260742188
QKV Transform: 0.0020723342895507812
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007574558258056641
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.00012969970703125
LN2: 0.00012564659118652344
MLP_h_4h: 0.0030367374420166016
MLP_4h_h: 0.0025970935821533203
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014543294906616211
LN1: 0.00012350082397460938
QKV Transform: 0.0020673274993896484
Attention Score: 0.0011241436004638672
fused
Attention Softmax: 0.0026397705078125
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.00074005126953125
Post-attention Dropout: 0.0003800392150878906
Post-attention residual: 0.0001285076141357422
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030286312103271484
MLP_4h_h: 0.0025963783264160156
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014501571655273438
LN1: 0.00012421607971191406
QKV Transform: 0.0020673274993896484
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.000732421875
Post-attention Dropout: 0.00038051605224609375
Post-attention residual: 0.00012755393981933594
LN2: 0.00012564659118652344
MLP_h_4h: 0.0030171871185302734
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.014522790908813477
LN1: 0.000125885009765625
QKV Transform: 0.002073049545288086
Attention Score: 0.0011272430419921875
fused
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.00012874603271484375
LN2: 0.0001277923583984375
MLP_h_4h: 0.003026247024536133
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014528989791870117
LN1: 0.0001246929168701172
QKV Transform: 0.0020647048950195312
Attention Score: 0.0011403560638427734
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.00012731552124023438
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030181407928466797
MLP_4h_h: 0.0025959014892578125
Post-MLP residual: 0.00037670135498046875
Attention layer time: 0.014510631561279297
LN1: 0.00012493133544921875
QKV Transform: 0.0020668506622314453
Attention Score: 0.001140594482421875
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007469654083251953
Post-attention Dropout: 0.0003800392150878906
Post-attention residual: 0.00012874603271484375
LN2: 0.0001266002655029297
MLP_h_4h: 0.003025054931640625
MLP_4h_h: 0.0025949478149414062
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014537811279296875
LN1: 0.00012636184692382812
QKV Transform: 0.002070903778076172
Attention Score: 0.001125335693359375
fused
Attention Softmax: 0.002639293670654297
Attention Dropout: 0.00005674362182617188
Attention Over Value: 0.0007798671722412109
Attention linproj: 0.0007550716400146484
Post-attention Dropout: 0.00038051605224609375
Post-attention residual: 0.00012803077697753906
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030221939086914062
MLP_4h_h: 0.002596139907836914
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014555931091308594
LN1: 0.00012421607971191406
QKV Transform: 0.0020666122436523438
Attention Score: 0.001127004623413086
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.0003771781921386719
Post-attention residual: 0.00012803077697753906
LN2: 0.000125885009765625
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.014508247375488281
LN1: 0.0001392364501953125
QKV Transform: 0.0020666122436523438
Attention Score: 0.0011208057403564453
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007472038269042969
Post-attention Dropout: 0.0003781318664550781
Post-attention residual: 0.0001285076141357422
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.0003867149353027344
Attention layer time: 0.014540910720825195
LN1: 0.00012755393981933594
QKV Transform: 0.002073526382446289
Attention Score: 0.0011258125305175781
fused
Attention Softmax: 0.002639293670654297
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.00013136863708496094
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030221939086914062
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.0003769397735595703
Attention layer time: 0.014510154724121094
LN1: 0.0001239776611328125
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011250972747802734
fused
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007691383361816406
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.00012731552124023438
LN2: 0.00012636184692382812
MLP_h_4h: 0.00301361083984375
MLP_4h_h: 0.0025970935821533203
Post-MLP residual: 0.0003821849822998047
Attention layer time: 0.014518260955810547
LN1: 0.00012540817260742188
QKV Transform: 0.0020699501037597656
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007474422454833984
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.00014162063598632812
LN2: 0.0001289844512939453
MLP_h_4h: 0.003022909164428711
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014563560485839844
LN1: 0.00012445449829101562
QKV Transform: 0.0020673274993896484
Attention Score: 0.001123189926147461
fused
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.0003840923309326172
Post-attention residual: 0.00012874603271484375
LN2: 0.0001285076141357422
MLP_h_4h: 0.0030236244201660156
MLP_4h_h: 0.0025954246520996094
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.014512062072753906
LN1: 0.00012350082397460938
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011196136474609375
fused
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007393360137939453
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.00012803077697753906
LN2: 0.00012564659118652344
MLP_h_4h: 0.0030171871185302734
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014493227005004883
LN1: 0.0001246929168701172
QKV Transform: 0.002072572708129883
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.0003833770751953125
Post-attention residual: 0.00012803077697753906
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030159950256347656
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.014526605606079102
LN1: 0.00012421607971191406
QKV Transform: 0.00206756591796875
Attention Score: 0.0011191368103027344
fused
Attention Softmax: 0.0026390552520751953
Attention Dropout: 0.00006604194641113281
Attention Over Value: 0.0007522106170654297
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.0003809928894042969
Post-attention residual: 0.00012826919555664062
LN2: 0.000125885009765625
MLP_h_4h: 0.0030164718627929688
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014531612396240234
LN1: 0.00012421607971191406
QKV Transform: 0.002071380615234375
Attention Score: 0.001119375228881836
fused
Attention Softmax: 0.0026397705078125
Attention Dropout: 0.00007390975952148438
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.00012755393981933594
LN2: 0.00012612342834472656
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.002611398696899414
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014538764953613281
LN1: 0.0001251697540283203
QKV Transform: 0.0020606517791748047
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.0003838539123535156
Post-attention residual: 0.00012922286987304688
LN2: 0.00012683868408203125
MLP_h_4h: 0.003039836883544922
MLP_4h_h: 0.002597332000732422
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014518022537231445
LN1: 0.0001239776611328125
QKV Transform: 0.0020666122436523438
Attention Score: 0.0011186599731445312
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.0001285076141357422
LN2: 0.0001251697540283203
MLP_h_4h: 0.0030159950256347656
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014495134353637695
LN1: 0.00012540817260742188
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007464885711669922
Post-attention Dropout: 0.00037860870361328125
Post-attention residual: 0.00012803077697753906
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030202865600585938
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014511585235595703
LN1: 0.000125885009765625
QKV Transform: 0.0020720958709716797
Attention Score: 0.0011227130889892578
fused
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.0003905296325683594
Post-attention residual: 0.00012826919555664062
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030241012573242188
MLP_4h_h: 0.0025975704193115234
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.01452016830444336
LN1: 0.0001239776611328125
QKV Transform: 0.002068042755126953
Attention Score: 0.0011279582977294922
fused
Attention Softmax: 0.002640247344970703
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0003800392150878906
Post-attention residual: 0.00012803077697753906
LN2: 0.000125885009765625
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014508724212646484
LN1: 0.00012350082397460938
QKV Transform: 0.002084016799926758
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007467269897460938
Post-attention Dropout: 0.0003979206085205078
Post-attention residual: 0.00012874603271484375
LN2: 0.00012683868408203125
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.01454472541809082
LN1: 0.0001251697540283203
QKV Transform: 0.0020797252655029297
Attention Score: 0.0011157989501953125
fused
Attention Softmax: 0.0026395320892333984
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.0003857612609863281
Post-attention residual: 0.00013017654418945312
LN2: 0.0001277923583984375
MLP_h_4h: 0.003023386001586914
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014518976211547852
LN1: 0.0001239776611328125
QKV Transform: 0.0020678043365478516
Attention Score: 0.001125335693359375
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007548332214355469
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.00012683868408203125
LN2: 0.00012493133544921875
MLP_h_4h: 0.003017902374267578
MLP_4h_h: 0.0025963783264160156
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014513969421386719
LN1: 0.00012564659118652344
QKV Transform: 0.002068042755126953
Attention Score: 0.001123189926147461
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007569789886474609
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.00012826919555664062
LN2: 0.00012612342834472656
MLP_h_4h: 0.0030252933502197266
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.00037550926208496094
Attention layer time: 0.014524698257446289
LN1: 0.00012373924255371094
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011279582977294922
fused
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.00037741661071777344
Post-attention residual: 0.00012922286987304688
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030303001403808594
MLP_4h_h: 0.0025997161865234375
Post-MLP residual: 0.00038123130798339844
Attention layer time: 0.014541864395141602
LN1: 0.00012421607971191406
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011293888092041016
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007352828979492188
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.00012803077697753906
LN2: 0.00012564659118652344
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0025975704193115234
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014506101608276367
LN1: 0.00012540817260742188
QKV Transform: 0.0020723342895507812
Attention Score: 0.0011243820190429688
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007455348968505859
Post-attention Dropout: 0.0003829002380371094
Post-attention residual: 0.00012826919555664062
LN2: 0.0001266002655029297
MLP_h_4h: 0.003024578094482422
MLP_4h_h: 0.002603292465209961
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014526844024658203
LN1: 0.00012445449829101562
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011394023895263672
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.0003764629364013672
Post-attention residual: 0.00012803077697753906
LN2: 0.0001251697540283203
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.0025975704193115234
Post-MLP residual: 0.0003764629364013672
Attention layer time: 0.014506101608276367
LN1: 0.00012493133544921875
QKV Transform: 0.002067089080810547
Attention Score: 0.0011432170867919922
fused
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007519721984863281
Attention linproj: 0.000759124755859375
Post-attention Dropout: 0.0003864765167236328
Post-attention residual: 0.00012826919555664062
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030202865600585938
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.0003771781921386719
Attention layer time: 0.01455068588256836
LN1: 0.00012564659118652344
QKV Transform: 0.0020728111267089844
Attention Score: 0.00112152099609375
fused
Attention Softmax: 0.0026395320892333984
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.0003840923309326172
Post-attention residual: 0.00012874603271484375
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030252933502197266
MLP_4h_h: 0.002597332000732422
Post-MLP residual: 0.00037550926208496094
Attention layer time: 0.014522552490234375
LN1: 0.00012350082397460938
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011186599731445312
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.000377655029296875
Post-attention residual: 0.00012826919555664062
LN2: 0.00012564659118652344
MLP_h_4h: 0.003019094467163086
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014505863189697266
LN1: 0.00012445449829101562
QKV Transform: 0.0020716190338134766
Attention Score: 0.0011227130889892578
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007491111755371094
Post-attention Dropout: 0.0003948211669921875
Post-attention residual: 0.00012969970703125
LN2: 0.00012731552124023438
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.00037670135498046875
Attention layer time: 0.01452326774597168
LN1: 0.0001430511474609375
QKV Transform: 0.0020761489868164062
Attention Score: 0.0011243820190429688
fused
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.00037741661071777344
Post-attention residual: 0.00013017654418945312
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030260086059570312
MLP_4h_h: 0.0025975704193115234
Post-MLP residual: 0.0003762245178222656
Attention layer time: 0.014527320861816406
LN1: 0.00012421607971191406
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.0026397705078125
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007445812225341797
Post-attention Dropout: 0.0003800392150878906
Post-attention residual: 0.0001277923583984375
LN2: 0.000125885009765625
MLP_h_4h: 0.003020048141479492
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014507293701171875
LN1: 0.00012540817260742188
QKV Transform: 0.0020661354064941406
Attention Score: 0.0011229515075683594
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007469654083251953
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.00012803077697753906
LN2: 0.00014591217041015625
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.002600431442260742
Post-MLP residual: 0.0003757476806640625
Attention layer time: 0.014556169509887695
LN1: 0.00012421607971191406
QKV Transform: 0.002067089080810547
Attention Score: 0.0011210441589355469
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007348060607910156
Post-attention Dropout: 0.0003771781921386719
Post-attention residual: 0.00012803077697753906
LN2: 0.00014638900756835938
MLP_h_4h: 0.003026247024536133
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.014522790908813477
LN1: 0.00012540817260742188
QKV Transform: 0.0020704269409179688
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.00012755393981933594
LN2: 0.00012493133544921875
MLP_h_4h: 0.0030202865600585938
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.014522075653076172
LN1: 0.0001251697540283203
QKV Transform: 0.0020737648010253906
Attention Score: 0.0011196136474609375
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.00012803077697753906
LN2: 0.00012755393981933594
MLP_h_4h: 0.003019571304321289
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.0003757476806640625
Attention layer time: 0.01451253890991211
LN1: 0.00012373924255371094
QKV Transform: 0.0020694732666015625
Attention Score: 0.0011265277862548828
fused
Attention Softmax: 0.0026395320892333984
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.0003771781921386719
Post-attention residual: 0.0001277923583984375
LN2: 0.00012421607971191406
MLP_h_4h: 0.0030198097229003906
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014521360397338867
LN1: 0.00012421607971191406
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00007414817810058594
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.00012946128845214844
LN2: 0.00012731552124023438
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0026102066040039062
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.01455831527709961
LN1: 0.00012564659118652344
QKV Transform: 0.002074003219604492
Attention Score: 0.00112152099609375
fused
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005292892456054688
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007462501525878906
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.00012922286987304688
LN2: 0.00012731552124023438
MLP_h_4h: 0.003023386001586914
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014534950256347656
LN1: 0.0001227855682373047
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.000766754150390625
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.0003800392150878906
Post-attention residual: 0.00012755393981933594
LN2: 0.00012493133544921875
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0026018619537353516
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014521360397338867
LN1: 0.00012636184692382812
QKV Transform: 0.002068758010864258
Attention Score: 0.0011208057403564453
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007462501525878906
Post-attention Dropout: 0.0003924369812011719
Post-attention residual: 0.00012969970703125
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030202865600585938
MLP_4h_h: 0.0025997161865234375
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014519929885864258
LN1: 0.000125885009765625
QKV Transform: 0.002074718475341797
Attention Score: 0.0011172294616699219
fused
Attention Softmax: 0.002638578414916992
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.0003955364227294922
Post-attention residual: 0.0001285076141357422
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030260086059570312
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.0003960132598876953
Attention layer time: 0.014546632766723633
LN1: 0.00012373924255371094
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011258125305175781
fused
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.00012755393981933594
LN2: 0.0001246929168701172
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014513015747070312
LN1: 0.0001239776611328125
QKV Transform: 0.002087116241455078
Attention Score: 0.0011258125305175781
fused
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007460117340087891
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012874603271484375
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030198097229003906
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014530181884765625
LN1: 0.00013637542724609375
QKV Transform: 0.002064943313598633
Attention Score: 0.0011250972747802734
fused
Attention Softmax: 0.002640247344970703
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007410049438476562
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.00012922286987304688
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030264854431152344
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.014514923095703125
LN1: 0.00012350082397460938
QKV Transform: 0.002069234848022461
Attention Score: 0.0011248588562011719
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007483959197998047
Post-attention Dropout: 0.000377655029296875
Post-attention residual: 0.00012731552124023438
LN2: 0.000125885009765625
MLP_h_4h: 0.003017902374267578
MLP_4h_h: 0.002600431442260742
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014524698257446289
LN1: 0.0001251697540283203
QKV Transform: 0.002061605453491211
Attention Score: 0.001123666763305664
fused
Attention Softmax: 0.0026428699493408203
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007393360137939453
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012803077697753906
LN2: 0.00012540817260742188
MLP_h_4h: 0.003033876419067383
MLP_4h_h: 0.002596616744995117
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014508485794067383
LN1: 0.0001246929168701172
QKV Transform: 0.0020699501037597656
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.00037550926208496094
Post-attention residual: 0.00012826919555664062
LN2: 0.00012612342834472656
MLP_h_4h: 0.003030538558959961
MLP_4h_h: 0.002595663070678711
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014508485794067383
LN1: 0.00012350082397460938
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011370182037353516
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.00038361549377441406
Post-attention residual: 0.00012731552124023438
LN2: 0.00012636184692382812
MLP_h_4h: 0.003024578094482422
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.014537572860717773
LN1: 0.00012540817260742188
QKV Transform: 0.0020759105682373047
Attention Score: 0.0011222362518310547
fused
Attention Softmax: 0.0026397705078125
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007443428039550781
Post-attention Dropout: 0.0003838539123535156
Post-attention residual: 0.0001289844512939453
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030133724212646484
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.01451420783996582
LN1: 0.00012493133544921875
QKV Transform: 0.0020668506622314453
Attention Score: 0.0011246204376220703
fused
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007445812225341797
Post-attention Dropout: 0.00037789344787597656
Post-attention residual: 0.0001277923583984375
LN2: 0.00012564659118652344
MLP_h_4h: 0.003021240234375
MLP_4h_h: 0.0026018619537353516
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.01455068588256836
LN1: 0.00012445449829101562
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011246204376220703
fused
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007464885711669922
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.00012803077697753906
LN2: 0.00012612342834472656
MLP_h_4h: 0.003019094467163086
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.00038313865661621094
Attention layer time: 0.014542579650878906
LN1: 0.0001251697540283203
QKV Transform: 0.0020706653594970703
Attention Score: 0.00112152099609375
fused
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012969970703125
LN2: 0.0001285076141357422
MLP_h_4h: 0.0030281543731689453
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014540433883666992
LN1: 0.0001251697540283203
QKV Transform: 0.002070903778076172
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.00012803077697753906
LN2: 0.00012493133544921875
MLP_h_4h: 0.0030183792114257812
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.01453089714050293
LN1: 0.00012683868408203125
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011262893676757812
fused
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007481575012207031
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012969970703125
LN2: 0.00012755393981933594
MLP_h_4h: 0.003020763397216797
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014513731002807617
LN1: 0.00012683868408203125
QKV Transform: 0.0020737648010253906
Attention Score: 0.0011169910430908203
fused
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.00012969970703125
LN2: 0.00012826919555664062
MLP_h_4h: 0.003025531768798828
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.0145111083984375
LN1: 0.00012540817260742188
QKV Transform: 0.00206756591796875
Attention Score: 0.0011246204376220703
fused
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005340576171875000
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007445812225341797
Post-attention Dropout: 0.00037741661071777344
Post-attention residual: 0.00012803077697753906
LN2: 0.00012540817260742188
MLP_h_4h: 0.003016948699951172
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.01450800895690918
LN1: 0.00012493133544921875
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011267662048339844
fused
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007474422454833984
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.0001285076141357422
LN2: 0.00012826919555664062
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.0003752708435058594
Attention layer time: 0.014521121978759766
LN1: 0.00012493133544921875
QKV Transform: 0.002068042755126953
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.0001285076141357422
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030241012573242188
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.01450657844543457
LN1: 0.00012421607971191406
QKV Transform: 0.0020699501037597656
Attention Score: 0.0011265277862548828
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.0003781318664550781
Post-attention residual: 0.0001277923583984375
LN2: 0.00012493133544921875
MLP_h_4h: 0.003021717071533203
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014517545700073242
LN1: 0.00012493133544921875
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.000751495361328125
Attention linproj: 0.0007474422454833984
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.0001285076141357422
LN2: 0.0001270771026611328
MLP_h_4h: 0.003017902374267578
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.01450657844543457
LN1: 0.00012350082397460938
QKV Transform: 0.002068042755126953
Attention Score: 0.0011241436004638672
fused
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00006365776062011719
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.0001270771026611328
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030159950256347656
MLP_4h_h: 0.0026102066040039062
Post-MLP residual: 0.00037741661071777344
Attention layer time: 0.014515161514282227
LN1: 0.0001239776611328125
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011296272277832031
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00007128715515136719
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007474422454833984
Post-attention Dropout: 0.0003960132598876953
Post-attention residual: 0.00012874603271484375
LN2: 0.00012731552124023438
MLP_h_4h: 0.003020763397216797
MLP_4h_h: 0.0026209354400634766
Post-MLP residual: 0.0003826618194580078
Attention layer time: 0.014600992202758789
LN1: 0.0001251697540283203
QKV Transform: 0.0020749568939208984
Attention Score: 0.001123666763305664
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007443428039550781
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.00012803077697753906
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030279159545898438
MLP_4h_h: 0.0026063919067382812
Post-MLP residual: 0.00037598609924316406
Attention layer time: 0.014526844024658203
LN1: 0.00012445449829101562
QKV Transform: 0.0020673274993896484
Attention Score: 0.0011284351348876953
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005578994750976562
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.00037741661071777344
Post-attention residual: 0.0001277923583984375
LN2: 0.00012612342834472656
MLP_h_4h: 0.003019094467163086
MLP_4h_h: 0.002610445022583008
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014531850814819336
LN1: 0.00012493133544921875
QKV Transform: 0.002068758010864258
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007596015930175781
Post-attention Dropout: 0.0003809928894042969
Post-attention residual: 0.0001285076141357422
LN2: 0.00012540817260742188
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.0025997161865234375
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014523029327392578
LN1: 0.00012683868408203125
QKV Transform: 0.002074718475341797
Attention Score: 0.0011174678802490234
fused
Attention Softmax: 0.0026383399963378906
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007631778717041016
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.00039839744567871094
Post-attention residual: 0.00012922286987304688
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030264854431152344
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.014539003372192383
LN1: 0.0001239776611328125
QKV Transform: 0.00206756591796875
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007448196411132812
Post-attention Dropout: 0.0003764629364013672
Post-attention residual: 0.0001289844512939453
LN2: 0.000125885009765625
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014525413513183594
LN1: 0.0001246929168701172
QKV Transform: 0.0020711421966552734
Attention Score: 0.0011239051818847656
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007464885711669922
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.00012922286987304688
LN2: 0.00012564659118652344
MLP_h_4h: 0.0030214786529541016
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.0003762245178222656
Attention layer time: 0.014518022537231445
LN1: 0.0001304149627685547
QKV Transform: 0.0020775794982910156
Attention Score: 0.0011208057403564453
fused
Attention Softmax: 0.0026395320892333984
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.00037741661071777344
Post-attention residual: 0.0001289844512939453
LN2: 0.0001285076141357422
MLP_h_4h: 0.003026247024536133
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.00037741661071777344
Attention layer time: 0.014522552490234375
LN1: 0.00012493133544921875
QKV Transform: 0.0020699501037597656
Attention Score: 0.001125335693359375
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.00012803077697753906
LN2: 0.0001251697540283203
MLP_h_4h: 0.0030205249786376953
MLP_4h_h: 0.0026035308837890625
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014521360397338867
LN1: 0.00012564659118652344
QKV Transform: 0.002081632614135742
Attention Score: 0.0011255741119384766
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007638931274414062
Attention linproj: 0.0007474422454833984
Post-attention Dropout: 0.00038051605224609375
Post-attention residual: 0.00012826919555664062
LN2: 0.0001251697540283203
MLP_h_4h: 0.0030210018157958984
MLP_4h_h: 0.002600431442260742
Post-MLP residual: 0.00037479400634765625
Attention layer time: 0.01456141471862793
LN1: 0.00012421607971191406
QKV Transform: 0.002069711685180664
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.002640247344970703
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.00037741661071777344
Post-attention residual: 0.0001277923583984375
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030269622802734375
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014514684677124023
LN1: 0.0001246929168701172
QKV Transform: 0.0020711421966552734
Attention Score: 0.001131296157836914
fused
Attention Softmax: 0.002640247344970703
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.0001285076141357422
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030236244201660156
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014521360397338867
LN1: 0.00012564659118652344
QKV Transform: 0.0020639896392822266
Attention Score: 0.0011281967163085938
fused
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.0003848075866699219
Post-attention residual: 0.0001285076141357422
LN2: 0.00012826919555664062
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.0026030540466308594
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.014531373977661133
LN1: 0.00012421607971191406
QKV Transform: 0.002069711685180664
Attention Score: 0.0011200904846191406
fused
Attention Softmax: 0.002638101577758789
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007710456848144531
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012803077697753906
LN2: 0.00012612342834472656
MLP_h_4h: 0.003020763397216797
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.00039124488830566406
Attention layer time: 0.014535188674926758
LN1: 0.0001246929168701172
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011212825775146484
fused
Attention Softmax: 0.0026390552520751953
Attention Dropout: 0.00007534027099609375
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007462501525878906
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.00012755393981933594
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030171871185302734
MLP_4h_h: 0.002608776092529297
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.014539480209350586
LN1: 0.00012493133544921875
QKV Transform: 0.0020699501037597656
Attention Score: 0.0011196136474609375
fused
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.0003840923309326172
Post-attention residual: 0.0001285076141357422
LN2: 0.00012683868408203125
MLP_h_4h: 0.003032684326171875
MLP_4h_h: 0.002617359161376953
Post-MLP residual: 0.0003838539123535156
Attention layer time: 0.014542818069458008
LN1: 0.00012350082397460938
QKV Transform: 0.0020706653594970703
Attention Score: 0.0011222362518310547
fused
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.00012755393981933594
LN2: 0.000125885009765625
MLP_h_4h: 0.0030202865600585938
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014506816864013672
LN1: 0.00012445449829101562
QKV Transform: 0.0020689964294433594
Attention Score: 0.001117706298828125
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007464885711669922
Post-attention Dropout: 0.000377655029296875
Post-attention residual: 0.00012969970703125
LN2: 0.00012683868408203125
MLP_h_4h: 0.003022432327270508
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.0003745555877685547
Attention layer time: 0.01450037956237793
LN1: 0.0001270771026611328
QKV Transform: 0.002074718475341797
Attention Score: 0.0011234283447265625
fused
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.0003871917724609375
Post-attention residual: 0.0001285076141357422
LN2: 0.00012731552124023438
MLP_h_4h: 0.003027200698852539
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014519453048706055
LN1: 0.00012350082397460938
QKV Transform: 0.002069711685180664
Attention Score: 0.0011284351348876953
fused
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.00037670135498046875
Post-attention residual: 0.00012826919555664062
LN2: 0.00012540817260742188
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.00037741661071777344
Attention layer time: 0.014513969421386719
LN1: 0.00012421607971191406
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011250972747802734
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.00012946128845214844
LN2: 0.00013017654418945312
MLP_h_4h: 0.003025054931640625
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.01453399658203125
LN1: 0.0001232624053955078
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.0026397705078125
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.00039458274841308594
Post-attention residual: 0.00012993812561035156
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030248165130615234
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014543533325195312
LN1: 0.00012493133544921875
QKV Transform: 0.0020694732666015625
Attention Score: 0.0011265277862548828
fused
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007593631744384766
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.0001380443572998047
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030183792114257812
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014546871185302734
LN1: 0.00012421607971191406
QKV Transform: 0.002064228057861328
Attention Score: 0.0011217594146728516
fused
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007371902465820312
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.00012803077697753906
LN2: 0.0001246929168701172
MLP_h_4h: 0.003035306930541992
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014523983001708984
LN1: 0.00012445449829101562
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.002638101577758789
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.0003781318664550781
Post-attention residual: 0.00012731552124023438
LN2: 0.00012612342834472656
MLP_h_4h: 0.003032207489013672
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014505147933959961
LN1: 0.00012350082397460938
QKV Transform: 0.0020699501037597656
Attention Score: 0.0011382102966308594
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005292892456054688
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.000736236572265625
Post-attention Dropout: 0.0003838539123535156
Post-attention residual: 0.00012803077697753906
LN2: 0.00012540817260742188
MLP_h_4h: 0.003021240234375
MLP_4h_h: 0.002600431442260742
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.01452493667602539
LN1: 0.00012540817260742188
QKV Transform: 0.002074718475341797
Attention Score: 0.0011260509490966797
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.00012826919555664062
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030176639556884766
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.00037670135498046875
Attention layer time: 0.01452016830444336
LN1: 0.00012350082397460938
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011386871337890625
fused
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007522106170654297
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.00037670135498046875
Post-attention residual: 0.00012826919555664062
LN2: 0.0001251697540283203
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.00037741661071777344
Attention layer time: 0.01451253890991211
LN1: 0.0001266002655029297
QKV Transform: 0.002069234848022461
Attention Score: 0.0011401176452636719
fused
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007481575012207031
Post-attention Dropout: 0.0003781318664550781
Post-attention residual: 0.00012874603271484375
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030219554901123047
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.014542341232299805
LN1: 0.0001266002655029297
QKV Transform: 0.0020749568939208984
Attention Score: 0.001123666763305664
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.0001285076141357422
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030274391174316406
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.0003769397735595703
Attention layer time: 0.014541864395141602
LN1: 0.00012350082397460938
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011227130889892578
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.00037670135498046875
Post-attention residual: 0.0001270771026611328
LN2: 0.00012564659118652344
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.00038123130798339844
Attention layer time: 0.014503955841064453
LN1: 0.00012421607971191406
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011208057403564453
fused
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007464885711669922
Post-attention Dropout: 0.0003902912139892578
Post-attention residual: 0.0001289844512939453
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030183792114257812
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014510631561279297
LN1: 0.0001430511474609375
QKV Transform: 0.0020742416381835938
Attention Score: 0.001117706298828125
fused
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.00013017654418945312
LN2: 0.00012874603271484375
MLP_h_4h: 0.0030269622802734375
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003771781921386719
Attention layer time: 0.014528989791870117
LN1: 0.00012445449829101562
QKV Transform: 0.002069234848022461
Attention Score: 0.0011246204376220703
fused
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.00012755393981933594
LN2: 0.00012540817260742188
MLP_h_4h: 0.0030167102813720703
MLP_4h_h: 0.0025997161865234375
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014521360397338867
LN1: 0.0001251697540283203
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011227130889892578
fused
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007467269897460938
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.0001277923583984375
LN2: 0.00020551681518554688
MLP_h_4h: 0.003023862838745117
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014611244201660156
LN1: 0.00012373924255371094
QKV Transform: 0.0020759105682373047
Attention Score: 0.0011186599731445312
fused
Attention Softmax: 0.0026404857635498047
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.0003771781921386719
Post-attention residual: 0.00013017654418945312
LN2: 0.00012874603271484375
MLP_h_4h: 0.0030295848846435547
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014562845230102539
LN1: 0.0001246929168701172
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011241436004638672
fused
Attention Softmax: 0.0026607513427734375
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007734298706054688
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.0001285076141357422
LN2: 0.00012636184692382812
MLP_h_4h: 0.003020048141479492
MLP_4h_h: 0.0026013851165771484
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014564752578735352
LN1: 0.00012493133544921875
QKV Transform: 0.0020630359649658203
Attention Score: 0.0011227130889892578
fused
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007550716400146484
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.0001285076141357422
LN2: 0.00012540817260742188
MLP_h_4h: 0.0030498504638671875
MLP_4h_h: 0.00260162353515625
Post-MLP residual: 0.00037598609924316406
Attention layer time: 0.014538288116455078
LN1: 0.0001239776611328125
QKV Transform: 0.002069711685180664
Attention Score: 0.0011217594146728516
fused
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.000377655029296875
Post-attention residual: 0.0001277923583984375
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030307769775390625
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.01450347900390625
LN1: 0.0001246929168701172
QKV Transform: 0.0020723342895507812
Attention Score: 0.001138448715209961
fused
Attention Softmax: 0.002646207809448242
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007338523864746094
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012826919555664062
LN2: 0.00012540817260742188
MLP_h_4h: 0.0030210018157958984
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014534235000610352
LN1: 0.0001251697540283203
QKV Transform: 0.0020744800567626953
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005054473876953125
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007462501525878906
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.0001277923583984375
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030145645141601562
MLP_4h_h: 0.0026056766510009766
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.01452183723449707
LN1: 0.0001239776611328125
QKV Transform: 0.0020706653594970703
Attention Score: 0.0011243820190429688
fused
Attention Softmax: 0.0026390552520751953
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.00037860870361328125
Post-attention residual: 0.0001277923583984375
LN2: 0.00012493133544921875
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.00037670135498046875
Attention layer time: 0.01450347900390625
LN1: 0.00012636184692382812
QKV Transform: 0.0020685195922851562
Attention Score: 0.001142740249633789
fused
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007576942443847656
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.0001285076141357422
LN2: 0.00012564659118652344
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003762245178222656
Attention layer time: 0.014537334442138672
LN1: 0.00012612342834472656
QKV Transform: 0.002074003219604492
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.002638578414916992
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.0003840923309326172
Post-attention residual: 0.0001277923583984375
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030257701873779297
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014535903930664062
LN1: 0.0001239776611328125
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011272430419921875
fused
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.0003771781921386719
Post-attention residual: 0.0001277923583984375
LN2: 0.000125885009765625
MLP_h_4h: 0.0030198097229003906
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014521598815917969
LN1: 0.00013828277587890625
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011222362518310547
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007467269897460938
Post-attention Dropout: 0.0003800392150878906
Post-attention residual: 0.00012922286987304688
LN2: 0.0001266002655029297
MLP_h_4h: 0.003020763397216797
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.00038361549377441406
Attention layer time: 0.014544486999511719
LN1: 0.0001270771026611328
QKV Transform: 0.002073049545288086
Attention Score: 0.0011248588562011719
fused
Attention Softmax: 0.0026395320892333984
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.00012946128845214844
LN2: 0.00012803077697753906
MLP_h_4h: 0.003027200698852539
MLP_4h_h: 0.002596616744995117
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014510631561279297
LN1: 0.0001239776611328125
QKV Transform: 0.002067089080810547
Attention Score: 0.0011248588562011719
fused
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007660388946533203
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.0003762245178222656
Post-attention residual: 0.00012874603271484375
LN2: 0.0001251697540283203
MLP_h_4h: 0.0030181407928466797
MLP_4h_h: 0.0026018619537353516
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014527082443237305
LN1: 0.00012493133544921875
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011246204376220703
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005078315734863281
Attention Over Value: 0.0007526874542236328
Attention linproj: 0.0007464885711669922
Post-attention Dropout: 0.0003800392150878906
Post-attention residual: 0.00012874603271484375
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030221939086914062
MLP_4h_h: 0.0026035308837890625
Post-MLP residual: 0.0003764629364013672
Attention layer time: 0.014528036117553711
LN1: 0.00012421607971191406
QKV Transform: 0.002069711685180664
Attention Score: 0.0011217594146728516
fused
Attention Softmax: 0.002639293670654297
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007405281066894531
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.00013256072998046875
LN2: 0.00012946128845214844
MLP_h_4h: 0.0030241012573242188
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.00038123130798339844
Attention layer time: 0.014513731002807617
LN1: 0.00012445449829101562
QKV Transform: 0.002071857452392578
Attention Score: 0.0011255741119384766
fused
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.00037860870361328125
Post-attention residual: 0.00012946128845214844
LN2: 0.00012731552124023438
MLP_h_4h: 0.0030183792114257812
MLP_4h_h: 0.0026035308837890625
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014548540115356445
LN1: 0.00012540817260742188
QKV Transform: 0.0020728111267089844
Attention Score: 0.001117706298828125
fused
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0003845691680908203
Post-attention residual: 0.0001277923583984375
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030231475830078125
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014512777328491211
LN1: 0.00012421607971191406
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011224746704101562
fused
Attention Softmax: 0.0026395320892333984
Attention Dropout: 0.00007200241088867188
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.0003781318664550781
Post-attention residual: 0.00012826919555664062
LN2: 0.00012540817260742188
MLP_h_4h: 0.0030176639556884766
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.01451253890991211
LN1: 0.0001232624053955078
QKV Transform: 0.0020668506622314453
Attention Score: 0.0011277198791503906
fused
Attention Softmax: 0.002639293670654297
Attention Dropout: 0.00007081031799316406
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012803077697753906
LN2: 0.000125885009765625
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.0026111602783203125
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.01453709602355957
LN1: 0.00012612342834472656
QKV Transform: 0.002069234848022461
Attention Score: 0.0011239051818847656
fused
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007448196411132812
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.0001289844512939453
LN2: 0.00012731552124023438
MLP_h_4h: 0.003026723861694336
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.014533281326293945
LN1: 0.0001232624053955078
QKV Transform: 0.002069711685180664
Attention Score: 0.0011184215545654297
fused
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.00012826919555664062
LN2: 0.000125885009765625
MLP_h_4h: 0.0030205249786376953
MLP_4h_h: 0.0026013851165771484
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.0145111083984375
LN1: 0.00012564659118652344
QKV Transform: 0.002069234848022461
Attention Score: 0.0011219978332519531
fused
Attention Softmax: 0.0026428699493408203
Attention Dropout: 0.00005030632019042969
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007467269897460938
Post-attention Dropout: 0.0004642009735107422
Post-attention residual: 0.0001277923583984375
LN2: 0.0001266002655029297
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.0026092529296875
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014595746994018555
LN1: 0.00012612342834472656
QKV Transform: 0.0020761489868164062
Attention Score: 0.0011222362518310547
fused
Attention Softmax: 0.0026412010192871094
Attention Dropout: 0.00005340576171875000
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.0003936290740966797
Post-attention residual: 0.00012826919555664062
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030286312103271484
MLP_4h_h: 0.0025985240936279297
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014540672302246094
LN1: 0.00012350082397460938
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011250972747802734
fused
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.0001277923583984375
LN2: 0.000125885009765625
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.01451563835144043
LN1: 0.00012445449829101562
QKV Transform: 0.0020749568939208984
Attention Score: 0.0011172294616699219
fused
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007479190826416016
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.00012946128845214844
LN2: 0.00012636184692382812
MLP_h_4h: 0.003020048141479492
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014514923095703125
LN1: 0.00013256072998046875
QKV Transform: 0.0020742416381835938
Attention Score: 0.0011234283447265625
fused
Attention Softmax: 0.0026390552520751953
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.0003783702850341797
Post-attention residual: 0.00012922286987304688
LN2: 0.00012874603271484375
MLP_h_4h: 0.003027677536010742
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.01452946662902832
LN1: 0.00012421607971191406
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011243820190429688
fused
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.0001277923583984375
LN2: 0.00012564659118652344
MLP_h_4h: 0.0030183792114257812
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014507293701171875
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 141.599
Transformer - MLP - Attention (in seconds): 0.0011
========================================================================================================================
1.13.1 

[2023-11-02 19:53:46,379] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-02 19:53:47,313] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.157.218, master_port=6000
[2023-11-02 19:53:47,313] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-02 19:53:50,560] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 24, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200


Actual
------
QKV Transform: 2.0414836406707764
Attention Score: 0.005762577056884766
Attention Softmax: 0.009610176086425781
Attention Dropout: 0.00101566314697265625
Attention Over Value: 0.00562286376953125
Attention linproj: 0.009144783020019531
QKV Transform: 0.0022695064544677734
Attention Score: 0.0011441707611083984
Attention Softmax: 0.0026726722717285156
Attention Dropout: 0.00007390975952148438
Attention Over Value: 0.0010149478912353516
Attention linproj: 0.0007755756378173828
QKV Transform: 0.0020987987518310547
Attention Score: 0.0011336803436279297
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00006389617919921875
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007505416870117188
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011360645294189453
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00006270408630371094
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007472038269042969
QKV Transform: 0.002080202102661133
Attention Score: 0.0011334419250488281
Attention Softmax: 0.0026535987854003906
Attention Dropout: 0.00006055831909179688
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007586479187011719
QKV Transform: 0.0020775794982910156
Attention Score: 0.0011365413665771484
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00007390975952148438
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007483959197998047
QKV Transform: 0.0020797252655029297
Attention Score: 0.0011322498321533203
Attention Softmax: 0.0026559829711914062
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007491111755371094
QKV Transform: 0.002079486846923828
Attention Score: 0.0011487007141113281
Attention Softmax: 0.0026557445526123047
Attention Dropout: 0.00006318092346191406
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007495880126953125
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011341571807861328
Attention Softmax: 0.0026559829711914062
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007503032684326172
QKV Transform: 0.002081155776977539
Attention Score: 0.0011434555053710938
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007634162902832031
Attention linproj: 0.0007417201995849609
QKV Transform: 0.0020818710327148438
Attention Score: 0.001138925552368164
Attention Softmax: 0.0026559829711914062
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007488727569580078
QKV Transform: 0.002087116241455078
Attention Score: 0.0011348724365234375
Attention Softmax: 0.002650737762451172
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007483959197998047
QKV Transform: 0.002102375030517578
Attention Score: 0.0011456012725830078
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00006580352783203125
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007522106170654297
QKV Transform: 0.0020847320556640625
Attention Score: 0.0011243820190429688
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007545948028564453
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011324882507324219
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007545948028564453
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011305809020996094
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007560253143310547
QKV Transform: 0.002084970474243164
Attention Score: 0.0011277198791503906
Attention Softmax: 0.0026543140411376953
Attention Dropout: 0.00005745887756347656
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007674694061279297
QKV Transform: 0.002084493637084961
Attention Score: 0.0011281967163085938
Attention Softmax: 0.0026526451110839844
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007491111755371094
QKV Transform: 0.0020897388458251953
Attention Score: 0.0011360645294189453
Attention Softmax: 0.00264739990234375
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007510185241699219
QKV Transform: 0.0020902156829833984
Attention Score: 0.0011487007141113281
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007483959197998047
QKV Transform: 0.0020911693572998047
Attention Score: 0.0011267662048339844
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007476806640625
QKV Transform: 0.002081155776977539
Attention Score: 0.0011293888092041016
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007491111755371094
QKV Transform: 0.0020813941955566406
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00007200241088867188
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007500648498535156
QKV Transform: 0.002081632614135742
Attention Score: 0.0011317729949951172
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007507801055908203
QKV Transform: 0.0020742416381835938
Attention Score: 0.0011417865753173828
Attention Softmax: 0.002650737762451172
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007524490356445312
QKV Transform: 0.002080202102661133
Attention Score: 0.0011363029479980469
Attention Softmax: 0.002651214599609375
Attention Dropout: 0.00006127357482910156
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007455348968505859
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011355876922607422
Attention Softmax: 0.0026488304138183594
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007507801055908203
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011394023895263672
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007488727569580078
QKV Transform: 0.002086162567138672
Attention Score: 0.0011301040649414062
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007529258728027344
QKV Transform: 0.002085447311401367
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007765293121337891
Attention linproj: 0.0007562637329101562
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011250972747802734
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005722045898437500
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007534027099609375
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011277198791503906
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005745887756347656
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007557868957519531
QKV Transform: 0.0020704269409179688
Attention Score: 0.0011298656463623047
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00005722045898437500
Attention Over Value: 0.0007772445678710938
Attention linproj: 0.0007522106170654297
QKV Transform: 0.002087116241455078
Attention Score: 0.0011343955993652344
Attention Softmax: 0.0026526451110839844
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007600784301757812
QKV Transform: 0.0020875930786132812
Attention Score: 0.001127481460571289
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007488727569580078
QKV Transform: 0.0020902156829833984
Attention Score: 0.001129150390625
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007510185241699219
QKV Transform: 0.0020895004272460938
Attention Score: 0.0011286735534667969
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00007057189941406250
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007481575012207031
QKV Transform: 0.0020890235900878906
Attention Score: 0.0011279582977294922
Attention Softmax: 0.002646207809448242
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007507801055908203
QKV Transform: 0.0020818710327148438
Attention Score: 0.001138925552368164
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007481575012207031
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011265277862548828
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005745887756347656
Attention Over Value: 0.0007729530334472656
Attention linproj: 0.0007491111755371094
QKV Transform: 0.002082347869873047
Attention Score: 0.0011293888092041016
Attention Softmax: 0.0026607513427734375
Attention Dropout: 0.00005698204040527344
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007426738739013672
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011484622955322266
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.004761695861816406
Attention linproj: 0.0007636547088623047
QKV Transform: 0.00209808349609375
Attention Score: 0.0011425018310546875
Attention Softmax: 0.0026547908782958984
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007498264312744141
QKV Transform: 0.002080678939819336
Attention Score: 0.0011317729949951172
Attention Softmax: 0.0026526451110839844
Attention Dropout: 0.00006127357482910156
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007538795471191406
QKV Transform: 0.002083301544189453
Attention Score: 0.0011317729949951172
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00006055831909179688
Attention Over Value: 0.0007674694061279297
Attention linproj: 0.0007483959197998047
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011293888092041016
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007545948028564453
QKV Transform: 0.002087116241455078
Attention Score: 0.0011320114135742188
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00006604194641113281
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007531642913818359
QKV Transform: 0.0020859241485595703
Attention Score: 0.001134634017944336
Attention Softmax: 0.0026531219482421875
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.00075531005859375
QKV Transform: 0.0020978450775146484
Attention Score: 0.0011279582977294922
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007560253143310547
QKV Transform: 0.002081155776977539
Attention Score: 0.0011408329010009766
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005722045898437500
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007557868957519531
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011363029479980469
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007457733154296875
QKV Transform: 0.002084493637084961
Attention Score: 0.0011324882507324219
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005745887756347656
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007495880126953125
QKV Transform: 0.0020923614501953125
Attention Score: 0.0011281967163085938
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007526874542236328
QKV Transform: 0.0020897388458251953
Attention Score: 0.0011355876922607422
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007505416870117188
QKV Transform: 0.0020914077758789062
Attention Score: 0.0011317729949951172
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006103515625000000
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007500648498535156
QKV Transform: 0.0020813941955566406
Attention Score: 0.001135110855102539
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007491111755371094
QKV Transform: 0.0020842552185058594
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00007200241088867188
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007493495941162109
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011289119720458984
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007505416870117188
QKV Transform: 0.002080202102661133
Attention Score: 0.001127004623413086
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00006127357482910156
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007517337799072266
QKV Transform: 0.0020813941955566406
Attention Score: 0.0011336803436279297
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007495880126953125
QKV Transform: 0.0020835399627685547
Attention Score: 0.0011355876922607422
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006103515625000000
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007569789886474609
QKV Transform: 0.0020835399627685547
Attention Score: 0.0011289119720458984
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007500648498535156
QKV Transform: 0.002086639404296875
Attention Score: 0.00113677978515625
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007550716400146484
QKV Transform: 0.002105712890625
Attention Score: 0.0011334419250488281
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007762908935546875
Attention linproj: 0.0007636547088623047
QKV Transform: 0.002088785171508789
Attention Score: 0.0011348724365234375
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005745887756347656
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007548332214355469
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011315345764160156
Attention Softmax: 0.0026526451110839844
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007545948028564453
QKV Transform: 0.002082347869873047
Attention Score: 0.0011379718780517578
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007631778717041016
QKV Transform: 0.0020890235900878906
Attention Score: 0.0011339187622070312
Attention Softmax: 0.0026540756225585938
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007467269897460938
QKV Transform: 0.002089262008666992
Attention Score: 0.0011301040649414062
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00007057189941406250
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.000751495361328125
QKV Transform: 0.002092599868774414
Attention Score: 0.0011434555053710938
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007507801055908203
QKV Transform: 0.002092599868774414
Attention Score: 0.001129150390625
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007510185241699219
QKV Transform: 0.0020911693572998047
Attention Score: 0.0011277198791503906
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007510185241699219
QKV Transform: 0.0020825862884521484
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007486343383789062
QKV Transform: 0.002657175064086914
Attention Score: 0.0011386871337890625
Attention Softmax: 0.0026535987854003906
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007519721984863281
QKV Transform: 0.0021059513092041016
Attention Score: 0.0011260509490966797
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007512569427490234
QKV Transform: 0.002081632614135742
Attention Score: 0.0011339187622070312
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007526874542236328
QKV Transform: 0.0020835399627685547
Attention Score: 0.001138925552368164
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00005722045898437500
Attention Over Value: 0.0007700920104980469
Attention linproj: 0.0007512569427490234
QKV Transform: 0.002084016799926758
Attention Score: 0.0011334419250488281
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007541179656982422
QKV Transform: 0.0020847320556640625
Attention Score: 0.0011436939239501953
Attention Softmax: 0.002651214599609375
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.000743865966796875
QKV Transform: 0.002083301544189453
Attention Score: 0.0011401176452636719
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007648468017578125
Attention linproj: 0.000751495361328125
QKV Transform: 0.002082347869873047
Attention Score: 0.0011413097381591797
Attention Softmax: 0.002651214599609375
Attention Dropout: 0.00006127357482910156
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007750988006591797
QKV Transform: 0.002087831497192383
Attention Score: 0.001135110855102539
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007522106170654297
QKV Transform: 0.002090930938720703
Attention Score: 0.0011341571807861328
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00006318092346191406
Attention Over Value: 0.0007638931274414062
Attention linproj: 0.0007448196411132812
QKV Transform: 0.0021049976348876953
Attention Score: 0.0011436939239501953
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00006318092346191406
Attention Over Value: 0.0007746219635009766
Attention linproj: 0.0007557868957519531
QKV Transform: 0.002090930938720703
Attention Score: 0.0011315345764160156
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007557868957519531
QKV Transform: 0.0020825862884521484
Attention Score: 0.0011348724365234375
Attention Softmax: 0.0026543140411376953
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.00075531005859375
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011286735534667969
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007562637329101562
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011265277862548828
Attention Softmax: 0.0026540756225585938
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007615089416503906
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011298656463623047
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007503032684326172
QKV Transform: 0.0020918846130371094
Attention Score: 0.0011315345764160156
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007600784301757812
QKV Transform: 0.0020906925201416016
Attention Score: 0.0011341571807861328
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007505416870117188
QKV Transform: 0.002092123031616211
Attention Score: 0.0011336803436279297
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007481575012207031
QKV Transform: 0.002084016799926758
Attention Score: 0.0011332035064697266
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007500648498535156
QKV Transform: 0.002085447311401367
Attention Score: 0.001129150390625
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00005745887756347656
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007505416870117188
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011332035064697266
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007550716400146484
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011444091796875
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007436275482177734
QKV Transform: 0.0020830631256103516
Attention Score: 0.0011353492736816406
Attention Softmax: 0.0026552677154541016
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007636547088623047
Attention linproj: 0.0007543563842773438
QKV Transform: 0.0020837783813476562
Attention Score: 0.0011336803436279297
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007429122924804688
QKV Transform: 0.0020856857299804688
Attention Score: 0.001131296157836914
Attention Softmax: 0.0026531219482421875
Attention Dropout: 0.00006318092346191406
Attention Over Value: 0.0007619857788085938
Attention linproj: 0.0007488727569580078
QKV Transform: 0.002092123031616211
Attention Score: 0.001131296157836914
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00006270408630371094
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007433891296386719
QKV Transform: 0.002094268798828125
Attention Score: 0.001138925552368164
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006818771362304688
Attention Over Value: 0.0007801055908203125
Attention linproj: 0.0007548332214355469
QKV Transform: 0.0020890235900878906
Attention Score: 0.0011305809020996094
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007622241973876953
Attention linproj: 0.0007576942443847656
QKV Transform: 0.002112150192260742
Attention Score: 0.0011289119720458984
Attention Softmax: 0.0026535987854003906
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007598400115966797
QKV Transform: 0.0020842552185058594
Attention Score: 0.0011296272277832031
Attention Softmax: 0.0026526451110839844
Attention Dropout: 0.00005745887756347656
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007581710815429688
QKV Transform: 0.0020885467529296875
Attention Score: 0.0011293888092041016
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007450580596923828
QKV Transform: 0.002089977264404297
Attention Score: 0.0011315345764160156
Attention Softmax: 0.0026540756225585938
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.000751495361328125
QKV Transform: 0.0020933151245117188
Attention Score: 0.0011310577392578125
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007512569427490234
QKV Transform: 0.0020935535430908203
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00006103515625000000
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.000751495361328125
QKV Transform: 0.0020956993103027344
Attention Score: 0.0011348724365234375
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007398128509521484
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011429786682128906
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007505416870117188
QKV Transform: 0.002085447311401367
Attention Score: 0.0011315345764160156
Attention Softmax: 0.0026743412017822266
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007483959197998047
QKV Transform: 0.002085447311401367
Attention Score: 0.001148223876953125
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007529258728027344
QKV Transform: 0.002084493637084961
Attention Score: 0.0011310577392578125
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007717609405517578
Attention linproj: 0.0007517337799072266
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011417865753173828
Attention Softmax: 0.002651214599609375
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007538795471191406
QKV Transform: 0.0020864009857177734
Attention Score: 0.001130819320678711
Attention Softmax: 0.0026547908782958984
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007581710815429688
QKV Transform: 0.0020904541015625
Attention Score: 0.0011324882507324219
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007522106170654297
QKV Transform: 0.002090930938720703
Attention Score: 0.001132965087890625
Attention Softmax: 0.002663850784301758
Attention Dropout: 0.00006270408630371094
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007510185241699219
QKV Transform: 0.0020799636840820312
Attention Score: 0.0011413097381591797
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00008010864257812500
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007536411285400391
QKV Transform: 0.0020928382873535156
Attention Score: 0.001155853271484375
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007572174072265625
QKV Transform: 0.0020890235900878906
Attention Score: 0.0011358261108398438
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007560253143310547
QKV Transform: 0.0020856857299804688
Attention Score: 0.0011315345764160156
Attention Softmax: 0.0026526451110839844
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007557868957519531
QKV Transform: 0.0020797252655029297
Attention Score: 0.001130819320678711
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007576942443847656
QKV Transform: 0.002089977264404297
Attention Score: 0.0011241436004638672
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007491111755371094
QKV Transform: 0.002092599868774414
Attention Score: 0.0011293888092041016
Attention Softmax: 0.0026590824127197266
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007503032684326172
QKV Transform: 0.002093076705932617
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007581710815429688
QKV Transform: 0.0020956993103027344
Attention Score: 0.0011305809020996094
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007517337799072266
QKV Transform: 0.0021026134490966797
Attention Score: 0.0011310577392578125
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007503032684326172
QKV Transform: 0.002084493637084961
Attention Score: 0.001142263412475586
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00006103515625000000
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007522106170654297
QKV Transform: 0.0020835399627685547
Attention Score: 0.001135110855102539
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007510185241699219
QKV Transform: 0.002085447311401367
Attention Score: 0.001129150390625
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007503032684326172
QKV Transform: 0.002085447311401367
Attention Score: 0.001127481460571289
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007543563842773438
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011425018310546875
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007519721984863281
QKV Transform: 0.002084493637084961
Attention Score: 0.0011310577392578125
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007493495941162109
QKV Transform: 0.002088308334350586
Attention Score: 0.0011305809020996094
Attention Softmax: 0.0026657581329345703
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007498264312744141
QKV Transform: 0.002093791961669922
Attention Score: 0.0011343955993652344
Attention Softmax: 0.0026509761810302734
Attention Dropout: 0.00006365776062011719
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007467269897460938
QKV Transform: 0.0020895004272460938
Attention Score: 0.0011513233184814453
Attention Softmax: 0.002651214599609375
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007650852203369141
QKV Transform: 0.002090930938720703
Attention Score: 0.0011303424835205078
Attention Softmax: 0.002654552459716797
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007584095001220703
QKV Transform: 0.002084970474243164
Attention Score: 0.001127004623413086
Attention Softmax: 0.0026531219482421875
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007531642913818359
QKV Transform: 0.0020961761474609375
Attention Score: 0.0011382102966308594
Attention Softmax: 0.0026535987854003906
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.00075531005859375
QKV Transform: 0.0020880699157714844
Attention Score: 0.0011272430419921875
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005698204040527344
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007579326629638672
QKV Transform: 0.002104520797729492
Attention Score: 0.0011415481567382812
Attention Softmax: 0.0026543140411376953
Attention Dropout: 0.00006103515625000000
Attention Over Value: 0.0007679462432861328
Attention linproj: 0.0007493495941162109
QKV Transform: 0.002094745635986328
Attention Score: 0.0011360645294189453
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007503032684326172
QKV Transform: 0.0020940303802490234
Attention Score: 0.0011348724365234375
Attention Softmax: 0.006299257278442383
Attention Dropout: 0.00006914138793945312
Attention Over Value: 0.0007741451263427734
Attention linproj: 0.0007576942443847656
QKV Transform: 0.0020914077758789062
Attention Score: 0.0011343955993652344
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007505416870117188
QKV Transform: 0.002086162567138672
Attention Score: 0.0011353492736816406
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007519721984863281
QKV Transform: 0.0020880699157714844
Attention Score: 0.0011298656463623047
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00007033348083496094
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007507801055908203
QKV Transform: 0.0020852088928222656
Attention Score: 0.0011322498321533203
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007619857788085938
Attention linproj: 0.0007522106170654297
QKV Transform: 0.002085447311401367
Attention Score: 0.0011341571807861328
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007531642913818359
QKV Transform: 0.0020847320556640625
Attention Score: 0.0011363029479980469
Attention Softmax: 0.0026547908782958984
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007650852203369141
QKV Transform: 0.002090930938720703
Attention Score: 0.0011374950408935547
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007507801055908203
Attention duration (in seconds): 0.0078
Attention throughput (in TFLOP/s): 106.201
MLP_h_4h: 2.2319748401641846
MLP_4h_h: 0.0028302669525146484
MLP_h_4h: 0.003081798553466797
MLP_4h_h: 0.0026268959045410156
MLP_h_4h: 0.003027677536010742
MLP_4h_h: 0.0026047229766845703
MLP_h_4h: 0.0030155181884765625
MLP_4h_h: 0.0026154518127441406
MLP_h_4h: 0.003014087677001953
MLP_4h_h: 0.0026009082794189453
MLP_h_4h: 0.003025054931640625
MLP_4h_h: 0.0026171207427978516
MLP_h_4h: 0.0030481815338134766
MLP_4h_h: 0.0026133060455322266
MLP_h_4h: 0.0030333995819091797
MLP_4h_h: 0.002614736557006836
MLP_h_4h: 0.0030264854431152344
MLP_4h_h: 0.002615690231323242
MLP_h_4h: 0.003051280975341797
MLP_4h_h: 0.0026307106018066406
MLP_h_4h: 0.0030694007873535156
MLP_4h_h: 0.0026307106018066406
MLP_h_4h: 0.003072023391723633
MLP_4h_h: 0.0026268959045410156
MLP_h_4h: 0.0030663013458251953
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.003123044967651367
MLP_4h_h: 0.002647876739501953
MLP_h_4h: 0.0031175613403320312
MLP_4h_h: 0.002636432647705078
MLP_h_4h: 0.0031223297119140625
MLP_4h_h: 0.0026404857635498047
MLP_h_4h: 0.0031583309173583984
MLP_4h_h: 0.0026488304138183594
MLP_h_4h: 0.0031692981719970703
MLP_4h_h: 0.0026481151580810547
MLP_h_4h: 0.0031561851501464844
MLP_4h_h: 0.0026502609252929688
MLP_h_4h: 0.003167390823364258
MLP_4h_h: 0.002649068832397461
MLP_h_4h: 0.003156900405883789
MLP_4h_h: 0.002649545669555664
MLP_h_4h: 0.003155946731567383
MLP_4h_h: 0.002640247344970703
MLP_h_4h: 0.0031495094299316406
MLP_4h_h: 0.0026378631591796875
MLP_h_4h: 0.003154277801513672
MLP_4h_h: 0.0026366710662841797
MLP_h_4h: 0.003148794174194336
MLP_4h_h: 0.0026357173919677734
MLP_h_4h: 0.003153085708618164
MLP_4h_h: 0.0026400089263916016
MLP_h_4h: 0.003149271011352539
MLP_4h_h: 0.002634763717651367
MLP_h_4h: 0.0031638145446777344
MLP_4h_h: 0.002635478973388672
MLP_h_4h: 0.003150463104248047
MLP_4h_h: 0.002635478973388672
MLP_h_4h: 0.0031490325927734375
MLP_4h_h: 0.0026352405548095703
MLP_h_4h: 0.003148317337036133
MLP_4h_h: 0.0026369094848632812
MLP_h_4h: 0.0031495094299316406
MLP_4h_h: 0.0026361942291259766
MLP_h_4h: 0.0031423568725585938
MLP_4h_h: 0.0026481151580810547
MLP_h_4h: 0.0031528472900390625
MLP_4h_h: 0.0026483535766601562
MLP_h_4h: 0.003156423568725586
MLP_4h_h: 0.0026564598083496094
MLP_h_4h: 0.003148794174194336
MLP_4h_h: 0.002645254135131836
MLP_h_4h: 0.0031540393829345703
MLP_4h_h: 0.002635955810546875
MLP_h_4h: 0.0031518936157226562
MLP_4h_h: 0.002646207809448242
MLP_h_4h: 0.0031557083129882812
MLP_4h_h: 0.002641439437866211
MLP_h_4h: 0.003152132034301758
MLP_4h_h: 0.002649068832397461
MLP_h_4h: 0.003149747848510742
MLP_4h_h: 0.002641439437866211
MLP_h_4h: 0.0031538009643554688
MLP_4h_h: 0.0026464462280273438
MLP_h_4h: 0.003149271011352539
MLP_4h_h: 0.002644777297973633
MLP_h_4h: 0.003148317337036133
MLP_4h_h: 0.002650022506713867
MLP_h_4h: 0.003149271011352539
MLP_4h_h: 0.002644777297973633
MLP_h_4h: 0.0031533241271972656
MLP_4h_h: 0.0026502609252929688
MLP_h_4h: 0.003150463104248047
MLP_4h_h: 0.0026464462280273438
MLP_h_4h: 0.003154754638671875
MLP_4h_h: 0.0026569366455078125
MLP_h_4h: 0.0031516551971435547
MLP_4h_h: 0.0026488304138183594
MLP_h_4h: 0.0031557083129882812
MLP_4h_h: 0.002645730972290039
MLP_h_4h: 0.003152608871459961
MLP_4h_h: 0.002646207809448242
MLP_h_4h: 0.003155946731567383
MLP_4h_h: 0.002640962600708008
MLP_h_4h: 0.0031545162200927734
MLP_4h_h: 0.0026483535766601562
MLP_h_4h: 0.0031502246856689453
MLP_4h_h: 0.0026445388793945312
MLP_h_4h: 0.003153562545776367
MLP_4h_h: 0.0026488304138183594
MLP_h_4h: 0.0031175613403320312
MLP_4h_h: 0.0026197433471679688
MLP_h_4h: 0.0031142234802246094
MLP_4h_h: 0.005450010299682617
MLP_h_4h: 0.0031757354736328125
MLP_4h_h: 0.002618551254272461
MLP_h_4h: 0.003123044967651367
MLP_4h_h: 0.002634286880493164
MLP_h_4h: 0.0031576156616210938
MLP_4h_h: 0.0026197433471679688
MLP_h_4h: 0.0031516551971435547
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.0031557083129882812
MLP_4h_h: 0.0026273727416992188
MLP_h_4h: 0.0030624866485595703
MLP_4h_h: 0.0026149749755859375
MLP_h_4h: 0.0030968189239501953
MLP_4h_h: 0.0026311874389648438
MLP_h_4h: 0.0030934810638427734
MLP_4h_h: 0.002619504928588867
MLP_h_4h: 0.0031218528747558594
MLP_4h_h: 0.002621173858642578
MLP_h_4h: 0.003124713897705078
MLP_4h_h: 0.002619504928588867
MLP_h_4h: 0.0031404495239257812
MLP_4h_h: 0.0026204586029052734
MLP_h_4h: 0.003123044967651367
MLP_4h_h: 0.0026209354400634766
MLP_h_4h: 0.0031180381774902344
MLP_4h_h: 0.00262451171875
MLP_h_4h: 0.0031213760375976562
MLP_4h_h: 0.002621889114379883
MLP_h_4h: 0.003118276596069336
MLP_4h_h: 0.002620220184326172
MLP_h_4h: 0.0031251907348632812
MLP_4h_h: 0.0026254653930664062
MLP_h_4h: 0.0031249523162841797
MLP_4h_h: 0.002622365951538086
MLP_h_4h: 0.0031304359436035156
MLP_4h_h: 0.0026237964630126953
MLP_h_4h: 0.0031280517578125
MLP_4h_h: 0.002622842788696289
MLP_h_4h: 0.0031332969665527344
MLP_4h_h: 0.002622842788696289
MLP_h_4h: 0.003125429153442383
MLP_4h_h: 0.0026221275329589844
MLP_h_4h: 0.003127574920654297
MLP_4h_h: 0.0026352405548095703
MLP_h_4h: 0.0031507015228271484
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.003126382827758789
MLP_4h_h: 0.0026264190673828125
MLP_h_4h: 0.0031173229217529297
MLP_4h_h: 0.002624988555908203
MLP_h_4h: 0.0031239986419677734
MLP_4h_h: 0.002622365951538086
MLP_h_4h: 0.003130674362182617
MLP_4h_h: 0.0026230812072753906
MLP_h_4h: 0.0031290054321289062
MLP_4h_h: 0.0026264190673828125
MLP_h_4h: 0.0031290054321289062
MLP_4h_h: 0.002620697021484375
MLP_h_4h: 0.0031294822692871094
MLP_4h_h: 0.0026221275329589844
MLP_h_4h: 0.0031201839447021484
MLP_4h_h: 0.0026216506958007812
MLP_h_4h: 0.0031282901763916016
MLP_4h_h: 0.002622365951538086
MLP_h_4h: 0.0031244754791259766
MLP_4h_h: 0.00262451171875
MLP_h_4h: 0.003123044967651367
MLP_4h_h: 0.002622842788696289
MLP_h_4h: 0.0031287670135498047
MLP_4h_h: 0.0026216506958007812
MLP_h_4h: 0.0031266212463378906
MLP_4h_h: 0.002624034881591797
MLP_h_4h: 0.003119230270385742
MLP_4h_h: 0.0026204586029052734
MLP_h_4h: 0.0031261444091796875
MLP_4h_h: 0.0026175975799560547
MLP_h_4h: 0.0031228065490722656
MLP_4h_h: 0.002620697021484375
MLP_h_4h: 0.0031290054321289062
MLP_4h_h: 0.002620220184326172
MLP_h_4h: 0.003123044967651367
MLP_4h_h: 0.0026235580444335938
MLP_h_4h: 0.003117799758911133
MLP_4h_h: 0.002621173858642578
MLP_h_4h: 0.003121614456176758
MLP_4h_h: 0.0026230812072753906
MLP_h_4h: 0.0031185150146484375
MLP_4h_h: 0.002622842788696289
MLP_h_4h: 0.0031249523162841797
MLP_4h_h: 0.0026264190673828125
MLP_h_4h: 0.0031180381774902344
MLP_4h_h: 0.0026221275329589844
MLP_h_4h: 0.0031273365020751953
MLP_4h_h: 0.002616405487060547
MLP_h_4h: 0.0031228065490722656
MLP_4h_h: 0.0026497840881347656
MLP_h_4h: 0.0031235218048095703
MLP_4h_h: 0.002619504928588867
MLP_h_4h: 0.0031244754791259766
MLP_4h_h: 0.0026221275329589844
MLP_h_4h: 0.00312042236328125
MLP_4h_h: 0.0026235580444335938
MLP_h_4h: 0.0031583309173583984
MLP_4h_h: 0.0026187896728515625
MLP_h_4h: 0.003118276596069336
MLP_4h_h: 0.002621889114379883
MLP_h_4h: 0.0031239986419677734
MLP_4h_h: 0.0026252269744873047
MLP_h_4h: 0.003123044967651367
MLP_4h_h: 0.002620697021484375
MLP_h_4h: 0.0031218528747558594
MLP_4h_h: 0.002644777297973633
MLP_h_4h: 0.0031244754791259766
MLP_4h_h: 0.0026273727416992188
MLP_h_4h: 0.0031232833862304688
MLP_4h_h: 0.0026209354400634766
MLP_h_4h: 0.003122091293334961
MLP_4h_h: 0.002621173858642578
MLP_h_4h: 0.0031137466430664062
MLP_4h_h: 0.0026214122772216797
MLP_h_4h: 0.0031523704528808594
MLP_4h_h: 0.002631664276123047
MLP_h_4h: 0.003153562545776367
MLP_4h_h: 0.0026319026947021484
MLP_h_4h: 0.0031571388244628906
MLP_4h_h: 0.0026307106018066406
MLP_h_4h: 0.0031533241271972656
MLP_4h_h: 0.002632617950439453
MLP_h_4h: 0.0031480789184570312
MLP_4h_h: 0.00263214111328125
MLP_h_4h: 0.003156423568725586
MLP_4h_h: 0.0026280879974365234
MLP_h_4h: 0.003157377243041992
MLP_4h_h: 0.002631664276123047
MLP_h_4h: 0.0031538009643554688
MLP_4h_h: 0.0026292800903320312
MLP_h_4h: 0.0031571388244628906
MLP_4h_h: 0.002625703811645508
MLP_h_4h: 0.003149747848510742
MLP_4h_h: 0.0026175975799560547
MLP_h_4h: 0.003157377243041992
MLP_4h_h: 0.0026199817657470703
MLP_h_4h: 0.003152132034301758
MLP_4h_h: 0.0026242733001708984
MLP_h_4h: 0.0031499862670898438
MLP_4h_h: 0.0026235580444335938
MLP_h_4h: 0.0031647682189941406
MLP_4h_h: 0.0026199817657470703
MLP_h_4h: 0.0031163692474365234
MLP_4h_h: 0.002621889114379883
MLP_h_4h: 0.0031125545501708984
MLP_4h_h: 0.0026247501373291016
MLP_h_4h: 0.003114938735961914
MLP_4h_h: 0.002622365951538086
MLP_h_4h: 0.0031235218048095703
MLP_4h_h: 0.0026183128356933594
MLP_h_4h: 0.003118753433227539
MLP_4h_h: 0.0026204586029052734
MLP_h_4h: 0.003122568130493164
MLP_4h_h: 0.0026178359985351562
MLP_h_4h: 0.003131866455078125
MLP_4h_h: 0.0026221275329589844
MLP_h_4h: 0.0031545162200927734
MLP_4h_h: 0.0026216506958007812
MLP_h_4h: 0.0031518936157226562
MLP_4h_h: 0.002619504928588867
MLP_h_4h: 0.0031502246856689453
MLP_4h_h: 0.0026187896728515625
MLP_h_4h: 0.003116130828857422
MLP_4h_h: 0.0026175975799560547
MLP_h_4h: 0.0031185150146484375
MLP_4h_h: 0.0026242733001708984
MLP_h_4h: 0.0031211376190185547
MLP_4h_h: 0.0026276111602783203
MLP_h_4h: 0.003118753433227539
MLP_4h_h: 0.002630472183227539
MLP_h_4h: 0.0031578540802001953
MLP_4h_h: 0.002629518508911133
MLP_h_4h: 0.003153562545776367
MLP_4h_h: 0.0026328563690185547
MLP_h_4h: 0.003159046173095703
MLP_4h_h: 0.0026302337646484375
MLP_h_4h: 0.0031571388244628906
MLP_4h_h: 0.002630949020385742
MLP_h_4h: 0.0031540393829345703
MLP_4h_h: 0.0026311874389648438
MLP duration (in seconds): 0.0058
MLP throughput (in TFLOP/s): 213.311
LN1: 0.004140377044677734
QKV Transform: 0.002235889434814453
Attention Score: 0.0012409687042236328
Attention Softmax: 0.002875089645385742
Attention Dropout: 0.00009655952453613281
Attention Over Value: 0.001226186752319336
Attention linproj: 0.0007746219635009766
Post-attention Dropout: 0.0685126781463623
Post-attention residual: 0.0032744407653808594
LN2: 0.00015115737915039062
MLP_h_4h: 0.0030279159545898438
MLP_4h_h: 0.002592325210571289
Post-MLP residual: 0.002010822296142578
Attention layer time: 0.09269428253173828
LN1: 0.0001308917999267578
QKV Transform: 0.0020859241485595703
Attention Score: 0.002248048782348633
Attention Softmax: 0.002691984176635742
Attention Dropout: 0.00006604194641113281
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.00038814544677734375
Post-attention residual: 0.0001304149627685547
LN2: 0.0001316070556640625
MLP_h_4h: 0.003022432327270508
MLP_4h_h: 0.0025899410247802734
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.015772581100463867
LN1: 0.00018715858459472656
QKV Transform: 0.002168416976928711
Attention Score: 0.001134634017944336
Attention Softmax: 0.002670764923095703
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007443428039550781
Post-attention Dropout: 0.00038623809814453125
Post-attention residual: 0.00012731552124023438
LN2: 0.00012874603271484375
MLP_h_4h: 0.0030205249786376953
MLP_4h_h: 0.0025911331176757812
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014779806137084961
LN1: 0.0001251697540283203
QKV Transform: 0.0020673274993896484
Attention Score: 0.0011477470397949219
Attention Softmax: 0.002656698226928711
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007395744323730469
Post-attention Dropout: 0.0003795623779296875
Post-attention residual: 0.00012731552124023438
LN2: 0.00012683868408203125
MLP_h_4h: 0.003008127212524414
MLP_4h_h: 0.002588510513305664
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014529228210449219
LN1: 0.0001251697540283203
QKV Transform: 0.0020630359649658203
Attention Score: 0.001123189926147461
Attention Softmax: 0.0026547908782958984
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.0003826618194580078
Post-attention residual: 0.00012922286987304688
LN2: 0.0001289844512939453
MLP_h_4h: 0.0030128955841064453
MLP_4h_h: 0.0025892257690429688
Post-MLP residual: 0.0003771781921386719
Attention layer time: 0.014506101608276367
LN1: 0.00012683868408203125
QKV Transform: 0.0020647048950195312
Attention Score: 0.0011205673217773438
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.00012922286987304688
LN2: 0.0001285076141357422
MLP_h_4h: 0.0030126571655273438
MLP_4h_h: 0.0025849342346191406
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014522314071655273
LN1: 0.00013184547424316406
QKV Transform: 0.0020606517791748047
Attention Score: 0.0011277198791503906
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012874603271484375
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030083656311035156
MLP_4h_h: 0.002586841583251953
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014496803283691406
LN1: 0.00012636184692382812
QKV Transform: 0.002061128616333008
Attention Score: 0.0011243820190429688
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007724761962890625
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.0003833770751953125
Post-attention residual: 0.0001285076141357422
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030107498168945312
MLP_4h_h: 0.0025870800018310547
Post-MLP residual: 0.00037550926208496094
Attention layer time: 0.014507770538330078
LN1: 0.00012421607971191406
QKV Transform: 0.002060413360595703
Attention Score: 0.0011229515075683594
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005292892456054688
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007379055023193359
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.0001289844512939453
LN2: 0.00014638900756835938
MLP_h_4h: 0.003014087677001953
MLP_4h_h: 0.002584218978881836
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014493465423583984
LN1: 0.00012373924255371094
QKV Transform: 0.0020592212677001953
Attention Score: 0.0011289119720458984
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007410049438476562
Post-attention Dropout: 0.00038504600524902344
Post-attention residual: 0.0001285076141357422
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030145645141601562
MLP_4h_h: 0.0025904178619384766
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.01450800895690918
LN1: 0.000125885009765625
QKV Transform: 0.0020623207092285156
Attention Score: 0.0011200904846191406
Attention Softmax: 0.0026650428771972656
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007452964782714844
Post-attention Dropout: 0.00038623809814453125
Post-attention residual: 0.00012731552124023438
LN2: 0.0001285076141357422
MLP_h_4h: 0.003014802932739258
MLP_4h_h: 0.002593517303466797
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.0145263671875
LN1: 0.0001251697540283203
QKV Transform: 0.0020627975463867188
Attention Score: 0.0011382102966308594
Attention Softmax: 0.002665281295776367
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007393360137939453
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.00012731552124023438
LN2: 0.00012731552124023438
MLP_h_4h: 0.003009796142578125
MLP_4h_h: 0.002592325210571289
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014534473419189453
LN1: 0.00012612342834472656
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011372566223144531
Attention Softmax: 0.002666473388671875
Attention Dropout: 0.00007343292236328125
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.0003857612609863281
Post-attention residual: 0.00012826919555664062
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030133724212646484
MLP_4h_h: 0.0025925636291503906
Post-MLP residual: 0.0003883838653564453
Attention layer time: 0.014579534530639648
LN1: 0.0001270771026611328
QKV Transform: 0.002068758010864258
Attention Score: 0.0011434555053710938
Attention Softmax: 0.0026721954345703125
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.0003857612609863281
Post-attention residual: 0.0001285076141357422
LN2: 0.0001289844512939453
MLP_h_4h: 0.0030193328857421875
MLP_4h_h: 0.002593517303466797
Post-MLP residual: 0.0003972053527832031
Attention layer time: 0.014590978622436523
LN1: 0.00012612342834472656
QKV Transform: 0.002063274383544922
Attention Score: 0.0011315345764160156
Attention Softmax: 0.002666950225830078
Attention Dropout: 0.00006294250488281250
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007383823394775391
Post-attention Dropout: 0.0003833770751953125
Post-attention residual: 0.0001285076141357422
LN2: 0.00012612342834472656
MLP_h_4h: 0.0030138492584228516
MLP_4h_h: 0.0025937557220458984
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.014540672302246094
LN1: 0.0001270771026611328
QKV Transform: 0.002062559127807617
Attention Score: 0.0011508464813232422
Attention Softmax: 0.0026662349700927734
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.00040030479431152344
Post-attention residual: 0.0001285076141357422
LN2: 0.0001266002655029297
MLP_h_4h: 0.003016233444213867
MLP_4h_h: 0.002591848373413086
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014568328857421875
LN1: 0.00012826919555664062
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011377334594726562
Attention Softmax: 0.00266265869140625
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.00038504600524902344
Post-attention residual: 0.0001289844512939453
LN2: 0.0001289844512939453
MLP_h_4h: 0.0030198097229003906
MLP_4h_h: 0.0025908946990966797
Post-MLP residual: 0.0003826618194580078
Attention layer time: 0.01455831527709961
LN1: 0.00012540817260742188
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011355876922607422
Attention Softmax: 0.0026662349700927734
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007395744323730469
Post-attention Dropout: 0.0003833770751953125
Post-attention residual: 0.00012826919555664062
LN2: 0.000125885009765625
MLP_h_4h: 0.003012418746948242
MLP_4h_h: 0.0025904178619384766
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014531135559082031
LN1: 0.000125885009765625
QKV Transform: 0.0020651817321777344
Attention Score: 0.0011310577392578125
Attention Softmax: 0.0026655197143554688
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.00012874603271484375
LN2: 0.00012826919555664062
MLP_h_4h: 0.003025531768798828
MLP_4h_h: 0.002592325210571289
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.01455545425415039
LN1: 0.00013399124145507812
QKV Transform: 0.002069234848022461
Attention Score: 0.0011310577392578125
Attention Softmax: 0.00266265869140625
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007376670837402344
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.00012826919555664062
LN2: 0.00012922286987304688
MLP_h_4h: 0.0030159950256347656
MLP_4h_h: 0.0025887489318847656
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014534235000610352
LN1: 0.00012373924255371094
QKV Transform: 0.0020601749420166016
Attention Score: 0.0011217594146728516
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005435943603515625
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.0004010200500488281
Post-attention residual: 0.0001277923583984375
LN2: 0.000125885009765625
MLP_h_4h: 0.003010272979736328
MLP_4h_h: 0.002590179443359375
Post-MLP residual: 0.00038313865661621094
Attention layer time: 0.01452946662902832
LN1: 0.0001354217529296875
QKV Transform: 0.0020613670349121094
Attention Score: 0.0011262893676757812
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.00012874603271484375
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030105113983154297
MLP_4h_h: 0.002590179443359375
Post-MLP residual: 0.0003769397735595703
Attention layer time: 0.014500856399536133
LN1: 0.0001251697540283203
QKV Transform: 0.0020608901977539062
Attention Score: 0.0011222362518310547
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007381439208984375
Post-attention Dropout: 0.0003826618194580078
Post-attention residual: 0.0001277923583984375
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030128955841064453
MLP_4h_h: 0.002586841583251953
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014477968215942383
LN1: 0.00012540817260742188
QKV Transform: 0.0020780563354492188
Attention Score: 0.001123666763305664
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.0003848075866699219
Post-attention residual: 0.00012874603271484375
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030150413513183594
MLP_4h_h: 0.002592802047729492
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014521121978759766
LN1: 0.00012564659118652344
QKV Transform: 0.002062559127807617
Attention Score: 0.0011203289031982422
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007305145263671875
Post-attention Dropout: 0.00038361549377441406
Post-attention residual: 0.00012826919555664062
LN2: 0.0001289844512939453
MLP_h_4h: 0.003009796142578125
MLP_4h_h: 0.0025937557220458984
Post-MLP residual: 0.0003771781921386719
Attention layer time: 0.01449131965637207
LN1: 0.0001246929168701172
QKV Transform: 0.002061128616333008
Attention Score: 0.001125335693359375
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007386207580566406
Post-attention Dropout: 0.00037860870361328125
Post-attention residual: 0.00012826919555664062
LN2: 0.00012636184692382812
MLP_h_4h: 0.003010272979736328
MLP_4h_h: 0.002592325210571289
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014492273330688477
LN1: 0.00012612342834472656
QKV Transform: 0.002064228057861328
Attention Score: 0.0011289119720458984
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.0001285076141357422
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030138492584228516
MLP_4h_h: 0.0025920867919921875
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014563322067260742
LN1: 0.00012564659118652344
QKV Transform: 0.0020630359649658203
Attention Score: 0.0011222362518310547
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.00038504600524902344
Post-attention residual: 0.00012803077697753906
LN2: 0.00012731552124023438
MLP_h_4h: 0.00301361083984375
MLP_4h_h: 0.0025937557220458984
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014501094818115234
LN1: 0.000125885009765625
QKV Transform: 0.0020601749420166016
Attention Score: 0.0011212825775146484
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005555152893066406
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.0001270771026611328
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030074119567871094
MLP_4h_h: 0.002587556838989258
Post-MLP residual: 0.0003821849822998047
Attention layer time: 0.014502525329589844
LN1: 0.0001251697540283203
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011210441589355469
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.0001285076141357422
LN2: 0.00012731552124023438
MLP_h_4h: 0.0030095577239990234
MLP_4h_h: 0.0025866031646728516
Post-MLP residual: 0.000392913818359375
Attention layer time: 0.014497756958007812
LN1: 0.0001277923583984375
QKV Transform: 0.002066373825073242
Attention Score: 0.0011227130889892578
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007364749908447266
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.00012946128845214844
LN2: 0.00012874603271484375
MLP_h_4h: 0.003014802932739258
MLP_4h_h: 0.0025949478149414062
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014497756958007812
LN1: 0.00012612342834472656
QKV Transform: 0.002072572708129883
Attention Score: 0.0011281967163085938
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005602836608886719
Attention Over Value: 0.0007522106170654297
Attention linproj: 0.0007407665252685547
Post-attention Dropout: 0.0003864765167236328
Post-attention residual: 0.00012826919555664062
LN2: 0.00012683868408203125
MLP_h_4h: 0.003007650375366211
MLP_4h_h: 0.002590656280517578
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.014509201049804688
LN1: 0.000125885009765625
QKV Transform: 0.0020580291748046875
Attention Score: 0.0011265277862548828
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.0001277923583984375
LN2: 0.00013327598571777344
MLP_h_4h: 0.003015756607055664
MLP_4h_h: 0.0025882720947265625
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014500856399536133
LN1: 0.0001246929168701172
QKV Transform: 0.0020613670349121094
Attention Score: 0.0011224746704101562
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007340908050537109
Post-attention Dropout: 0.00038051605224609375
Post-attention residual: 0.0001442432403564453
LN2: 0.00012969970703125
MLP_h_4h: 0.0030143260955810547
MLP_4h_h: 0.002588510513305664
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014505624771118164
LN1: 0.00012445449829101562
QKV Transform: 0.0020613670349121094
Attention Score: 0.0011227130889892578
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.000736236572265625
Post-attention Dropout: 0.0003826618194580078
Post-attention residual: 0.0001277923583984375
LN2: 0.0001270771026611328
MLP_h_4h: 0.0029997825622558594
MLP_4h_h: 0.0025866031646728516
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014479398727416992
LN1: 0.00012636184692382812
QKV Transform: 0.0020644664764404297
Attention Score: 0.0011203289031982422
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.0001285076141357422
LN2: 0.00012946128845214844
MLP_h_4h: 0.0030117034912109375
MLP_4h_h: 0.0025894641876220703
Post-MLP residual: 0.00038242340087890625
Attention layer time: 0.014522314071655273
LN1: 0.00012421607971191406
QKV Transform: 0.002061128616333008
Attention Score: 0.0011692047119140625
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.00012755393981933594
LN2: 0.00012636184692382812
MLP_h_4h: 0.003008127212524414
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014556169509887695
LN1: 0.00012731552124023438
QKV Transform: 0.0020639896392822266
Attention Score: 0.0011322498321533203
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00006961822509765625
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007476806640625
Post-attention Dropout: 0.00038504600524902344
Post-attention residual: 0.00012803077697753906
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030150413513183594
MLP_4h_h: 0.0025866031646728516
Post-MLP residual: 0.0003829002380371094
Attention layer time: 0.014542818069458008
LN1: 0.00012683868408203125
QKV Transform: 0.0020585060119628906
Attention Score: 0.0011169910430908203
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.00012922286987304688
LN2: 0.00012731552124023438
MLP_h_4h: 0.0030205249786376953
MLP_4h_h: 0.0026063919067382812
Post-MLP residual: 0.0003800392150878906
Attention layer time: 0.014513731002807617
LN1: 0.0001251697540283203
QKV Transform: 0.002063274383544922
Attention Score: 0.001119852066040039
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007355213165283203
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.00012946128845214844
LN2: 0.000125885009765625
MLP_h_4h: 0.0030095577239990234
MLP_4h_h: 0.0025899410247802734
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014483213424682617
LN1: 0.00013875961303710938
QKV Transform: 0.0020601749420166016
Attention Score: 0.0011191368103027344
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012946128845214844
LN2: 0.0001277923583984375
MLP_h_4h: 0.003009319305419922
MLP_4h_h: 0.0025882720947265625
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014504194259643555
LN1: 0.0001266002655029297
QKV Transform: 0.0020647048950195312
Attention Score: 0.0011239051818847656
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005555152893066406
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007367134094238281
Post-attention Dropout: 0.0003809928894042969
Post-attention residual: 0.00012969970703125
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030171871185302734
MLP_4h_h: 0.002588033676147461
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.01449441909790039
LN1: 0.00012493133544921875
QKV Transform: 0.0020585060119628906
Attention Score: 0.001131296157836914
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005555152893066406
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.00012755393981933594
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030050277709960938
MLP_4h_h: 0.002592325210571289
Post-MLP residual: 0.00038123130798339844
Attention layer time: 0.014492511749267578
LN1: 0.00012612342834472656
QKV Transform: 0.002062082290649414
Attention Score: 0.0011212825775146484
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007519721984863281
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.00012826919555664062
LN2: 0.00012612342834472656
MLP_h_4h: 0.003009796142578125
MLP_4h_h: 0.002588510513305664
Post-MLP residual: 0.0003821849822998047
Attention layer time: 0.014501810073852539
LN1: 0.00012373924255371094
QKV Transform: 0.0020608901977539062
Attention Score: 0.001125335693359375
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007369518280029297
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012874603271484375
LN2: 0.00013065338134765625
MLP_h_4h: 0.0030133724212646484
MLP_4h_h: 0.0025854110717773438
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014498710632324219
LN1: 0.00012373924255371094
QKV Transform: 0.0020592212677001953
Attention Score: 0.0011248588562011719
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007407665252685547
Post-attention Dropout: 0.0003848075866699219
Post-attention residual: 0.00012826919555664062
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030100345611572266
MLP_4h_h: 0.002585887908935547
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014487743377685547
LN1: 0.0001266002655029297
QKV Transform: 0.0020596981048583984
Attention Score: 0.00112152099609375
Attention Softmax: 0.002663850784301758
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.000385284423828125
Post-attention residual: 0.00012803077697753906
LN2: 0.0001361370086669922
MLP_h_4h: 0.0030133724212646484
MLP_4h_h: 0.0025959014892578125
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014528751373291016
LN1: 0.00012493133544921875
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011336803436279297
Attention Softmax: 0.0026617050170898438
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007379055023193359
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.0001277923583984375
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0025904178619384766
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.01452946662902832
LN1: 0.000125885009765625
QKV Transform: 0.0020651817321777344
Attention Score: 0.0011394023895263672
Attention Softmax: 0.002660989761352539
Attention Dropout: 0.00007057189941406250
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.0004010200500488281
Post-attention residual: 0.0001277923583984375
LN2: 0.00012826919555664062
MLP_h_4h: 0.003016233444213867
MLP_4h_h: 0.002595663070678711
Post-MLP residual: 0.0003845691680908203
Attention layer time: 0.014590978622436523
LN1: 0.00012540817260742188
QKV Transform: 0.002068042755126953
Attention Score: 0.0011296272277832031
Attention Softmax: 0.0026612281799316406
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.00038552284240722656
Post-attention residual: 0.00012803077697753906
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030241012573242188
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.01454019546508789
LN1: 0.0001251697540283203
QKV Transform: 0.0020651817321777344
Attention Score: 0.0011327266693115234
Attention Softmax: 0.002664327621459961
Attention Dropout: 0.00005578994750976562
Attention Over Value: 0.0007669925689697266
Attention linproj: 0.0007410049438476562
Post-attention Dropout: 0.00038433074951171875
Post-attention residual: 0.0001285076141357422
LN2: 0.00012803077697753906
MLP_h_4h: 0.003022432327270508
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.0003821849822998047
Attention layer time: 0.014585494995117188
LN1: 0.00012731552124023438
QKV Transform: 0.002066373825073242
Attention Score: 0.0011515617370605469
Attention Softmax: 0.002665996551513672
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0004017353057861328
Post-attention residual: 0.0001270771026611328
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030128955841064453
MLP_4h_h: 0.0025942325592041016
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.014573335647583008
LN1: 0.00012803077697753906
QKV Transform: 0.00206756591796875
Attention Score: 0.0011286735534667969
Attention Softmax: 0.0026607513427734375
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.0003867149353027344
Post-attention residual: 0.0001277923583984375
LN2: 0.00012755393981933594
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.002590656280517578
Post-MLP residual: 0.0003829002380371094
Attention layer time: 0.014543533325195312
LN1: 0.00012445449829101562
QKV Transform: 0.0020647048950195312
Attention Score: 0.0011339187622070312
Attention Softmax: 0.0026640892028808594
Attention Dropout: 0.00005555152893066406
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007393360137939453
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.00012803077697753906
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030167102813720703
MLP_4h_h: 0.002594470977783203
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014536619186401367
LN1: 0.0001266002655029297
QKV Transform: 0.002066373825073242
Attention Score: 0.001134634017944336
Attention Softmax: 0.0026655197143554688
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.00038051605224609375
Post-attention residual: 0.00012969970703125
LN2: 0.00012969970703125
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0025925636291503906
Post-MLP residual: 0.0003826618194580078
Attention layer time: 0.014564990997314453
LN1: 0.00012683868408203125
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011353492736816406
Attention Softmax: 0.0026628971099853516
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.0003826618194580078
Post-attention residual: 0.0001289844512939453
LN2: 0.00012969970703125
MLP_h_4h: 0.003019571304321289
MLP_4h_h: 0.0025916099548339844
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014540672302246094
LN1: 0.0001251697540283203
QKV Transform: 0.0020639896392822266
Attention Score: 0.0011348724365234375
Attention Softmax: 0.002663135528564453
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.00038361549377441406
Post-attention residual: 0.0001277923583984375
LN2: 0.0001270771026611328
MLP_h_4h: 0.00301361083984375
MLP_4h_h: 0.0025904178619384766
Post-MLP residual: 0.00038433074951171875
Attention layer time: 0.014552593231201172
LN1: 0.0001246929168701172
QKV Transform: 0.0020627975463867188
Attention Score: 0.0011272430419921875
Attention Softmax: 0.0026667118072509766
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007379055023193359
Post-attention Dropout: 0.0003838539123535156
Post-attention residual: 0.0001342296600341797
LN2: 0.0001289844512939453
MLP_h_4h: 0.003016233444213867
MLP_4h_h: 0.0025942325592041016
Post-MLP residual: 0.00037741661071777344
Attention layer time: 0.014549970626831055
LN1: 0.0001251697540283203
QKV Transform: 0.003435373306274414
Attention Score: 0.0011332035064697266
Attention Softmax: 0.002667665481567383
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.0003826618194580078
Post-attention residual: 0.0001380443572998047
LN2: 0.0001285076141357422
MLP_h_4h: 0.0030128955841064453
MLP_4h_h: 0.002589702606201172
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.01594090461730957
LN1: 0.0001251697540283203
QKV Transform: 0.0020818710327148438
Attention Score: 0.0011353492736816406
Attention Softmax: 0.00266265869140625
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007381439208984375
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.00012969970703125
LN2: 0.0001285076141357422
MLP_h_4h: 0.0030198097229003906
MLP_4h_h: 0.0025920867919921875
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014545202255249023
LN1: 0.0001246929168701172
QKV Transform: 0.002063751220703125
Attention Score: 0.0011365413665771484
Attention Softmax: 0.002664804458618164
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007405281066894531
Post-attention Dropout: 0.0003864765167236328
Post-attention residual: 0.0001285076141357422
LN2: 0.00012826919555664062
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.0025911331176757812
Post-MLP residual: 0.00038313865661621094
Attention layer time: 0.01454925537109375
LN1: 0.00012636184692382812
QKV Transform: 0.002056598663330078
Attention Score: 0.0011343955993652344
Attention Softmax: 0.0026667118072509766
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007534027099609375
Attention linproj: 0.0007395744323730469
Post-attention Dropout: 0.00038433074951171875
Post-attention residual: 0.00012826919555664062
LN2: 0.000125885009765625
MLP_h_4h: 0.003014087677001953
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014525175094604492
LN1: 0.0001246929168701172
QKV Transform: 0.002063274383544922
Attention Score: 0.0011293888092041016
Attention Softmax: 0.002663135528564453
Attention Dropout: 0.00006985664367675781
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007390975952148438
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.0001277923583984375
LN2: 0.00012731552124023438
MLP_h_4h: 0.003022432327270508
MLP_4h_h: 0.0025844573974609375
Post-MLP residual: 0.0003826618194580078
Attention layer time: 0.014553308486938477
LN1: 0.00012755393981933594
QKV Transform: 0.002065896987915039
Attention Score: 0.0011267662048339844
Attention Softmax: 0.0026586055755615234
Attention Dropout: 0.00005602836608886719
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007500648498535156
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.00012826919555664062
LN2: 0.00012683868408203125
MLP_h_4h: 0.003009796142578125
MLP_4h_h: 0.0025873184204101562
Post-MLP residual: 0.0003829002380371094
Attention layer time: 0.014537334442138672
LN1: 0.00012612342834472656
QKV Transform: 0.0020644664764404297
Attention Score: 0.0011279582977294922
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00006628036499023438
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.00038504600524902344
Post-attention residual: 0.0001277923583984375
LN2: 0.0001289844512939453
MLP_h_4h: 0.0030214786529541016
MLP_4h_h: 0.002590179443359375
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014545440673828125
LN1: 0.00012493133544921875
QKV Transform: 0.0020623207092285156
Attention Score: 0.001138925552368164
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.0001277923583984375
LN2: 0.00012731552124023438
MLP_h_4h: 0.0030117034912109375
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014508247375488281
LN1: 0.0001289844512939453
QKV Transform: 0.002065896987915039
Attention Score: 0.0011289119720458984
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007410049438476562
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.00012755393981933594
LN2: 0.0001277923583984375
MLP_h_4h: 0.003009796142578125
MLP_4h_h: 0.0025870800018310547
Post-MLP residual: 0.00040078163146972656
Attention layer time: 0.014557838439941406
LN1: 0.00012540817260742188
QKV Transform: 0.0020644664764404297
Attention Score: 0.0011205673217773438
Attention Softmax: 0.002641916275024414
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.00038552284240722656
Post-attention residual: 0.00012803077697753906
LN2: 0.00012731552124023438
MLP_h_4h: 0.0030126571655273438
MLP_4h_h: 0.0025949478149414062
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014501571655273438
LN1: 0.0001232624053955078
QKV Transform: 0.0020737648010253906
Attention Score: 0.0011224746704101562
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007388591766357422
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012755393981933594
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030078887939453125
MLP_4h_h: 0.0025866031646728516
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014488697052001953
LN1: 0.0001239776611328125
QKV Transform: 0.0020627975463867188
Attention Score: 0.0011208057403564453
Attention Softmax: 0.0026428699493408203
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.0001285076141357422
LN2: 0.00012731552124023438
MLP_h_4h: 0.0030126571655273438
MLP_4h_h: 0.0025861263275146484
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014482259750366211
LN1: 0.0001266002655029297
QKV Transform: 0.0020651817321777344
Attention Score: 0.0011210441589355469
Attention Softmax: 0.0026412010192871094
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007376670837402344
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.00013017654418945312
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030155181884765625
MLP_4h_h: 0.0025866031646728516
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.014482975006103516
LN1: 0.00014138221740722656
QKV Transform: 0.0020623207092285156
Attention Score: 0.0011281967163085938
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.0001285076141357422
LN2: 0.00012612342834472656
MLP_h_4h: 0.003008127212524414
MLP_4h_h: 0.00258636474609375
Post-MLP residual: 0.00038123130798339844
Attention layer time: 0.014511823654174805
LN1: 0.00012493133544921875
QKV Transform: 0.0020592212677001953
Attention Score: 0.0011217594146728516
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.0003826618194580078
Post-attention residual: 0.00012755393981933594
LN2: 0.00013947486877441406
MLP_h_4h: 0.0030095577239990234
MLP_4h_h: 0.002589702606201172
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.0145111083984375
LN1: 0.00012350082397460938
QKV Transform: 0.0020599365234375
Attention Score: 0.0011255741119384766
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007395744323730469
Post-attention Dropout: 0.0003769397735595703
Post-attention residual: 0.00012946128845214844
LN2: 0.0001456737518310547
MLP_h_4h: 0.003016948699951172
MLP_4h_h: 0.002585887908935547
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014506816864013672
LN1: 0.00012373924255371094
QKV Transform: 0.002057790756225586
Attention Score: 0.0011293888092041016
Attention Softmax: 0.002644062042236328
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007369518280029297
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.0001277923583984375
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030062198638916016
MLP_4h_h: 0.002590179443359375
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014488935470581055
LN1: 0.00012683868408203125
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011203289031982422
Attention Softmax: 0.0026667118072509766
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.00038623809814453125
Post-attention residual: 0.0001285076141357422
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030155181884765625
MLP_4h_h: 0.0025920867919921875
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014539241790771484
LN1: 0.000125885009765625
QKV Transform: 0.0020656585693359375
Attention Score: 0.0011496543884277344
Attention Softmax: 0.0026657581329345703
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007398128509521484
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.0001289844512939453
LN2: 0.0001277923583984375
MLP_h_4h: 0.003008604049682617
MLP_4h_h: 0.002594470977783203
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014550447463989258
LN1: 0.00012755393981933594
QKV Transform: 0.002065420150756836
Attention Score: 0.0011327266693115234
Attention Softmax: 0.0026633739471435547
Attention Dropout: 0.00007295608520507812
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.0003829002380371094
Post-attention residual: 0.00012803077697753906
LN2: 0.0001266002655029297
MLP_h_4h: 0.003016233444213867
MLP_4h_h: 0.002591848373413086
Post-MLP residual: 0.0003829002380371094
Attention layer time: 0.01456141471862793
LN1: 0.00012612342834472656
QKV Transform: 0.0020661354064941406
Attention Score: 0.0011336803436279297
Attention Softmax: 0.002663850784301758
Attention Dropout: 0.00005555152893066406
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0003871917724609375
Post-attention residual: 0.00012826919555664062
LN2: 0.0001285076141357422
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0025963783264160156
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.01455998420715332
LN1: 0.00012540817260742188
QKV Transform: 0.002063751220703125
Attention Score: 0.001130819320678711
Attention Softmax: 0.0026628971099853516
Attention Dropout: 0.00005650520324707031
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.00037932395935058594
Post-attention residual: 0.00012755393981933594
LN2: 0.00012636184692382812
MLP_h_4h: 0.00301361083984375
MLP_4h_h: 0.0025908946990966797
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014541864395141602
LN1: 0.00012636184692382812
QKV Transform: 0.0020651817321777344
Attention Score: 0.0011415481567382812
Attention Softmax: 0.0026650428771972656
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.0001285076141357422
LN2: 0.0001289844512939453
MLP_h_4h: 0.0030150413513183594
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014543771743774414
LN1: 0.00012636184692382812
QKV Transform: 0.0020647048950195312
Attention Score: 0.0011229515075683594
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005388259887695312
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007383823394775391
Post-attention Dropout: 0.0003986358642578125
Post-attention residual: 0.0001285076141357422
LN2: 0.0001270771026611328
MLP_h_4h: 0.003014802932739258
MLP_4h_h: 0.002588510513305664
Post-MLP residual: 0.00037789344787597656
Attention layer time: 0.01455068588256836
LN1: 0.00012540817260742188
QKV Transform: 0.0020608901977539062
Attention Score: 0.0011205673217773438
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007390975952148438
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.00012755393981933594
LN2: 0.00012564659118652344
MLP_h_4h: 0.003010272979736328
MLP_4h_h: 0.0025887489318847656
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014484405517578125
LN1: 0.0001239776611328125
QKV Transform: 0.0020618438720703125
Attention Score: 0.001123666763305664
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012803077697753906
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030095577239990234
MLP_4h_h: 0.002593517303466797
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014505863189697266
LN1: 0.00012493133544921875
QKV Transform: 0.0020570755004882812
Attention Score: 0.0011298656463623047
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005292892456054688
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007388591766357422
Post-attention Dropout: 0.0003829002380371094
Post-attention residual: 0.00012946128845214844
LN2: 0.00012803077697753906
MLP_h_4h: 0.003016948699951172
MLP_4h_h: 0.002587556838989258
Post-MLP residual: 0.0003833770751953125
Attention layer time: 0.014496326446533203
LN1: 0.00012636184692382812
QKV Transform: 0.0020647048950195312
Attention Score: 0.0011191368103027344
Attention Softmax: 0.002655029296875
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007388591766357422
Post-attention Dropout: 0.0003833770751953125
Post-attention residual: 0.0001285076141357422
LN2: 0.0001251697540283203
MLP_h_4h: 0.0030100345611572266
MLP_4h_h: 0.002590179443359375
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014502763748168945
LN1: 0.00012564659118652344
QKV Transform: 0.002055644989013672
Attention Score: 0.0011258125305175781
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007522106170654297
Attention linproj: 0.0007586479187011719
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.0001289844512939453
LN2: 0.000125885009765625
MLP_h_4h: 0.0030107498168945312
MLP_4h_h: 0.0025892257690429688
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014497518539428711
LN1: 0.00012493133544921875
QKV Transform: 0.0020627975463867188
Attention Score: 0.0011281967163085938
Attention Softmax: 0.0026412010192871094
Attention Dropout: 0.00006508827209472656
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007374286651611328
Post-attention Dropout: 0.0003788471221923828
Post-attention residual: 0.00012826919555664062
LN2: 0.00012564659118652344
MLP_h_4h: 0.003022909164428711
MLP_4h_h: 0.002588033676147461
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.0145111083984375
LN1: 0.0001251697540283203
QKV Transform: 0.0020627975463867188
Attention Score: 0.0011248588562011719
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005578994750976562
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.0003829002380371094
Post-attention residual: 0.00012755393981933594
LN2: 0.00012731552124023438
MLP_h_4h: 0.003009319305419922
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.01451420783996582
LN1: 0.00012540817260742188
QKV Transform: 0.0020647048950195312
Attention Score: 0.0011179447174072266
Attention Softmax: 0.0026628971099853516
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007462501525878906
Post-attention Dropout: 0.0003838539123535156
Post-attention residual: 0.0001285076141357422
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030155181884765625
MLP_4h_h: 0.0025908946990966797
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014514684677124023
LN1: 0.00012445449829101562
QKV Transform: 0.0020608901977539062
Attention Score: 0.001123189926147461
Attention Softmax: 0.002643108367919922
Attention Dropout: 0.00006532669067382812
Attention Over Value: 0.0007536411285400391
Attention linproj: 0.0007393360137939453
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.00012755393981933594
LN2: 0.00012612342834472656
MLP_h_4h: 0.003009796142578125
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014498710632324219
LN1: 0.000125885009765625
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011289119720458984
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.0003802776336669922
Post-attention residual: 0.00013065338134765625
LN2: 0.0001277923583984375
MLP_h_4h: 0.003011941909790039
MLP_4h_h: 0.0025870800018310547
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014493465423583984
LN1: 0.00012826919555664062
QKV Transform: 0.0020656585693359375
Attention Score: 0.0011227130889892578
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.00039887428283691406
Post-attention residual: 0.00012803077697753906
LN2: 0.00012826919555664062
MLP_h_4h: 0.0030159950256347656
MLP_4h_h: 0.002590656280517578
Post-MLP residual: 0.000377655029296875
Attention layer time: 0.0145111083984375
LN1: 0.00012445449829101562
QKV Transform: 0.002061605453491211
Attention Score: 0.0011222362518310547
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007390975952148438
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.0001277923583984375
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030128955841064453
MLP_4h_h: 0.0025954246520996094
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014497041702270508
LN1: 0.00012445449829101562
QKV Transform: 0.002060413360595703
Attention Score: 0.0011260509490966797
Attention Softmax: 0.00264739990234375
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007445812225341797
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.0001289844512939453
LN2: 0.00012969970703125
MLP_h_4h: 0.0030117034912109375
MLP_4h_h: 0.002590656280517578
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014522790908813477
LN1: 0.0001246929168701172
QKV Transform: 0.0020737648010253906
Attention Score: 0.0011246204376220703
Attention Softmax: 0.0026428699493408203
Attention Dropout: 0.00006842613220214844
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007371902465820312
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.00012969970703125
LN2: 0.0001289844512939453
MLP_h_4h: 0.0030159950256347656
MLP_4h_h: 0.0025866031646728516
Post-MLP residual: 0.0003771781921386719
Attention layer time: 0.014518022537231445
LN1: 0.00012564659118652344
QKV Transform: 0.0020618438720703125
Attention Score: 0.0011241436004638672
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007402896881103516
Post-attention Dropout: 0.00038504600524902344
Post-attention residual: 0.00012803077697753906
LN2: 0.00012755393981933594
MLP_h_4h: 0.0030107498168945312
MLP_4h_h: 0.002591848373413086
Post-MLP residual: 0.00038123130798339844
Attention layer time: 0.014519691467285156
LN1: 0.000125885009765625
QKV Transform: 0.002066373825073242
Attention Score: 0.0011224746704101562
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007517337799072266
Attention linproj: 0.0007569789886474609
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.0001285076141357422
LN2: 0.00012564659118652344
MLP_h_4h: 0.0030181407928466797
MLP_4h_h: 0.002592802047729492
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014511346817016602
LN1: 0.00012445449829101562
QKV Transform: 0.0020780563354492188
Attention Score: 0.0011267662048339844
Attention Softmax: 0.002641439437866211
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007469654083251953
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.0001277923583984375
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030074119567871094
MLP_4h_h: 0.002585887908935547
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014544010162353516
LN1: 0.00012564659118652344
QKV Transform: 0.002064228057861328
Attention Score: 0.0011227130889892578
Attention Softmax: 0.0026400089263916016
Attention Dropout: 0.00005578994750976562
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007309913635253906
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.00012874603271484375
LN2: 0.00012636184692382812
MLP_h_4h: 0.003015756607055664
MLP_4h_h: 0.0025882720947265625
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014504671096801758
LN1: 0.00012540817260742188
QKV Transform: 0.0020644664764404297
Attention Score: 0.001125335693359375
Attention Softmax: 0.002645730972290039
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007517337799072266
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.0003840923309326172
Post-attention residual: 0.00012826919555664062
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030150413513183594
MLP_4h_h: 0.0025911331176757812
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014506101608276367
LN1: 0.0001239776611328125
QKV Transform: 0.0020618438720703125
Attention Score: 0.0011205673217773438
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007517337799072266
Attention linproj: 0.0007395744323730469
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.0001285076141357422
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030121803283691406
MLP_4h_h: 0.002591371536254883
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.01449728012084961
LN1: 0.00012564659118652344
QKV Transform: 0.0020606517791748047
Attention Score: 0.0011534690856933594
Attention Softmax: 0.0026426315307617188
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.0001277923583984375
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030107498168945312
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014527559280395508
LN1: 0.00012755393981933594
QKV Transform: 0.0027713775634765625
Attention Score: 0.0011792182922363281
Attention Softmax: 0.002733945846557617
Attention Dropout: 0.00014638900756835938
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007731914520263672
Post-attention Dropout: 0.0004093647003173828
Post-attention residual: 0.00012874603271484375
LN2: 0.00016164779663085938
MLP_h_4h: 0.0030138492584228516
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.0003833770751953125
Attention layer time: 0.015876054763793945
LN1: 0.00012969970703125
QKV Transform: 0.0020704269409179688
Attention Score: 0.0011246204376220703
Attention Softmax: 0.0026421546936035156
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007565021514892578
Attention linproj: 0.0007498264312744141
Post-attention Dropout: 0.000400543212890625
Post-attention residual: 0.0001285076141357422
LN2: 0.00012826919555664062
MLP_h_4h: 0.003015756607055664
MLP_4h_h: 0.0025930404663085938
Post-MLP residual: 0.0003845691680908203
Attention layer time: 0.01456308364868164
LN1: 0.0001246929168701172
QKV Transform: 0.0020630359649658203
Attention Score: 0.0011219978332519531
Attention Softmax: 0.0026438236236572266
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.00075531005859375
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.0003819465637207031
Post-attention residual: 0.0001277923583984375
LN2: 0.0001308917999267578
MLP_h_4h: 0.0030121803283691406
MLP_4h_h: 0.002591848373413086
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.014502763748168945
LN1: 0.00012564659118652344
QKV Transform: 0.0020766258239746094
Attention Score: 0.0011203289031982422
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.000743865966796875
Post-attention Dropout: 0.00038361549377441406
Post-attention residual: 0.0001289844512939453
LN2: 0.0001270771026611328
MLP_h_4h: 0.003011465072631836
MLP_4h_h: 0.002588033676147461
Post-MLP residual: 0.00037741661071777344
Attention layer time: 0.014520883560180664
LN1: 0.00012445449829101562
QKV Transform: 0.0020685195922851562
Attention Score: 0.0011212825775146484
Attention Softmax: 0.0026407241821289062
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007560253143310547
Attention linproj: 0.0007381439208984375
Post-attention Dropout: 0.00038433074951171875
Post-attention residual: 0.00013017654418945312
LN2: 0.00012874603271484375
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.002586841583251953
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014496326446533203
LN1: 0.00012445449829101562
QKV Transform: 0.0020613670349121094
Attention Score: 0.001125335693359375
Attention Softmax: 0.0026428699493408203
Attention Dropout: 0.00005555152893066406
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007379055023193359
Post-attention Dropout: 0.00037980079650878906
Post-attention residual: 0.0001289844512939453
LN2: 0.0001266002655029297
MLP_h_4h: 0.003008604049682617
MLP_4h_h: 0.002591371536254883
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014501094818115234
LN1: 0.000125885009765625
QKV Transform: 0.0020551681518554688
Attention Score: 0.0011200904846191406
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007541179656982422
Post-attention Dropout: 0.00038361549377441406
Post-attention residual: 0.0001285076141357422
LN2: 0.00012540817260742188
MLP_h_4h: 0.003011465072631836
MLP_4h_h: 0.0025911331176757812
Post-MLP residual: 0.00037741661071777344
Attention layer time: 0.01448822021484375
LN1: 0.00012493133544921875
QKV Transform: 0.002058744430541992
Attention Score: 0.0011217594146728516
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00008559226989746094
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007417201995849609
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.00012803077697753906
LN2: 0.00012636184692382812
MLP_h_4h: 0.003007650375366211
MLP_4h_h: 0.00258636474609375
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.014529943466186523
LN1: 0.00012493133544921875
QKV Transform: 0.0020623207092285156
Attention Score: 0.001123666763305664
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00005650520324707031
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007317066192626953
Post-attention Dropout: 0.0004036426544189453
Post-attention residual: 0.00012826919555664062
LN2: 0.00012803077697753906
MLP_h_4h: 0.0030143260955810547
MLP_4h_h: 0.0025959014892578125
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.01454782485961914
LN1: 0.000125885009765625
QKV Transform: 0.0020651817321777344
Attention Score: 0.0011239051818847656
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.000766754150390625
Attention linproj: 0.0007450580596923828
Post-attention Dropout: 0.0003848075866699219
Post-attention residual: 0.0001277923583984375
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030078887939453125
MLP_4h_h: 0.0025877952575683594
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.014520883560180664
LN1: 0.00012445449829101562
QKV Transform: 0.002062082290649414
Attention Score: 0.0011229515075683594
Attention Softmax: 0.002643585205078125
Attention Dropout: 0.00005173683166503906
Attention Over Value: 0.0007507801055908203
Attention linproj: 0.0007390975952148438
Post-attention Dropout: 0.0003783702850341797
Post-attention residual: 0.00012803077697753906
LN2: 0.00012564659118652344
MLP_h_4h: 0.003007650375366211
MLP_4h_h: 0.002589702606201172
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014467000961303711
LN1: 0.00012612342834472656
QKV Transform: 0.0020599365234375
Attention Score: 0.0011451244354248047
Attention Softmax: 0.0026433467864990234
Attention Dropout: 0.00005102157592773438
Attention Over Value: 0.0007538795471191406
Attention linproj: 0.0007426738739013672
Post-attention Dropout: 0.0003974437713623047
Post-attention residual: 0.00012755393981933594
LN2: 0.0001266002655029297
MLP_h_4h: 0.003008604049682617
MLP_4h_h: 0.0025882720947265625
Post-MLP residual: 0.00038886070251464844
Attention layer time: 0.014535903930664062
LN1: 0.0001266002655029297
QKV Transform: 0.0020656585693359375
Attention Score: 0.0011174678802490234
Attention Softmax: 0.002640962600708008
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007524490356445312
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.00039577484130859375
Post-attention residual: 0.0001277923583984375
LN2: 0.00012826919555664062
MLP_h_4h: 0.0030145645141601562
MLP_4h_h: 0.0025877952575683594
Post-MLP residual: 0.0003826618194580078
Attention layer time: 0.014506101608276367
LN1: 0.00012540817260742188
QKV Transform: 0.0020635128021240234
Attention Score: 0.0011289119720458984
Attention Softmax: 0.0026443004608154297
Attention Dropout: 0.00005650520324707031
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007412433624267578
Post-attention Dropout: 0.00038170814514160156
Post-attention residual: 0.00012755393981933594
LN2: 0.00012612342834472656
MLP_h_4h: 0.0030133724212646484
MLP_4h_h: 0.0025937557220458984
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014511346817016602
LN1: 0.00012421607971191406
QKV Transform: 0.002078533172607422
Attention Score: 0.0011184215545654297
Attention Softmax: 0.0026445388793945312
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.000400543212890625
Post-attention residual: 0.00012874603271484375
LN2: 0.00012826919555664062
MLP_h_4h: 0.003017902374267578
MLP_4h_h: 0.002592325210571289
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.014528751373291016
LN1: 0.00013399124145507812
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011208057403564453
Attention Softmax: 0.002642393112182617
Attention Dropout: 0.00005364418029785156
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007383823394775391
Post-attention Dropout: 0.00038552284240722656
Post-attention residual: 0.00012946128845214844
LN2: 0.00012922286987304688
MLP_h_4h: 0.0030143260955810547
MLP_4h_h: 0.0025877952575683594
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.01450657844543457
LN1: 0.00012421607971191406
QKV Transform: 0.002061605453491211
Attention Score: 0.0011241436004638672
Attention Softmax: 0.002645254135131836
Attention Dropout: 0.00005412101745605469
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007731914520263672
Post-attention Dropout: 0.0003833770751953125
Post-attention residual: 0.00012803077697753906
LN2: 0.00012636184692382812
MLP_h_4h: 0.0030121803283691406
MLP_4h_h: 0.0025908946990966797
Post-MLP residual: 0.0003802776336669922
Attention layer time: 0.014529943466186523
LN1: 0.00012493133544921875
QKV Transform: 0.0020732879638671875
Attention Score: 0.0011208057403564453
Attention Softmax: 0.002644777297973633
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007526874542236328
Attention linproj: 0.0007433891296386719
Post-attention Dropout: 0.0003826618194580078
Post-attention residual: 0.00012826919555664062
LN2: 0.0001266002655029297
MLP_h_4h: 0.0030269622802734375
MLP_4h_h: 0.002588987350463867
Post-MLP residual: 0.0003795623779296875
Attention layer time: 0.01452946662902832
LN1: 0.00012373924255371094
QKV Transform: 0.0020623207092285156
Attention Score: 0.001119375228881836
Attention Softmax: 0.0026416778564453125
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007386207580566406
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.0001277923583984375
LN2: 0.00012636184692382812
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.0025920867919921875
Post-MLP residual: 0.0003790855407714844
Attention layer time: 0.014485597610473633
LN1: 0.0001251697540283203
QKV Transform: 0.002062559127807617
Attention Score: 0.0011239051818847656
Attention Softmax: 0.002664804458618164
Attention Dropout: 0.00005626678466796875
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007526874542236328
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012826919555664062
LN2: 0.00012683868408203125
MLP_h_4h: 0.0030176639556884766
MLP_4h_h: 0.0025937557220458984
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.01454615592956543
LN1: 0.00012636184692382812
QKV Transform: 0.002070188522338867
Attention Score: 0.001127481460571289
Attention Softmax: 0.002668142318725586
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.00012826919555664062
LN2: 0.00013637542724609375
MLP_h_4h: 0.003022432327270508
MLP_4h_h: 0.002590656280517578
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014553546905517578
LN1: 0.00012421607971191406
QKV Transform: 0.0020627975463867188
Attention Score: 0.0011436939239501953
Attention Softmax: 0.0026450157165527344
Attention Dropout: 0.00005125999450683594
Attention Over Value: 0.0007529258728027344
Attention linproj: 0.0007522106170654297
Post-attention Dropout: 0.00037860870361328125
Post-attention residual: 0.00012755393981933594
LN2: 0.000125885009765625
MLP_h_4h: 0.003014087677001953
MLP_4h_h: 0.0025970935821533203
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014545202255249023
LN1: 0.00012636184692382812
QKV Transform: 0.0020706653594970703
Attention Score: 0.0011327266693115234
Attention Softmax: 0.0026607513427734375
Attention Dropout: 0.00007343292236328125
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007441043853759766
Post-attention Dropout: 0.00038123130798339844
Post-attention residual: 0.00012755393981933594
LN2: 0.00012636184692382812
MLP_h_4h: 0.003014087677001953
MLP_4h_h: 0.002589702606201172
Post-MLP residual: 0.00038433074951171875
Attention layer time: 0.014559507369995117
LN1: 0.00012636184692382812
QKV Transform: 0.002071380615234375
Attention Score: 0.0011534690856933594
Attention Softmax: 0.0026712417602539062
Attention Dropout: 0.00005245208740234375
Attention Over Value: 0.0007557868957519531
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.0003857612609863281
Post-attention residual: 0.00012755393981933594
LN2: 0.00012969970703125
MLP_h_4h: 0.003022909164428711
MLP_4h_h: 0.0025968551635742188
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014606475830078125
LN1: 0.00012373924255371094
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011279582977294922
Attention Softmax: 0.002665996551513672
Attention Dropout: 0.00006318092346191406
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.00012755393981933594
LN2: 0.00012612342834472656
MLP_h_4h: 0.003014087677001953
MLP_4h_h: 0.002595186233520508
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014540433883666992
LN1: 0.00012540817260742188
QKV Transform: 0.0020678043365478516
Attention Score: 0.0011463165283203125
Attention Softmax: 0.0026662349700927734
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.0003986358642578125
Post-attention residual: 0.0001277923583984375
LN2: 0.00012755393981933594
MLP_h_4h: 0.003015279769897461
MLP_4h_h: 0.00258636474609375
Post-MLP residual: 0.0003829002380371094
Attention layer time: 0.0145721435546875
LN1: 0.00012731552124023438
QKV Transform: 0.002072572708129883
Attention Score: 0.0011272430419921875
Attention Softmax: 0.0026636123657226562
Attention Dropout: 0.00005459785461425781
Attention Over Value: 0.0007531642913818359
Attention linproj: 0.0007455348968505859
Post-attention Dropout: 0.00038433074951171875
Post-attention residual: 0.0001285076141357422
LN2: 0.00012826919555664062
MLP_h_4h: 0.003026247024536133
MLP_4h_h: 0.0025899410247802734
Post-MLP residual: 0.0003814697265625
Attention layer time: 0.014552116394042969
LN1: 0.00012421607971191406
QKV Transform: 0.0020682811737060547
Attention Score: 0.0011322498321533203
Attention Softmax: 0.0026645660400390625
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007421970367431641
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012755393981933594
LN2: 0.00012683868408203125
MLP_h_4h: 0.003019094467163086
MLP_4h_h: 0.002593994140625
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014547109603881836
LN1: 0.0001308917999267578
QKV Transform: 0.0020720958709716797
Attention Score: 0.0011370182037353516
Attention Softmax: 0.0026667118072509766
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007450580596923828
Post-attention Dropout: 0.00038051605224609375
Post-attention residual: 0.00012946128845214844
LN2: 0.00012874603271484375
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.0025932788848876953
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014560222625732422
LN1: 0.00012731552124023438
QKV Transform: 0.0020711421966552734
Attention Score: 0.001131296157836914
Attention Softmax: 0.002661466598510742
Attention Dropout: 0.00005316734313964844
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.00074005126953125
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.00012803077697753906
LN2: 0.00012826919555664062
MLP_h_4h: 0.0030219554901123047
MLP_4h_h: 0.0025920867919921875
Post-MLP residual: 0.0003829002380371094
Attention layer time: 0.014544010162353516
LN1: 0.000133514404296875
QKV Transform: 0.00206756591796875
Attention Score: 0.001135110855102539
Attention Softmax: 0.0026636123657226562
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007436275482177734
Post-attention Dropout: 0.0003840923309326172
Post-attention residual: 0.00012826919555664062
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030181407928466797
MLP_4h_h: 0.002593994140625
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014573335647583008
LN1: 0.0001246929168701172
QKV Transform: 0.0020661354064941406
Attention Score: 0.001131296157836914
Attention Softmax: 0.002666950225830078
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007450580596923828
Post-attention Dropout: 0.0003867149353027344
Post-attention residual: 0.00012803077697753906
LN2: 0.0001285076141357422
MLP_h_4h: 0.003017902374267578
MLP_4h_h: 0.0025920867919921875
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014545917510986328
LN1: 0.0001251697540283203
QKV Transform: 0.002080202102661133
Attention Score: 0.0011296272277832031
Attention Softmax: 0.002662181854248047
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007548332214355469
Attention linproj: 0.0007410049438476562
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.00012803077697753906
LN2: 0.00012922286987304688
MLP_h_4h: 0.003024578094482422
MLP_4h_h: 0.002590656280517578
Post-MLP residual: 0.00037932395935058594
Attention layer time: 0.014547109603881836
LN1: 0.00012612342834472656
QKV Transform: 0.0020689964294433594
Attention Score: 0.0011336803436279297
Attention Softmax: 0.002666950225830078
Attention Dropout: 0.00005483627319335938
Attention Over Value: 0.0007562637329101562
Attention linproj: 0.0007574558258056641
Post-attention Dropout: 0.0003814697265625
Post-attention residual: 0.00012731552124023438
LN2: 0.00012612342834472656
MLP_h_4h: 0.0030159950256347656
MLP_4h_h: 0.002596139907836914
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014563560485839844
LN1: 0.00012636184692382812
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011279582977294922
Attention Softmax: 0.0026657581329345703
Attention Dropout: 0.00005197525024414062
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007445812225341797
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.00012874603271484375
LN2: 0.0001270771026611328
MLP_h_4h: 0.0030121803283691406
MLP_4h_h: 0.002592802047729492
Post-MLP residual: 0.00037860870361328125
Attention layer time: 0.01459646224975586
LN1: 0.00012540817260742188
QKV Transform: 0.0020699501037597656
Attention Score: 0.0011317729949951172
Attention Softmax: 0.0026612281799316406
Attention Dropout: 0.00005269050598144531
Attention Over Value: 0.0007541179656982422
Attention linproj: 0.0007407665252685547
Post-attention Dropout: 0.0003809928894042969
Post-attention residual: 0.0001277923583984375
LN2: 0.00012993812561035156
MLP_h_4h: 0.003022909164428711
MLP_4h_h: 0.0025932788848876953
Post-MLP residual: 0.0003819465637207031
Attention layer time: 0.014541864395141602
LN1: 0.0001246929168701172
QKV Transform: 0.0020673274993896484
Attention Score: 0.0011327266693115234
Attention Softmax: 0.0026640892028808594
Attention Dropout: 0.00005555152893066406
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007479190826416016
Post-attention Dropout: 0.0003848075866699219
Post-attention residual: 0.00012922286987304688
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030198097229003906
MLP_4h_h: 0.002596616744995117
Post-MLP residual: 0.0003783702850341797
Attention layer time: 0.01455998420715332
LN1: 0.00012564659118652344
QKV Transform: 0.0020704269409179688
Attention Score: 0.0011343955993652344
Attention Softmax: 0.0026674270629882812
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007545948028564453
Attention linproj: 0.0007486343383789062
Post-attention Dropout: 0.0003840923309326172
Post-attention residual: 0.00012803077697753906
LN2: 0.0001380443572998047
MLP_h_4h: 0.0030171871185302734
MLP_4h_h: 0.0025925636291503906
Post-MLP residual: 0.0003788471221923828
Attention layer time: 0.014557838439941406
LN1: 0.0001246929168701172
QKV Transform: 0.0020661354064941406
Attention Score: 0.0011315345764160156
Attention Softmax: 0.0026617050170898438
Attention Dropout: 0.00006461143493652344
Attention Over Value: 0.0007550716400146484
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.00038242340087890625
Post-attention residual: 0.0001289844512939453
LN2: 0.00012612342834472656
MLP_h_4h: 0.0030345916748046875
MLP_4h_h: 0.0025949478149414062
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014578580856323242
LN1: 0.000125885009765625
QKV Transform: 0.002067089080810547
Attention Score: 0.0011358261108398438
Attention Softmax: 0.0026645660400390625
Attention Dropout: 0.00005531311035156250
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007429122924804688
Post-attention Dropout: 0.0003848075866699219
Post-attention residual: 0.0001277923583984375
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030188560485839844
MLP_4h_h: 0.0025942325592041016
Post-MLP residual: 0.0003809928894042969
Attention layer time: 0.014573097229003906
LN1: 0.00012540817260742188
QKV Transform: 0.0020723342895507812
Attention Score: 0.0011317729949951172
Attention Softmax: 0.0026674270629882812
Attention Dropout: 0.00006508827209472656
Attention Over Value: 0.0007522106170654297
Attention linproj: 0.0007460117340087891
Post-attention Dropout: 0.00038313865661621094
Post-attention residual: 0.0001289844512939453
LN2: 0.0001277923583984375
MLP_h_4h: 0.0030317306518554688
MLP_4h_h: 0.002597332000732422
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014577150344848633
LN1: 0.00012493133544921875
QKV Transform: 0.0020673274993896484
Attention Score: 0.0011432170867919922
Attention Softmax: 0.0026655197143554688
Attention Dropout: 0.00005149841308593750
Attention Over Value: 0.0007569789886474609
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.0003807544708251953
Post-attention residual: 0.0001285076141357422
LN2: 0.00012636184692382812
MLP_h_4h: 0.003017425537109375
MLP_4h_h: 0.0025937557220458984
Post-MLP residual: 0.00038051605224609375
Attention layer time: 0.014547348022460938
LN1: 0.00012731552124023438
QKV Transform: 0.002070903778076172
Attention Score: 0.0011322498321533203
Attention Softmax: 0.002663135528564453
Attention Dropout: 0.00007510185241699219
Attention Over Value: 0.0007567405700683594
Attention linproj: 0.0007357597351074219
Post-attention Dropout: 0.00038361549377441406
Post-attention residual: 0.0001277923583984375
LN2: 0.00012612342834472656
MLP_h_4h: 0.0030183792114257812
MLP_4h_h: 0.0025970935821533203
Post-MLP residual: 0.00038170814514160156
Attention layer time: 0.014571905136108398
LN1: 0.000125885009765625
QKV Transform: 0.0020711421966552734
Attention Score: 0.0011334419250488281
Attention Softmax: 0.0026633739471435547
Attention Dropout: 0.00005507469177246094
Attention Over Value: 0.0007555484771728516
Attention linproj: 0.0007431507110595703
Post-attention Dropout: 0.00038433074951171875
Post-attention residual: 0.00012922286987304688
LN2: 0.00012826919555664062
MLP_h_4h: 0.0030312538146972656
MLP_4h_h: 0.0025970935821533203
Post-MLP residual: 0.00037980079650878906
Attention layer time: 0.014572381973266602
LN1: 0.00012445449829101562
QKV Transform: 0.00206756591796875
Attention Score: 0.0011315345764160156
Attention Softmax: 0.002663850784301758
Attention Dropout: 0.00005602836608886719
Attention Over Value: 0.0007638931274414062
Attention linproj: 0.0007419586181640625
Post-attention Dropout: 0.0003790855407714844
Post-attention residual: 0.00012826919555664062
LN2: 0.00012564659118652344
MLP_h_4h: 0.003018617630004883
MLP_4h_h: 0.002594470977783203
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014553546905517578
LN1: 0.0001266002655029297
QKV Transform: 0.0020673274993896484
Attention Score: 0.001138925552368164
Attention Softmax: 0.002665281295776367
Attention Dropout: 0.00005221366882324219
Attention Over Value: 0.0007543563842773438
Attention linproj: 0.0007457733154296875
Post-attention Dropout: 0.0003821849822998047
Post-attention residual: 0.0001289844512939453
LN2: 0.0001277923583984375
MLP_h_4h: 0.003019571304321289
MLP_4h_h: 0.0026061534881591797
Post-MLP residual: 0.0003807544708251953
Attention layer time: 0.014558792114257812
LN1: 0.00012731552124023438
QKV Transform: 0.002072572708129883
Attention Score: 0.001130819320678711
Attention Softmax: 0.002663135528564453
Attention Dropout: 0.00005292892456054688
Attention Over Value: 0.0007579326629638672
Attention linproj: 0.0007414817810058594
Post-attention Dropout: 0.00039577484130859375
Post-attention residual: 0.00012803077697753906
LN2: 0.0001277923583984375
MLP_h_4h: 0.003021717071533203
MLP_4h_h: 0.0025899410247802734
Post-MLP residual: 0.0003781318664550781
Attention layer time: 0.014557838439941406
Transformer duration (in seconds): 0.0146
Transformer throughput (in TFLOP/s): 141.437
Transformer - MLP - Attention (in seconds): 0.0010
========================================================================================================================
