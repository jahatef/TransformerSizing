num_attention_heads: 128, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0391
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 84.332
Elapsed time for attention_key_query_prob (512x2048x64x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x64x2048): 56.870
Elapsed time for attention_prob_times_values (512x2048x2048x64): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x64): 51.161
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 100.729
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0512
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 85.849
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0814
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 54.022

Attention duration (in seconds): 0.0602
Attention throughput (in TFLOP/s): 82.141
MLP duration (in seconds): 0.1326
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1929
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2892
Attention throughput (in TFLOP/s): 17.108
MLP duration (in seconds): 0.1240
MLP throughput (in TFLOP/s): 70.911
Transformer duration (in seconds): 0.2897
Transformer throughput (in TFLOP/s): 47.448
Transformer - MLP - Attention (in seconds): -0.1236
========================================================================================================================
num_attention_heads: 128, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 87.463
Elapsed time for attention_key_query_prob (512x2048x65x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x65x2048): 43.000
Elapsed time for attention_prob_times_values (512x2048x2048x65): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x65): 33.313
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 101.574
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 101.774
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 81.131

Attention duration (in seconds): 0.0649
Attention throughput (in TFLOP/s): 78.456
MLP duration (in seconds): 0.1005
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1654
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 17.642
MLP duration (in seconds): 0.0925
MLP throughput (in TFLOP/s): 98.072
Transformer duration (in seconds): 0.2892
Transformer throughput (in TFLOP/s): 48.996
Transformer - MLP - Attention (in seconds): -0.0921
========================================================================================================================
num_attention_heads: 128, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 83.292
Elapsed time for attention_key_query_prob (512x2048x66x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x66x2048): 43.905
Elapsed time for attention_prob_times_values (512x2048x2048x66): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x66): 34.692
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 101.583
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0459
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 101.902
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0684
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 68.391

Attention duration (in seconds): 0.0683
Attention throughput (in TFLOP/s): 76.833
MLP duration (in seconds): 0.1143
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1825
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 18.160
MLP duration (in seconds): 0.1089
MLP throughput (in TFLOP/s): 85.876
Transformer duration (in seconds): 0.2887
Transformer throughput (in TFLOP/s): 50.558
Transformer - MLP - Attention (in seconds): -0.1090
========================================================================================================================
num_attention_heads: 128, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 85.149
Elapsed time for attention_key_query_prob (512x2048x67x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x67x2048): 43.518
Elapsed time for attention_prob_times_values (512x2048x2048x67): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x67): 34.169
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 100.966
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0474
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 101.745
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0634
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 76.001

Attention duration (in seconds): 0.0694
Attention throughput (in TFLOP/s): 77.718
MLP duration (in seconds): 0.1108
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1802
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 18.683
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 97.939
Transformer duration (in seconds): 0.2890
Transformer throughput (in TFLOP/s): 52.024
Transformer - MLP - Attention (in seconds): -0.0982
========================================================================================================================
num_attention_heads: 128, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 97.982
Elapsed time for attention_key_query_prob (512x2048x68x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x68x2048): 44.396
Elapsed time for attention_prob_times_values (512x2048x2048x68): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x68): 34.119
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 97.609
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0489
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 101.553
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0698
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 71.110

Attention duration (in seconds): 0.0659
Attention throughput (in TFLOP/s): 84.257
MLP duration (in seconds): 0.1187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1846
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 19.212
MLP duration (in seconds): 0.1055
MLP throughput (in TFLOP/s): 94.143
Transformer duration (in seconds): 0.2903
Transformer throughput (in TFLOP/s): 53.319
Transformer - MLP - Attention (in seconds): -0.1040
========================================================================================================================
num_attention_heads: 128, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0377
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 101.598
Elapsed time for attention_key_query_prob (512x2048x69x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x69x2048): 44.184
Elapsed time for attention_prob_times_values (512x2048x2048x69): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x69): 35.166
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 101.523
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 101.773
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0686
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 74.564

Attention duration (in seconds): 0.0655
Attention throughput (in TFLOP/s): 87.149
MLP duration (in seconds): 0.1188
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1843
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2890
Attention throughput (in TFLOP/s): 19.738
MLP duration (in seconds): 0.1040
MLP throughput (in TFLOP/s): 98.298
Transformer duration (in seconds): 0.2969
Transformer throughput (in TFLOP/s): 53.649
Transformer - MLP - Attention (in seconds): -0.0961
========================================================================================================================
num_attention_heads: 128, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 101.576
Elapsed time for attention_key_query_prob (512x2048x70x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x70x2048): 45.256
Elapsed time for attention_prob_times_values (512x2048x2048x70): 0.0082
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x70): 36.548
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 101.634
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 101.838
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0729
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 72.142

Attention duration (in seconds): 0.0667
Attention throughput (in TFLOP/s): 87.949
MLP duration (in seconds): 0.1246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1913
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 20.303
MLP duration (in seconds): 0.1228
MLP throughput (in TFLOP/s): 85.688
Transformer duration (in seconds): 0.2942
Transformer throughput (in TFLOP/s): 55.704
Transformer - MLP - Attention (in seconds): -0.1174
========================================================================================================================
num_attention_heads: 128, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 101.718
Elapsed time for attention_key_query_prob (512x2048x71x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x71x2048): 44.935
Elapsed time for attention_prob_times_values (512x2048x2048x71): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x71): 36.109
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 101.416
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 101.603
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0735
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 73.608

Attention duration (in seconds): 0.0685
Attention throughput (in TFLOP/s): 87.942
MLP duration (in seconds): 0.1268
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1953
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 20.854
MLP duration (in seconds): 0.1245
MLP throughput (in TFLOP/s): 86.958
Transformer duration (in seconds): 0.2981
Transformer throughput (in TFLOP/s): 56.515
Transformer - MLP - Attention (in seconds): -0.1152
========================================================================================================================
num_attention_heads: 128, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 101.824
Elapsed time for attention_key_query_prob (512x2048x72x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x72x2048): 46.192
Elapsed time for attention_prob_times_values (512x2048x2048x72): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x72): 36.067
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 101.162
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 101.693
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 72.603

Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 88.324
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2014
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000


Actual
------
Attention duration (in seconds): 0.2889
Attention throughput (in TFLOP/s): 21.411
MLP duration (in seconds): 0.1138
MLP throughput (in TFLOP/s): 97.797
Transformer duration (in seconds): 0.2891
Transformer throughput (in TFLOP/s): 59.903
Transformer - MLP - Attention (in seconds): -0.1136
========================================================================================================================
num_attention_heads: 128, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 101.754
Elapsed time for attention_key_query_prob (512x2048x73x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x73x2048): 44.859
Elapsed time for attention_prob_times_values (512x2048x2048x73): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x73): 36.832
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 101.492
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0559
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 102.287
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0777
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 73.617

Attention duration (in seconds): 0.0718
Attention throughput (in TFLOP/s): 88.462
MLP duration (in seconds): 0.1337
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2054
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 102.213
Elapsed time for attention_key_query_prob (512x2048x74x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x74x2048): 45.851
Elapsed time for attention_prob_times_values (512x2048x2048x74): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x74): 38.401
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 102.202
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 102.209
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0827
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 71.073

Attention duration (in seconds): 0.0727
Attention throughput (in TFLOP/s): 89.578
MLP duration (in seconds): 0.1403
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2130
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 102.150
Elapsed time for attention_key_query_prob (512x2048x75x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x75x2048): 45.699
Elapsed time for attention_prob_times_values (512x2048x2048x75): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x75): 37.714
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 102.135
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 102.136
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 69.445

Attention duration (in seconds): 0.0747
Attention throughput (in TFLOP/s): 89.456
MLP duration (in seconds): 0.1461
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2208
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0459
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 101.252
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 46.730
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 39.460
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 101.698
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0607
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 102.176
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0853
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 72.670

Attention duration (in seconds): 0.0764
Attention throughput (in TFLOP/s): 89.672
MLP duration (in seconds): 0.1460
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 102.022
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 46.524
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 38.489
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 101.466
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0634
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 100.472
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 72.291

Attention duration (in seconds): 0.0782
Attention throughput (in TFLOP/s): 89.883
MLP duration (in seconds): 0.1514
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 101.852
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 47.604
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0083
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 40.146
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 102.168
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0639
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 102.215
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0886
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 73.764

Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 90.632
MLP duration (in seconds): 0.1525
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2319
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 101.303
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 47.512
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 39.386
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 101.751
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0657
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 102.028
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0924
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 72.526

Attention duration (in seconds): 0.0818
Attention throughput (in TFLOP/s): 90.181
MLP duration (in seconds): 0.1581
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2399
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 102.181
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 49.285
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 39.652
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 102.148
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0684
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 100.434
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0980
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 70.155

Attention duration (in seconds): 0.0829
Attention throughput (in TFLOP/s): 91.190
MLP duration (in seconds): 0.1664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2493
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 101.709
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 47.371
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 40.238
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 101.691
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0690
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 102.139
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0975
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 72.243

Attention duration (in seconds): 0.0853
Attention throughput (in TFLOP/s): 90.791
MLP duration (in seconds): 0.1665
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2517
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 101.947
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 48.245
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 41.903
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 101.587
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0706
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 102.308
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.1010
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 71.466

Attention duration (in seconds): 0.0866
Attention throughput (in TFLOP/s): 91.517
MLP duration (in seconds): 0.1716
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 102.173
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 45.653
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 40.986
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 102.260
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0733
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 100.973
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1023
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 72.320

Attention duration (in seconds): 0.0889
Attention throughput (in TFLOP/s): 91.238
MLP duration (in seconds): 0.1755
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2644
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
[2023-11-22 14:50:03,989] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0550
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 100.807
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 48.299
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 41.054
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 102.673
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0721
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 102.611
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.1020
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 72.525

Attention duration (in seconds): 0.0891
Attention throughput (in TFLOP/s): 91.012
MLP duration (in seconds): 0.1741
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2632
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 99.667
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 48.820
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 43.000
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0190
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 99.841
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0739
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 102.497
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.1031
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 73.518

Attention duration (in seconds): 0.0918
Attention throughput (in TFLOP/s): 90.427
MLP duration (in seconds): 0.1770
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 100.488
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 48.954
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 42.055
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 102.751
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0768
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 100.986
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0900
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 86.240

Attention duration (in seconds): 0.0929
Attention throughput (in TFLOP/s): 91.352
MLP duration (in seconds): 0.1668
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2597
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 102.511
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 49.598
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0084
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 43.798
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 102.248
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0774
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 102.578
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.1073
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 74.003

Attention duration (in seconds): 0.0934
Attention throughput (in TFLOP/s): 92.936
MLP duration (in seconds): 0.1847
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2781
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 102.492
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 49.838
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 42.820
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 101.791
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0792
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 102.557
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.1113
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 73.051

Attention duration (in seconds): 0.0957
Attention throughput (in TFLOP/s): 92.775
MLP duration (in seconds): 0.1905
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2862
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0611
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 102.064
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 50.694
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 42.950
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 101.653
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0811
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 102.476
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.1191
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 69.794

Attention duration (in seconds): 0.0978
Attention throughput (in TFLOP/s): 92.744
MLP duration (in seconds): 0.2003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2981
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0622
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 102.497
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 48.863
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 43.745
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 102.462
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0831
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 102.407
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.1175
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 72.393

Attention duration (in seconds): 0.0995
Attention throughput (in TFLOP/s): 93.117
MLP duration (in seconds): 0.2005
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3001
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0638
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 102.262
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 49.596
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 45.477
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 102.240
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0848
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 102.538
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.1212
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 71.756

Attention duration (in seconds): 0.1013
Attention throughput (in TFLOP/s): 93.445
MLP duration (in seconds): 0.2060
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3074
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0651
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 102.455
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 49.604
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 44.447
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 102.043
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0866
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 102.668
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.3526
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 25.220

Attention duration (in seconds): 0.1035
Attention throughput (in TFLOP/s): 93.420
MLP duration (in seconds): 0.4392
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5427
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0663
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 102.805
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 50.482
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0085
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 46.553
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 102.848
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 101.813
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.1504
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 60.414

Attention duration (in seconds): 0.1047
Attention throughput (in TFLOP/s): 94.343
MLP duration (in seconds): 0.2397
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3444
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0679
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 102.508
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 50.395
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 45.324
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 102.792
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.1015
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 91.461
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.1649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 56.321

Attention duration (in seconds): 0.1073
Attention throughput (in TFLOP/s): 94.019
MLP duration (in seconds): 0.2664
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3737
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0720
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 98.897
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 51.210
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 47.207
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 102.604
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.1146
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 82.799
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.1669
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 56.850

Attention duration (in seconds): 0.1115
Attention throughput (in TFLOP/s): 92.329
MLP duration (in seconds): 0.2815
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0725
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 100.314
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 51.167
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 46.068
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 102.417
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.1075
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 90.124
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.1711
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 56.645

Attention duration (in seconds): 0.1129
Attention throughput (in TFLOP/s): 93.030
MLP duration (in seconds): 0.2786
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3915
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0724
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 102.512
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 66.166
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 47.805
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 102.434
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.1328
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 74.526
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.2092
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 47.306

Attention duration (in seconds): 0.1114
Attention throughput (in TFLOP/s): 96.226
MLP duration (in seconds): 0.3420
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4534
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0894
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 84.747
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 51.856
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 46.989
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 102.902
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.1184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 85.325
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.1879
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 53.776

Attention duration (in seconds): 0.1309
Attention throughput (in TFLOP/s): 83.575
MLP duration (in seconds): 0.3063
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4371
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0796
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 97.125
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 52.884
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 48.874
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 102.781
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.1294
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 79.667
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.2036
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 50.638

Attention duration (in seconds): 0.1213
Attention throughput (in TFLOP/s): 91.965
MLP duration (in seconds): 0.3331
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4544
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0931
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 84.801
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 52.443
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 47.963
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 102.601
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.1241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 84.789
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.2112
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 49.825

Attention duration (in seconds): 0.1357
Attention throughput (in TFLOP/s): 83.824
MLP duration (in seconds): 0.3353
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4710
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0911
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 88.400
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 53.737
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0086
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 49.802
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 102.450
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.1369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 78.428
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 56.680

Attention duration (in seconds): 0.1339
Attention throughput (in TFLOP/s): 86.594
MLP duration (in seconds): 0.3263
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4603
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.1009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 81.456
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 53.031
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 48.736
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 100.613
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.1418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 77.239
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.2119
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 51.702

Attention duration (in seconds): 0.1451
Attention throughput (in TFLOP/s): 81.440
MLP duration (in seconds): 0.3537
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4988
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 92.726
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 54.038
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 50.590
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 103.036
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.1302
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 85.800
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.1938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 57.658

Attention duration (in seconds): 0.1342
Attention throughput (in TFLOP/s): 89.753
MLP duration (in seconds): 0.3240
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4582
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.1011
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 84.533
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 53.602
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 49.569
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 102.799
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.1398
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 81.469
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.1693
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 67.285

Attention duration (in seconds): 0.1459
Attention throughput (in TFLOP/s): 84.113
MLP duration (in seconds): 0.3091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4551
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.1023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 85.153
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 55.121
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 50.108
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 102.593
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.1429
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 81.268
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.1680
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 69.134

Attention duration (in seconds): 0.1476
Attention throughput (in TFLOP/s): 84.732
MLP duration (in seconds): 0.3109
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.1074
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 82.665
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 53.439
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 50.104
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0289
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 102.295
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.1511
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 78.330
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.1806
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 65.536

Attention duration (in seconds): 0.1538
Attention throughput (in TFLOP/s): 82.848
MLP duration (in seconds): 0.3318
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4855
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.1152
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 78.557
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 54.429
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0087
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 52.157
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0293
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 102.892
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.1636
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 73.729
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.1857
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 64.954

Attention duration (in seconds): 0.1616
Attention throughput (in TFLOP/s): 80.296
MLP duration (in seconds): 0.3494
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5110
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.1154
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 79.906
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 54.076
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 50.662
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 102.954
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.1603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 76.677
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.1788
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 68.759

Attention duration (in seconds): 0.1628
Attention throughput (in TFLOP/s): 81.154
MLP duration (in seconds): 0.3391
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5019
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.1139
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 82.490
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 55.418
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 52.936
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0304
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 102.853
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.1661
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 75.395
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.1907
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 65.674

Attention duration (in seconds): 0.1614
Attention throughput (in TFLOP/s): 83.322
MLP duration (in seconds): 0.3568
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5183
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.1265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 75.629
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 54.766
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 51.508
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 102.663
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.1727
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 73.857
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.1977
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 64.524

Attention duration (in seconds): 0.1752
Attention throughput (in TFLOP/s): 78.153
MLP duration (in seconds): 0.3704
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5457
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.1229
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 79.306
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 55.945
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0088
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 53.649
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 103.291
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.1720
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 75.524
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.1823
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 71.256

Attention duration (in seconds): 0.1716
Attention throughput (in TFLOP/s): 81.235
MLP duration (in seconds): 0.3544
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5259
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.1305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 76.011
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 55.490
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 52.191
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 102.998
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.1753
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 75.450
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.1991
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 66.450

Attention duration (in seconds): 0.1804
Attention throughput (in TFLOP/s): 78.632
MLP duration (in seconds): 0.3744
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5548
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.1315
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 76.800
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 57.551
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 54.139
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 102.994
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.1851
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 72.759
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.2273
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 59.245

Attention duration (in seconds): 0.1815
Attention throughput (in TFLOP/s): 79.523
MLP duration (in seconds): 0.4125
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5939
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.1366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 75.278
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 55.193
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 52.392
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 102.533
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.1858
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 73.798
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.2759
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 49.689

Attention duration (in seconds): 0.1881
Attention throughput (in TFLOP/s): 78.056
MLP duration (in seconds): 0.4617
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6498
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.1380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 75.834
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 56.476
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 55.128
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 101.552
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.1918
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 72.737
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.2572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 54.259

Attention duration (in seconds): 0.1899
Attention throughput (in TFLOP/s): 78.634
MLP duration (in seconds): 0.4490
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6389
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.1382
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 77.045
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 55.933
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0092
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 53.476
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 103.137
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.1939
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 73.251
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.2841
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 49.981

Attention duration (in seconds): 0.1907
Attention throughput (in TFLOP/s): 79.635
MLP duration (in seconds): 0.4780
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6687
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.1434
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 75.578
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 57.217
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0089
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 55.771
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 103.203
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.1987
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 72.698
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.3005
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 48.084

Attention duration (in seconds): 0.1960
Attention throughput (in TFLOP/s): 78.793
MLP duration (in seconds): 0.4992
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6952
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.1481
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 74.426
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 56.588
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 54.297
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 103.053
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.2014
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 72.986
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.3086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 47.635

Attention duration (in seconds): 0.2019
Attention throughput (in TFLOP/s): 77.774
MLP duration (in seconds): 0.5100
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7119
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.1485
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 75.526
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 57.641
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 56.530
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 101.689
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.2049
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 72.972
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.3284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 45.526

Attention duration (in seconds): 0.2030
Attention throughput (in TFLOP/s): 78.650
MLP duration (in seconds): 0.5333
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7363
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.1581
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 72.138
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 57.270
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0093
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 54.804
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 102.563
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.2099
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 72.448
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.3196
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 47.576

Attention duration (in seconds): 0.2134
Attention throughput (in TFLOP/s): 76.043
MLP duration (in seconds): 0.5295
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7429
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.1621
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 71.536
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 58.961
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 56.728
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 103.298
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.2213
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 69.879
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.3144
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 49.176

Attention duration (in seconds): 0.2174
Attention throughput (in TFLOP/s): 75.880
MLP duration (in seconds): 0.5357
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7530
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.1606
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 73.411
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0092
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 56.498
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 43.503
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 99.950
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.2180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 72.110
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.3333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 47.161

Attention duration (in seconds): 0.2211
Attention throughput (in TFLOP/s): 75.812
MLP duration (in seconds): 0.5513
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7724
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.1571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 76.309
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 57.731
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0090
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 57.910
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 102.917
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.2253
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 70.925
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.3589
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 44.532

Attention duration (in seconds): 0.2140
Attention throughput (in TFLOP/s): 79.570
MLP duration (in seconds): 0.5842
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7982
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.1676
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 72.687
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 56.707
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0120
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 44.173
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 102.788
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.3463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 46.903
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.3528
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 46.047

Attention duration (in seconds): 0.2284
Attention throughput (in TFLOP/s): 75.749
MLP duration (in seconds): 0.6991
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.9275
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.1729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 71.620
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 58.809
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 58.494
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0400
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 103.269
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.2409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 68.521
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.3768
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 43.814

Attention duration (in seconds): 0.2310
Attention throughput (in TFLOP/s): 76.076
MLP duration (in seconds): 0.6178
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8488
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.1728
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 72.798
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 57.634
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0123
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 43.751
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 103.077
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.2355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 71.256
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.3684
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 45.538

Attention duration (in seconds): 0.2351
Attention throughput (in TFLOP/s): 75.922
MLP duration (in seconds): 0.6039
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8390
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.1786
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 71.569
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 58.135
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0091
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 59.346
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 99.970
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.2452
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 69.530
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.3847
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 44.309

Attention duration (in seconds): 0.2397
Attention throughput (in TFLOP/s): 75.633
MLP duration (in seconds): 0.6299
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8696
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.1753
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 74.094
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 57.825
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0124
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 43.881
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 103.043
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.2439
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 70.993
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.3864
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 44.819

Attention duration (in seconds): 0.2392
Attention throughput (in TFLOP/s): 76.967
MLP duration (in seconds): 0.6303
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8695
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
[2023-11-22 19:53:43,203] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2.1.1+rocm5.6 

num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.5574
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 62.508
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 70.917
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0166
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 53.943
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.1773
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 65.520
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.7542
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 61.596
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 1.3110
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 35.435

Attention duration (in seconds): 0.7638
Attention throughput (in TFLOP/s): 63.160
MLP duration (in seconds): 2.0652
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.5724
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 61.459
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 67.660
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0169
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 53.108
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.1800
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 65.148
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.7673
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 61.123
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 1.3113
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 35.769

Attention duration (in seconds): 0.7825
Attention throughput (in TFLOP/s): 62.232
MLP duration (in seconds): 2.0786
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8611
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.5802
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 61.213
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 68.916
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 55.770
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.1818
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 65.122
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.7812
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 60.616
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 1.3229
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 35.793

Attention duration (in seconds): 0.7912
Attention throughput (in TFLOP/s): 62.127
MLP duration (in seconds): 2.1041
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8953
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.5831
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 61.486
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 67.996
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 53.454
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.1832
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 65.217
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.7817
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 61.151
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 1.3282
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 35.992

Attention duration (in seconds): 0.7966
Attention throughput (in TFLOP/s): 62.283
MLP duration (in seconds): 2.1099
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.9066
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.5965
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 60.673
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 69.565
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0162
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 56.349
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.1871
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 64.476
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.7286
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 66.232
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 1.2903
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 37.402

Attention duration (in seconds): 0.8129
Attention throughput (in TFLOP/s): 61.606
MLP duration (in seconds): 2.0189
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.5960
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 61.307
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 68.386
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 53.854
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.1865
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 65.291
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.7131
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 68.309
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 1.3323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 36.565

Attention duration (in seconds): 0.8128
Attention throughput (in TFLOP/s): 62.182
MLP duration (in seconds): 2.0454
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8583
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.6097
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 60.491
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0132
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 69.768
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 56.510
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.1906
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 64.486
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.7252
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 67.808
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 1.3336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 36.874

Attention duration (in seconds): 0.8297
Attention throughput (in TFLOP/s): 61.478
MLP duration (in seconds): 2.0587
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.8885
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.6135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 60.681
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 68.866
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 54.317
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.1922
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 64.571
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.7360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 67.440
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 1.3598
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 36.501

Attention duration (in seconds): 0.8360
Attention throughput (in TFLOP/s): 61.577
MLP duration (in seconds): 2.0957
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.9318
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.6145
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 61.144
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 71.026
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0168
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 55.110
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.1923
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 65.122
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.7401
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 67.690
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 1.3810
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 36.276

Attention duration (in seconds): 0.8367
Attention throughput (in TFLOP/s): 62.091
MLP duration (in seconds): 2.1211
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 2.9578
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.6279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 60.391
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 68.519
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0170
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 54.753
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.1955
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 64.665
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.8396
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 60.224
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 1.4290
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 35.383

Attention duration (in seconds): 0.8540
Attention throughput (in TFLOP/s): 61.387
MLP duration (in seconds): 2.2685
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.1225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.6311
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 60.640
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0134
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 69.861
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0163
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 57.548
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.1973
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 64.674
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.8497
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 60.056
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 1.4408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 35.418

Attention duration (in seconds): 0.8581
Attention throughput (in TFLOP/s): 61.653
MLP duration (in seconds): 2.2904
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 3.1485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
