cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
cupy is not loaded.
[2023-09-06 22:45:07,834] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-09-06 22:45:08,766] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.145.10, master_port=6000
[2023-09-06 22:45:08,766] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-06 22:45:08,796] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 8192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8192x24576, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8192x24576, b=2048): 243.601
Elapsed time for attention_key_query_prob (256x2048x128x2048): 0.0025
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x128x2048): 108.164
Elapsed time for attention_prob_times_values (256x2048x2048x128): 0.0019
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x128): 141.689
Elapsed time for attention_linear_projection (4x8192x8192, b=2048): 0.0043
Throughput (in TFLOP/s) for attention_linear_projection (4x8192x8192, b=2048): 252.972
Elapsed time for mlp_h_to_4h (4x8192x32768, b=2048): 0.0180
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8192x32768, b=2048): 243.976
Elapsed time for mlp_4h_to_h (4x32768x8192, b=2048): 0.0198
Throughput (in TFLOP/s) for mlp_4h_to_h (4x32768x8192, b=2048): 222.483

Attention duration (in seconds): 0.0224
Attention throughput (in TFLOP/s): 221.196
MLP duration (in seconds): 0.0378
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0602
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8320x24960, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8320x24960, b=2048): 245.663
Elapsed time for attention_key_query_prob (256x2048x130x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x130x2048): 71.233
Elapsed time for attention_prob_times_values (256x2048x2048x130): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x130): 93.652
Elapsed time for attention_linear_projection (4x8320x8320, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x8320x8320, b=2048): 246.473
Elapsed time for mlp_h_to_4h (4x8320x33280, b=2048): 0.0183
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8320x33280, b=2048): 248.005
Elapsed time for mlp_4h_to_h (4x33280x8320, b=2048): 0.0197
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33280x8320, b=2048): 229.806

Attention duration (in seconds): 0.0254
Attention throughput (in TFLOP/s): 200.970
MLP duration (in seconds): 0.0380
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0634
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8448x25344, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8448x25344, b=2048): 237.215
Elapsed time for attention_key_query_prob (256x2048x132x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x132x2048): 71.905
Elapsed time for attention_prob_times_values (256x2048x2048x132): 0.0030
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x132): 94.219
Elapsed time for attention_linear_projection (4x8448x8448, b=2048): 0.0046
Throughput (in TFLOP/s) for attention_linear_projection (4x8448x8448, b=2048): 251.741
Elapsed time for mlp_h_to_4h (4x8448x33792, b=2048): 0.0193
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8448x33792, b=2048): 242.679
Elapsed time for mlp_4h_to_h (4x33792x8448, b=2048): 0.0201
Throughput (in TFLOP/s) for mlp_4h_to_h (4x33792x8448, b=2048): 232.909

Attention duration (in seconds): 0.0264
Attention throughput (in TFLOP/s): 198.765
MLP duration (in seconds): 0.0394
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0657
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8576x25728, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8576x25728, b=2048): 231.234
Elapsed time for attention_key_query_prob (256x2048x134x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x134x2048): 72.770
Elapsed time for attention_prob_times_values (256x2048x2048x134): 0.0031
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x134): 92.177
Elapsed time for attention_linear_projection (4x8576x8576, b=2048): 0.0047
Throughput (in TFLOP/s) for attention_linear_projection (4x8576x8576, b=2048): 255.571
Elapsed time for mlp_h_to_4h (4x8576x34304, b=2048): 0.0202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8576x34304, b=2048): 238.587
Elapsed time for mlp_4h_to_h (4x34304x8576, b=2048): 0.0207
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34304x8576, b=2048): 232.924

Attention duration (in seconds): 0.0274
Attention throughput (in TFLOP/s): 196.740
MLP duration (in seconds): 0.0409
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0683
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8704x26112, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8704x26112, b=2048): 236.543
Elapsed time for attention_key_query_prob (256x2048x136x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x136x2048): 98.773
Elapsed time for attention_prob_times_values (256x2048x2048x136): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x136): 122.095
Elapsed time for attention_linear_projection (4x8704x8704, b=2048): 0.0050
Throughput (in TFLOP/s) for attention_linear_projection (4x8704x8704, b=2048): 246.574
Elapsed time for mlp_h_to_4h (4x8704x34816, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8704x34816, b=2048): 241.378
Elapsed time for mlp_4h_to_h (4x34816x8704, b=2048): 0.0209
Throughput (in TFLOP/s) for mlp_4h_to_h (4x34816x8704, b=2048): 238.012

Attention duration (in seconds): 0.0261
Attention throughput (in TFLOP/s): 212.404
MLP duration (in seconds): 0.0414
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0676
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8832x26496, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8832x26496, b=2048): 233.902
Elapsed time for attention_key_query_prob (256x2048x138x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x138x2048): 74.556
Elapsed time for attention_prob_times_values (256x2048x2048x138): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x138): 92.990
Elapsed time for attention_linear_projection (4x8832x8832, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x8832x8832, b=2048): 247.058
Elapsed time for mlp_h_to_4h (4x8832x35328, b=2048): 0.0206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8832x35328, b=2048): 247.829
Elapsed time for mlp_4h_to_h (4x35328x8832, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35328x8832, b=2048): 232.783

Attention duration (in seconds): 0.0287
Attention throughput (in TFLOP/s): 198.589
MLP duration (in seconds): 0.0426
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0713
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 8960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x8960x26880, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x8960x26880, b=2048): 236.206
Elapsed time for attention_key_query_prob (256x2048x140x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x140x2048): 75.541
Elapsed time for attention_prob_times_values (256x2048x2048x140): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x140): 94.805
Elapsed time for attention_linear_projection (4x8960x8960, b=2048): 0.0052
Throughput (in TFLOP/s) for attention_linear_projection (4x8960x8960, b=2048): 254.917
Elapsed time for mlp_h_to_4h (4x8960x35840, b=2048): 0.0210
Throughput (in TFLOP/s) for mlp_h_to_4h (4x8960x35840, b=2048): 250.769
Elapsed time for mlp_4h_to_h (4x35840x8960, b=2048): 0.0215
Throughput (in TFLOP/s) for mlp_4h_to_h (4x35840x8960, b=2048): 244.547

Attention duration (in seconds): 0.0290
Attention throughput (in TFLOP/s): 202.043
MLP duration (in seconds): 0.0425
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0715
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9088x27264, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9088x27264, b=2048): 235.192
Elapsed time for attention_key_query_prob (256x2048x142x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x142x2048): 75.794
Elapsed time for attention_prob_times_values (256x2048x2048x142): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x142): 95.172
Elapsed time for attention_linear_projection (4x9088x9088, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x9088x9088, b=2048): 245.817
Elapsed time for mlp_h_to_4h (4x9088x36352, b=2048): 0.0222
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9088x36352, b=2048): 243.750
Elapsed time for mlp_4h_to_h (4x36352x9088, b=2048): 0.0220
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36352x9088, b=2048): 246.246

Attention duration (in seconds): 0.0300
Attention throughput (in TFLOP/s): 200.802
MLP duration (in seconds): 0.0442
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0742
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9216x27648, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9216x27648, b=2048): 235.489
Elapsed time for attention_key_query_prob (256x2048x144x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x144x2048): 108.739
Elapsed time for attention_prob_times_values (256x2048x2048x144): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x144): 134.255
Elapsed time for attention_linear_projection (4x9216x9216, b=2048): 0.0055
Throughput (in TFLOP/s) for attention_linear_projection (4x9216x9216, b=2048): 250.921
Elapsed time for mlp_h_to_4h (4x9216x36864, b=2048): 0.0228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9216x36864, b=2048): 244.186
Elapsed time for mlp_4h_to_h (4x36864x9216, b=2048): 0.0238
Throughput (in TFLOP/s) for mlp_4h_to_h (4x36864x9216, b=2048): 234.113

Attention duration (in seconds): 0.0284
Attention throughput (in TFLOP/s): 217.613
MLP duration (in seconds): 0.0466
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0750
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9344x28032, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9344x28032, b=2048): 233.357
Elapsed time for attention_key_query_prob (256x2048x146x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x146x2048): 78.156
Elapsed time for attention_prob_times_values (256x2048x2048x146): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x146): 96.242
Elapsed time for attention_linear_projection (4x9344x9344, b=2048): 0.0056
Throughput (in TFLOP/s) for attention_linear_projection (4x9344x9344, b=2048): 253.836
Elapsed time for mlp_h_to_4h (4x9344x37376, b=2048): 0.0226
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9344x37376, b=2048): 252.828
Elapsed time for mlp_4h_to_h (4x37376x9344, b=2048): 0.0246
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37376x9344, b=2048): 232.792

Attention duration (in seconds): 0.0313
Attention throughput (in TFLOP/s): 202.877
MLP duration (in seconds): 0.0472
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0785
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9472x28416, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9472x28416, b=2048): 228.931
Elapsed time for attention_key_query_prob (256x2048x148x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x148x2048): 78.973
Elapsed time for attention_prob_times_values (256x2048x2048x148): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x148): 97.325
Elapsed time for attention_linear_projection (4x9472x9472, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9472x9472, b=2048): 233.663
Elapsed time for mlp_h_to_4h (4x9472x37888, b=2048): 0.0250
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9472x37888, b=2048): 235.596
Elapsed time for mlp_4h_to_h (4x37888x9472, b=2048): 0.0251
Throughput (in TFLOP/s) for mlp_4h_to_h (4x37888x9472, b=2048): 234.353

Attention duration (in seconds): 0.0328
Attention throughput (in TFLOP/s): 198.377
MLP duration (in seconds): 0.0500
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0829
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9600x28800, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9600x28800, b=2048): 230.116
Elapsed time for attention_key_query_prob (256x2048x150x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x150x2048): 80.307
Elapsed time for attention_prob_times_values (256x2048x2048x150): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x150): 98.633
Elapsed time for attention_linear_projection (4x9600x9600, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9600x9600, b=2048): 232.795
Elapsed time for mlp_h_to_4h (4x9600x38400, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9600x38400, b=2048): 238.211
Elapsed time for mlp_4h_to_h (4x38400x9600, b=2048): 0.0259
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38400x9600, b=2048): 233.441

Attention duration (in seconds): 0.0334
Attention throughput (in TFLOP/s): 199.833
MLP duration (in seconds): 0.0512
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0847
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 229.990
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 104.783
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 140.176
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0066
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 234.349
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0263
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 235.837
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0252
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 245.854

Attention duration (in seconds): 0.0323
Attention throughput (in TFLOP/s): 212.324
MLP duration (in seconds): 0.0515
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0838
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 235.707
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 82.034
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 100.319
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 231.169
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 241.545
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 232.144

Attention duration (in seconds): 0.0345
Attention throughput (in TFLOP/s): 203.879
MLP duration (in seconds): 0.0538
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0882
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 238.429
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 82.649
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 101.132
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 230.058
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 241.931
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 243.228

Attention duration (in seconds): 0.0350
Attention throughput (in TFLOP/s): 205.709
MLP duration (in seconds): 0.0539
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0889
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 241.604
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 84.001
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 102.657
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0073
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 229.347
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 242.561
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 235.977

Attention duration (in seconds): 0.0355
Attention throughput (in TFLOP/s): 208.167
MLP duration (in seconds): 0.0560
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0915
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 244.503
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 123.102
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 148.236
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 231.347
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0284
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 242.062
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 247.827

Attention duration (in seconds): 0.0336
Attention throughput (in TFLOP/s): 224.878
MLP duration (in seconds): 0.0561
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0897
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 229.100
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 79.937
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 104.712
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 235.443
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0294
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 239.831
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0303
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 232.324

Attention duration (in seconds): 0.0382
Attention throughput (in TFLOP/s): 202.542
MLP duration (in seconds): 0.0597
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0979
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 239.625
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 80.470
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 105.800
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 231.841
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0300
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 240.721
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 236.242

Attention duration (in seconds): 0.0381
Attention throughput (in TFLOP/s): 208.050
MLP duration (in seconds): 0.0606
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0986
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0231
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 239.906
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 81.584
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 107.098
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 231.989
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0305
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 242.448
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 235.324

Attention duration (in seconds): 0.0388
Attention throughput (in TFLOP/s): 209.052
MLP duration (in seconds): 0.0619
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1007
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 243.966
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 109.423
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 151.731
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 224.766
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 242.865
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0308
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 245.607

Attention duration (in seconds): 0.0374
Attention throughput (in TFLOP/s): 221.911
MLP duration (in seconds): 0.0620
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.0994
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 245.344
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 83.783
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 109.687
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 232.153
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 240.396
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 235.368

Attention duration (in seconds): 0.0398
Attention throughput (in TFLOP/s): 213.507
MLP duration (in seconds): 0.0652
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1050
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 242.937
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 84.093
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 111.160
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 230.842
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 238.972
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0329
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 241.246

Attention duration (in seconds): 0.0408
Attention throughput (in TFLOP/s): 212.579
MLP duration (in seconds): 0.0661
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1070
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0251
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 243.127
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 84.716
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 111.572
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 225.806
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 241.109
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0346
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 234.992

Attention duration (in seconds): 0.0418
Attention throughput (in TFLOP/s): 212.164
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1101
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 237.820
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 122.348
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 160.664
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 232.184
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0346
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 240.240
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 247.226

Attention duration (in seconds): 0.0406
Attention throughput (in TFLOP/s): 223.327
MLP duration (in seconds): 0.0682
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1089
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0266
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 240.155
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 86.013
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 113.370
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 229.031
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 241.262
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0347
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 245.265

Attention duration (in seconds): 0.0437
Attention throughput (in TFLOP/s): 212.309
MLP duration (in seconds): 0.0699
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1136
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 243.608
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 86.770
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 114.337
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 228.297
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 240.898
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0368
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 236.184

Attention duration (in seconds): 0.0441
Attention throughput (in TFLOP/s): 214.572
MLP duration (in seconds): 0.0729
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1171
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 239.404
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 88.031
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 115.136
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 226.344
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0365
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 243.811
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0377
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 236.061

Attention duration (in seconds): 0.0455
Attention throughput (in TFLOP/s): 212.549
MLP duration (in seconds): 0.0741
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1196
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 241.905
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 113.275
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 163.686
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 234.080
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 241.554
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 248.635

Attention duration (in seconds): 0.0438
Attention throughput (in TFLOP/s): 225.610
MLP duration (in seconds): 0.0742
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1180
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0286
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 243.783
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 60.197
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 117.576
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0100
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 233.193
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 241.587
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 235.903

Attention duration (in seconds): 0.0486
Attention throughput (in TFLOP/s): 207.696
MLP duration (in seconds): 0.0778
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1264
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 251.734
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 90.704
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 119.166
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 235.717
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 243.548
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0386
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 245.691

Attention duration (in seconds): 0.0462
Attention throughput (in TFLOP/s): 222.990
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1237
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 249.030
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 91.185
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 120.578
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 229.939
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0388
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 249.740
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0417
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 232.649

Attention duration (in seconds): 0.0476
Attention throughput (in TFLOP/s): 220.822
MLP duration (in seconds): 0.0805
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1280
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 246.074
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 135.150
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 173.077
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 241.433
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0396
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 249.842
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0402
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 246.373

Attention duration (in seconds): 0.0458
Attention throughput (in TFLOP/s): 233.861
MLP duration (in seconds): 0.0798
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1256
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 248.672
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 87.637
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 120.162
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0106
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 237.647
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 249.566
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 234.744

Attention duration (in seconds): 0.0493
Attention throughput (in TFLOP/s): 221.740
MLP duration (in seconds): 0.0835
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1328
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 248.388
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 87.936
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 118.979
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 226.838
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 248.388
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0419
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 246.313

Attention duration (in seconds): 0.0508
Attention throughput (in TFLOP/s): 219.452
MLP duration (in seconds): 0.0834
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1342
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 251.375
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 88.896
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 118.998
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0109
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 241.075
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0422
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 249.444
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 233.398

Attention duration (in seconds): 0.0507
Attention throughput (in TFLOP/s): 224.483
MLP duration (in seconds): 0.0873
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1379
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0323
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 249.104
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 120.699
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 175.067
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0113
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 238.563
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0430
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 249.770
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 247.460

Attention duration (in seconds): 0.0496
Attention throughput (in TFLOP/s): 233.836
MLP duration (in seconds): 0.0864
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1360
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 252.535
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 90.139
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 118.446
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0114
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 239.342
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 250.222
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 244.385

Attention duration (in seconds): 0.0524
Attention throughput (in TFLOP/s): 225.392
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1410
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0339
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 247.083
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 90.950
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 120.293
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 240.620
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0448
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 249.402
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0451
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 247.690

Attention duration (in seconds): 0.0540
Attention throughput (in TFLOP/s): 223.205
MLP duration (in seconds): 0.0899
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1439
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0345
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 247.693
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 91.390
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 120.674
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 233.687
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0458
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 248.985
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0472
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 241.221

Attention duration (in seconds): 0.0552
Attention throughput (in TFLOP/s): 222.452
MLP duration (in seconds): 0.0930
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1482
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0352
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 247.393
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 133.640
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 183.748
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 238.877
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0469
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 247.618
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0536
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 216.564

Attention duration (in seconds): 0.0531
Attention throughput (in TFLOP/s): 235.378
MLP duration (in seconds): 0.1005
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1537
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0357
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 248.787
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 92.780
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 121.829
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0122
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 243.198
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0476
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 248.956
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0486
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 243.807

Attention duration (in seconds): 0.0564
Attention throughput (in TFLOP/s): 225.811
MLP duration (in seconds): 0.0961
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1525
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 249.252
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 93.669
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 123.108
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 242.293
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 248.832
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0499
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 241.790

Attention duration (in seconds): 0.0573
Attention throughput (in TFLOP/s): 226.405
MLP duration (in seconds): 0.0984
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1557
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 247.387
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 95.168
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 124.486
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0129
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 238.350
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0493
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 249.175
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0505
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 243.569

Attention duration (in seconds): 0.0587
Attention throughput (in TFLOP/s): 225.144
MLP duration (in seconds): 0.0998
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1585
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0376
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 250.057
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 123.512
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 187.379
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 240.060
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0509
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 246.139
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0511
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 245.029

Attention duration (in seconds): 0.0568
Attention throughput (in TFLOP/s): 236.672
MLP duration (in seconds): 0.1020
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1588
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 245.278
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 96.338
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 126.218
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 241.825
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0513
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 248.486
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0526
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 242.554

Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 225.351
MLP duration (in seconds): 0.1039
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1647
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 250.370
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 96.918
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 126.951
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 243.906
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 247.331
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0535
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 242.710

Attention duration (in seconds): 0.0608
Attention throughput (in TFLOP/s): 229.108
MLP duration (in seconds): 0.1061
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1669
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0399
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 248.420
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 98.396
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 128.890
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 236.521
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 249.140
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0550
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 240.476

Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 227.043
MLP duration (in seconds): 0.1081
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1706
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 248.638
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 147.400
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 195.373
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0136
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 247.395
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0539
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 249.996
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 243.700

Attention duration (in seconds): 0.0600
Attention throughput (in TFLOP/s): 240.659
MLP duration (in seconds): 0.1091
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1691
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 252.282
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 94.255
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 130.976
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 243.234
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0551
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 248.923
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0565
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 242.635

Attention duration (in seconds): 0.0637
Attention throughput (in TFLOP/s): 230.452
MLP duration (in seconds): 0.1116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1753
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 224.144
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 94.856
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 131.813
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 236.133
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0567
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 246.014
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0577
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 241.995

Attention duration (in seconds): 0.0703
Attention throughput (in TFLOP/s): 212.299
MLP duration (in seconds): 0.1144
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1847
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0436
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 244.163
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 95.534
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 132.476
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 238.065
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 246.748
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0587
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 241.985

Attention duration (in seconds): 0.0674
Attention throughput (in TFLOP/s): 225.243
MLP duration (in seconds): 0.1162
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1837
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 246.512
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 129.512
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 196.176
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0153
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 236.854
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0589
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 245.287
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0602
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 240.045

Attention duration (in seconds): 0.0656
Attention throughput (in TFLOP/s): 235.456
MLP duration (in seconds): 0.1191
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1847
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 250.487
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 97.936
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 134.522
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 234.463
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0590
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 249.198
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0605
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 243.029

Attention duration (in seconds): 0.0685
Attention throughput (in TFLOP/s): 229.085
MLP duration (in seconds): 0.1195
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1880
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 250.132
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 98.158
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 135.958
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 243.296
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0605
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 247.171
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0635
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 235.591

Attention duration (in seconds): 0.0691
Attention throughput (in TFLOP/s): 231.093
MLP duration (in seconds): 0.1239
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1930
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0457
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 249.350
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 98.639
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 136.603
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0157
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 242.463
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0615
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 247.170
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0627
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 242.420

Attention duration (in seconds): 0.0703
Attention throughput (in TFLOP/s): 230.715
MLP duration (in seconds): 0.1242
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1946
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0469
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 247.169
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 143.751
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 206.706
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 238.777
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0629
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 245.749
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0640
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 241.661

Attention duration (in seconds): 0.0692
Attention throughput (in TFLOP/s): 238.388
MLP duration (in seconds): 0.1269
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.1961
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0477
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 247.323
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 100.066
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 139.727
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 236.942
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0635
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 247.541
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0649
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 242.351

Attention duration (in seconds): 0.0732
Attention throughput (in TFLOP/s): 229.049
MLP duration (in seconds): 0.1284
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2015
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0484
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 247.515
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 101.776
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 141.572
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 242.226
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.1040
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 153.705
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0678
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 235.565

Attention duration (in seconds): 0.0738
Attention throughput (in TFLOP/s): 230.846
MLP duration (in seconds): 0.1718
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2456
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0490
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 248.489
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 102.175
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 142.852
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0168
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 241.490
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 244.802
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0677
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 240.006

Attention duration (in seconds): 0.0747
Attention throughput (in TFLOP/s): 231.560
MLP duration (in seconds): 0.1340
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2088
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 244.409
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 128.224
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 210.675
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 242.561
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0675
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 244.525
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0726
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 227.442

Attention duration (in seconds): 0.0744
Attention throughput (in TFLOP/s): 236.350
MLP duration (in seconds): 0.1401
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2145
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0511
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 246.314
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 104.182
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 145.456
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 242.087
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0681
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 246.413
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0694
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 241.910

Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 231.067
MLP duration (in seconds): 0.1374
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2147
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 247.614
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 105.313
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 147.419
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0177
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 240.379
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0694
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 245.462
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0705
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 241.737

Attention duration (in seconds): 0.0782
Attention throughput (in TFLOP/s): 231.914
MLP duration (in seconds): 0.1400
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2181
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 247.118
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 105.605
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 148.233
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 239.131
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0706
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 245.425
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0718
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 241.199

Attention duration (in seconds): 0.0795
Attention throughput (in TFLOP/s): 231.530
MLP duration (in seconds): 0.1424
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2219
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 248.454
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 156.392
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 219.709
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 243.808
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0719
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 244.687
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0725
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 242.542

Attention duration (in seconds): 0.0772
Attention throughput (in TFLOP/s): 242.242
MLP duration (in seconds): 0.1444
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2216
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 250.635
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 102.558
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 96.326
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 237.937
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0722
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 247.331
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0739
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 241.741

Attention duration (in seconds): 0.0834
Attention throughput (in TFLOP/s): 227.541
MLP duration (in seconds): 0.1462
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2296
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0555
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 245.355
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 102.435
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 95.661
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 235.448
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.0736
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 246.557
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0752
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 241.415

Attention duration (in seconds): 0.0860
Attention throughput (in TFLOP/s): 223.924
MLP duration (in seconds): 0.1488
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2348
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 219.681
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 102.568
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 96.452
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 234.050
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.0748
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 246.231
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0791
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 232.909

Attention duration (in seconds): 0.0939
Attention throughput (in TFLOP/s): 208.197
MLP duration (in seconds): 0.1539
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2479
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0572
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 245.389
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 134.535
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 132.062
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 240.007
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0770
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 242.964
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0783
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 239.053

Attention duration (in seconds): 0.0852
Attention throughput (in TFLOP/s): 232.961
MLP duration (in seconds): 0.1553
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2404
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 228.368
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 103.068
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 96.458
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 238.122
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0771
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 246.323
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0791
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 240.004

Attention duration (in seconds): 0.0938
Attention throughput (in TFLOP/s): 214.707
MLP duration (in seconds): 0.1562
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2500
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0588
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 245.970
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 104.112
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 96.364
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 244.529
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0789
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 244.473
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0805
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 239.497

Attention duration (in seconds): 0.0900
Attention throughput (in TFLOP/s): 227.013
MLP duration (in seconds): 0.1594
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2494
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0596
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 246.401
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 104.546
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 96.906
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 232.424
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0863
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 226.798
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0826
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 236.891

Attention duration (in seconds): 0.0921
Attention throughput (in TFLOP/s): 224.962
MLP duration (in seconds): 0.1689
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2610
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0605
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 246.309
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 149.945
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 136.404
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 248.855
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.0814
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 243.831
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 216.773

Attention duration (in seconds): 0.0886
Attention throughput (in TFLOP/s): 237.334
MLP duration (in seconds): 0.1731
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2617
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0617
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 245.094
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 104.886
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 96.943
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0205
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 245.964
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0819
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 245.998
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0834
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 241.710

Attention duration (in seconds): 0.0938
Attention throughput (in TFLOP/s): 227.318
MLP duration (in seconds): 0.1653
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2591
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0621
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 246.919
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 105.949
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 97.455
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 247.568
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0832
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 245.884
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0883
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 231.569

Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 229.084
MLP duration (in seconds): 0.1715
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2659
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0631
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 246.698
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 107.219
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 98.012
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 247.320
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0846
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 245.174
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0865
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 239.775

Attention duration (in seconds): 0.0957
Attention throughput (in TFLOP/s): 229.256
MLP duration (in seconds): 0.1711
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2668
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0639
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 246.831
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 133.844
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 137.702
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0212
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 247.931
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0857
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 245.514
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0883
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 238.472

Attention duration (in seconds): 0.0940
Attention throughput (in TFLOP/s): 236.613
MLP duration (in seconds): 0.1740
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2680
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0642
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 249.411
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 108.188
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 96.785
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 253.044
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0875
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 243.988
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0915
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 233.348

Attention duration (in seconds): 0.0971
Attention throughput (in TFLOP/s): 232.231
MLP duration (in seconds): 0.1790
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2761
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0659
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 246.562
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 108.231
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 96.701
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 246.406
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.0884
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 244.958
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0903
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 239.820

Attention duration (in seconds): 0.0998
Attention throughput (in TFLOP/s): 229.241
MLP duration (in seconds): 0.1787
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2784
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0672
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 245.079
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 109.596
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 98.094
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 248.237
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.1073
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 204.584
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0941
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 233.426

Attention duration (in seconds): 0.1012
Attention throughput (in TFLOP/s): 229.168
MLP duration (in seconds): 0.2014
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3026
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0670
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 249.368
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 159.763
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 144.525
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 252.806
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.0906
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 245.875
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0928
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 239.840

Attention duration (in seconds): 0.0971
Attention throughput (in TFLOP/s): 241.957
MLP duration (in seconds): 0.1834
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2805
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0689
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 245.709
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 104.844
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 99.828
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 253.595
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.0922
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 244.974
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.1018
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 221.799

Attention duration (in seconds): 0.1033
Attention throughput (in TFLOP/s): 230.504
MLP duration (in seconds): 0.1939
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2973
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0701
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 244.971
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 105.133
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 96.706
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0229
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 250.401
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0934
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 244.943
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0949
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 241.111

Attention duration (in seconds): 0.1054
Attention throughput (in TFLOP/s): 229.109
MLP duration (in seconds): 0.1884
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2937
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0799
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 217.877
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 105.832
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0144
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 43.770
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 249.342
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0944
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 245.754
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0963
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 240.841

Attention duration (in seconds): 0.1235
Attention throughput (in TFLOP/s): 198.060
MLP duration (in seconds): 0.1908
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3143
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 247.484
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 137.062
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 142.665
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 253.515
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0962
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 244.442
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0970
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 242.542

Attention duration (in seconds): 0.1036
Attention throughput (in TFLOP/s): 239.380
MLP duration (in seconds): 0.1932
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.2967
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0728
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 245.742
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 107.478
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 101.499
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 250.001
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0970
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 245.656
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0975
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 244.452

Attention duration (in seconds): 0.1089
Attention throughput (in TFLOP/s): 230.756
MLP duration (in seconds): 0.1946
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3034
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 248.576
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 107.348
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 102.401
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 251.944
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.0982
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 245.928
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0987
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 244.866

Attention duration (in seconds): 0.1092
Attention throughput (in TFLOP/s): 233.126
MLP duration (in seconds): 0.1969
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3061
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 245.144
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 106.929
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0229
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 28.374
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 253.157
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0998
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 245.356
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.1022
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 239.630

Attention duration (in seconds): 0.1280
Attention throughput (in TFLOP/s): 201.400
MLP duration (in seconds): 0.2020
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3300
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0756
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 246.044
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 152.071
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 146.899
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0248
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 250.267
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.1016
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 244.266
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.1114
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 222.705

Attention duration (in seconds): 0.1091
Attention throughput (in TFLOP/s): 239.269
MLP duration (in seconds): 0.2130
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3221
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 245.013
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 107.951
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 104.958
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 251.738
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.1033
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 243.304
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.1047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 240.016

Attention duration (in seconds): 0.1143
Attention throughput (in TFLOP/s): 231.504
MLP duration (in seconds): 0.2080
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3223
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0777
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 245.703
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 109.462
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 103.516
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 252.930
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.1041
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 244.679
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.1058
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 240.647

Attention duration (in seconds): 0.1153
Attention throughput (in TFLOP/s): 232.265
MLP duration (in seconds): 0.2099
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3252
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0824
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 234.703
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 109.952
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 103.955
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 250.995
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.1104
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 233.737
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 238.040

Attention duration (in seconds): 0.1206
Attention throughput (in TFLOP/s): 224.967
MLP duration (in seconds): 0.2187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3393
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0795
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 246.486
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 135.669
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 151.048
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 248.388
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1078
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 242.303
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.1133
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 230.647

Attention duration (in seconds): 0.1152
Attention throughput (in TFLOP/s): 238.494
MLP duration (in seconds): 0.2211
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3363
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0804
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 246.850
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 110.501
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 104.980
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0262
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 252.532
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1071
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 247.012
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1129
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 234.430

Attention duration (in seconds): 0.1191
Attention throughput (in TFLOP/s): 233.467
MLP duration (in seconds): 0.2200
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3392
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0812
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 247.459
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 111.439
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 106.983
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 250.989
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1093
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 245.222
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1240
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 216.178

Attention duration (in seconds): 0.1204
Attention throughput (in TFLOP/s): 233.958
MLP duration (in seconds): 0.2333
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3537
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0827
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 246.271
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 113.057
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 107.104
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0271
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 250.856
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1135
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 239.264
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1127
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 240.880

Attention duration (in seconds): 0.1221
Attention throughput (in TFLOP/s): 233.433
MLP duration (in seconds): 0.2261
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3483
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0828
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 248.996
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 163.804
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 159.085
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0273
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 252.066
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1123
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 244.795
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 201.240

Attention duration (in seconds): 0.1186
Attention throughput (in TFLOP/s): 243.412
MLP duration (in seconds): 0.2489
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.3675
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0851
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 245.206
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 109.104
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 106.672
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0275
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 253.344
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1134
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 245.397
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1787
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 155.741

Attention duration (in seconds): 0.1254
Attention throughput (in TFLOP/s): 232.950
MLP duration (in seconds): 0.2921
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4175
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0857
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 246.746
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 109.092
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 107.747
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 254.392
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1148
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 245.420
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 148.815

Attention duration (in seconds): 0.1262
Attention throughput (in TFLOP/s): 234.352
MLP duration (in seconds): 0.3042
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4304
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0869
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 246.273
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 109.329
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 108.955
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 251.903
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1160
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 245.910
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1823
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 156.518

Attention duration (in seconds): 0.1280
Attention throughput (in TFLOP/s): 233.776
MLP duration (in seconds): 0.2983
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4263
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0878
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 246.682
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 138.833
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 157.979
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 253.100
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 243.317
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1917
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 150.620

Attention duration (in seconds): 0.1259
Attention throughput (in TFLOP/s): 240.647
MLP duration (in seconds): 0.3104
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4363
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0891
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 245.988
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 110.608
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 106.663
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 250.801
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 245.633
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1933
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 151.232

Attention duration (in seconds): 0.1313
Attention throughput (in TFLOP/s): 233.402
MLP duration (in seconds): 0.3123
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4436
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.0906
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 244.951
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 110.994
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 108.438
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 251.431
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1214
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 243.817
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1915
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 154.547

Attention duration (in seconds): 0.1330
Attention throughput (in TFLOP/s): 233.167
MLP duration (in seconds): 0.3128
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4458
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.0907
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 247.727
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 111.870
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 110.218
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 253.262
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 243.920
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1941
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 154.305

Attention duration (in seconds): 0.1331
Attention throughput (in TFLOP/s): 235.692
MLP duration (in seconds): 0.3168
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4500
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0918
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 247.638
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 155.249
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 163.000
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 254.048
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1244
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 243.649
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1911
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 158.622

Attention duration (in seconds): 0.1307
Attention throughput (in TFLOP/s): 242.948
MLP duration (in seconds): 0.3154
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4461
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.0939
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 244.849
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 112.453
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 110.658
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0307
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 249.545
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1251
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 245.087
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 155.781

Attention duration (in seconds): 0.1377
Attention throughput (in TFLOP/s): 233.296
MLP duration (in seconds): 0.3220
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4597
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.0977
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 238.248
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 113.618
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 111.752
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0392
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 197.686
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1269
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 244.532
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1973
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 157.278

Attention duration (in seconds): 0.1499
Attention throughput (in TFLOP/s): 216.771
MLP duration (in seconds): 0.3242
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4741
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.0960
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 245.292
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 115.257
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 112.833
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0312
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 251.862
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1282
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 244.891
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.2248
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 139.641

Attention duration (in seconds): 0.1400
Attention throughput (in TFLOP/s): 234.681
MLP duration (in seconds): 0.3531
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4931
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.0958
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 248.651
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 175.035
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 164.489
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 253.095
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1308
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 242.842
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.2153
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 147.563

Attention duration (in seconds): 0.1359
Attention throughput (in TFLOP/s): 244.610
MLP duration (in seconds): 0.3461
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4820
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.0978
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 246.405
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 115.046
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 105.494
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 251.692
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1312
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 244.921
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.2105
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 152.676

Attention duration (in seconds): 0.1432
Attention throughput (in TFLOP/s): 234.731
MLP duration (in seconds): 0.3417
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4849
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.0989
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 246.583
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 114.737
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 114.115
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 250.009
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 171.754
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.2079
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 156.396

Attention duration (in seconds): 0.1444
Attention throughput (in TFLOP/s): 235.403
MLP duration (in seconds): 0.3971
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5416
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.1006
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 245.258
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 116.687
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 112.614
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0412
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 199.716
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1397
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 235.454
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1424
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 230.881

Attention duration (in seconds): 0.1548
Attention throughput (in TFLOP/s): 222.084
MLP duration (in seconds): 0.2821
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4369
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.1013
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 246.290
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 191.400
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 170.808
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0332
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 250.653
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1359
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 244.653
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1408
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 236.281

Attention duration (in seconds): 0.1428
Attention throughput (in TFLOP/s): 243.447
MLP duration (in seconds): 0.2767
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4195
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.1014
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 248.923
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 111.496
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 112.949
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0333
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 252.796
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.1374
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 244.812
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1422
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 236.571

Attention duration (in seconds): 0.1482
Attention throughput (in TFLOP/s): 237.292
MLP duration (in seconds): 0.2796
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4278
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1049
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 243.178
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0200
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 38.235
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 115.182
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0339
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 250.740
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1392
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 244.406
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1436
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 236.860

Attention duration (in seconds): 0.1655
Attention throughput (in TFLOP/s): 214.830
MLP duration (in seconds): 0.2828
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4483
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1047
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 246.550
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 112.732
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 114.958
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0361
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 238.070
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.1413
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 243.457
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1452
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 236.986

Attention duration (in seconds): 0.1543
Attention throughput (in TFLOP/s): 232.946
MLP duration (in seconds): 0.2865
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4408
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1066
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 244.698
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 175.846
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 170.143
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0460
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 189.026
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.1424
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 244.320
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1478
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 235.363

Attention duration (in seconds): 0.1616
Attention throughput (in TFLOP/s): 224.875
MLP duration (in seconds): 0.2902
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4518
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1082
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 243.799
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 110.026
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 113.673
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0371
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 237.156
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.1449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 242.689
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1497
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 235.027

Attention duration (in seconds): 0.1592
Attention throughput (in TFLOP/s): 230.725
MLP duration (in seconds): 0.2946
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4538
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 246.315
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 112.212
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 113.928
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 250.156
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.1447
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 245.759
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1539
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 231.070

Attention duration (in seconds): 0.1577
Attention throughput (in TFLOP/s): 235.495
MLP duration (in seconds): 0.2986
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4563
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1102
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 244.756
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 113.018
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 114.213
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 236.873
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.1475
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 243.752
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1523
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 236.044

Attention duration (in seconds): 0.1620
Attention throughput (in TFLOP/s): 231.706
MLP duration (in seconds): 0.2999
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4618
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1114
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 244.675
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 186.667
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 178.255
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 250.205
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1503
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 241.871
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1542
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 235.690

Attention duration (in seconds): 0.1564
Attention throughput (in TFLOP/s): 242.506
MLP duration (in seconds): 0.3045
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4610
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1118
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 246.635
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 112.887
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 118.634
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0365
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 251.371
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.1514
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 242.793
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1563
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 235.171

Attention duration (in seconds): 0.1620
Attention throughput (in TFLOP/s): 236.603
MLP duration (in seconds): 0.3076
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4697
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1135
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 245.418
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 112.609
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 117.957
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 238.826
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.1527
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 243.209
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1574
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 235.976

Attention duration (in seconds): 0.1663
Attention throughput (in TFLOP/s): 233.018
MLP duration (in seconds): 0.3102
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4764
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1159
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 243.053
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 110.609
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 120.186
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0381
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 246.672
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.1547
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 242.676
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1591
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 236.030

Attention duration (in seconds): 0.1679
Attention throughput (in TFLOP/s): 233.253
MLP duration (in seconds): 0.3138
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4817
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1164
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 244.539
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 180.039
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 174.987
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 250.957
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.1562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 242.958
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 233.504

Attention duration (in seconds): 0.1633
Attention throughput (in TFLOP/s): 242.288
MLP duration (in seconds): 0.3187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4820
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1174
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 244.962
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 113.140
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 121.005
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0385
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 249.039
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.1564
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 245.169
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1622
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 236.455

Attention duration (in seconds): 0.1698
Attention throughput (in TFLOP/s): 235.420
MLP duration (in seconds): 0.3187
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4885
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1179
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 246.569
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 114.327
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 122.600
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 238.485
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.1591
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 243.644
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1641
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 236.238

Attention duration (in seconds): 0.1723
Attention throughput (in TFLOP/s): 234.398
MLP duration (in seconds): 0.3232
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4955
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 244.187
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 114.814
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 122.116
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0387
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 253.003
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.1610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 243.374
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1653
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 236.934

Attention duration (in seconds): 0.1729
Attention throughput (in TFLOP/s): 236.071
MLP duration (in seconds): 0.3263
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4992
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 246.823
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 193.259
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 184.684
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 251.537
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.1628
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 243.138
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1679
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 235.684

Attention duration (in seconds): 0.1683
Attention throughput (in TFLOP/s): 244.919
MLP duration (in seconds): 0.3307
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.4991
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1222
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 245.550
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 108.619
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 122.517
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 238.242
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.1675
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 238.807
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 236.013

Attention duration (in seconds): 0.1785
Attention throughput (in TFLOP/s): 233.316
MLP duration (in seconds): 0.3369
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5155
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 244.956
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 109.219
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 120.877
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 248.780
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.1672
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 241.638
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.2218
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 182.167

Attention duration (in seconds): 0.1789
Attention throughput (in TFLOP/s): 235.252
MLP duration (in seconds): 0.3891
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5679
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 242.134
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 109.290
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 120.607
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 249.920
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.1685
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 242.241
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1733
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 235.593

Attention duration (in seconds): 0.1819
Attention throughput (in TFLOP/s): 233.647
MLP duration (in seconds): 0.3419
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5238
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 244.477
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 177.759
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 182.189
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0433
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 238.036
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.1706
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 241.731
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.2177
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 189.473

Attention duration (in seconds): 0.1792
Attention throughput (in TFLOP/s): 239.551
MLP duration (in seconds): 0.3883
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5676
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1278
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 244.516
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 110.123
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 118.945
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 249.927
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.1711
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 243.577
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.1767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 235.850

Attention duration (in seconds): 0.1843
Attention throughput (in TFLOP/s): 235.290
MLP duration (in seconds): 0.3478
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5321
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1289
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 244.851
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 110.552
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 121.969
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0421
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 249.829
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.1720
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 244.776
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1791
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 235.067

Attention duration (in seconds): 0.1857
Attention throughput (in TFLOP/s): 235.805
MLP duration (in seconds): 0.3510
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5368
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1295
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 246.246
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 111.186
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 122.375
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0427
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 248.829
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.1745
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 243.720
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1799
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 236.342

Attention duration (in seconds): 0.1869
Attention throughput (in TFLOP/s): 236.653
MLP duration (in seconds): 0.3544
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5413
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1311
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 245.699
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 187.914
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 188.939
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 248.987
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.1762
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 243.753
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.1819
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 236.166

Attention duration (in seconds): 0.1833
Attention throughput (in TFLOP/s): 243.624
MLP duration (in seconds): 0.3581
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5414
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.1327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 245.090
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 110.228
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 120.411
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 244.169
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.1769
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 245.197
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.1841
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 235.642

Attention duration (in seconds): 0.1922
Attention throughput (in TFLOP/s): 234.729
MLP duration (in seconds): 0.3610
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5532
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 243.528
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 112.388
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 122.795
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 251.582
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1801
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 243.311
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1855
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 236.160

Attention duration (in seconds): 0.1933
Attention throughput (in TFLOP/s): 235.690
MLP duration (in seconds): 0.3656
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5588
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1360
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 243.981
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 111.679
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 122.158
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 248.991
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1815
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 243.836
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.2049
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 215.973

Attention duration (in seconds): 0.1954
Attention throughput (in TFLOP/s): 235.384
MLP duration (in seconds): 0.3863
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5817
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1392
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 240.814
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 184.041
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 187.363
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 249.335
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.1844
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 242.353
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1893
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 236.072

Attention duration (in seconds): 0.1934
Attention throughput (in TFLOP/s): 240.098
MLP duration (in seconds): 0.3737
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5671
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1385
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 244.432
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 112.539
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 122.849
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 250.160
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.1894
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 238.306
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1965
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 229.690

Attention duration (in seconds): 0.1985
Attention throughput (in TFLOP/s): 236.147
MLP duration (in seconds): 0.3858
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5844
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1393
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 245.373
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 113.125
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 123.312
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0460
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 247.711
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1883
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 241.939
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1928
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 236.299

Attention duration (in seconds): 0.2003
Attention throughput (in TFLOP/s): 236.372
MLP duration (in seconds): 0.3812
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5814
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1407
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 245.260
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 112.305
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 125.314
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 249.539
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1956
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 235.167
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1948
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 236.236

Attention duration (in seconds): 0.2018
Attention throughput (in TFLOP/s): 236.805
MLP duration (in seconds): 0.3904
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5922
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1426
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 244.301
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 198.348
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 195.156
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0466
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 249.258
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.1921
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 241.824
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1963
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 236.697

Attention duration (in seconds): 0.1983
Attention throughput (in TFLOP/s): 243.287
MLP duration (in seconds): 0.3884
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5867
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1450
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 242.614
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 114.644
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 122.595
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0473
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 248.085
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.1933
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 242.632
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1989
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 235.788

Attention duration (in seconds): 0.2074
Attention throughput (in TFLOP/s): 234.793
MLP duration (in seconds): 0.3922
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.5996
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1458
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 243.566
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 115.799
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 128.705
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 245.883
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.1952
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 242.553
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.2009
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 235.713

Attention duration (in seconds): 0.2088
Attention throughput (in TFLOP/s): 235.477
MLP duration (in seconds): 0.3961
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6049
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1470
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 243.946
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 113.153
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 124.722
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 249.941
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.1972
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 242.390
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.2039
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 234.496

Attention duration (in seconds): 0.2101
Attention throughput (in TFLOP/s): 236.200
MLP duration (in seconds): 0.4011
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6111
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1482
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 244.303
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 181.032
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 194.245
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0491
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 245.963
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.1995
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 241.927
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.2057
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 234.607

Attention duration (in seconds): 0.2069
Attention throughput (in TFLOP/s): 242.025
MLP duration (in seconds): 0.4052
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6121
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1498
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 243.944
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 113.476
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 125.423
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0495
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 246.087
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.2048
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 237.898
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.2059
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 236.605

Attention duration (in seconds): 0.2146
Attention throughput (in TFLOP/s): 235.509
MLP duration (in seconds): 0.4107
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6253
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1519
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 242.811
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 115.988
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 128.438
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0499
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 246.556
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.2031
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 242.155
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 235.779

Attention duration (in seconds): 0.2168
Attention throughput (in TFLOP/s): 235.262
MLP duration (in seconds): 0.4116
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6284
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.2046
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 181.944
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 115.089
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 126.630
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0501
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 247.749
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.2050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 242.164
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.2090
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 237.453

Attention duration (in seconds): 0.2700
Attention throughput (in TFLOP/s): 190.670
MLP duration (in seconds): 0.4140
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6840
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1539
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 244.078
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 191.860
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 202.199
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0504
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 248.465
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.2063
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 242.858
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.2132
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 235.029

Attention duration (in seconds): 0.2138
Attention throughput (in TFLOP/s): 243.032
MLP duration (in seconds): 0.4194
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6332
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1562
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 242.760
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 119.982
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 126.333
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0511
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 247.343
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.2118
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 238.706
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.2143
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 235.978

Attention duration (in seconds): 0.2225
Attention throughput (in TFLOP/s): 235.664
MLP duration (in seconds): 0.4261
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6485
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1568
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 244.154
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 119.098
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 129.566
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0515
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 247.947
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.2109
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 242.009
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.2176
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 234.507

Attention duration (in seconds): 0.2233
Attention throughput (in TFLOP/s): 236.916
MLP duration (in seconds): 0.4285
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6517
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1582
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 244.176
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 119.275
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 128.578
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 247.898
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2123
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 242.582
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.2190
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 235.169

Attention duration (in seconds): 0.2253
Attention throughput (in TFLOP/s): 236.909
MLP duration (in seconds): 0.4313
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6566
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.1602
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 243.265
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 183.293
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 199.435
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0526
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 247.215
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.2144
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 242.420
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.2211
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 235.078

Attention duration (in seconds): 0.2227
Attention throughput (in TFLOP/s): 241.876
MLP duration (in seconds): 0.4354
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6581
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 240.595
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 122.130
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 129.989
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 248.351
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.2172
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 241.482
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.2222
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 235.984

Attention duration (in seconds): 0.2313
Attention throughput (in TFLOP/s): 234.894
MLP duration (in seconds): 0.4394
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6707
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 242.931
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 122.705
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 129.478
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0531
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 249.098
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.2184
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 242.352
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.2238
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 236.496

Attention duration (in seconds): 0.2316
Attention throughput (in TFLOP/s): 236.704
MLP duration (in seconds): 0.4421
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6737
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1646
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 243.323
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 122.566
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 130.365
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0536
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 249.007
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.2300
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 232.161
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2259
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 236.376

Attention duration (in seconds): 0.2334
Attention throughput (in TFLOP/s): 237.029
MLP duration (in seconds): 0.4559
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6892
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1947
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 207.559
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0049
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 197.883
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 208.442
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 247.919
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.2232
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 241.338
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2283
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 236.019

Attention duration (in seconds): 0.2585
Attention throughput (in TFLOP/s): 215.875
MLP duration (in seconds): 0.4515
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7100
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1674
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 243.584
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 112.098
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 132.027
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 246.724
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.2241
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 242.537
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2307
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 235.646

Attention duration (in seconds): 0.2384
Attention throughput (in TFLOP/s): 236.130
MLP duration (in seconds): 0.4548
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.6932
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.1871
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 219.802
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 113.757
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 132.696
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0558
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 245.705
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.2257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 242.981
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2332
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 235.145

Attention duration (in seconds): 0.2588
Attention throughput (in TFLOP/s): 219.429
MLP duration (in seconds): 0.4589
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7177
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1699
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 244.196
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 111.940
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 133.249
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0566
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 244.222
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.2281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 242.616
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2377
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 232.799

Attention duration (in seconds): 0.2426
Attention throughput (in TFLOP/s): 236.108
MLP duration (in seconds): 0.4657
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7083
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1731
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 241.871
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 184.655
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 203.704
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 249.624
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2313
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 241.322
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2361
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 236.462

Attention duration (in seconds): 0.2391
Attention throughput (in TFLOP/s): 241.647
MLP duration (in seconds): 0.4674
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7064
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1727
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 244.544
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 114.789
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 134.436
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0564
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 249.455
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.2308
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 243.922
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2385
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 236.084

Attention duration (in seconds): 0.2450
Attention throughput (in TFLOP/s): 237.849
MLP duration (in seconds): 0.4694
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7144
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.1746
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 243.969
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 115.583
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 135.243
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 244.609
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.2325
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 244.316
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2415
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 235.191

Attention duration (in seconds): 0.2485
Attention throughput (in TFLOP/s): 236.508
MLP duration (in seconds): 0.4740
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7225
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1760
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 244.123
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 115.641
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 134.223
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0578
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 247.774
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.2384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 240.301
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2696
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 212.516

Attention duration (in seconds): 0.2498
Attention throughput (in TFLOP/s): 237.303
MLP duration (in seconds): 0.5080
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7578
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1774
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 244.319
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 194.641
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 213.908
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 247.773
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.2393
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 241.488
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2442
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 236.647

Attention duration (in seconds): 0.2455
Attention throughput (in TFLOP/s): 243.526
MLP duration (in seconds): 0.4835
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7290
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1794
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 243.720
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 118.801
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 135.521
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 247.189
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.2403
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 242.620
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2459
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 237.064

Attention duration (in seconds): 0.2541
Attention throughput (in TFLOP/s): 237.240
MLP duration (in seconds): 0.4862
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7403
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1826
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 241.494
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 119.721
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 134.616
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 246.965
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.2434
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 241.543
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2538
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 231.652

Attention duration (in seconds): 0.2580
Attention throughput (in TFLOP/s): 235.700
MLP duration (in seconds): 0.4972
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7552
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.1821
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 244.228
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 119.133
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 133.706
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0605
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 244.887
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.2448
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 242.231
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2776
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 213.581

Attention duration (in seconds): 0.2587
Attention throughput (in TFLOP/s): 237.059
MLP duration (in seconds): 0.5224
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7811
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1851
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 242.327
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 184.235
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 210.091
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0600
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 249.027
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.2462
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 242.901
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2541
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 235.334

Attention duration (in seconds): 0.2555
Attention throughput (in TFLOP/s): 242.041
MLP duration (in seconds): 0.5003
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7558
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.1856
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 243.662
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 119.786
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 136.403
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0608
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 247.903
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.2483
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 242.917
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2564
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 235.221

Attention duration (in seconds): 0.2624
Attention throughput (in TFLOP/s): 237.583
MLP duration (in seconds): 0.5047
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7671
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1876
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 243.146
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 120.650
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 134.364
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0611
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 248.776
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.2517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 241.615
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2569
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 236.752

Attention duration (in seconds): 0.2648
Attention throughput (in TFLOP/s): 237.401
MLP duration (in seconds): 0.5086
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7734
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 244.306
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 119.942
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0075
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 137.660
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0612
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 250.664
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.2528
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 242.619
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 236.476

Attention duration (in seconds): 0.2655
Attention throughput (in TFLOP/s): 238.766
MLP duration (in seconds): 0.5122
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7776
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1891
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 245.288
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 201.795
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 221.013
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0619
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 249.943
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.3312
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 186.755
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2608
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 237.182

Attention duration (in seconds): 0.2607
Attention throughput (in TFLOP/s): 245.106
MLP duration (in seconds): 0.5919
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8527
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1917
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 243.972
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 122.412
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 137.020
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0621
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 251.087
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.2570
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 242.695
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2635
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 236.690

Attention duration (in seconds): 0.2698
Attention throughput (in TFLOP/s): 238.805
MLP duration (in seconds): 0.5204
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.7903
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1956
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 241.072
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 122.872
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0076
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 137.484
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0639
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 245.918
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.2589
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 242.896
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2657
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 236.642

Attention duration (in seconds): 0.2756
Attention throughput (in TFLOP/s): 235.726
MLP duration (in seconds): 0.5246
MLP throughput (in TFLOP/s): 1.000
Transformer duration (in seconds): 0.8002
Transformer throughput (in TFLOP/s): 1.000
Transformer - MLP - Attention (in seconds): 0.0000
========================================================================================================================
