num_attention_heads: 24, hidden_size: 3072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1, vocab_size: 51200

LN1: 0.00409245491027832
QKV Transform: 0.0022890567779541016
Attention Score: 0.0012273788452148438
Attention Softmax: 0.0028247833251953125
Attention Dropout: 0.00010204315185546875
Attention Over Value: 0.0012440681457519531
Attention linproj: 0.0007884502410888672
Post-attention Dropout: 0.07147455215454102
Post-attention residual: 0.0036079883575439453
LN2: 0.00021195411682128906
MLP_h_4h: 0.003177642822265625
MLP_4h_h: 0.002603769302368164
Post-MLP residual: 0.002073049545288086
Attention layer time: 0.09635519981384277
LN1: 0.0001380443572998047
QKV Transform: 0.002101898193359375
Attention Score: 0.0022263526916503906
Attention Softmax: 0.002696990966796875
Attention Dropout: 0.00007581710815429688
Attention Over Value: 0.0007736682891845703
Attention linproj: 0.0007631778717041016
Post-attention Dropout: 0.00039768218994140625
Post-attention residual: 0.00013113021850585938
LN2: 0.0001327991485595703
MLP_h_4h: 0.0030364990234375
MLP_4h_h: 0.0026121139526367188
Post-MLP residual: 0.000392913818359375
Attention layer time: 0.01593184471130371
LN1: 0.00017786026000976562
QKV Transform: 0.0020852088928222656
Attention Score: 0.001153707504272461
Attention Softmax: 0.002679586410522461
Attention Dropout: 0.00006604194641113281
Attention Over Value: 0.0007631778717041016
Attention linproj: 0.0007557868957519531
Post-attention Dropout: 0.0003936290740966797
Post-attention residual: 0.0001304149627685547
LN2: 0.00013136863708496094
MLP_h_4h: 0.003034830093383789
MLP_4h_h: 0.0026030540466308594
Post-MLP residual: 0.0003876686096191406
Attention layer time: 0.014811992645263672
LN1: 0.00012874603271484375
QKV Transform: 0.0020818710327148438
Attention Score: 0.0011410713195800781
Attention Softmax: 0.0026731491088867188
Attention Dropout: 0.00006365776062011719
Attention Over Value: 0.0007631778717041016
Attention linproj: 0.0007488727569580078
Post-attention Dropout: 0.0003879070281982422
Post-attention residual: 0.00013208389282226562
LN2: 0.0001342296600341797
MLP_h_4h: 0.003040313720703125
MLP_4h_h: 0.002602815628051758
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014687061309814453
LN1: 0.00012946128845214844
QKV Transform: 0.0020978450775146484
Attention Score: 0.0011594295501708984
Attention Softmax: 0.0026962757110595703
Attention Dropout: 0.00006675720214843750
Attention Over Value: 0.0007636547088623047
Attention linproj: 0.0007565021514892578
Post-attention Dropout: 0.00039076805114746094
Post-attention residual: 0.00012922286987304688
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030646324157714844
MLP_4h_h: 0.0026345252990722656
Post-MLP residual: 0.0003924369812011719
Attention layer time: 0.014834165573120117
LN1: 0.00012969970703125
QKV Transform: 0.0021202564239501953
Attention Score: 0.0011589527130126953
Attention Softmax: 0.0026993751525878906
Attention Dropout: 0.00006389617919921875
Attention Over Value: 0.0007634162902832031
Attention linproj: 0.0007660388946533203
Post-attention Dropout: 0.00039196014404296875
Post-attention residual: 0.00013065338134765625
LN2: 0.00013208389282226562
MLP_h_4h: 0.003064870834350586
MLP_4h_h: 0.0026302337646484375
Post-MLP residual: 0.0003864765167236328
Attention layer time: 0.014869213104248047
LN1: 0.000141143798828125
QKV Transform: 0.0021076202392578125
Attention Score: 0.0011403560638427734
Attention Softmax: 0.0026619434356689453
Attention Dropout: 0.00006103515625000000
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007548332214355469
Post-attention Dropout: 0.000392913818359375
Post-attention residual: 0.0001304149627685547
LN2: 0.00013184547424316406
MLP_h_4h: 0.0030634403228759766
MLP_4h_h: 0.0026547908782958984
Post-MLP residual: 0.0003910064697265625
Attention layer time: 0.014804840087890625
LN1: 0.00012969970703125
QKV Transform: 0.002099275588989258
Attention Score: 0.0011386871337890625
Attention Softmax: 0.0026595592498779297
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007562637329101562
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.0001304149627685547
LN2: 0.00013065338134765625
MLP_h_4h: 0.003054380416870117
MLP_4h_h: 0.0026252269744873047
Post-MLP residual: 0.00038623809814453125
Attention layer time: 0.014716148376464844
LN1: 0.00012969970703125
QKV Transform: 0.0020990371704101562
Attention Score: 0.0011415481567382812
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006484985351562500
Attention Over Value: 0.0007636547088623047
Attention linproj: 0.000759124755859375
Post-attention Dropout: 0.0003910064697265625
Post-attention residual: 0.00013065338134765625
LN2: 0.00013065338134765625
MLP_h_4h: 0.003072500228881836
MLP_4h_h: 0.0026235580444335938
Post-MLP residual: 0.00039124488830566406
Attention layer time: 0.014778375625610352
LN1: 0.00012993812561035156
QKV Transform: 0.002101898193359375
Attention Score: 0.0011334419250488281
Attention Softmax: 0.002655506134033203
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007534027099609375
Post-attention Dropout: 0.00039315223693847656
Post-attention residual: 0.0001308917999267578
LN2: 0.00013327598571777344
MLP_h_4h: 0.003056764602661133
MLP_4h_h: 0.002629518508911133
Post-MLP residual: 0.0003867149353027344
Attention layer time: 0.014729499816894531
LN1: 0.0001289844512939453
QKV Transform: 0.0020971298217773438
Attention Score: 0.0011353492736816406
Attention Softmax: 0.002650737762451172
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007526874542236328
Post-attention Dropout: 0.00038886070251464844
Post-attention residual: 0.00012993812561035156
LN2: 0.00013113021850585938
MLP_h_4h: 0.003068208694458008
MLP_4h_h: 0.0026237964630126953
Post-MLP residual: 0.0003902912139892578
Attention layer time: 0.014731645584106445
LN1: 0.00012922286987304688
QKV Transform: 0.0020990371704101562
Attention Score: 0.0011441707611083984
Attention Softmax: 0.002671480178833008
Attention Dropout: 0.00006508827209472656
Attention Over Value: 0.0007772445678710938
Attention linproj: 0.0007545948028564453
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.00012969970703125
LN2: 0.0001323223114013672
MLP_h_4h: 0.0030603408813476562
MLP_4h_h: 0.0026285648345947266
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014785289764404297
LN1: 0.00013017654418945312
QKV Transform: 0.002112150192260742
Attention Score: 0.0011446475982666016
Attention Softmax: 0.0026743412017822266
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007622241973876953
Post-attention Dropout: 0.00039196014404296875
Post-attention residual: 0.00012993812561035156
LN2: 0.00014853477478027344
MLP_h_4h: 0.003059864044189453
MLP_4h_h: 0.002629995346069336
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.01479959487915039
LN1: 0.0001308917999267578
QKV Transform: 0.0021092891693115234
Attention Score: 0.0011413097381591797
Attention Softmax: 0.0026683807373046875
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007619857788085938
Attention linproj: 0.0007543563842773438
Post-attention Dropout: 0.0003883838653564453
Post-attention residual: 0.00013208389282226562
LN2: 0.00013327598571777344
MLP_h_4h: 0.0030639171600341797
MLP_4h_h: 0.0026276111602783203
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.014766454696655273
LN1: 0.00012874603271484375
QKV Transform: 0.0021009445190429688
Attention Score: 0.001146554946899414
Attention Softmax: 0.002670764923095703
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007660388946533203
Attention linproj: 0.0007555484771728516
Post-attention Dropout: 0.0003883838653564453
Post-attention residual: 0.00012969970703125
LN2: 0.00013113021850585938
MLP_h_4h: 0.003059864044189453
MLP_4h_h: 0.0026285648345947266
Post-MLP residual: 0.00038814544677734375
Attention layer time: 0.01476287841796875
LN1: 0.00012993812561035156
QKV Transform: 0.002101898193359375
Attention Score: 0.0011386871337890625
Attention Softmax: 0.0026731491088867188
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007617473602294922
Post-attention Dropout: 0.00038886070251464844
Post-attention residual: 0.0001304149627685547
LN2: 0.00013184547424316406
MLP_h_4h: 0.0030646324157714844
MLP_4h_h: 0.002637624740600586
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014765501022338867
LN1: 0.0001316070556640625
QKV Transform: 0.002106904983520508
Attention Score: 0.0011420249938964844
Attention Softmax: 0.002670764923095703
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007655620574951172
Post-attention Dropout: 0.0003943443298339844
Post-attention residual: 0.0001316070556640625
LN2: 0.00013327598571777344
MLP_h_4h: 0.003068685531616211
MLP_4h_h: 0.0026311874389648438
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014789104461669922
LN1: 0.00012946128845214844
QKV Transform: 0.00209808349609375
Attention Score: 0.001142740249633789
Attention Softmax: 0.0026702880859375
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007565021514892578
Post-attention Dropout: 0.0003876686096191406
Post-attention residual: 0.00013136863708496094
LN2: 0.00012993812561035156
MLP_h_4h: 0.0030562877655029297
MLP_4h_h: 0.0026273727416992188
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014732122421264648
LN1: 0.00013065338134765625
QKV Transform: 0.0021011829376220703
Attention Score: 0.001146078109741211
Attention Softmax: 0.0026683807373046875
Attention Dropout: 0.00006389617919921875
Attention Over Value: 0.0007658004760742188
Attention linproj: 0.0007617473602294922
Post-attention Dropout: 0.0003917217254638672
Post-attention residual: 0.00013065338134765625
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030553340911865234
MLP_4h_h: 0.0026247501373291016
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.014768600463867188
LN1: 0.00013875961303710938
QKV Transform: 0.002103567123413086
Attention Score: 0.0011332035064697266
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007534027099609375
Post-attention Dropout: 0.0003902912139892578
Post-attention residual: 0.00013065338134765625
LN2: 0.0001327991485595703
MLP_h_4h: 0.003058910369873047
MLP_4h_h: 0.0026237964630126953
Post-MLP residual: 0.00038695335388183594
Attention layer time: 0.0147247314453125
LN1: 0.00012874603271484375
QKV Transform: 0.0020983219146728516
Attention Score: 0.0011398792266845703
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007529258728027344
Post-attention Dropout: 0.0003879070281982422
Post-attention residual: 0.0001308917999267578
LN2: 0.00014162063598632812
MLP_h_4h: 0.003062725067138672
MLP_4h_h: 0.0026192665100097656
Post-MLP residual: 0.0003921985626220703
Attention layer time: 0.014747142791748047
LN1: 0.00012946128845214844
QKV Transform: 0.0020978450775146484
Attention Score: 0.0011444091796875
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007688999176025391
Attention linproj: 0.0007538795471191406
Post-attention Dropout: 0.00039076805114746094
Post-attention residual: 0.0001304149627685547
LN2: 0.00012993812561035156
MLP_h_4h: 0.003053903579711914
MLP_4h_h: 0.0026242733001708984
Post-MLP residual: 0.0003883838653564453
Attention layer time: 0.014726400375366211
LN1: 0.00012969970703125
QKV Transform: 0.0021102428436279297
Attention Score: 0.0011332035064697266
Attention Softmax: 0.0026540756225585938
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007698535919189453
Attention linproj: 0.0007486343383789062
Post-attention Dropout: 0.0003921985626220703
Post-attention residual: 0.0001308917999267578
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030617713928222656
MLP_4h_h: 0.0026204586029052734
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014731168746948242
LN1: 0.00012826919555664062
QKV Transform: 0.0021157264709472656
Attention Score: 0.0011396408081054688
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007531642913818359
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.00013136863708496094
LN2: 0.00013303756713867188
MLP_h_4h: 0.0030634403228759766
MLP_4h_h: 0.0026214122772216797
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014728307723999023
LN1: 0.00012826919555664062
QKV Transform: 0.0020971298217773438
Attention Score: 0.0011408329010009766
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007579326629638672
Post-attention Dropout: 0.00039076805114746094
Post-attention residual: 0.00013065338134765625
LN2: 0.0001304149627685547
MLP_h_4h: 0.003056049346923828
MLP_4h_h: 0.002623319625854492
Post-MLP residual: 0.000392913818359375
Attention layer time: 0.014758110046386719
LN1: 0.00012826919555664062
QKV Transform: 0.0021114349365234375
Attention Score: 0.0011372566223144531
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007605552673339844
Post-attention Dropout: 0.00038814544677734375
Post-attention residual: 0.00013184547424316406
LN2: 0.00013065338134765625
MLP_h_4h: 0.003055095672607422
MLP_4h_h: 0.0026230812072753906
Post-MLP residual: 0.0003859996795654297
Attention layer time: 0.014728784561157227
LN1: 0.0001316070556640625
QKV Transform: 0.002103090286254883
Attention Score: 0.0011322498321533203
Attention Softmax: 0.0026509761810302734
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007565021514892578
Post-attention Dropout: 0.0004086494445800781
Post-attention residual: 0.00013184547424316406
LN2: 0.00013113021850585938
MLP_h_4h: 0.0030608177185058594
MLP_4h_h: 0.0026237964630126953
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014751195907592773
LN1: 0.0001285076141357422
QKV Transform: 0.002096414566040039
Attention Score: 0.0011339187622070312
Attention Softmax: 0.002651214599609375
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.002444744110107422
Attention linproj: 0.0007562637329101562
Post-attention Dropout: 0.00039315223693847656
Post-attention residual: 0.0001304149627685547
LN2: 0.0001342296600341797
MLP_h_4h: 0.0030553340911865234
MLP_4h_h: 0.002628803253173828
Post-MLP residual: 0.00038814544677734375
Attention layer time: 0.01644730567932129
LN1: 0.0001285076141357422
QKV Transform: 0.002098560333251953
Attention Score: 0.0011358261108398438
Attention Softmax: 0.002650737762451172
Attention Dropout: 0.00007534027099609375
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007569789886474609
Post-attention Dropout: 0.0003876686096191406
Post-attention residual: 0.00012993812561035156
LN2: 0.0001304149627685547
MLP_h_4h: 0.003050565719604492
MLP_4h_h: 0.002624034881591797
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014732122421264648
LN1: 0.00012993812561035156
QKV Transform: 0.0020978450775146484
Attention Score: 0.0011386871337890625
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007534027099609375
Post-attention Dropout: 0.0003902912139892578
Post-attention residual: 0.00013065338134765625
LN2: 0.0001304149627685547
MLP_h_4h: 0.003055095672607422
MLP_4h_h: 0.002624988555908203
Post-MLP residual: 0.00038814544677734375
Attention layer time: 0.014719963073730469
LN1: 0.0001308917999267578
QKV Transform: 0.0021033287048339844
Attention Score: 0.0011341571807861328
Attention Softmax: 0.0026552677154541016
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007588863372802734
Post-attention Dropout: 0.0003910064697265625
Post-attention residual: 0.0001304149627685547
LN2: 0.00013065338134765625
MLP_h_4h: 0.0030634403228759766
MLP_4h_h: 0.002623319625854492
Post-MLP residual: 0.000385284423828125
Attention layer time: 0.014729976654052734
LN1: 0.00012969970703125
QKV Transform: 0.0020966529846191406
Attention Score: 0.0011334419250488281
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007531642913818359
Post-attention Dropout: 0.0003895759582519531
Post-attention residual: 0.00013256072998046875
LN2: 0.0001327991485595703
MLP_h_4h: 0.0030617713928222656
MLP_4h_h: 0.0026197433471679688
Post-MLP residual: 0.0003886222839355469
Attention layer time: 0.01472020149230957
LN1: 0.0001289844512939453
QKV Transform: 0.0020966529846191406
Attention Score: 0.0011372566223144531
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.000762939453125
Attention linproj: 0.0007550716400146484
Post-attention Dropout: 0.0003871917724609375
Post-attention residual: 0.00013017654418945312
LN2: 0.0001304149627685547
MLP_h_4h: 0.003053903579711914
MLP_4h_h: 0.0026273727416992188
Post-MLP residual: 0.0003886222839355469
Attention layer time: 0.014712810516357422
LN1: 0.0001285076141357422
QKV Transform: 0.0020978450775146484
Attention Score: 0.0011382102966308594
Attention Softmax: 0.0026581287384033203
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007672309875488281
Post-attention Dropout: 0.0003986358642578125
Post-attention residual: 0.00014710426330566406
LN2: 0.00013303756713867188
MLP_h_4h: 0.003063201904296875
MLP_4h_h: 0.002629518508911133
Post-MLP residual: 0.0003986358642578125
Attention layer time: 0.014800786972045898
LN1: 0.0001323223114013672
QKV Transform: 0.0021097660064697266
Attention Score: 0.0011467933654785156
Attention Softmax: 0.0026683807373046875
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007572174072265625
Post-attention Dropout: 0.000392913818359375
Post-attention residual: 0.0001308917999267578
LN2: 0.00013208389282226562
MLP_h_4h: 0.0030679702758789062
MLP_4h_h: 0.002628326416015625
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014795780181884766
LN1: 0.00012922286987304688
QKV Transform: 0.0021026134490966797
Attention Score: 0.0011534690856933594
Attention Softmax: 0.0026712417602539062
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007619857788085938
Attention linproj: 0.0007572174072265625
Post-attention Dropout: 0.0003886222839355469
Post-attention residual: 0.00012993812561035156
LN2: 0.0001304149627685547
MLP_h_4h: 0.003059864044189453
MLP_4h_h: 0.002632617950439453
Post-MLP residual: 0.0003917217254638672
Attention layer time: 0.014799118041992188
LN1: 0.00013136863708496094
QKV Transform: 0.002114534378051758
Attention Score: 0.0011456012725830078
Attention Softmax: 0.0026688575744628906
Attention Dropout: 0.00006604194641113281
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.000762939453125
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.00013017654418945312
LN2: 0.00013113021850585938
MLP_h_4h: 0.0030608177185058594
MLP_4h_h: 0.002629518508911133
Post-MLP residual: 0.0003921985626220703
Attention layer time: 0.014810562133789062
LN1: 0.0001316070556640625
QKV Transform: 0.0021066665649414062
Attention Score: 0.0011417865753173828
Attention Softmax: 0.002674579620361328
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007543563842773438
Post-attention Dropout: 0.0003943443298339844
Post-attention residual: 0.00013065338134765625
LN2: 0.00013566017150878906
MLP_h_4h: 0.003057718276977539
MLP_4h_h: 0.0026292800903320312
Post-MLP residual: 0.0003883838653564453
Attention layer time: 0.014766693115234375
LN1: 0.00012826919555664062
QKV Transform: 0.0020978450775146484
Attention Score: 0.0011401176452636719
Attention Softmax: 0.002650737762451172
Attention Dropout: 0.00007271766662597656
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007567405700683594
Post-attention Dropout: 0.0003895759582519531
Post-attention residual: 0.00013017654418945312
LN2: 0.00014638900756835938
MLP_h_4h: 0.0030641555786132812
MLP_4h_h: 0.002624988555908203
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014749765396118164
LN1: 0.00012993812561035156
QKV Transform: 0.002100706100463867
Attention Score: 0.0011363029479980469
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007543563842773438
Post-attention Dropout: 0.00039005279541015625
Post-attention residual: 0.0001323223114013672
LN2: 0.00012993812561035156
MLP_h_4h: 0.0030558109283447266
MLP_4h_h: 0.0026252269744873047
Post-MLP residual: 0.00038933753967285156
Attention layer time: 0.014729499816894531
LN1: 0.00012946128845214844
QKV Transform: 0.002091646194458008
Attention Score: 0.0011348724365234375
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007627010345458984
Attention linproj: 0.0007615089416503906
Post-attention Dropout: 0.00039267539978027344
Post-attention residual: 0.00013065338134765625
LN2: 0.00012969970703125
MLP_h_4h: 0.0030579566955566406
MLP_4h_h: 0.0026242733001708984
Post-MLP residual: 0.00038814544677734375
Attention layer time: 0.014739751815795898
LN1: 0.0001285076141357422
QKV Transform: 0.002110719680786133
Attention Score: 0.0011355876922607422
Attention Softmax: 0.0026502609252929688
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.00075531005859375
Post-attention Dropout: 0.0003883838653564453
Post-attention residual: 0.0001323223114013672
LN2: 0.000133514404296875
MLP_h_4h: 0.0030646324157714844
MLP_4h_h: 0.00262451171875
Post-MLP residual: 0.0003902912139892578
Attention layer time: 0.014731168746948242
LN1: 0.00012874603271484375
QKV Transform: 0.0020983219146728516
Attention Score: 0.0011382102966308594
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007567405700683594
Post-attention Dropout: 0.0003867149353027344
Post-attention residual: 0.00012922286987304688
LN2: 0.00012993812561035156
MLP_h_4h: 0.0030570030212402344
MLP_4h_h: 0.0026285648345947266
Post-MLP residual: 0.000400543212890625
Attention layer time: 0.014734029769897461
LN1: 0.0001285076141357422
QKV Transform: 0.002103567123413086
Attention Score: 0.0011339187622070312
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007615089416503906
Post-attention Dropout: 0.0004000663757324219
Post-attention residual: 0.00013113021850585938
LN2: 0.00013136863708496094
MLP_h_4h: 0.003056764602661133
MLP_4h_h: 0.0026247501373291016
Post-MLP residual: 0.00040221214294433594
Attention layer time: 0.014749526977539062
LN1: 0.00013065338134765625
QKV Transform: 0.0021033287048339844
Attention Score: 0.0011322498321533203
Attention Softmax: 0.0026488304138183594
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.000759124755859375
Post-attention Dropout: 0.0004096031188964844
Post-attention residual: 0.0001304149627685547
LN2: 0.0001316070556640625
MLP_h_4h: 0.0030608177185058594
MLP_4h_h: 0.0026259422302246094
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.01474142074584961
LN1: 0.00012874603271484375
QKV Transform: 0.002078533172607422
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026307106018066406
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007503032684326172
Post-attention Dropout: 0.0003883838653564453
Post-attention residual: 0.00013065338134765625
LN2: 0.00012946128845214844
MLP_h_4h: 0.003021717071533203
MLP_4h_h: 0.002596139907836914
Post-MLP residual: 0.0003921985626220703
Attention layer time: 0.014606952667236328
LN1: 0.00012803077697753906
QKV Transform: 0.002081632614135742
Attention Score: 0.0011255741119384766
Attention Softmax: 0.0026319026947021484
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007543563842773438
Post-attention Dropout: 0.0003876686096191406
Post-attention residual: 0.0001308917999267578
LN2: 0.00013017654418945312
MLP_h_4h: 0.003026247024536133
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.01459956169128418
LN1: 0.00013256072998046875
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011336803436279297
Attention Softmax: 0.0026450157165527344
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007498264312744141
Post-attention Dropout: 0.00039124488830566406
Post-attention residual: 0.00013065338134765625
LN2: 0.00013208389282226562
MLP_h_4h: 0.003036022186279297
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.00039649009704589844
Attention layer time: 0.014669418334960938
LN1: 0.00012874603271484375
QKV Transform: 0.0020787715911865234
Attention Score: 0.0011339187622070312
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007636547088623047
Attention linproj: 0.0007505416870117188
Post-attention Dropout: 0.0003876686096191406
Post-attention residual: 0.00012993812561035156
LN2: 0.00012922286987304688
MLP_h_4h: 0.003031492233276367
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.0003914833068847656
Attention layer time: 0.014642000198364258
LN1: 0.00012826919555664062
QKV Transform: 0.002081155776977539
Attention Score: 0.0011260509490966797
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007557868957519531
Post-attention Dropout: 0.0003986358642578125
Post-attention residual: 0.00013184547424316406
LN2: 0.00013256072998046875
MLP_h_4h: 0.003034353256225586
MLP_4h_h: 0.00260162353515625
Post-MLP residual: 0.0003883838653564453
Attention layer time: 0.014647722244262695
LN1: 0.00013256072998046875
QKV Transform: 0.0020856857299804688
Attention Score: 0.0011286735534667969
Attention Softmax: 0.00264739990234375
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.0004017353057861328
Post-attention residual: 0.0001308917999267578
LN2: 0.00013184547424316406
MLP_h_4h: 0.0030372142791748047
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014660358428955078
LN1: 0.00013017654418945312
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011324882507324219
Attention Softmax: 0.0026504993438720703
Attention Dropout: 0.00006341934204101562
Attention Over Value: 0.0007622241973876953
Attention linproj: 0.0007519721984863281
Post-attention Dropout: 0.00038623809814453125
Post-attention residual: 0.00012993812561035156
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030291080474853516
MLP_4h_h: 0.0026013851165771484
Post-MLP residual: 0.00040411949157714844
Attention layer time: 0.014666557312011719
LN1: 0.0001304149627685547
QKV Transform: 0.0020825862884521484
Attention Score: 0.0011403560638427734
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007619857788085938
Attention linproj: 0.0007560253143310547
Post-attention Dropout: 0.00040221214294433594
Post-attention residual: 0.00013017654418945312
LN2: 0.00013065338134765625
MLP_h_4h: 0.003031015396118164
MLP_4h_h: 0.002598285675048828
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014657020568847656
LN1: 0.00013065338134765625
QKV Transform: 0.0020873546600341797
Attention Score: 0.0011301040649414062
Attention Softmax: 0.002646207809448242
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007510185241699219
Post-attention Dropout: 0.00039315223693847656
Post-attention residual: 0.00012993812561035156
LN2: 0.00013256072998046875
MLP_h_4h: 0.0030374526977539062
MLP_4h_h: 0.0026171207427978516
Post-MLP residual: 0.00038433074951171875
Attention layer time: 0.014666557312011719
LN1: 0.0001285076141357422
QKV Transform: 0.002079486846923828
Attention Score: 0.0011358261108398438
Attention Softmax: 0.002658367156982422
Attention Dropout: 0.00007581710815429688
Attention Over Value: 0.0007643699645996094
Attention linproj: 0.0007548332214355469
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.0001304149627685547
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030384063720703125
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.0003857612609863281
Attention layer time: 0.014681339263916016
LN1: 0.00013017654418945312
QKV Transform: 0.00209808349609375
Attention Score: 0.001132965087890625
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00006437301635742188
Attention Over Value: 0.0007791519165039062
Attention linproj: 0.0007555484771728516
Post-attention Dropout: 0.0003917217254638672
Post-attention residual: 0.00013065338134765625
LN2: 0.00012969970703125
MLP_h_4h: 0.003032684326171875
MLP_4h_h: 0.0025975704193115234
Post-MLP residual: 0.0003902912139892578
Attention layer time: 0.01469731330871582
LN1: 0.0001304149627685547
QKV Transform: 0.002084970474243164
Attention Score: 0.001131296157836914
Attention Softmax: 0.0026726722717285156
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007510185241699219
Post-attention Dropout: 0.0003905296325683594
Post-attention residual: 0.00013113021850585938
LN2: 0.00013327598571777344
MLP_h_4h: 0.003023862838745117
MLP_4h_h: 0.0026035308837890625
Post-MLP residual: 0.0003867149353027344
Attention layer time: 0.014677286148071289
LN1: 0.00012874603271484375
QKV Transform: 0.002080678939819336
Attention Score: 0.0011265277862548828
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007512569427490234
Post-attention Dropout: 0.0003857612609863281
Post-attention residual: 0.00012969970703125
LN2: 0.0001308917999267578
MLP_h_4h: 0.0030336380004882812
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014617919921875
LN1: 0.00012969970703125
QKV Transform: 0.0020813941955566406
Attention Score: 0.0011410713195800781
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00008249282836914062
Attention Over Value: 0.0007627010345458984
Attention linproj: 0.0007557868957519531
Post-attention Dropout: 0.00038909912109375
Post-attention residual: 0.0001308917999267578
LN2: 0.00012922286987304688
MLP_h_4h: 0.0030307769775390625
MLP_4h_h: 0.002622365951538086
Post-MLP residual: 0.00039076805114746094
Attention layer time: 0.014704227447509766
LN1: 0.00012993812561035156
QKV Transform: 0.0020856857299804688
Attention Score: 0.001134634017944336
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00006175041198730469
Attention Over Value: 0.0007636547088623047
Attention linproj: 0.0007526874542236328
Post-attention Dropout: 0.0003933906555175781
Post-attention residual: 0.00013065338134765625
LN2: 0.00013208389282226562
MLP_h_4h: 0.0030317306518554688
MLP_4h_h: 0.002604961395263672
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014674901962280273
LN1: 0.00012803077697753906
QKV Transform: 0.002079486846923828
Attention Score: 0.0011370182037353516
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007512569427490234
Post-attention Dropout: 0.00038886070251464844
Post-attention residual: 0.00013017654418945312
LN2: 0.00013017654418945312
MLP_h_4h: 0.003047466278076172
MLP_4h_h: 0.0026116371154785156
Post-MLP residual: 0.00039076805114746094
Attention layer time: 0.014673709869384766
LN1: 0.00012922286987304688
QKV Transform: 0.0020797252655029297
Attention Score: 0.0011403560638427734
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00006365776062011719
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007600784301757812
Post-attention Dropout: 0.00038886070251464844
Post-attention residual: 0.00013017654418945312
LN2: 0.00012993812561035156
MLP_h_4h: 0.003032207489013672
MLP_4h_h: 0.0025997161865234375
Post-MLP residual: 0.0003910064697265625
Attention layer time: 0.014661550521850586
LN1: 0.00013017654418945312
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011353492736816406
Attention Softmax: 0.0026531219482421875
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007505416870117188
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.00013113021850585938
LN2: 0.0001316070556640625
MLP_h_4h: 0.0030307769775390625
MLP_4h_h: 0.002602815628051758
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014649629592895508
LN1: 0.00012874603271484375
QKV Transform: 0.0020797252655029297
Attention Score: 0.0011279582977294922
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00008940696716308594
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007522106170654297
Post-attention Dropout: 0.0003857612609863281
Post-attention residual: 0.0001304149627685547
LN2: 0.0001327991485595703
MLP_h_4h: 0.0030417442321777344
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014666080474853516
LN1: 0.00012826919555664062
QKV Transform: 0.002080202102661133
Attention Score: 0.0011451244354248047
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00006151199340820312
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007474422454833984
Post-attention Dropout: 0.0003924369812011719
Post-attention residual: 0.00013113021850585938
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030295848846435547
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.0003886222839355469
Attention layer time: 0.014649629592895508
LN1: 0.00013017654418945312
QKV Transform: 0.0020852088928222656
Attention Score: 0.0011403560638427734
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007615089416503906
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.00013113021850585938
LN2: 0.0001304149627685547
MLP_h_4h: 0.003028392791748047
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.0003848075866699219
Attention layer time: 0.014664649963378906
LN1: 0.00012755393981933594
QKV Transform: 0.0020809173583984375
Attention Score: 0.001142263412475586
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007510185241699219
Post-attention Dropout: 0.00038814544677734375
Post-attention residual: 0.00012993812561035156
LN2: 0.00014972686767578125
MLP_h_4h: 0.0030393600463867188
MLP_4h_h: 0.0025992393493652344
Post-MLP residual: 0.00038886070251464844
Attention layer time: 0.01468801498413086
LN1: 0.00012874603271484375
QKV Transform: 0.0020792484283447266
Attention Score: 0.0011374950408935547
Attention Softmax: 0.0026497840881347656
Attention Dropout: 0.00006341934204101562
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007503032684326172
Post-attention Dropout: 0.00038933753967285156
Post-attention residual: 0.00013017654418945312
LN2: 0.00012946128845214844
MLP_h_4h: 0.003029346466064453
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.014652729034423828
LN1: 0.00012969970703125
QKV Transform: 0.0020737648010253906
Attention Score: 0.001130819320678711
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007572174072265625
Post-attention Dropout: 0.0003955364227294922
Post-attention residual: 0.00012969970703125
LN2: 0.00012874603271484375
MLP_h_4h: 0.0030670166015625
MLP_4h_h: 0.0026035308837890625
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014676332473754883
LN1: 0.00012826919555664062
QKV Transform: 0.0020799636840820312
Attention Score: 0.0011315345764160156
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006604194641113281
Attention Over Value: 0.0007660388946533203
Attention linproj: 0.0007495880126953125
Post-attention Dropout: 0.0003962516784667969
Post-attention residual: 0.00013208389282226562
LN2: 0.00013303756713867188
MLP_h_4h: 0.0030374526977539062
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.00038695335388183594
Attention layer time: 0.014662981033325195
LN1: 0.00012874603271484375
QKV Transform: 0.002078533172607422
Attention Score: 0.0011372566223144531
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007627010345458984
Attention linproj: 0.0007498264312744141
Post-attention Dropout: 0.00038743019104003906
Post-attention residual: 0.00013017654418945312
LN2: 0.00012993812561035156
MLP_h_4h: 0.003029346466064453
MLP_4h_h: 0.002603769302368164
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.014641523361206055
LN1: 0.00012969970703125
QKV Transform: 0.0020933151245117188
Attention Score: 0.0011403560638427734
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007534027099609375
Post-attention Dropout: 0.0003898143768310547
Post-attention residual: 0.00012969970703125
LN2: 0.00012946128845214844
MLP_h_4h: 0.0030333995819091797
MLP_4h_h: 0.0026428699493408203
Post-MLP residual: 0.0005252361297607422
Attention layer time: 0.014988422393798828
LN1: 0.00019097328186035156
QKV Transform: 0.002155780792236328
Attention Score: 0.0011458396911621094
Attention Softmax: 0.0026865005493164062
Attention Dropout: 0.00014281272888183594
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007603168487548828
Post-attention Dropout: 0.00039267539978027344
Post-attention residual: 0.0001304149627685547
LN2: 0.00013208389282226562
MLP_h_4h: 0.0030281543731689453
MLP_4h_h: 0.0026106834411621094
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.01503443717956543
LN1: 0.00012874603271484375
QKV Transform: 0.0020787715911865234
Attention Score: 0.0011243820190429688
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007505416870117188
Post-attention Dropout: 0.0003871917724609375
Post-attention residual: 0.0001308917999267578
LN2: 0.00012922286987304688
MLP_h_4h: 0.0030286312103271484
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.0003864765167236328
Attention layer time: 0.014614105224609375
LN1: 0.00012969970703125
QKV Transform: 0.002081155776977539
Attention Score: 0.0011401176452636719
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00008273124694824219
Attention Over Value: 0.0007655620574951172
Attention linproj: 0.0007548332214355469
Post-attention Dropout: 0.0003910064697265625
Post-attention residual: 0.00012946128845214844
LN2: 0.00012993812561035156
MLP_h_4h: 0.003030538558959961
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.00039267539978027344
Attention layer time: 0.014682292938232422
LN1: 0.0001304149627685547
QKV Transform: 0.0020859241485595703
Attention Score: 0.0011398792266845703
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007581710815429688
Attention linproj: 0.0007503032684326172
Post-attention Dropout: 0.0003905296325683594
Post-attention residual: 0.00013065338134765625
LN2: 0.0001316070556640625
MLP_h_4h: 0.0030546188354492188
MLP_4h_h: 0.0026094913482666016
Post-MLP residual: 0.0003952980041503906
Attention layer time: 0.014700889587402344
LN1: 0.0001285076141357422
QKV Transform: 0.002082347869873047
Attention Score: 0.00113677978515625
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007498264312744141
Post-attention Dropout: 0.0003867149353027344
Post-attention residual: 0.00013065338134765625
LN2: 0.00012969970703125
MLP_h_4h: 0.0030374526977539062
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.0004057884216308594
Attention layer time: 0.014676332473754883
LN1: 0.00013017654418945312
QKV Transform: 0.0020864009857177734
Attention Score: 0.0011339187622070312
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00006365776062011719
Attention Over Value: 0.0007627010345458984
Attention linproj: 0.0007624626159667969
Post-attention Dropout: 0.0003898143768310547
Post-attention residual: 0.00012993812561035156
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030324459075927734
MLP_4h_h: 0.0026044845581054688
Post-MLP residual: 0.00039076805114746094
Attention layer time: 0.014673471450805664
LN1: 0.00012946128845214844
QKV Transform: 0.0020852088928222656
Attention Score: 0.0011377334594726562
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007529258728027344
Post-attention Dropout: 0.00039315223693847656
Post-attention residual: 0.00013065338134765625
LN2: 0.00014066696166992188
MLP_h_4h: 0.0030303001403808594
MLP_4h_h: 0.002603769302368164
Post-MLP residual: 0.00038814544677734375
Attention layer time: 0.014665842056274414
LN1: 0.00012874603271484375
QKV Transform: 0.002076864242553711
Attention Score: 0.0011301040649414062
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00007200241088867188
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007486343383789062
Post-attention Dropout: 0.00039196014404296875
Post-attention residual: 0.0001304149627685547
LN2: 0.00013065338134765625
MLP_h_4h: 0.0030395984649658203
MLP_4h_h: 0.002597808837890625
Post-MLP residual: 0.00038886070251464844
Attention layer time: 0.01463937759399414
LN1: 0.0001289844512939453
QKV Transform: 0.0020792484283447266
Attention Score: 0.0011479854583740234
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007495880126953125
Post-attention Dropout: 0.00039124488830566406
Post-attention residual: 0.00012969970703125
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030286312103271484
MLP_4h_h: 0.002603769302368164
Post-MLP residual: 0.00039076805114746094
Attention layer time: 0.01465606689453125
LN1: 0.00013065338134765625
QKV Transform: 0.002072572708129883
Attention Score: 0.0011305809020996094
Attention Softmax: 0.002652883529663086
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007605552673339844
Post-attention Dropout: 0.0003895759582519531
Post-attention residual: 0.00013113021850585938
LN2: 0.00012969970703125
MLP_h_4h: 0.003028392791748047
MLP_4h_h: 0.0026044845581054688
Post-MLP residual: 0.0003864765167236328
Attention layer time: 0.014667034149169922
LN1: 0.00012755393981933594
QKV Transform: 0.002080202102661133
Attention Score: 0.001130819320678711
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007612705230712891
Attention linproj: 0.0007481575012207031
Post-attention Dropout: 0.00038814544677734375
Post-attention residual: 0.0001392364501953125
LN2: 0.00014901161193847656
MLP_h_4h: 0.0030367374420166016
MLP_4h_h: 0.0025980472564697266
Post-MLP residual: 0.00038623809814453125
Attention layer time: 0.014663457870483398
LN1: 0.00012922286987304688
QKV Transform: 0.002080202102661133
Attention Score: 0.0011377334594726562
Attention Softmax: 0.0026493072509765625
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007483959197998047
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.00013113021850585938
LN2: 0.00014138221740722656
MLP_h_4h: 0.00302886962890625
MLP_4h_h: 0.0026030540466308594
Post-MLP residual: 0.00038933753967285156
Attention layer time: 0.014669179916381836
LN1: 0.00012922286987304688
QKV Transform: 0.002072572708129883
Attention Score: 0.0011391639709472656
Attention Softmax: 0.002653837203979492
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007574558258056641
Post-attention Dropout: 0.00039768218994140625
Post-attention residual: 0.0001304149627685547
LN2: 0.0001289844512939453
MLP_h_4h: 0.0030469894409179688
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.0003864765167236328
Attention layer time: 0.014661550521850586
LN1: 0.00012826919555664062
QKV Transform: 0.0020797252655029297
Attention Score: 0.001131296157836914
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007505416870117188
Post-attention Dropout: 0.0003867149353027344
Post-attention residual: 0.00013065338134765625
LN2: 0.000133514404296875
MLP_h_4h: 0.0030410289764404297
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014634847640991211
LN1: 0.0001285076141357422
QKV Transform: 0.002079010009765625
Attention Score: 0.0011458396911621094
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00006294250488281250
Attention Over Value: 0.0007627010345458984
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.00039124488830566406
Post-attention residual: 0.00012969970703125
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030286312103271484
MLP_4h_h: 0.0026018619537353516
Post-MLP residual: 0.00039005279541015625
Attention layer time: 0.014654397964477539
LN1: 0.00013017654418945312
QKV Transform: 0.002091646194458008
Attention Score: 0.0011327266693115234
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007560253143310547
Post-attention Dropout: 0.00039076805114746094
Post-attention residual: 0.00013017654418945312
LN2: 0.00013017654418945312
MLP_h_4h: 0.003035306930541992
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.0003845691680908203
Attention layer time: 0.014679193496704102
LN1: 0.0001285076141357422
QKV Transform: 0.002089262008666992
Attention Score: 0.0011358261108398438
Attention Softmax: 0.0026471614837646484
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007479190826416016
Post-attention Dropout: 0.0003886222839355469
Post-attention residual: 0.0001316070556640625
LN2: 0.0001327991485595703
MLP_h_4h: 0.0030379295349121094
MLP_4h_h: 0.0025997161865234375
Post-MLP residual: 0.0003921985626220703
Attention layer time: 0.014662027359008789
LN1: 0.00012946128845214844
QKV Transform: 0.0020842552185058594
Attention Score: 0.001135110855102539
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00006294250488281250
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007669925689697266
Post-attention Dropout: 0.00038623809814453125
Post-attention residual: 0.00012993812561035156
LN2: 0.00013113021850585938
MLP_h_4h: 0.003028392791748047
MLP_4h_h: 0.002602815628051758
Post-MLP residual: 0.0003883838653564453
Attention layer time: 0.014659404754638672
LN1: 0.00013017654418945312
QKV Transform: 0.002079010009765625
Attention Score: 0.0011317729949951172
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007545948028564453
Post-attention Dropout: 0.00039005279541015625
Post-attention residual: 0.00012969970703125
LN2: 0.00013303756713867188
MLP_h_4h: 0.003036022186279297
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.00038623809814453125
Attention layer time: 0.014666557312011719
LN1: 0.00013017654418945312
QKV Transform: 0.002076864242553711
Attention Score: 0.001129150390625
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007503032684326172
Post-attention Dropout: 0.00038814544677734375
Post-attention residual: 0.00013113021850585938
LN2: 0.0001327991485595703
MLP_h_4h: 0.0030364990234375
MLP_4h_h: 0.0026013851165771484
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014645576477050781
LN1: 0.00012922286987304688
QKV Transform: 0.0020706653594970703
Attention Score: 0.0011301040649414062
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007650852203369141
Attention linproj: 0.0007517337799072266
Post-attention Dropout: 0.00038743019104003906
Post-attention residual: 0.00013017654418945312
LN2: 0.00012946128845214844
MLP_h_4h: 0.0030295848846435547
MLP_4h_h: 0.0026018619537353516
Post-MLP residual: 0.0003921985626220703
Attention layer time: 0.014632940292358398
LN1: 0.0001404285430908203
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011310577392578125
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007619857788085938
Attention linproj: 0.0007560253143310547
Post-attention Dropout: 0.0003910064697265625
Post-attention residual: 0.00013065338134765625
LN2: 0.0001327991485595703
MLP_h_4h: 0.0030336380004882812
MLP_4h_h: 0.0026035308837890625
Post-MLP residual: 0.000385284423828125
Attention layer time: 0.014664888381958008
LN1: 0.00012922286987304688
QKV Transform: 0.002089977264404297
Attention Score: 0.0011324882507324219
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007576942443847656
Attention linproj: 0.0007498264312744141
Post-attention Dropout: 0.00038909912109375
Post-attention residual: 0.0001323223114013672
LN2: 0.00013327598571777344
MLP_h_4h: 0.0030412673950195312
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.0003905296325683594
Attention layer time: 0.014652252197265625
LN1: 0.0001289844512939453
QKV Transform: 0.0020809173583984375
Attention Score: 0.0011391639709472656
Attention Softmax: 0.0026488304138183594
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007631778717041016
Attention linproj: 0.0007503032684326172
Post-attention Dropout: 0.0004031658172607422
Post-attention residual: 0.00013208389282226562
LN2: 0.0001323223114013672
MLP_h_4h: 0.0030374526977539062
MLP_4h_h: 0.002605438232421875
Post-MLP residual: 0.0003902912139892578
Attention layer time: 0.0146942138671875
LN1: 0.00012874603271484375
QKV Transform: 0.0020799636840820312
Attention Score: 0.0011398792266845703
Attention Softmax: 0.0026519298553466797
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.000762939453125
Attention linproj: 0.0007557868957519531
Post-attention Dropout: 0.00039005279541015625
Post-attention residual: 0.00013136863708496094
LN2: 0.00013256072998046875
MLP_h_4h: 0.003031492233276367
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.00039958953857421875
Attention layer time: 0.014698982238769531
LN1: 0.00013208389282226562
QKV Transform: 0.002086639404296875
Attention Score: 0.0011324882507324219
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007619857788085938
Attention linproj: 0.0007503032684326172
Post-attention Dropout: 0.0004074573516845703
Post-attention residual: 0.0001308917999267578
LN2: 0.0001323223114013672
MLP_h_4h: 0.0030384063720703125
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014681816101074219
LN1: 0.00012826919555664062
QKV Transform: 0.002080678939819336
Attention Score: 0.0011353492736816406
Attention Softmax: 0.0026488304138183594
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007636547088623047
Attention linproj: 0.0007505416870117188
Post-attention Dropout: 0.0003886222839355469
Post-attention residual: 0.00013017654418945312
LN2: 0.00013065338134765625
MLP_h_4h: 0.0030176639556884766
MLP_4h_h: 0.002603769302368164
Post-MLP residual: 0.0003905296325683594
Attention layer time: 0.014632701873779297
LN1: 0.00012803077697753906
QKV Transform: 0.0020835399627685547
Attention Score: 0.0011248588562011719
Attention Softmax: 0.0026521682739257812
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007550716400146484
Post-attention Dropout: 0.00038814544677734375
Post-attention residual: 0.00013208389282226562
LN2: 0.0001316070556640625
MLP_h_4h: 0.0030324459075927734
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.0003867149353027344
Attention layer time: 0.01463627815246582
LN1: 0.00013184547424316406
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011343955993652344
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007565021514892578
Post-attention Dropout: 0.000408172607421875
Post-attention residual: 0.0001316070556640625
LN2: 0.00013113021850585938
MLP_h_4h: 0.003038167953491211
MLP_4h_h: 0.0025987625122070312
Post-MLP residual: 0.0003876686096191406
Attention layer time: 0.014678955078125
LN1: 0.0001289844512939453
QKV Transform: 0.0020782947540283203
Attention Score: 0.0011456012725830078
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007610321044921875
Attention linproj: 0.0007505416870117188
Post-attention Dropout: 0.00038909912109375
Post-attention residual: 0.0001304149627685547
LN2: 0.00012993812561035156
MLP_h_4h: 0.0030269622802734375
MLP_4h_h: 0.0026051998138427734
Post-MLP residual: 0.0003905296325683594
Attention layer time: 0.014652252197265625
LN1: 0.00014472007751464844
QKV Transform: 0.002079486846923828
Attention Score: 0.0011289119720458984
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00005865097045898438
Attention Over Value: 0.0007586479187011719
Attention linproj: 0.0007548332214355469
Post-attention Dropout: 0.0003898143768310547
Post-attention residual: 0.0001316070556640625
LN2: 0.0001316070556640625
MLP_h_4h: 0.003031492233276367
MLP_4h_h: 0.0026009082794189453
Post-MLP residual: 0.0003886222839355469
Attention layer time: 0.014650344848632812
LN1: 0.0001323223114013672
QKV Transform: 0.002085447311401367
Attention Score: 0.001130819320678711
Attention Softmax: 0.0026454925537109375
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.000759124755859375
Attention linproj: 0.0007486343383789062
Post-attention Dropout: 0.0003933906555175781
Post-attention residual: 0.00013065338134765625
LN2: 0.0001323223114013672
MLP_h_4h: 0.003036022186279297
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.0003859996795654297
Attention layer time: 0.014654159545898438
LN1: 0.0001289844512939453
QKV Transform: 0.002080678939819336
Attention Score: 0.0011322498321533203
Attention Softmax: 0.0026509761810302734
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007622241973876953
Attention linproj: 0.0007522106170654297
Post-attention Dropout: 0.00038814544677734375
Post-attention residual: 0.00013017654418945312
LN2: 0.00012874603271484375
MLP_h_4h: 0.0030303001403808594
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.00040030479431152344
Attention layer time: 0.014681339263916016
LN1: 0.00013065338134765625
QKV Transform: 0.0020818710327148438
Attention Score: 0.0011587142944335938
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007574558258056641
Attention linproj: 0.0007555484771728516
Post-attention Dropout: 0.00040435791015625
Post-attention residual: 0.0001304149627685547
LN2: 0.00013113021850585938
MLP_h_4h: 0.003030538558959961
MLP_4h_h: 0.002599477767944336
Post-MLP residual: 0.0003933906555175781
Attention layer time: 0.014677286148071289
LN1: 0.00013136863708496094
QKV Transform: 0.0020875930786132812
Attention Score: 0.001135110855102539
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00006031990051269531
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007526874542236328
Post-attention Dropout: 0.0003941059112548828
Post-attention residual: 0.0001308917999267578
LN2: 0.0001316070556640625
MLP_h_4h: 0.003051280975341797
MLP_4h_h: 0.002602815628051758
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014676809310913086
LN1: 0.0001285076141357422
QKV Transform: 0.0020797252655029297
Attention Score: 0.0011277198791503906
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00007367134094238281
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007522106170654297
Post-attention Dropout: 0.0003864765167236328
Post-attention residual: 0.00012946128845214844
LN2: 0.00012993812561035156
MLP_h_4h: 0.00302886962890625
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014632701873779297
LN1: 0.00012993812561035156
QKV Transform: 0.0020818710327148438
Attention Score: 0.0011279582977294922
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007774829864501953
Attention linproj: 0.0007550716400146484
Post-attention Dropout: 0.00038933753967285156
Post-attention residual: 0.0001304149627685547
LN2: 0.00012946128845214844
MLP_h_4h: 0.003030061721801758
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.00039124488830566406
Attention layer time: 0.014687299728393555
LN1: 0.00013017654418945312
QKV Transform: 0.002086639404296875
Attention Score: 0.001138925552368164
Attention Softmax: 0.0026738643646240234
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007495880126953125
Post-attention Dropout: 0.0003936290740966797
Post-attention residual: 0.00013136863708496094
LN2: 0.00013184547424316406
MLP_h_4h: 0.0030400753021240234
MLP_4h_h: 0.002604246139526367
Post-MLP residual: 0.0003857612609863281
Attention layer time: 0.014685392379760742
LN1: 0.0001285076141357422
QKV Transform: 0.0020818710327148438
Attention Score: 0.0011310577392578125
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007596015930175781
Attention linproj: 0.0007495880126953125
Post-attention Dropout: 0.0003871917724609375
Post-attention residual: 0.00013017654418945312
LN2: 0.00013065338134765625
MLP_h_4h: 0.0030303001403808594
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014623165130615234
LN1: 0.0001308917999267578
QKV Transform: 0.00208282470703125
Attention Score: 0.0011303424835205078
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00008082389831542969
Attention Over Value: 0.0007631778717041016
Attention linproj: 0.0007541179656982422
Post-attention Dropout: 0.0003902912139892578
Post-attention residual: 0.00013065338134765625
LN2: 0.00013184547424316406
MLP_h_4h: 0.0030336380004882812
MLP_4h_h: 0.0026149749755859375
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014701604843139648
LN1: 0.00013017654418945312
QKV Transform: 0.0020859241485595703
Attention Score: 0.0011327266693115234
Attention Softmax: 0.0026628971099853516
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007503032684326172
Post-attention Dropout: 0.0003933906555175781
Post-attention residual: 0.00013065338134765625
LN2: 0.00013136863708496094
MLP_h_4h: 0.003041505813598633
MLP_4h_h: 0.0026061534881591797
Post-MLP residual: 0.00038695335388183594
Attention layer time: 0.014672279357910156
LN1: 0.0001277923583984375
QKV Transform: 0.0020821094512939453
Attention Score: 0.0011317729949951172
Attention Softmax: 0.0026476383209228516
Attention Dropout: 0.00005793571472167969
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007517337799072266
Post-attention Dropout: 0.00038623809814453125
Post-attention residual: 0.00012993812561035156
LN2: 0.00013065338134765625
MLP_h_4h: 0.003039121627807617
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.0003876686096191406
Attention layer time: 0.01465606689453125
LN1: 0.0001285076141357422
QKV Transform: 0.0020804405212402344
Attention Score: 0.0011339187622070312
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006365776062011719
Attention Over Value: 0.0007615089416503906
Attention linproj: 0.0007424354553222656
Post-attention Dropout: 0.00038909912109375
Post-attention residual: 0.0001304149627685547
LN2: 0.0001308917999267578
MLP_h_4h: 0.0030319690704345703
MLP_4h_h: 0.0026030540466308594
Post-MLP residual: 0.0003902912139892578
Attention layer time: 0.014641046524047852
LN1: 0.00013017654418945312
QKV Transform: 0.002084016799926758
Attention Score: 0.001129150390625
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.00039458274841308594
Post-attention residual: 0.00013136863708496094
LN2: 0.0001323223114013672
MLP_h_4h: 0.0030355453491210938
MLP_4h_h: 0.0026056766510009766
Post-MLP residual: 0.00038552284240722656
Attention layer time: 0.014654874801635742
LN1: 0.00012826919555664062
QKV Transform: 0.0020813941955566406
Attention Score: 0.0011320114135742188
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007481575012207031
Post-attention Dropout: 0.00038552284240722656
Post-attention residual: 0.00012993812561035156
LN2: 0.00012993812561035156
MLP_h_4h: 0.0030412673950195312
MLP_4h_h: 0.002601146697998047
Post-MLP residual: 0.00038933753967285156
Attention layer time: 0.014651298522949219
LN1: 0.00012874603271484375
QKV Transform: 0.0020797252655029297
Attention Score: 0.001132965087890625
Attention Softmax: 0.002681732177734375
Attention Dropout: 0.00006294250488281250
Attention Over Value: 0.0007638931274414062
Attention linproj: 0.0007495880126953125
Post-attention Dropout: 0.00039267539978027344
Post-attention residual: 0.0001304149627685547
LN2: 0.0001366138458251953
MLP_h_4h: 0.0030221939086914062
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.014684677124023438
LN1: 0.00012993812561035156
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011332035064697266
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007593631744384766
Attention linproj: 0.0007619857788085938
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.00013113021850585938
LN2: 0.00012969970703125
MLP_h_4h: 0.0030274391174316406
MLP_4h_h: 0.0026078224182128906
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014655590057373047
LN1: 0.00012826919555664062
QKV Transform: 0.002080678939819336
Attention Score: 0.0011332035064697266
Attention Softmax: 0.0026464462280273438
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007588863372802734
Attention linproj: 0.0007491111755371094
Post-attention Dropout: 0.00038743019104003906
Post-attention residual: 0.00012993812561035156
LN2: 0.00013065338134765625
MLP_h_4h: 0.0030400753021240234
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014630317687988281
LN1: 0.00012803077697753906
QKV Transform: 0.0020813941955566406
Attention Score: 0.0011332035064697266
Attention Softmax: 0.002649068832397461
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007643699645996094
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.0003895759582519531
Post-attention residual: 0.00012969970703125
LN2: 0.00012993812561035156
MLP_h_4h: 0.003033876419067383
MLP_4h_h: 0.002604246139526367
Post-MLP residual: 0.0003886222839355469
Attention layer time: 0.014646291732788086
LN1: 0.00012993812561035156
QKV Transform: 0.0020868778228759766
Attention Score: 0.0011317729949951172
Attention Softmax: 0.002651214599609375
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007572174072265625
Attention linproj: 0.0007636547088623047
Post-attention Dropout: 0.00038933753967285156
Post-attention residual: 0.0001304149627685547
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030307769775390625
MLP_4h_h: 0.0026040077209472656
Post-MLP residual: 0.00039005279541015625
Attention layer time: 0.014656782150268555
LN1: 0.00012803077697753906
QKV Transform: 0.0020825862884521484
Attention Score: 0.001130819320678711
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.00038552284240722656
Post-attention residual: 0.0001361370086669922
LN2: 0.00014901161193847656
MLP_h_4h: 0.003039121627807617
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.00039076805114746094
Attention layer time: 0.01465749740600586
LN1: 0.00012826919555664062
QKV Transform: 0.0020792484283447266
Attention Score: 0.0011320114135742188
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00006341934204101562
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.0003895759582519531
Post-attention residual: 0.00012969970703125
LN2: 0.00013065338134765625
MLP_h_4h: 0.0030307769775390625
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014642477035522461
LN1: 0.00013017654418945312
QKV Transform: 0.002088785171508789
Attention Score: 0.0011298656463623047
Attention Softmax: 0.0026531219482421875
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007622241973876953
Attention linproj: 0.0007550716400146484
Post-attention Dropout: 0.0003948211669921875
Post-attention residual: 0.0001308917999267578
LN2: 0.00012969970703125
MLP_h_4h: 0.0030481815338134766
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.0003864765167236328
Attention layer time: 0.014669179916381836
LN1: 0.0001285076141357422
QKV Transform: 0.002080678939819336
Attention Score: 0.0011348724365234375
Attention Softmax: 0.002648591995239258
Attention Dropout: 0.00005936622619628906
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007479190826416016
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.0001316070556640625
LN2: 0.0001323223114013672
MLP_h_4h: 0.0030372142791748047
MLP_4h_h: 0.002599000930786133
Post-MLP residual: 0.0003857612609863281
Attention layer time: 0.014634132385253906
LN1: 0.00012803077697753906
QKV Transform: 0.0020799636840820312
Attention Score: 0.0011332035064697266
Attention Softmax: 0.0026488304138183594
Attention Dropout: 0.00006222724914550781
Attention Over Value: 0.0007643699645996094
Attention linproj: 0.0007510185241699219
Post-attention Dropout: 0.0003933906555175781
Post-attention residual: 0.0001304149627685547
LN2: 0.0001316070556640625
MLP_h_4h: 0.0030357837677001953
MLP_4h_h: 0.002605438232421875
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.014678716659545898
LN1: 0.00013113021850585938
QKV Transform: 0.0020902156829833984
Attention Score: 0.0011348724365234375
Attention Softmax: 0.002651691436767578
Attention Dropout: 0.00005817413330078125
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007550716400146484
Post-attention Dropout: 0.00039386749267578125
Post-attention residual: 0.00013017654418945312
LN2: 0.0001308917999267578
MLP_h_4h: 0.0030367374420166016
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.0003895759582519531
Attention layer time: 0.014685630798339844
LN1: 0.0001285076141357422
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011374950408935547
Attention Softmax: 0.0026466846466064453
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007474422454833984
Post-attention Dropout: 0.00038933753967285156
Post-attention residual: 0.00013184547424316406
LN2: 0.0001323223114013672
MLP_h_4h: 0.0030372142791748047
MLP_4h_h: 0.0026006698608398438
Post-MLP residual: 0.0003879070281982422
Attention layer time: 0.014644861221313477
LN1: 0.00012874603271484375
QKV Transform: 0.002079010009765625
Attention Score: 0.0011322498321533203
Attention Softmax: 0.002646923065185547
Attention Dropout: 0.00006270408630371094
Attention Over Value: 0.0007636547088623047
Attention linproj: 0.0007672309875488281
Post-attention Dropout: 0.00038886070251464844
Post-attention residual: 0.00012993812561035156
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030295848846435547
MLP_4h_h: 0.002603769302368164
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.01465296745300293
LN1: 0.00013113021850585938
QKV Transform: 0.0021257400512695312
Attention Score: 0.0011653900146484375
Attention Softmax: 0.0026955604553222656
Attention Dropout: 0.00008249282836914062
Attention Over Value: 0.0007669925689697266
Attention linproj: 0.0007808208465576172
Post-attention Dropout: 0.00041985511779785156
Post-attention residual: 0.0001347064971923828
LN2: 0.0001475811004638672
MLP_h_4h: 0.003039121627807617
MLP_4h_h: 0.0026018619537353516
Post-MLP residual: 0.0003871917724609375
Attention layer time: 0.0150604248046875
LN1: 0.00013256072998046875
QKV Transform: 0.0020875930786132812
Attention Score: 0.0011348724365234375
Attention Softmax: 0.0026483535766601562
Attention Dropout: 0.00005984306335449219
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007536411285400391
Post-attention Dropout: 0.00039076805114746094
Post-attention residual: 0.00013113021850585938
LN2: 0.0001316070556640625
MLP_h_4h: 0.0030395984649658203
MLP_4h_h: 0.0026001930236816406
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014663934707641602
LN1: 0.00012874603271484375
QKV Transform: 0.002079010009765625
Attention Score: 0.0011310577392578125
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007624626159667969
Attention linproj: 0.0007510185241699219
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.00013017654418945312
LN2: 0.00012969970703125
MLP_h_4h: 0.003030061721801758
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.0003993511199951172
Attention layer time: 0.014656782150268555
LN1: 0.00013065338134765625
QKV Transform: 0.002079486846923828
Attention Score: 0.0011601448059082031
Attention Softmax: 0.0026509761810302734
Attention Dropout: 0.00005841255187988281
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.00075531005859375
Post-attention Dropout: 0.00040435791015625
Post-attention residual: 0.00013113021850585938
LN2: 0.00013017654418945312
MLP_h_4h: 0.0030295848846435547
MLP_4h_h: 0.002599954605102539
Post-MLP residual: 0.0003955364227294922
Attention layer time: 0.014683246612548828
LN1: 0.0001304149627685547
QKV Transform: 0.002084970474243164
Attention Score: 0.0011322498321533203
Attention Softmax: 0.0026488304138183594
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007598400115966797
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.0003924369812011719
Post-attention residual: 0.0001316070556640625
LN2: 0.00013208389282226562
MLP_h_4h: 0.0030388832092285156
MLP_4h_h: 0.002603769302368164
Post-MLP residual: 0.00038886070251464844
Attention layer time: 0.014672040939331055
LN1: 0.0001285076141357422
QKV Transform: 0.002079486846923828
Attention Score: 0.0011296272277832031
Attention Softmax: 0.0026488304138183594
Attention Dropout: 0.00007319450378417969
Attention Over Value: 0.0007584095001220703
Attention linproj: 0.0007519721984863281
Post-attention Dropout: 0.00038743019104003906
Post-attention residual: 0.00012993812561035156
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030317306518554688
MLP_4h_h: 0.0026025772094726562
Post-MLP residual: 0.0003886222839355469
Attention layer time: 0.014638662338256836
LN1: 0.00012946128845214844
QKV Transform: 0.0020983219146728516
Attention Score: 0.0011475086212158203
Attention Softmax: 0.0026459693908691406
Attention Dropout: 0.00006628036499023438
Attention Over Value: 0.0007798671722412109
Attention linproj: 0.0007567405700683594
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.00013065338134765625
LN2: 0.00013113021850585938
MLP_h_4h: 0.003031492233276367
MLP_4h_h: 0.0026023387908935547
Post-MLP residual: 0.0003933906555175781
Attention layer time: 0.014712333679199219
LN1: 0.0001304149627685547
QKV Transform: 0.0020856857299804688
Attention Score: 0.0011279582977294922
Attention Softmax: 0.002672910690307617
Attention Dropout: 0.00006127357482910156
Attention Over Value: 0.0007739067077636719
Attention linproj: 0.0007541179656982422
Post-attention Dropout: 0.0003914833068847656
Post-attention residual: 0.0001304149627685547
LN2: 0.0001316070556640625
MLP_h_4h: 0.003033876419067383
MLP_4h_h: 0.0026051998138427734
Post-MLP residual: 0.000385284423828125
Attention layer time: 0.014699459075927734
LN1: 0.00012826919555664062
QKV Transform: 0.002079010009765625
Attention Score: 0.001163482666015625
Attention Softmax: 0.002649545669555664
Attention Dropout: 0.00006079673767089844
Attention Over Value: 0.0007622241973876953
Attention linproj: 0.0007545948028564453
Post-attention Dropout: 0.0003879070281982422
Post-attention residual: 0.00013065338134765625
LN2: 0.00012993812561035156
MLP_h_4h: 0.003030538558959961
MLP_4h_h: 0.00260162353515625
Post-MLP residual: 0.0003998279571533203
Attention layer time: 0.014675140380859375
LN1: 0.0001289844512939453
QKV Transform: 0.002080678939819336
Attention Score: 0.0011289119720458984
Attention Softmax: 0.0026481151580810547
Attention Dropout: 0.00006389617919921875
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.00075531005859375
Post-attention Dropout: 0.00038909912109375
Post-attention residual: 0.00013113021850585938
LN2: 0.0001308917999267578
MLP_h_4h: 0.0030341148376464844
MLP_4h_h: 0.002603292465209961
Post-MLP residual: 0.00039076805114746094
Attention layer time: 0.014652490615844727
LN1: 0.0001308917999267578
QKV Transform: 0.002085447311401367
Attention Score: 0.0011360645294189453
Attention Softmax: 0.0026526451110839844
Attention Dropout: 0.00005888938903808594
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007517337799072266
Post-attention Dropout: 0.0003924369812011719
Post-attention residual: 0.00013065338134765625
LN2: 0.00013184547424316406
MLP_h_4h: 0.0030298233032226562
MLP_4h_h: 0.0026051998138427734
Post-MLP residual: 0.0003871917724609375
Attention layer time: 0.014653205871582031
LN1: 0.0001285076141357422
QKV Transform: 0.002077817916870117
Attention Score: 0.001146554946899414
Attention Softmax: 0.002653360366821289
Attention Dropout: 0.00007271766662597656
Attention Over Value: 0.0007658004760742188
Attention linproj: 0.0007543563842773438
Post-attention Dropout: 0.00038695335388183594
Post-attention residual: 0.00012969970703125
LN2: 0.0001468658447265625
MLP_h_4h: 0.003037691116333008
MLP_4h_h: 0.0026085376739501953
Post-MLP residual: 0.0003864765167236328
Attention layer time: 0.014751434326171875
LN1: 0.00012803077697753906
QKV Transform: 0.0020825862884521484
Attention Score: 0.0011339187622070312
Attention Softmax: 0.002650022506713867
Attention Dropout: 0.00006246566772460938
Attention Over Value: 0.0007634162902832031
Attention linproj: 0.0007488727569580078
Post-attention Dropout: 0.0003902912139892578
Post-attention residual: 0.00012993812561035156
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030324459075927734
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.00038909912109375
Attention layer time: 0.014672279357910156
LN1: 0.00013017654418945312
QKV Transform: 0.002086162567138672
Attention Score: 0.0011315345764160156
Attention Softmax: 0.002655506134033203
Attention Dropout: 0.00006699562072753906
Attention Over Value: 0.0007662773132324219
Attention linproj: 0.0007481575012207031
Post-attention Dropout: 0.00038933753967285156
Post-attention residual: 0.00013184547424316406
LN2: 0.00012969970703125
MLP_h_4h: 0.0030295848846435547
MLP_4h_h: 0.0026056766510009766
Post-MLP residual: 0.00038695335388183594
Attention layer time: 0.014679431915283203
LN1: 0.00012874603271484375
QKV Transform: 0.0020809173583984375
Attention Score: 0.001127481460571289
Attention Softmax: 0.00264739990234375
Attention Dropout: 0.00006008148193359375
Attention Over Value: 0.0007605552673339844
Attention linproj: 0.0007479190826416016
Post-attention Dropout: 0.00038623809814453125
Post-attention residual: 0.00013065338134765625
LN2: 0.0001506805419921875
MLP_h_4h: 0.003036975860595703
MLP_4h_h: 0.002603292465209961
Post-MLP residual: 0.0003859996795654297
Attention layer time: 0.014645814895629883
LN1: 0.00012874603271484375
QKV Transform: 0.0020782947540283203
Attention Score: 0.0011343955993652344
Attention Softmax: 0.0026514530181884766
Attention Dropout: 0.00006198883056640625
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007479190826416016
Post-attention Dropout: 0.0003895759582519531
Post-attention residual: 0.00012969970703125
LN2: 0.0001304149627685547
MLP_h_4h: 0.003032684326171875
MLP_4h_h: 0.002602100372314453
Post-MLP residual: 0.0003883838653564453
Attention layer time: 0.014655113220214844
LN1: 0.00013065338134765625
QKV Transform: 0.0020749568939208984
Attention Score: 0.001130819320678711
Attention Softmax: 0.002652406692504883
Attention Dropout: 0.00005912780761718750
Attention Over Value: 0.0007603168487548828
Attention linproj: 0.0007569789886474609
Post-attention Dropout: 0.00039267539978027344
Post-attention residual: 0.00013136863708496094
LN2: 0.00012993812561035156
MLP_h_4h: 0.0030508041381835938
MLP_4h_h: 0.002602815628051758
Post-MLP residual: 0.00038743019104003906
Attention layer time: 0.014658451080322266
LN1: 0.00012922286987304688
QKV Transform: 0.002078533172607422
Attention Score: 0.0011324882507324219
Attention Softmax: 0.002647876739501953
Attention Dropout: 0.00005960464477539062
Attention Over Value: 0.0007617473602294922
Attention linproj: 0.0007476806640625
Post-attention Dropout: 0.00038909912109375
Post-attention residual: 0.0001308917999267578
LN2: 0.0001327991485595703
MLP_h_4h: 0.0030355453491210938
MLP_4h_h: 0.0026047229766845703
Post-MLP residual: 0.0003898143768310547
Attention layer time: 0.014641761779785156
LN1: 0.00012922286987304688
QKV Transform: 0.002079486846923828
Attention Score: 0.0011415481567382812
Attention Softmax: 0.0026509761810302734
Attention Dropout: 0.00006270408630371094
Attention Over Value: 0.0007607936859130859
Attention linproj: 0.0007493495941162109
Post-attention Dropout: 0.00039005279541015625
Post-attention residual: 0.00013065338134765625
LN2: 0.00012969970703125
MLP_h_4h: 0.0030298233032226562
MLP_4h_h: 0.002617359161376953
Post-MLP residual: 0.0003886222839355469
Attention layer time: 0.014666557312011719
LN1: 0.00013065338134765625
QKV Transform: 0.0020923614501953125
Attention Score: 0.0011286735534667969
Attention Softmax: 0.002650737762451172
Attention Dropout: 0.00005769729614257812
Attention Over Value: 0.0007600784301757812
Attention linproj: 0.0007543563842773438
Post-attention Dropout: 0.0003917217254638672
Post-attention residual: 0.0001304149627685547
LN2: 0.0001304149627685547
MLP_h_4h: 0.0030336380004882812
MLP_4h_h: 0.0026051998138427734
Post-MLP residual: 0.00038504600524902344
Attention layer time: 0.014671802520751953
Transformer duration (in seconds): 0.0147
Transformer throughput (in TFLOP/s): 140.204
Transformer - MLP - Attention (in seconds): 0.0011
========================================================================================================================
