[2023-06-06 20:36:02,500] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-06 20:36:03,193] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.132.141, master_port=6000
[2023-06-06 20:36:03,193] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-06 20:36:06,472] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 248.411
Elapsed time for attention_key_query_prob (512x2048x76x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x76x2048): 52.570
Elapsed time for attention_prob_times_values (512x2048x2048x76): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x76): 79.261
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0064
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 241.964
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0252
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 246.334
Elapsed time for mlp_fused_gelu (2048x4x38912): 0.0011
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0246
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 252.277
Elapsed time for transformer_add_bias_dropout (2048x4x9728): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9728): 0.0004

Attention duration (in seconds): 0.1054
Attention throughput (in TFLOP/s): 65.013
MLP duration (in seconds): 0.0508
MLP throughput (in TFLOP/s): 243.978
Transformer duration (in seconds): 0.1585
Transformer throughput (in TFLOP/s): 121.506
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0495
Attention throughput (in TFLOP/s): 138.553
MLP duration (in seconds): 0.0513
MLP throughput (in TFLOP/s): 241.809
Transformer duration (in seconds): 0.1036
Transformer throughput (in TFLOP/s): 185.865
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 128, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 245.560
Elapsed time for attention_key_query_prob (512x2048x77x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x77x2048): 30.070
Elapsed time for attention_prob_times_values (512x2048x2048x77): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x77): 48.382
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 243.210
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 246.924
Elapsed time for mlp_fused_gelu (2048x4x39424): 0.0011
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0264
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 241.529
Elapsed time for transformer_add_bias_dropout (2048x4x9856): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9856): 0.0004

Attention duration (in seconds): 0.1138
Attention throughput (in TFLOP/s): 61.746
MLP duration (in seconds): 0.0532
MLP throughput (in TFLOP/s): 239.180
Transformer duration (in seconds): 0.1693
Transformer throughput (in TFLOP/s): 116.691
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0624
Attention throughput (in TFLOP/s): 112.549
MLP duration (in seconds): 0.0539
MLP throughput (in TFLOP/s): 236.397
Transformer duration (in seconds): 0.1205
Transformer throughput (in TFLOP/s): 164.018
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0199
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 246.361
Elapsed time for attention_key_query_prob (512x2048x78x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x78x2048): 54.370
Elapsed time for attention_prob_times_values (512x2048x2048x78): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x78): 81.210
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 241.934
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 246.740
Elapsed time for mlp_fused_gelu (2048x4x39936): 0.0011
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0258
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 253.007
Elapsed time for transformer_add_bias_dropout (2048x4x9984): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9984): 0.0004

Attention duration (in seconds): 0.1069
Attention throughput (in TFLOP/s): 67.383
MLP duration (in seconds): 0.0534
MLP throughput (in TFLOP/s): 244.651
Transformer duration (in seconds): 0.1626
Transformer throughput (in TFLOP/s): 124.667
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0507
Attention throughput (in TFLOP/s): 141.948
MLP duration (in seconds): 0.0539
MLP throughput (in TFLOP/s): 242.409
Transformer duration (in seconds): 0.1077
Transformer throughput (in TFLOP/s): 188.252
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 246.224
Elapsed time for attention_key_query_prob (512x2048x79x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x79x2048): 30.181
Elapsed time for attention_prob_times_values (512x2048x2048x79): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x79): 49.442
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0069
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 244.083
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0271
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 246.994
Elapsed time for mlp_fused_gelu (2048x4x40448): 0.0011
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 241.690
Elapsed time for transformer_add_bias_dropout (2048x4x10112): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10112): 0.0004

Attention duration (in seconds): 0.1154
Attention throughput (in TFLOP/s): 63.976
MLP duration (in seconds): 0.0560
MLP throughput (in TFLOP/s): 239.422
Transformer duration (in seconds): 0.1736
Transformer throughput (in TFLOP/s): 119.687
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0641
Attention throughput (in TFLOP/s): 115.167
MLP duration (in seconds): 0.0566
MLP throughput (in TFLOP/s): 236.668
Transformer duration (in seconds): 0.1248
Transformer throughput (in TFLOP/s): 166.528
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0208
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 247.305
Elapsed time for attention_key_query_prob (512x2048x80x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x80x2048): 77.454
Elapsed time for attention_prob_times_values (512x2048x2048x80): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x80): 94.338
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 243.641
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0278
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 247.506
Elapsed time for mlp_fused_gelu (2048x4x40960): 0.0011
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0273
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 251.987
Elapsed time for transformer_add_bias_dropout (2048x4x10240): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10240): 0.0004

Attention duration (in seconds): 0.1059
Attention throughput (in TFLOP/s): 71.355
MLP duration (in seconds): 0.0562
MLP throughput (in TFLOP/s): 244.656
Transformer duration (in seconds): 0.1645
Transformer throughput (in TFLOP/s): 129.524
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0499
Attention throughput (in TFLOP/s): 151.584
MLP duration (in seconds): 0.0565
MLP throughput (in TFLOP/s): 243.350
Transformer duration (in seconds): 0.1092
Transformer throughput (in TFLOP/s): 195.081
Transformer - MLP - Attention (in seconds): 0.0029
========================================================================================================================
num_attention_heads: 128, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 246.194
Elapsed time for attention_key_query_prob (512x2048x81x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x81x2048): 30.660
Elapsed time for attention_prob_times_values (512x2048x2048x81): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x81): 50.497
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0611
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 243.385
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0285
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 247.006
Elapsed time for mlp_fused_gelu (2048x4x41472): 0.0011
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0290
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 242.542
Elapsed time for transformer_add_bias_dropout (2048x4x10368): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10368): 0.0004

Attention duration (in seconds): 0.1171
Attention throughput (in TFLOP/s): 66.098
MLP duration (in seconds): 0.0587
MLP throughput (in TFLOP/s): 239.967
Transformer duration (in seconds): 0.1782
Transformer throughput (in TFLOP/s): 122.498
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0653
Attention throughput (in TFLOP/s): 118.500
MLP duration (in seconds): 0.0594
MLP throughput (in TFLOP/s): 237.197
Transformer duration (in seconds): 0.1288
Transformer throughput (in TFLOP/s): 169.440
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 246.550
Elapsed time for attention_key_query_prob (512x2048x82x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x82x2048): 56.861
Elapsed time for attention_prob_times_values (512x2048x2048x82): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x82): 84.927
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 242.961
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0293
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 246.158
Elapsed time for mlp_fused_gelu (2048x4x41984): 0.0012
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 242.129
Elapsed time for transformer_add_bias_dropout (2048x4x10496): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10496): 0.0004

Attention duration (in seconds): 0.1097
Attention throughput (in TFLOP/s): 72.239
MLP duration (in seconds): 0.0603
MLP throughput (in TFLOP/s): 239.422
Transformer duration (in seconds): 0.1724
Transformer throughput (in TFLOP/s): 129.723
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0536
Attention throughput (in TFLOP/s): 147.868
MLP duration (in seconds): 0.0610
MLP throughput (in TFLOP/s): 236.824
Transformer duration (in seconds): 0.1179
Transformer throughput (in TFLOP/s): 189.627
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 246.424
Elapsed time for attention_key_query_prob (512x2048x83x2048): 0.0114
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x83x2048): 31.372
Elapsed time for attention_prob_times_values (512x2048x2048x83): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x83): 51.936
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 242.008
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0300
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 246.648
Elapsed time for mlp_fused_gelu (2048x4x42496): 0.0012
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0306
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 242.014
Elapsed time for transformer_add_bias_dropout (2048x4x10624): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10624): 0.0004

Attention duration (in seconds): 0.1184
Attention throughput (in TFLOP/s): 68.523
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 239.654
Transformer duration (in seconds): 0.1825
Transformer throughput (in TFLOP/s): 125.470
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0669
Attention throughput (in TFLOP/s): 121.268
MLP duration (in seconds): 0.0623
MLP throughput (in TFLOP/s): 237.355
Transformer duration (in seconds): 0.1336
Transformer throughput (in TFLOP/s): 171.377
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 247.030
Elapsed time for attention_key_query_prob (512x2048x84x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x84x2048): 57.994
Elapsed time for attention_prob_times_values (512x2048x2048x84): 0.0041
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x84): 86.964
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 244.751
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0307
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 247.101
Elapsed time for mlp_fused_gelu (2048x4x43008): 0.0012
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0299
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 253.416
Elapsed time for transformer_add_bias_dropout (2048x4x10752): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10752): 0.0004

Attention duration (in seconds): 0.1111
Attention throughput (in TFLOP/s): 74.717
MLP duration (in seconds): 0.0617
MLP throughput (in TFLOP/s): 245.393
Transformer duration (in seconds): 0.1753
Transformer throughput (in TFLOP/s): 133.800
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0548
Attention throughput (in TFLOP/s): 151.338
MLP duration (in seconds): 0.0624
MLP throughput (in TFLOP/s): 242.991
Transformer duration (in seconds): 0.1233
Transformer throughput (in TFLOP/s): 190.148
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 246.834
Elapsed time for attention_key_query_prob (512x2048x85x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x85x2048): 31.779
Elapsed time for attention_prob_times_values (512x2048x2048x85): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x85): 53.139
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 243.703
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 246.117
Elapsed time for mlp_fused_gelu (2048x4x43520): 0.0012
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0321
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 241.639
Elapsed time for transformer_add_bias_dropout (2048x4x10880): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10880): 0.0004

Attention duration (in seconds): 0.1199
Attention throughput (in TFLOP/s): 70.813
MLP duration (in seconds): 0.0648
MLP throughput (in TFLOP/s): 239.331
Transformer duration (in seconds): 0.1872
Transformer throughput (in TFLOP/s): 128.236
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0688
Attention throughput (in TFLOP/s): 123.376
MLP duration (in seconds): 0.0655
MLP throughput (in TFLOP/s): 237.025
Transformer duration (in seconds): 0.1386
Transformer throughput (in TFLOP/s): 173.218
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 246.521
Elapsed time for attention_key_query_prob (512x2048x86x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x86x2048): 59.594
Elapsed time for attention_prob_times_values (512x2048x2048x86): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x86): 88.688
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 243.909
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0322
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 246.349
Elapsed time for mlp_fused_gelu (2048x4x44032): 0.0012
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0315
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 252.149
Elapsed time for transformer_add_bias_dropout (2048x4x11008): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11008): 0.0004

Attention duration (in seconds): 0.1126
Attention throughput (in TFLOP/s): 77.061
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 244.530
Transformer duration (in seconds): 0.1801
Transformer throughput (in TFLOP/s): 136.378
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0563
Attention throughput (in TFLOP/s): 154.124
MLP duration (in seconds): 0.0654
MLP throughput (in TFLOP/s): 242.867
Transformer duration (in seconds): 0.1252
Transformer throughput (in TFLOP/s): 196.254
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 246.886
Elapsed time for attention_key_query_prob (512x2048x87x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x87x2048): 31.996
Elapsed time for attention_prob_times_values (512x2048x2048x87): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x87): 54.028
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 244.013
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 246.144
Elapsed time for mlp_fused_gelu (2048x4x44544): 0.0012
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 241.113
Elapsed time for transformer_add_bias_dropout (2048x4x11136): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11136): 0.0005

Attention duration (in seconds): 0.1216
Attention throughput (in TFLOP/s): 72.995
MLP duration (in seconds): 0.0680
MLP throughput (in TFLOP/s): 239.185
Transformer duration (in seconds): 0.1921
Transformer throughput (in TFLOP/s): 130.813
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 126.781
MLP duration (in seconds): 0.0684
MLP throughput (in TFLOP/s): 237.760
Transformer duration (in seconds): 0.1427
Transformer throughput (in TFLOP/s): 176.132
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 247.310
Elapsed time for attention_key_query_prob (512x2048x88x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x88x2048): 76.319
Elapsed time for attention_prob_times_values (512x2048x2048x88): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x88): 102.220
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 244.611
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0337
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 246.752
Elapsed time for mlp_fused_gelu (2048x4x45056): 0.0012
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0331
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 251.572
Elapsed time for transformer_add_bias_dropout (2048x4x11264): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11264): 0.0005

Attention duration (in seconds): 0.1123
Attention throughput (in TFLOP/s): 80.764
MLP duration (in seconds): 0.0680
MLP throughput (in TFLOP/s): 244.571
Transformer duration (in seconds): 0.1829
Transformer throughput (in TFLOP/s): 140.519
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0561
Attention throughput (in TFLOP/s): 161.638
MLP duration (in seconds): 0.0686
MLP throughput (in TFLOP/s): 242.465
Transformer duration (in seconds): 0.1283
Transformer throughput (in TFLOP/s): 200.302
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 128, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 246.036
Elapsed time for attention_key_query_prob (512x2048x89x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x89x2048): 32.579
Elapsed time for attention_prob_times_values (512x2048x2048x89): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x89): 55.128
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 244.341
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0346
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 245.699
Elapsed time for mlp_fused_gelu (2048x4x45568): 0.0013
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0339
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 250.585
Elapsed time for transformer_add_bias_dropout (2048x4x11392): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11392): 0.0005

Attention duration (in seconds): 0.1233
Attention throughput (in TFLOP/s): 75.206
MLP duration (in seconds): 0.0698
MLP throughput (in TFLOP/s): 243.641
Transformer duration (in seconds): 0.1957
Transformer throughput (in TFLOP/s): 134.300
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0716
Attention throughput (in TFLOP/s): 129.452
MLP duration (in seconds): 0.0703
MLP throughput (in TFLOP/s): 242.015
Transformer duration (in seconds): 0.1462
Transformer throughput (in TFLOP/s): 179.768
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 246.557
Elapsed time for attention_key_query_prob (512x2048x90x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x90x2048): 62.106
Elapsed time for attention_prob_times_values (512x2048x2048x90): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x90): 92.474
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 243.740
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0353
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 246.343
Elapsed time for mlp_fused_gelu (2048x4x46080): 0.0013
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0359
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 242.340
Elapsed time for transformer_add_bias_dropout (2048x4x11520): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11520): 0.0005

Attention duration (in seconds): 0.1157
Attention throughput (in TFLOP/s): 81.824
MLP duration (in seconds): 0.0725
MLP throughput (in TFLOP/s): 240.032
Transformer duration (in seconds): 0.1908
Transformer throughput (in TFLOP/s): 140.771
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0596
Attention throughput (in TFLOP/s): 158.770
MLP duration (in seconds): 0.0733
MLP throughput (in TFLOP/s): 237.441
Transformer duration (in seconds): 0.1372
Transformer throughput (in TFLOP/s): 195.794
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 246.711
Elapsed time for attention_key_query_prob (512x2048x91x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x91x2048): 33.218
Elapsed time for attention_prob_times_values (512x2048x2048x91): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x91): 56.666
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 245.605
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 246.113
Elapsed time for mlp_fused_gelu (2048x4x46592): 0.0013
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0367
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 242.193
Elapsed time for transformer_add_bias_dropout (2048x4x11648): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11648): 0.0005

Attention duration (in seconds): 0.1247
Attention throughput (in TFLOP/s): 77.570
MLP duration (in seconds): 0.0741
MLP throughput (in TFLOP/s): 239.893
Transformer duration (in seconds): 0.2015
Transformer throughput (in TFLOP/s): 136.259
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0730
Attention throughput (in TFLOP/s): 132.477
MLP duration (in seconds): 0.0749
MLP throughput (in TFLOP/s): 237.417
Transformer duration (in seconds): 0.1523
Transformer throughput (in TFLOP/s): 180.325
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 246.499
Elapsed time for attention_key_query_prob (512x2048x92x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x92x2048): 63.171
Elapsed time for attention_prob_times_values (512x2048x2048x92): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x92): 94.406
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0093
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 245.589
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 246.423
Elapsed time for mlp_fused_gelu (2048x4x47104): 0.0013
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0360
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 252.610
Elapsed time for transformer_add_bias_dropout (2048x4x11776): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11776): 0.0005

Attention duration (in seconds): 0.1173
Attention throughput (in TFLOP/s): 84.225
MLP duration (in seconds): 0.0742
MLP throughput (in TFLOP/s): 245.096
Transformer duration (in seconds): 0.1941
Transformer throughput (in TFLOP/s): 144.501
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 162.159
MLP duration (in seconds): 0.0747
MLP throughput (in TFLOP/s): 243.225
Transformer duration (in seconds): 0.1395
Transformer throughput (in TFLOP/s): 201.152
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0283
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 246.542
Elapsed time for attention_key_query_prob (512x2048x93x2048): 0.0118
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x93x2048): 33.896
Elapsed time for attention_prob_times_values (512x2048x2048x93): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x93): 57.933
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0095
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 243.820
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0379
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 245.183
Elapsed time for mlp_fused_gelu (2048x4x47616): 0.0013
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 241.882
Elapsed time for transformer_add_bias_dropout (2048x4x11904): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11904): 0.0005

Attention duration (in seconds): 0.1264
Attention throughput (in TFLOP/s): 79.789
MLP duration (in seconds): 0.0776
MLP throughput (in TFLOP/s): 239.396
Transformer duration (in seconds): 0.2067
Transformer throughput (in TFLOP/s): 138.643
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0748
Attention throughput (in TFLOP/s): 134.840
MLP duration (in seconds): 0.0783
MLP throughput (in TFLOP/s): 237.208
Transformer duration (in seconds): 0.1575
Transformer throughput (in TFLOP/s): 181.983
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 255.603
Elapsed time for attention_key_query_prob (512x2048x94x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x94x2048): 64.679
Elapsed time for attention_prob_times_values (512x2048x2048x94): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x94): 96.290
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 257.992
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0374
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 253.723
Elapsed time for mlp_fused_gelu (2048x4x48128): 0.0013
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0378
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 250.746
Elapsed time for transformer_add_bias_dropout (2048x4x12032): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12032): 0.0005

Attention duration (in seconds): 0.1174
Attention throughput (in TFLOP/s): 87.679
MLP duration (in seconds): 0.0766
MLP throughput (in TFLOP/s): 247.845
Transformer duration (in seconds): 0.1967
Transformer throughput (in TFLOP/s): 148.781
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0620
Attention throughput (in TFLOP/s): 166.090
MLP duration (in seconds): 0.0769
MLP throughput (in TFLOP/s): 246.782
Transformer duration (in seconds): 0.1423
Transformer throughput (in TFLOP/s): 205.717
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 128, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 255.886
Elapsed time for attention_key_query_prob (512x2048x95x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x95x2048): 35.350
Elapsed time for attention_prob_times_values (512x2048x2048x95): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x95): 58.716
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 248.560
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0379
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 255.659
Elapsed time for mlp_fused_gelu (2048x4x48640): 0.0013
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0400
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 242.526
Elapsed time for transformer_add_bias_dropout (2048x4x12160): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12160): 0.0005

Attention duration (in seconds): 0.1266
Attention throughput (in TFLOP/s): 83.000
MLP duration (in seconds): 0.0792
MLP throughput (in TFLOP/s): 244.694
Transformer duration (in seconds): 0.2086
Transformer throughput (in TFLOP/s): 143.284
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0763
Attention throughput (in TFLOP/s): 137.783
MLP duration (in seconds): 0.0802
MLP throughput (in TFLOP/s): 241.513
Transformer duration (in seconds): 0.1610
Transformer throughput (in TFLOP/s): 185.580
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 255.908
Elapsed time for attention_key_query_prob (512x2048x96x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x96x2048): 93.655
Elapsed time for attention_prob_times_values (512x2048x2048x96): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x96): 111.438
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 256.800
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0389
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 254.585
Elapsed time for mlp_fused_gelu (2048x4x49152): 0.0014
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0394
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 251.047
Elapsed time for transformer_add_bias_dropout (2048x4x12288): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12288): 0.0005

Attention duration (in seconds): 0.1167
Attention throughput (in TFLOP/s): 91.868
MLP duration (in seconds): 0.0796
MLP throughput (in TFLOP/s): 248.489
Transformer duration (in seconds): 0.1992
Transformer throughput (in TFLOP/s): 153.207
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0614
Attention throughput (in TFLOP/s): 174.551
MLP duration (in seconds): 0.0803
MLP throughput (in TFLOP/s): 246.396
Transformer duration (in seconds): 0.1453
Transformer throughput (in TFLOP/s): 210.032
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 128, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 255.066
Elapsed time for attention_key_query_prob (512x2048x97x2048): 0.0130
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x97x2048): 32.068
Elapsed time for attention_prob_times_values (512x2048x2048x97): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x97): 59.703
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 258.265
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0398
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 254.139
Elapsed time for mlp_fused_gelu (2048x4x49664): 0.0014
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0417
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 242.222
Elapsed time for transformer_add_bias_dropout (2048x4x12416): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12416): 0.0005

Attention duration (in seconds): 0.1294
Attention throughput (in TFLOP/s): 84.508
MLP duration (in seconds): 0.0828
MLP throughput (in TFLOP/s): 243.930
Transformer duration (in seconds): 0.2151
Transformer throughput (in TFLOP/s): 144.781
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0798
Attention throughput (in TFLOP/s): 137.088
MLP duration (in seconds): 0.0835
MLP throughput (in TFLOP/s): 241.873
Transformer duration (in seconds): 0.1679
Transformer throughput (in TFLOP/s): 185.434
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0303
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 255.575
Elapsed time for attention_key_query_prob (512x2048x98x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x98x2048): 60.427
Elapsed time for attention_prob_times_values (512x2048x2048x98): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x98): 99.965
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 248.533
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0406
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 253.870
Elapsed time for mlp_fused_gelu (2048x4x50176): 0.0014
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 250.446
Elapsed time for transformer_add_bias_dropout (2048x4x12544): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12544): 0.0005

Attention duration (in seconds): 0.1218
Attention throughput (in TFLOP/s): 91.593
MLP duration (in seconds): 0.0832
MLP throughput (in TFLOP/s): 247.946
Transformer duration (in seconds): 0.2078
Transformer throughput (in TFLOP/s): 152.909
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0666
Attention throughput (in TFLOP/s): 167.451
MLP duration (in seconds): 0.0835
MLP throughput (in TFLOP/s): 246.862
Transformer duration (in seconds): 0.1541
Transformer throughput (in TFLOP/s): 206.235
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 256.046
Elapsed time for attention_key_query_prob (512x2048x99x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x99x2048): 31.449
Elapsed time for attention_prob_times_values (512x2048x2048x99): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x99): 61.428
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 255.818
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0414
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 254.191
Elapsed time for mlp_fused_gelu (2048x4x50688): 0.0014
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 242.222
Elapsed time for transformer_add_bias_dropout (2048x4x12672): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12672): 0.0005

Attention duration (in seconds): 0.1315
Attention throughput (in TFLOP/s): 86.481
MLP duration (in seconds): 0.0862
MLP throughput (in TFLOP/s): 244.033
Transformer duration (in seconds): 0.2207
Transformer throughput (in TFLOP/s): 146.923
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0820
Attention throughput (in TFLOP/s): 138.788
MLP duration (in seconds): 0.0870
MLP throughput (in TFLOP/s): 241.818
Transformer duration (in seconds): 0.1739
Transformer throughput (in TFLOP/s): 186.468
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 256.217
Elapsed time for attention_key_query_prob (512x2048x100x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x100x2048): 61.442
Elapsed time for attention_prob_times_values (512x2048x2048x100): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x100): 102.006
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 258.286
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0421
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 255.026
Elapsed time for mlp_fused_gelu (2048x4x51200): 0.0014
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0427
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 251.564
Elapsed time for transformer_add_bias_dropout (2048x4x12800): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12800): 0.0005

Attention duration (in seconds): 0.1230
Attention throughput (in TFLOP/s): 94.286
MLP duration (in seconds): 0.0862
MLP throughput (in TFLOP/s): 249.126
Transformer duration (in seconds): 0.2121
Transformer throughput (in TFLOP/s): 155.913
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0679
Attention throughput (in TFLOP/s): 170.765
MLP duration (in seconds): 0.0869
MLP throughput (in TFLOP/s): 247.020
Transformer duration (in seconds): 0.1622
Transformer throughput (in TFLOP/s): 203.861
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 255.665
Elapsed time for attention_key_query_prob (512x2048x101x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x101x2048): 31.419
Elapsed time for attention_prob_times_values (512x2048x2048x101): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x101): 62.601
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 259.655
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0431
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 254.193
Elapsed time for mlp_fused_gelu (2048x4x51712): 0.0014
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0441
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 248.520
Elapsed time for transformer_add_bias_dropout (2048x4x12928): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12928): 0.0005

Attention duration (in seconds): 0.1334
Attention throughput (in TFLOP/s): 88.639
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 247.276
Transformer duration (in seconds): 0.2249
Transformer throughput (in TFLOP/s): 149.962
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0843
Attention throughput (in TFLOP/s): 140.179
MLP duration (in seconds): 0.0896
MLP throughput (in TFLOP/s): 244.459
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 189.021
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0328
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 255.618
Elapsed time for attention_key_query_prob (512x2048x102x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x102x2048): 62.525
Elapsed time for attention_prob_times_values (512x2048x2048x102): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x102): 104.036
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 257.743
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0440
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 253.911
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0014
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0446
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 250.648
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.1248
Attention throughput (in TFLOP/s): 96.543
MLP duration (in seconds): 0.0900
MLP throughput (in TFLOP/s): 248.228
Transformer duration (in seconds): 0.2178
Transformer throughput (in TFLOP/s): 157.906
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0697
Attention throughput (in TFLOP/s): 172.735
MLP duration (in seconds): 0.0906
MLP throughput (in TFLOP/s): 246.500
Transformer duration (in seconds): 0.1643
Transformer throughput (in TFLOP/s): 209.356
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 254.764
Elapsed time for attention_key_query_prob (512x2048x103x2048): 0.0142
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x103x2048): 31.051
Elapsed time for attention_prob_times_values (512x2048x2048x103): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x103): 62.866
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 258.652
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0448
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 254.426
Elapsed time for mlp_fused_gelu (2048x4x52736): 0.0015
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0457
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 249.008
Elapsed time for transformer_add_bias_dropout (2048x4x13184): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13184): 0.0005

Attention duration (in seconds): 0.1358
Attention throughput (in TFLOP/s): 90.408
MLP duration (in seconds): 0.0920
MLP throughput (in TFLOP/s): 247.701
Transformer duration (in seconds): 0.2308
Transformer throughput (in TFLOP/s): 151.913
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0867
Attention throughput (in TFLOP/s): 141.531
MLP duration (in seconds): 0.0928
MLP throughput (in TFLOP/s): 245.598
Transformer duration (in seconds): 0.1838
Transformer throughput (in TFLOP/s): 190.700
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 128, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 255.509
Elapsed time for attention_key_query_prob (512x2048x104x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x104x2048): 85.059
Elapsed time for attention_prob_times_values (512x2048x2048x104): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x104): 118.918
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 260.222
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0457
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 254.145
Elapsed time for mlp_fused_gelu (2048x4x53248): 0.0015
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0464
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 250.394
Elapsed time for transformer_add_bias_dropout (2048x4x13312): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13312): 0.0005

Attention duration (in seconds): 0.1242
Attention throughput (in TFLOP/s): 100.694
MLP duration (in seconds): 0.0935
MLP throughput (in TFLOP/s): 248.290
Transformer duration (in seconds): 0.2208
Transformer throughput (in TFLOP/s): 161.838
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0700
Attention throughput (in TFLOP/s): 178.667
MLP duration (in seconds): 0.0943
MLP throughput (in TFLOP/s): 246.257
Transformer duration (in seconds): 0.1680
Transformer throughput (in TFLOP/s): 212.652
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 254.700
Elapsed time for attention_key_query_prob (512x2048x105x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x105x2048): 31.459
Elapsed time for attention_prob_times_values (512x2048x2048x105): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x105): 63.936
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 256.889
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 254.628
Elapsed time for mlp_fused_gelu (2048x4x53760): 0.0015
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0477
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 248.196
Elapsed time for transformer_add_bias_dropout (2048x4x13440): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13440): 0.0005

Attention duration (in seconds): 0.1377
Attention throughput (in TFLOP/s): 92.509
MLP duration (in seconds): 0.0957
MLP throughput (in TFLOP/s): 247.468
Transformer duration (in seconds): 0.2365
Transformer throughput (in TFLOP/s): 154.006
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0893
Attention throughput (in TFLOP/s): 142.649
MLP duration (in seconds): 0.0964
MLP throughput (in TFLOP/s): 245.535
Transformer duration (in seconds): 0.1909
Transformer throughput (in TFLOP/s): 190.798
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 254.805
Elapsed time for attention_key_query_prob (512x2048x106x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x106x2048): 64.683
Elapsed time for attention_prob_times_values (512x2048x2048x106): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x106): 107.743
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 258.004
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0475
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 254.102
Elapsed time for mlp_fused_gelu (2048x4x54272): 0.0015
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0483
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 249.900
Elapsed time for transformer_add_bias_dropout (2048x4x13568): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13568): 0.0005

Attention duration (in seconds): 0.1284
Attention throughput (in TFLOP/s): 101.031
MLP duration (in seconds): 0.0973
MLP throughput (in TFLOP/s): 248.104
Transformer duration (in seconds): 0.2288
Transformer throughput (in TFLOP/s): 162.179
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0740
Attention throughput (in TFLOP/s): 175.394
MLP duration (in seconds): 0.0978
MLP throughput (in TFLOP/s): 246.841
Transformer duration (in seconds): 0.1766
Transformer throughput (in TFLOP/s): 210.053
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 254.643
Elapsed time for attention_key_query_prob (512x2048x107x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x107x2048): 32.025
Elapsed time for attention_prob_times_values (512x2048x2048x107): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x107): 66.087
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 258.966
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0484
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 253.918
Elapsed time for mlp_fused_gelu (2048x4x54784): 0.0015
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0495
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 248.307
Elapsed time for transformer_add_bias_dropout (2048x4x13696): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13696): 0.0006

Attention duration (in seconds): 0.1393
Attention throughput (in TFLOP/s): 94.818
MLP duration (in seconds): 0.0994
MLP throughput (in TFLOP/s): 247.260
Transformer duration (in seconds): 0.2419
Transformer throughput (in TFLOP/s): 156.252
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0912
Attention throughput (in TFLOP/s): 144.795
MLP duration (in seconds): 0.1002
MLP throughput (in TFLOP/s): 245.447
Transformer duration (in seconds): 0.1960
Transformer throughput (in TFLOP/s): 192.869
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 255.455
Elapsed time for attention_key_query_prob (512x2048x108x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x108x2048): 65.482
Elapsed time for attention_prob_times_values (512x2048x2048x108): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x108): 109.794
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 261.044
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0496
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 252.724
Elapsed time for mlp_fused_gelu (2048x4x55296): 0.0015
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0502
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 249.347
Elapsed time for transformer_add_bias_dropout (2048x4x13824): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13824): 0.0006

Attention duration (in seconds): 0.1300
Attention throughput (in TFLOP/s): 103.461
MLP duration (in seconds): 0.1013
MLP throughput (in TFLOP/s): 247.237
Transformer duration (in seconds): 0.2345
Transformer throughput (in TFLOP/s): 164.188
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0752
Attention throughput (in TFLOP/s): 178.797
MLP duration (in seconds): 0.1016
MLP throughput (in TFLOP/s): 246.642
Transformer duration (in seconds): 0.1813
Transformer throughput (in TFLOP/s): 212.375
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0375
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 254.818
Elapsed time for attention_key_query_prob (512x2048x109x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x109x2048): 32.596
Elapsed time for attention_prob_times_values (512x2048x2048x109): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x109): 67.228
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 256.817
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0503
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 253.642
Elapsed time for mlp_fused_gelu (2048x4x55808): 0.0015
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0515
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 247.642
Elapsed time for transformer_add_bias_dropout (2048x4x13952): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13952): 0.0006

Attention duration (in seconds): 0.1413
Attention throughput (in TFLOP/s): 96.941
MLP duration (in seconds): 0.1034
MLP throughput (in TFLOP/s): 246.870
Transformer duration (in seconds): 0.2478
Transformer throughput (in TFLOP/s): 158.220
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0930
Attention throughput (in TFLOP/s): 147.287
MLP duration (in seconds): 0.1042
MLP throughput (in TFLOP/s): 244.913
Transformer duration (in seconds): 0.2025
Transformer throughput (in TFLOP/s): 193.651
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 255.171
Elapsed time for attention_key_query_prob (512x2048x110x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x110x2048): 66.654
Elapsed time for attention_prob_times_values (512x2048x2048x110): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x110): 111.423
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 259.299
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0513
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 253.378
Elapsed time for mlp_fused_gelu (2048x4x56320): 0.0016
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0518
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 250.805
Elapsed time for transformer_add_bias_dropout (2048x4x14080): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14080): 0.0006

Attention duration (in seconds): 0.1320
Attention throughput (in TFLOP/s): 105.582
MLP duration (in seconds): 0.1046
MLP throughput (in TFLOP/s): 248.337
Transformer duration (in seconds): 0.2399
Transformer throughput (in TFLOP/s): 166.437
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 180.269
MLP duration (in seconds): 0.1055
MLP throughput (in TFLOP/s): 246.413
Transformer duration (in seconds): 0.1867
Transformer throughput (in TFLOP/s): 213.846
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 128, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0388
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 255.864
Elapsed time for attention_key_query_prob (512x2048x111x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x111x2048): 32.894
Elapsed time for attention_prob_times_values (512x2048x2048x111): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x111): 67.368
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0127
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 260.975
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 253.808
Elapsed time for mlp_fused_gelu (2048x4x56832): 0.0016
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 248.072
Elapsed time for transformer_add_bias_dropout (2048x4x14208): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14208): 0.0006

Attention duration (in seconds): 0.1430
Attention throughput (in TFLOP/s): 99.201
MLP duration (in seconds): 0.1070
MLP throughput (in TFLOP/s): 247.230
Transformer duration (in seconds): 0.2532
Transformer throughput (in TFLOP/s): 160.487
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0950
Attention throughput (in TFLOP/s): 149.337
MLP duration (in seconds): 0.1083
MLP throughput (in TFLOP/s): 244.259
Transformer duration (in seconds): 0.2078
Transformer throughput (in TFLOP/s): 195.627
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 254.848
Elapsed time for attention_key_query_prob (512x2048x112x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x112x2048): 95.633
Elapsed time for attention_prob_times_values (512x2048x2048x112): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x112): 127.509
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 257.701
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0529
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 254.803
Elapsed time for mlp_fused_gelu (2048x4x57344): 0.0016
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0539
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 249.887
Elapsed time for transformer_add_bias_dropout (2048x4x14336): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14336): 0.0006

Attention duration (in seconds): 0.1315
Attention throughput (in TFLOP/s): 109.781
MLP duration (in seconds): 0.1083
MLP throughput (in TFLOP/s): 248.638
Transformer duration (in seconds): 0.2431
Transformer throughput (in TFLOP/s): 170.189
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0776
Attention throughput (in TFLOP/s): 185.899
MLP duration (in seconds): 0.1092
MLP throughput (in TFLOP/s): 246.606
Transformer duration (in seconds): 0.1914
Transformer throughput (in TFLOP/s): 216.121
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0404
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 254.376
Elapsed time for attention_key_query_prob (512x2048x113x2048): 0.0145
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x113x2048): 33.544
Elapsed time for attention_prob_times_values (512x2048x2048x113): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x113): 68.496
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0133
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 258.253
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0539
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 254.347
Elapsed time for mlp_fused_gelu (2048x4x57856): 0.0016
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 248.046
Elapsed time for transformer_add_bias_dropout (2048x4x14464): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14464): 0.0006

Attention duration (in seconds): 0.1452
Attention throughput (in TFLOP/s): 101.106
MLP duration (in seconds): 0.1108
MLP throughput (in TFLOP/s): 247.536
Transformer duration (in seconds): 0.2593
Transformer throughput (in TFLOP/s): 162.378
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0970
Attention throughput (in TFLOP/s): 151.305
MLP duration (in seconds): 0.1122
MLP throughput (in TFLOP/s): 244.314
Transformer duration (in seconds): 0.2136
Transformer throughput (in TFLOP/s): 197.081
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 254.895
Elapsed time for attention_key_query_prob (512x2048x114x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x114x2048): 68.789
Elapsed time for attention_prob_times_values (512x2048x2048x114): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x114): 115.071
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0135
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 258.539
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0550
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 253.853
Elapsed time for mlp_fused_gelu (2048x4x58368): 0.0016
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0560
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 249.202
Elapsed time for transformer_add_bias_dropout (2048x4x14592): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14592): 0.0006

Attention duration (in seconds): 0.1359
Attention throughput (in TFLOP/s): 109.899
MLP duration (in seconds): 0.1126
MLP throughput (in TFLOP/s): 247.909
Transformer duration (in seconds): 0.2518
Transformer throughput (in TFLOP/s): 170.127
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0813
Attention throughput (in TFLOP/s): 183.738
MLP duration (in seconds): 0.1133
MLP throughput (in TFLOP/s): 246.431
Transformer duration (in seconds): 0.1997
Transformer throughput (in TFLOP/s): 214.515
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 254.418
Elapsed time for attention_key_query_prob (512x2048x115x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x115x2048): 34.394
Elapsed time for attention_prob_times_values (512x2048x2048x115): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x115): 70.654
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 256.209
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0557
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 254.819
Elapsed time for mlp_fused_gelu (2048x4x58880): 0.0016
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 248.376
Elapsed time for transformer_add_bias_dropout (2048x4x14720): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14720): 0.0006

Attention duration (in seconds): 0.1470
Attention throughput (in TFLOP/s): 103.309
MLP duration (in seconds): 0.1145
MLP throughput (in TFLOP/s): 247.989
Transformer duration (in seconds): 0.2649
Transformer throughput (in TFLOP/s): 164.544
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0996
Attention throughput (in TFLOP/s): 152.534
MLP duration (in seconds): 0.1163
MLP throughput (in TFLOP/s): 244.240
Transformer duration (in seconds): 0.2206
Transformer throughput (in TFLOP/s): 197.555
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 254.468
Elapsed time for attention_key_query_prob (512x2048x116x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x116x2048): 70.082
Elapsed time for attention_prob_times_values (512x2048x2048x116): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x116): 116.865
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 257.490
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0571
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 252.987
Elapsed time for mlp_fused_gelu (2048x4x59392): 0.0016
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 247.360
Elapsed time for transformer_add_bias_dropout (2048x4x14848): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14848): 0.0006

Attention duration (in seconds): 0.1380
Attention throughput (in TFLOP/s): 111.954
MLP duration (in seconds): 0.1172
MLP throughput (in TFLOP/s): 246.645
Transformer duration (in seconds): 0.2585
Transformer throughput (in TFLOP/s): 171.528
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0831
Attention throughput (in TFLOP/s): 185.960
MLP duration (in seconds): 0.1181
MLP throughput (in TFLOP/s): 244.722
Transformer duration (in seconds): 0.2060
Transformer throughput (in TFLOP/s): 215.200
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0431
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 255.722
Elapsed time for attention_key_query_prob (512x2048x117x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x117x2048): 35.252
Elapsed time for attention_prob_times_values (512x2048x2048x117): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x117): 71.789
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 258.680
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0578
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 254.483
Elapsed time for mlp_fused_gelu (2048x4x59904): 0.0017
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0595
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 246.892
Elapsed time for transformer_add_bias_dropout (2048x4x14976): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14976): 0.0006

Attention duration (in seconds): 0.1485
Attention throughput (in TFLOP/s): 105.725
MLP duration (in seconds): 0.1189
MLP throughput (in TFLOP/s): 247.149
Transformer duration (in seconds): 0.2709
Transformer throughput (in TFLOP/s): 166.487
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.1013
Attention throughput (in TFLOP/s): 154.948
MLP duration (in seconds): 0.1205
MLP throughput (in TFLOP/s): 243.935
Transformer duration (in seconds): 0.2264
Transformer throughput (in TFLOP/s): 199.202
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0439
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 255.281
Elapsed time for attention_key_query_prob (512x2048x118x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x118x2048): 71.334
Elapsed time for attention_prob_times_values (512x2048x2048x118): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x118): 118.498
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0144
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 258.676
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0592
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 252.690
Elapsed time for mlp_fused_gelu (2048x4x60416): 0.0017
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0609
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 245.689
Elapsed time for transformer_add_bias_dropout (2048x4x15104): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15104): 0.0006

Attention duration (in seconds): 0.1397
Attention throughput (in TFLOP/s): 114.266
MLP duration (in seconds): 0.1217
MLP throughput (in TFLOP/s): 245.727
Transformer duration (in seconds): 0.2649
Transformer throughput (in TFLOP/s): 173.176
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0850
Attention throughput (in TFLOP/s): 187.809
MLP duration (in seconds): 0.1221
MLP throughput (in TFLOP/s): 244.823
Transformer duration (in seconds): 0.2119
Transformer throughput (in TFLOP/s): 216.417
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0447
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 255.076
Elapsed time for attention_key_query_prob (512x2048x119x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x119x2048): 34.115
Elapsed time for attention_prob_times_values (512x2048x2048x119): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x119): 71.874
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 256.006
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0598
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 254.112
Elapsed time for mlp_fused_gelu (2048x4x60928): 0.0017
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 249.148
Elapsed time for transformer_add_bias_dropout (2048x4x15232): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15232): 0.0006

Attention duration (in seconds): 0.1516
Attention throughput (in TFLOP/s): 107.026
MLP duration (in seconds): 0.1225
MLP throughput (in TFLOP/s): 248.155
Transformer duration (in seconds): 0.2776
Transformer throughput (in TFLOP/s): 167.977
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.1037
Attention throughput (in TFLOP/s): 156.551
MLP duration (in seconds): 0.1244
MLP throughput (in TFLOP/s): 244.460
Transformer duration (in seconds): 0.2329
Transformer throughput (in TFLOP/s): 200.248
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 255.785
Elapsed time for attention_key_query_prob (512x2048x120x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x120x2048): 91.534
Elapsed time for attention_prob_times_values (512x2048x2048x120): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x120): 134.192
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0151
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 256.456
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0608
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 254.295
Elapsed time for mlp_fused_gelu (2048x4x61440): 0.0017
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0622
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 248.601
Elapsed time for transformer_add_bias_dropout (2048x4x15360): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15360): 0.0006

Attention duration (in seconds): 0.1399
Attention throughput (in TFLOP/s): 117.923
MLP duration (in seconds): 0.1247
MLP throughput (in TFLOP/s): 248.003
Transformer duration (in seconds): 0.2681
Transformer throughput (in TFLOP/s): 176.892
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 191.463
MLP duration (in seconds): 0.1261
MLP throughput (in TFLOP/s): 245.237
Transformer duration (in seconds): 0.2174
Transformer throughput (in TFLOP/s): 218.090
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0463
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 254.843
Elapsed time for attention_key_query_prob (512x2048x121x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x121x2048): 34.863
Elapsed time for attention_prob_times_values (512x2048x2048x121): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x121): 73.004
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 258.478
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0619
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 253.829
Elapsed time for mlp_fused_gelu (2048x4x61952): 0.0017
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0636
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 247.227
Elapsed time for transformer_add_bias_dropout (2048x4x15488): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15488): 0.0006

Attention duration (in seconds): 0.1535
Attention throughput (in TFLOP/s): 109.216
MLP duration (in seconds): 0.1272
MLP throughput (in TFLOP/s): 247.120
Transformer duration (in seconds): 0.2842
Transformer throughput (in TFLOP/s): 169.591
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.1056
Attention throughput (in TFLOP/s): 158.667
MLP duration (in seconds): 0.1285
MLP throughput (in TFLOP/s): 244.718
Transformer duration (in seconds): 0.2387
Transformer throughput (in TFLOP/s): 201.952
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0470
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 255.124
Elapsed time for attention_key_query_prob (512x2048x122x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x122x2048): 73.804
Elapsed time for attention_prob_times_values (512x2048x2048x122): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x122): 122.524
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0613
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 256.692
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0631
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 253.390
Elapsed time for mlp_fused_gelu (2048x4x62464): 0.0017
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0644
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 248.252
Elapsed time for transformer_add_bias_dropout (2048x4x15616): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15616): 0.0006

Attention duration (in seconds): 0.1443
Attention throughput (in TFLOP/s): 118.033
MLP duration (in seconds): 0.1292
MLP throughput (in TFLOP/s): 247.453
Transformer duration (in seconds): 0.2770
Transformer throughput (in TFLOP/s): 176.867
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0900
Attention throughput (in TFLOP/s): 189.162
MLP duration (in seconds): 0.1309
MLP throughput (in TFLOP/s): 244.184
Transformer duration (in seconds): 0.2264
Transformer throughput (in TFLOP/s): 216.403
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0480
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 253.933
Elapsed time for attention_key_query_prob (512x2048x123x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x123x2048): 36.645
Elapsed time for attention_prob_times_values (512x2048x2048x123): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x123): 75.308
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 256.711
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0643
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 252.506
Elapsed time for mlp_fused_gelu (2048x4x62976): 0.0017
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0656
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 247.610
Elapsed time for transformer_add_bias_dropout (2048x4x15744): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15744): 0.0006

Attention duration (in seconds): 0.1552
Attention throughput (in TFLOP/s): 111.468
MLP duration (in seconds): 0.1317
MLP throughput (in TFLOP/s): 246.736
Transformer duration (in seconds): 0.2905
Transformer throughput (in TFLOP/s): 171.410
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.1077
Attention throughput (in TFLOP/s): 160.653
MLP duration (in seconds): 0.1328
MLP throughput (in TFLOP/s): 244.641
Transformer duration (in seconds): 0.2453
Transformer throughput (in TFLOP/s): 203.010
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0486
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 254.779
Elapsed time for attention_key_query_prob (512x2048x124x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x124x2048): 75.205
Elapsed time for attention_prob_times_values (512x2048x2048x124): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x124): 124.606
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0160
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 258.351
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0650
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 253.889
Elapsed time for mlp_fused_gelu (2048x4x63488): 0.0018
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0673
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 245.216
Elapsed time for transformer_add_bias_dropout (2048x4x15872): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15872): 0.0006

Attention duration (in seconds): 0.1459
Attention throughput (in TFLOP/s): 120.457
MLP duration (in seconds): 0.1341
MLP throughput (in TFLOP/s): 246.221
Transformer duration (in seconds): 0.2836
Transformer throughput (in TFLOP/s): 178.385
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0919
Attention throughput (in TFLOP/s): 191.339
MLP duration (in seconds): 0.1356
MLP throughput (in TFLOP/s): 243.464
Transformer duration (in seconds): 0.2321
Transformer throughput (in TFLOP/s): 217.955
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 253.689
Elapsed time for attention_key_query_prob (512x2048x125x2048): 0.0143
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x125x2048): 37.419
Elapsed time for attention_prob_times_values (512x2048x2048x125): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x125): 76.454
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 255.890
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 254.311
Elapsed time for mlp_fused_gelu (2048x4x64000): 0.0018
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0679
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 247.141
Elapsed time for transformer_add_bias_dropout (2048x4x16000): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16000): 0.0006

Attention duration (in seconds): 0.1573
Attention throughput (in TFLOP/s): 113.475
MLP duration (in seconds): 0.1356
MLP throughput (in TFLOP/s): 247.415
Transformer duration (in seconds): 0.2966
Transformer throughput (in TFLOP/s): 173.323
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.1102
Attention throughput (in TFLOP/s): 161.953
MLP duration (in seconds): 0.1374
MLP throughput (in TFLOP/s): 244.274
Transformer duration (in seconds): 0.2530
Transformer throughput (in TFLOP/s): 203.208
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 254.415
Elapsed time for attention_key_query_prob (512x2048x126x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x126x2048): 76.559
Elapsed time for attention_prob_times_values (512x2048x2048x126): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x126): 126.336
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 256.549
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0672
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 253.551
Elapsed time for mlp_fused_gelu (2048x4x64512): 0.0018
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0686
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 248.597
Elapsed time for transformer_add_bias_dropout (2048x4x16128): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16128): 0.0006

Attention duration (in seconds): 0.1482
Attention throughput (in TFLOP/s): 122.340
MLP duration (in seconds): 0.1376
MLP throughput (in TFLOP/s): 247.804
Transformer duration (in seconds): 0.2894
Transformer throughput (in TFLOP/s): 180.428
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0934
Attention throughput (in TFLOP/s): 194.030
MLP duration (in seconds): 0.1390
MLP throughput (in TFLOP/s): 245.290
Transformer duration (in seconds): 0.2373
Transformer throughput (in TFLOP/s): 220.056
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0508
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 255.682
Elapsed time for attention_key_query_prob (512x2048x127x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x127x2048): 37.140
Elapsed time for attention_prob_times_values (512x2048x2048x127): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x127): 76.515
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0167
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 259.174
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0680
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 254.750
Elapsed time for mlp_fused_gelu (2048x4x65024): 0.0018
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0697
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 248.483
Elapsed time for transformer_add_bias_dropout (2048x4x16256): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16256): 0.0007

Attention duration (in seconds): 0.1593
Attention throughput (in TFLOP/s): 115.576
MLP duration (in seconds): 0.1395
MLP throughput (in TFLOP/s): 248.344
Transformer duration (in seconds): 0.3024
Transformer throughput (in TFLOP/s): 175.388
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.1119
Attention throughput (in TFLOP/s): 164.523
MLP duration (in seconds): 0.1413
MLP throughput (in TFLOP/s): 245.197
Transformer duration (in seconds): 0.2586
Transformer throughput (in TFLOP/s): 205.102
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 255.652
Elapsed time for attention_key_query_prob (512x2048x128x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x128x2048): 110.801
Elapsed time for attention_prob_times_values (512x2048x2048x128): 0.0038
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x128): 144.500
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 259.963
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0693
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 254.002
Elapsed time for mlp_fused_gelu (2048x4x65536): 0.0018
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0711
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 247.340
Elapsed time for transformer_add_bias_dropout (2048x4x16384): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16384): 0.0007

Attention duration (in seconds): 0.1473
Attention throughput (in TFLOP/s): 126.930
MLP duration (in seconds): 0.1422
MLP throughput (in TFLOP/s): 247.439
Transformer duration (in seconds): 0.2932
Transformer throughput (in TFLOP/s): 183.764
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0940
Attention throughput (in TFLOP/s): 198.749
MLP duration (in seconds): 0.1437
MLP throughput (in TFLOP/s): 244.817
Transformer duration (in seconds): 0.2433
Transformer throughput (in TFLOP/s): 221.443
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0525
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 255.374
Elapsed time for attention_key_query_prob (512x2048x129x2048): 0.0162
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x129x2048): 34.280
Elapsed time for attention_prob_times_values (512x2048x2048x129): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x129): 55.908
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 256.777
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0704
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 253.699
Elapsed time for mlp_fused_gelu (2048x4x66048): 0.0018
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0723
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 247.171
Elapsed time for transformer_add_bias_dropout (2048x4x16512): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16512): 0.0007

Attention duration (in seconds): 0.1659
Attention throughput (in TFLOP/s): 114.377
MLP duration (in seconds): 0.1445
MLP throughput (in TFLOP/s): 247.241
Transformer duration (in seconds): 0.3142
Transformer throughput (in TFLOP/s): 174.125
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.1196
Attention throughput (in TFLOP/s): 158.713
MLP duration (in seconds): 0.1468
MLP throughput (in TFLOP/s): 243.394
Transformer duration (in seconds): 0.2721
Transformer throughput (in TFLOP/s): 201.072
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 254.179
Elapsed time for attention_key_query_prob (512x2048x130x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x130x2048): 72.184
Elapsed time for attention_prob_times_values (512x2048x2048x130): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x130): 95.035
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 258.059
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.0715
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 253.758
Elapsed time for mlp_fused_gelu (2048x4x66560): 0.0018
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0732
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 247.750
Elapsed time for transformer_add_bias_dropout (2048x4x16640): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16640): 0.0007

Attention duration (in seconds): 0.1547
Attention throughput (in TFLOP/s): 124.513
MLP duration (in seconds): 0.1466
MLP throughput (in TFLOP/s): 247.584
Transformer duration (in seconds): 0.3051
Transformer throughput (in TFLOP/s): 182.102
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.1013
Attention throughput (in TFLOP/s): 190.124
MLP duration (in seconds): 0.1485
MLP throughput (in TFLOP/s): 244.471
Transformer duration (in seconds): 0.2546
Transformer throughput (in TFLOP/s): 218.172
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 128, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0542
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 254.965
Elapsed time for attention_key_query_prob (512x2048x131x2048): 0.0161
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x131x2048): 34.948
Elapsed time for attention_prob_times_values (512x2048x2048x131): 0.0095
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x131): 58.984
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0178
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 258.323
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.0729
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 252.724
Elapsed time for mlp_fused_gelu (2048x4x67072): 0.0018
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0748
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 246.503
Elapsed time for transformer_add_bias_dropout (2048x4x16768): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16768): 0.0007

Attention duration (in seconds): 0.1676
Attention throughput (in TFLOP/s): 116.642
MLP duration (in seconds): 0.1495
MLP throughput (in TFLOP/s): 246.492
Transformer duration (in seconds): 0.3210
Transformer throughput (in TFLOP/s): 175.732
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.1224
Attention throughput (in TFLOP/s): 159.718
MLP duration (in seconds): 0.1511
MLP throughput (in TFLOP/s): 243.868
Transformer duration (in seconds): 0.2784
Transformer throughput (in TFLOP/s): 202.637
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0550
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 255.060
Elapsed time for attention_key_query_prob (512x2048x132x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x132x2048): 72.937
Elapsed time for attention_prob_times_values (512x2048x2048x132): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x132): 94.956
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 255.619
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0740
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 252.749
Elapsed time for mlp_fused_gelu (2048x4x67584): 0.0019
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0757
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 247.227
Elapsed time for transformer_add_bias_dropout (2048x4x16896): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16896): 0.0007

Attention duration (in seconds): 0.1570
Attention throughput (in TFLOP/s): 126.380
MLP duration (in seconds): 0.1516
MLP throughput (in TFLOP/s): 246.891
Transformer duration (in seconds): 0.3124
Transformer throughput (in TFLOP/s): 183.286
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.1032
Attention throughput (in TFLOP/s): 192.334
MLP duration (in seconds): 0.1538
MLP throughput (in TFLOP/s): 243.294
Transformer duration (in seconds): 0.2634
Transformer throughput (in TFLOP/s): 217.382
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0564
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 252.430
Elapsed time for attention_key_query_prob (512x2048x133x2048): 0.0165
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x133x2048): 34.664
Elapsed time for attention_prob_times_values (512x2048x2048x133): 0.0096
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x133): 59.563
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 257.530
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0753
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 252.327
Elapsed time for mlp_fused_gelu (2048x4x68096): 0.0019
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0772
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 246.080
Elapsed time for transformer_add_bias_dropout (2048x4x17024): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17024): 0.0007

Attention duration (in seconds): 0.1709
Attention throughput (in TFLOP/s): 117.828
MLP duration (in seconds): 0.1543
MLP throughput (in TFLOP/s): 246.139
Transformer duration (in seconds): 0.3291
Transformer throughput (in TFLOP/s): 176.609
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1258
Attention throughput (in TFLOP/s): 160.094
MLP duration (in seconds): 0.1565
MLP throughput (in TFLOP/s): 242.775
Transformer duration (in seconds): 0.2868
Transformer throughput (in TFLOP/s): 202.640
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0570
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 253.474
Elapsed time for attention_key_query_prob (512x2048x134x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x134x2048): 73.606
Elapsed time for attention_prob_times_values (512x2048x2048x134): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x134): 93.673
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 258.372
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0764
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 252.335
Elapsed time for mlp_fused_gelu (2048x4x68608): 0.0019
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0784
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 245.857
Elapsed time for transformer_add_bias_dropout (2048x4x17152): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17152): 0.0007

Attention duration (in seconds): 0.1596
Attention throughput (in TFLOP/s): 127.977
MLP duration (in seconds): 0.1567
MLP throughput (in TFLOP/s): 246.052
Transformer duration (in seconds): 0.3203
Transformer throughput (in TFLOP/s): 184.197
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1061
Attention throughput (in TFLOP/s): 192.573
MLP duration (in seconds): 0.1590
MLP throughput (in TFLOP/s): 242.559
Transformer duration (in seconds): 0.2699
Transformer throughput (in TFLOP/s): 218.584
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0578
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 254.081
Elapsed time for attention_key_query_prob (512x2048x135x2048): 0.0173
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x135x2048): 33.545
Elapsed time for attention_prob_times_values (512x2048x2048x135): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x135): 58.535
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 259.668
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0775
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 252.664
Elapsed time for mlp_fused_gelu (2048x4x69120): 0.0019
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0792
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 246.950
Elapsed time for transformer_add_bias_dropout (2048x4x17280): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17280): 0.0007

Attention duration (in seconds): 0.1738
Attention throughput (in TFLOP/s): 119.296
MLP duration (in seconds): 0.1586
MLP throughput (in TFLOP/s): 246.778
Transformer duration (in seconds): 0.3363
Transformer throughput (in TFLOP/s): 178.022
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1310
Attention throughput (in TFLOP/s): 158.274
MLP duration (in seconds): 0.1606
MLP throughput (in TFLOP/s): 243.623
Transformer duration (in seconds): 0.2957
Transformer throughput (in TFLOP/s): 202.455
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 128, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0587
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 253.840
Elapsed time for attention_key_query_prob (512x2048x136x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x136x2048): 99.248
Elapsed time for attention_prob_times_values (512x2048x2048x136): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x136): 127.408
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0193
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 256.868
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.0787
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 252.235
Elapsed time for mlp_fused_gelu (2048x4x69632): 0.0019
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 247.252
Elapsed time for transformer_add_bias_dropout (2048x4x17408): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17408): 0.0007

Attention duration (in seconds): 0.1585
Attention throughput (in TFLOP/s): 132.707
MLP duration (in seconds): 0.1610
MLP throughput (in TFLOP/s): 246.745
Transformer duration (in seconds): 0.3234
Transformer throughput (in TFLOP/s): 187.843
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1064
Attention throughput (in TFLOP/s): 197.568
MLP duration (in seconds): 0.1633
MLP throughput (in TFLOP/s): 243.236
Transformer duration (in seconds): 0.2752
Transformer throughput (in TFLOP/s): 220.713
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0595
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 254.064
Elapsed time for attention_key_query_prob (512x2048x137x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x137x2048): 33.816
Elapsed time for attention_prob_times_values (512x2048x2048x137): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x137): 59.084
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 257.991
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0797
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 253.000
Elapsed time for mlp_fused_gelu (2048x4x70144): 0.0019
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0815
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 247.376
Elapsed time for transformer_add_bias_dropout (2048x4x17536): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17536): 0.0007

Attention duration (in seconds): 0.1763
Attention throughput (in TFLOP/s): 120.957
MLP duration (in seconds): 0.1631
MLP throughput (in TFLOP/s): 247.195
Transformer duration (in seconds): 0.3434
Transformer throughput (in TFLOP/s): 179.492
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1317
Attention throughput (in TFLOP/s): 161.931
MLP duration (in seconds): 0.1657
MLP throughput (in TFLOP/s): 243.297
Transformer duration (in seconds): 0.3022
Transformer throughput (in TFLOP/s): 203.942
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 128, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 254.122
Elapsed time for attention_key_query_prob (512x2048x138x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x138x2048): 75.058
Elapsed time for attention_prob_times_values (512x2048x2048x138): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x138): 94.331
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 258.324
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 252.426
Elapsed time for mlp_fused_gelu (2048x4x70656): 0.0019
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 244.830
Elapsed time for transformer_add_bias_dropout (2048x4x17664): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17664): 0.0007

Attention duration (in seconds): 0.1643
Attention throughput (in TFLOP/s): 131.689
MLP duration (in seconds): 0.1665
MLP throughput (in TFLOP/s): 245.667
Transformer duration (in seconds): 0.3348
Transformer throughput (in TFLOP/s): 186.781
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1114
Attention throughput (in TFLOP/s): 194.157
MLP duration (in seconds): 0.1693
MLP throughput (in TFLOP/s): 241.627
Transformer duration (in seconds): 0.2868
Transformer throughput (in TFLOP/s): 218.048
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0616
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 252.404
Elapsed time for attention_key_query_prob (512x2048x139x2048): 0.0168
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x139x2048): 35.524
Elapsed time for attention_prob_times_values (512x2048x2048x139): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x139): 61.047
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 256.773
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0825
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 251.506
Elapsed time for mlp_fused_gelu (2048x4x71168): 0.0020
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0841
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 246.647
Elapsed time for transformer_add_bias_dropout (2048x4x17792): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17792): 0.0007

Attention duration (in seconds): 0.1784
Attention throughput (in TFLOP/s): 122.976
MLP duration (in seconds): 0.1686
MLP throughput (in TFLOP/s): 246.158
Transformer duration (in seconds): 0.3510
Transformer throughput (in TFLOP/s): 180.710
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1349
Attention throughput (in TFLOP/s): 162.601
MLP duration (in seconds): 0.1714
MLP throughput (in TFLOP/s): 242.103
Transformer duration (in seconds): 0.3117
Transformer throughput (in TFLOP/s): 203.508
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0627
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 251.900
Elapsed time for attention_key_query_prob (512x2048x140x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x140x2048): 76.182
Elapsed time for attention_prob_times_values (512x2048x2048x140): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x140): 95.739
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0204
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 258.047
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0835
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 252.023
Elapsed time for mlp_fused_gelu (2048x4x71680): 0.0020
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0858
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 245.350
Elapsed time for transformer_add_bias_dropout (2048x4x17920): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17920): 0.0007

Attention duration (in seconds): 0.1672
Attention throughput (in TFLOP/s): 133.073
MLP duration (in seconds): 0.1713
MLP throughput (in TFLOP/s): 245.765
Transformer duration (in seconds): 0.3425
Transformer throughput (in TFLOP/s): 187.832
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1135
Attention throughput (in TFLOP/s): 195.949
MLP duration (in seconds): 0.1737
MLP throughput (in TFLOP/s): 242.295
Transformer duration (in seconds): 0.2923
Transformer throughput (in TFLOP/s): 220.098
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0633
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 252.796
Elapsed time for attention_key_query_prob (512x2048x141x2048): 0.0172
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x141x2048): 35.174
Elapsed time for attention_prob_times_values (512x2048x2048x141): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x141): 60.068
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 258.077
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0850
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 251.167
Elapsed time for mlp_fused_gelu (2048x4x72192): 0.0020
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0866
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 246.396
Elapsed time for transformer_add_bias_dropout (2048x4x18048): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18048): 0.0007

Attention duration (in seconds): 0.1813
Attention throughput (in TFLOP/s): 124.431
MLP duration (in seconds): 0.1736
MLP throughput (in TFLOP/s): 245.913
Transformer duration (in seconds): 0.3590
Transformer throughput (in TFLOP/s): 181.755
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1377
Attention throughput (in TFLOP/s): 163.814
MLP duration (in seconds): 0.1767
MLP throughput (in TFLOP/s): 241.660
Transformer duration (in seconds): 0.3188
Transformer throughput (in TFLOP/s): 204.695
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 128, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0650
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 249.960
Elapsed time for attention_key_query_prob (512x2048x142x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x142x2048): 76.886
Elapsed time for attention_prob_times_values (512x2048x2048x142): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x142): 96.064
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 257.383
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.0863
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 250.935
Elapsed time for mlp_fused_gelu (2048x4x72704): 0.0020
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0888
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 243.948
Elapsed time for transformer_add_bias_dropout (2048x4x18176): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18176): 0.0007

Attention duration (in seconds): 0.1703
Attention throughput (in TFLOP/s): 134.324
MLP duration (in seconds): 0.1770
MLP throughput (in TFLOP/s): 244.599
Transformer duration (in seconds): 0.3514
Transformer throughput (in TFLOP/s): 188.294
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1165
Attention throughput (in TFLOP/s): 196.398
MLP duration (in seconds): 0.1793
MLP throughput (in TFLOP/s): 241.458
Transformer duration (in seconds): 0.3009
Transformer throughput (in TFLOP/s): 219.881
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0653
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 252.109
Elapsed time for attention_key_query_prob (512x2048x143x2048): 0.0174
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x143x2048): 35.250
Elapsed time for attention_prob_times_values (512x2048x2048x143): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x143): 59.823
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 256.778
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0871
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 252.053
Elapsed time for mlp_fused_gelu (2048x4x73216): 0.0020
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0893
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 245.920
Elapsed time for transformer_add_bias_dropout (2048x4x18304): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18304): 0.0007

Attention duration (in seconds): 0.1844
Attention throughput (in TFLOP/s): 125.766
MLP duration (in seconds): 0.1784
MLP throughput (in TFLOP/s): 246.141
Transformer duration (in seconds): 0.3669
Transformer throughput (in TFLOP/s): 182.867
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1401
Attention throughput (in TFLOP/s): 165.481
MLP duration (in seconds): 0.1812
MLP throughput (in TFLOP/s): 242.345
Transformer duration (in seconds): 0.3251
Transformer throughput (in TFLOP/s): 206.414
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 128, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0661
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 252.725
Elapsed time for attention_key_query_prob (512x2048x144x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x144x2048): 110.837
Elapsed time for attention_prob_times_values (512x2048x2048x144): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x144): 137.439
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 258.615
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.0884
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 251.736
Elapsed time for mlp_fused_gelu (2048x4x73728): 0.0020
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0909
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 244.861
Elapsed time for transformer_add_bias_dropout (2048x4x18432): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18432): 0.0007

Attention duration (in seconds): 0.1677
Attention throughput (in TFLOP/s): 140.183
MLP duration (in seconds): 0.1814
MLP throughput (in TFLOP/s): 245.474
Transformer duration (in seconds): 0.3533
Transformer throughput (in TFLOP/s): 192.583
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1164
Attention throughput (in TFLOP/s): 201.946
MLP duration (in seconds): 0.1835
MLP throughput (in TFLOP/s): 242.641
Transformer duration (in seconds): 0.3051
Transformer throughput (in TFLOP/s): 222.986
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0668
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 253.531
Elapsed time for attention_key_query_prob (512x2048x145x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x145x2048): 57.216
Elapsed time for attention_prob_times_values (512x2048x2048x145): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x145): 59.637
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 259.736
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.0892
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 253.070
Elapsed time for mlp_fused_gelu (2048x4x74240): 0.0020
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 246.404
Elapsed time for transformer_add_bias_dropout (2048x4x18560): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18560): 0.0007

Attention duration (in seconds): 0.1798
Attention throughput (in TFLOP/s): 132.456
MLP duration (in seconds): 0.1829
MLP throughput (in TFLOP/s): 246.905
Transformer duration (in seconds): 0.3669
Transformer throughput (in TFLOP/s): 187.970
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1303
Attention throughput (in TFLOP/s): 182.779
MLP duration (in seconds): 0.1861
MLP throughput (in TFLOP/s): 242.677
Transformer duration (in seconds): 0.3221
Transformer throughput (in TFLOP/s): 214.112
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0682
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 251.836
Elapsed time for attention_key_query_prob (512x2048x146x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x146x2048): 78.639
Elapsed time for attention_prob_times_values (512x2048x2048x146): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x146): 97.557
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0223
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 256.772
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0911
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 251.366
Elapsed time for mlp_fused_gelu (2048x4x74752): 0.0021
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0934
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 245.082
Elapsed time for transformer_add_bias_dropout (2048x4x18688): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18688): 0.0008

Attention duration (in seconds): 0.1748
Attention throughput (in TFLOP/s): 138.073
MLP duration (in seconds): 0.1865
MLP throughput (in TFLOP/s): 245.447
Transformer duration (in seconds): 0.3656
Transformer throughput (in TFLOP/s): 191.236
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1217
Attention throughput (in TFLOP/s): 198.292
MLP duration (in seconds): 0.1897
MLP throughput (in TFLOP/s): 241.253
Transformer duration (in seconds): 0.3162
Transformer throughput (in TFLOP/s): 221.141
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0688
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 253.070
Elapsed time for attention_key_query_prob (512x2048x147x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x147x2048): 57.860
Elapsed time for attention_prob_times_values (512x2048x2048x147): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x147): 60.312
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 257.913
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0921
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 251.931
Elapsed time for mlp_fused_gelu (2048x4x75264): 0.0021
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0944
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 245.799
Elapsed time for transformer_add_bias_dropout (2048x4x18816): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18816): 0.0008

Attention duration (in seconds): 0.1826
Attention throughput (in TFLOP/s): 133.967
MLP duration (in seconds): 0.1886
MLP throughput (in TFLOP/s): 246.097
Transformer duration (in seconds): 0.3755
Transformer throughput (in TFLOP/s): 188.754
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1329
Attention throughput (in TFLOP/s): 184.121
MLP duration (in seconds): 0.1913
MLP throughput (in TFLOP/s): 242.627
Transformer duration (in seconds): 0.3307
Transformer throughput (in TFLOP/s): 214.301
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0701
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 251.750
Elapsed time for attention_key_query_prob (512x2048x148x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x148x2048): 79.665
Elapsed time for attention_prob_times_values (512x2048x2048x148): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x148): 98.914
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0239
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 245.832
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0937
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 251.092
Elapsed time for mlp_fused_gelu (2048x4x75776): 0.0021
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0952
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 247.169
Elapsed time for transformer_add_bias_dropout (2048x4x18944): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18944): 0.0008

Attention duration (in seconds): 0.1784
Attention throughput (in TFLOP/s): 138.986
MLP duration (in seconds): 0.1909
MLP throughput (in TFLOP/s): 246.395
Transformer duration (in seconds): 0.3736
Transformer throughput (in TFLOP/s): 192.269
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1236
Attention throughput (in TFLOP/s): 200.635
MLP duration (in seconds): 0.1920
MLP throughput (in TFLOP/s): 245.049
Transformer duration (in seconds): 0.3209
Transformer throughput (in TFLOP/s): 223.851
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0710
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 251.979
Elapsed time for attention_key_query_prob (512x2048x149x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x149x2048): 58.185
Elapsed time for attention_prob_times_values (512x2048x2048x149): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x149): 60.588
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 256.515
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0945
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 252.203
Elapsed time for mlp_fused_gelu (2048x4x76288): 0.0021
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0963
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 247.417
Elapsed time for transformer_add_bias_dropout (2048x4x19072): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19072): 0.0008

Attention duration (in seconds): 0.1857
Attention throughput (in TFLOP/s): 135.236
MLP duration (in seconds): 0.1930
MLP throughput (in TFLOP/s): 247.071
Transformer duration (in seconds): 0.3830
Transformer throughput (in TFLOP/s): 190.042
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1359
Attention throughput (in TFLOP/s): 184.876
MLP duration (in seconds): 0.1955
MLP throughput (in TFLOP/s): 243.846
Transformer duration (in seconds): 0.3361
Transformer throughput (in TFLOP/s): 216.588
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 128, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0716
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 253.006
Elapsed time for attention_key_query_prob (512x2048x150x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x150x2048): 80.975
Elapsed time for attention_prob_times_values (512x2048x2048x150): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x150): 99.614
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 257.234
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.0962
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 251.176
Elapsed time for mlp_fused_gelu (2048x4x76800): 0.0021
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0972
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 248.586
Elapsed time for transformer_add_bias_dropout (2048x4x19200): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19200): 0.0008

Attention duration (in seconds): 0.1795
Attention throughput (in TFLOP/s): 141.775
MLP duration (in seconds): 0.1955
MLP throughput (in TFLOP/s): 247.173
Transformer duration (in seconds): 0.3793
Transformer throughput (in TFLOP/s): 194.456
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1265
Attention throughput (in TFLOP/s): 201.135
MLP duration (in seconds): 0.1979
MLP throughput (in TFLOP/s): 244.168
Transformer duration (in seconds): 0.3298
Transformer throughput (in TFLOP/s): 223.672
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0730
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 251.694
Elapsed time for attention_key_query_prob (512x2048x151x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x151x2048): 58.420
Elapsed time for attention_prob_times_values (512x2048x2048x151): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x151): 60.059
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 258.465
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0975
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 251.155
Elapsed time for mlp_fused_gelu (2048x4x77312): 0.0021
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0996
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 245.754
Elapsed time for transformer_add_bias_dropout (2048x4x19328): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19328): 0.0008

Attention duration (in seconds): 0.1885
Attention throughput (in TFLOP/s): 136.762
MLP duration (in seconds): 0.1992
MLP throughput (in TFLOP/s): 245.775
Transformer duration (in seconds): 0.3921
Transformer throughput (in TFLOP/s): 190.612
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1387
Attention throughput (in TFLOP/s): 185.818
MLP duration (in seconds): 0.2015
MLP throughput (in TFLOP/s): 243.049
Transformer duration (in seconds): 0.3455
Transformer throughput (in TFLOP/s): 216.327
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 128, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0735
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 252.981
Elapsed time for attention_key_query_prob (512x2048x152x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x152x2048): 104.946
Elapsed time for attention_prob_times_values (512x2048x2048x152): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x152): 143.199
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 256.534
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0987
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 251.427
Elapsed time for mlp_fused_gelu (2048x4x77824): 0.0021
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.1013
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 244.864
Elapsed time for transformer_add_bias_dropout (2048x4x19456): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19456): 0.0008

Attention duration (in seconds): 0.1785
Attention throughput (in TFLOP/s): 146.315
MLP duration (in seconds): 0.2021
MLP throughput (in TFLOP/s): 245.475
Transformer duration (in seconds): 0.3850
Transformer throughput (in TFLOP/s): 196.690
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1275
Attention throughput (in TFLOP/s): 204.758
MLP duration (in seconds): 0.2041
MLP throughput (in TFLOP/s): 243.053
Transformer duration (in seconds): 0.3385
Transformer throughput (in TFLOP/s): 223.708
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0726
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 259.720
Elapsed time for attention_key_query_prob (512x2048x153x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x153x2048): 59.070
Elapsed time for attention_prob_times_values (512x2048x2048x153): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x153): 60.452
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 259.545
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.0980
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 256.419
Elapsed time for mlp_fused_gelu (2048x4x78336): 0.0022
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.1000
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 251.397
Elapsed time for transformer_add_bias_dropout (2048x4x19584): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19584): 0.0008

Attention duration (in seconds): 0.1887
Attention throughput (in TFLOP/s): 140.139
MLP duration (in seconds): 0.2002
MLP throughput (in TFLOP/s): 251.149
Transformer duration (in seconds): 0.3934
Transformer throughput (in TFLOP/s): 195.039
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1388
Attention throughput (in TFLOP/s): 190.549
MLP duration (in seconds): 0.2025
MLP throughput (in TFLOP/s): 248.201
Transformer duration (in seconds): 0.3456
Transformer throughput (in TFLOP/s): 222.013
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 128, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0747
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 255.829
Elapsed time for attention_key_query_prob (512x2048x154x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x154x2048): 82.725
Elapsed time for attention_prob_times_values (512x2048x2048x154): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x154): 103.014
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0243
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 261.954
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.1001
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 254.471
Elapsed time for mlp_fused_gelu (2048x4x78848): 0.0022
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.1016
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 250.575
Elapsed time for transformer_add_bias_dropout (2048x4x19712): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19712): 0.0008

Attention duration (in seconds): 0.1833
Attention throughput (in TFLOP/s): 146.141
MLP duration (in seconds): 0.2039
MLP throughput (in TFLOP/s): 249.821
Transformer duration (in seconds): 0.3917
Transformer throughput (in TFLOP/s): 198.416
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1306
Attention throughput (in TFLOP/s): 205.154
MLP duration (in seconds): 0.2062
MLP throughput (in TFLOP/s): 246.944
Transformer duration (in seconds): 0.3444
Transformer throughput (in TFLOP/s): 225.636
Transformer - MLP - Attention (in seconds): 0.0076
========================================================================================================================
num_attention_heads: 128, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0763
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 253.451
Elapsed time for attention_key_query_prob (512x2048x155x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x155x2048): 59.444
Elapsed time for attention_prob_times_values (512x2048x2048x155): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x155): 63.055
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 262.136
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.1010
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 255.397
Elapsed time for mlp_fused_gelu (2048x4x79360): 0.0022
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.1033
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 249.733
Elapsed time for transformer_add_bias_dropout (2048x4x19840): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19840): 0.0008

Attention duration (in seconds): 0.1927
Attention throughput (in TFLOP/s): 140.811
MLP duration (in seconds): 0.2065
MLP throughput (in TFLOP/s): 249.857
Transformer duration (in seconds): 0.4037
Transformer throughput (in TFLOP/s): 195.011
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1415
Attention throughput (in TFLOP/s): 191.769
MLP duration (in seconds): 0.2091
MLP throughput (in TFLOP/s): 246.790
Transformer duration (in seconds): 0.3552
Transformer throughput (in TFLOP/s): 221.652
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 128, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0767
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 255.528
Elapsed time for attention_key_query_prob (512x2048x156x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x156x2048): 83.380
Elapsed time for attention_prob_times_values (512x2048x2048x156): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x156): 103.587
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 259.620
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1027
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 254.495
Elapsed time for mlp_fused_gelu (2048x4x79872): 0.0022
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.1052
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 248.290
Elapsed time for transformer_add_bias_dropout (2048x4x19968): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19968): 0.0008

Attention duration (in seconds): 0.1863
Attention throughput (in TFLOP/s): 147.463
MLP duration (in seconds): 0.2101
MLP throughput (in TFLOP/s): 248.727
Transformer duration (in seconds): 0.4009
Transformer throughput (in TFLOP/s): 198.861
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1335
Attention throughput (in TFLOP/s): 205.712
MLP duration (in seconds): 0.2131
MLP throughput (in TFLOP/s): 245.289
Transformer duration (in seconds): 0.3518
Transformer throughput (in TFLOP/s): 226.625
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 128, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0785
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 252.902
Elapsed time for attention_key_query_prob (512x2048x157x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x157x2048): 60.081
Elapsed time for attention_prob_times_values (512x2048x2048x157): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x157): 63.795
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 259.962
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1043
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 253.711
Elapsed time for mlp_fused_gelu (2048x4x80384): 0.0022
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1064
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 248.639
Elapsed time for transformer_add_bias_dropout (2048x4x20096): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20096): 0.0008

Attention duration (in seconds): 0.1957
Attention throughput (in TFLOP/s): 142.162
MLP duration (in seconds): 0.2130
MLP throughput (in TFLOP/s): 248.542
Transformer duration (in seconds): 0.4132
Transformer throughput (in TFLOP/s): 195.422
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1437
Attention throughput (in TFLOP/s): 193.528
MLP duration (in seconds): 0.2155
MLP throughput (in TFLOP/s): 245.586
Transformer duration (in seconds): 0.3644
Transformer throughput (in TFLOP/s): 221.617
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 128, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0789
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 254.658
Elapsed time for attention_key_query_prob (512x2048x158x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x158x2048): 84.573
Elapsed time for attention_prob_times_values (512x2048x2048x158): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x158): 104.586
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 262.392
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 255.391
Elapsed time for mlp_fused_gelu (2048x4x80896): 0.0022
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1075
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 249.333
Elapsed time for transformer_add_bias_dropout (2048x4x20224): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20224): 0.0008

Attention duration (in seconds): 0.1889
Attention throughput (in TFLOP/s): 149.071
MLP duration (in seconds): 0.2147
MLP throughput (in TFLOP/s): 249.709
Transformer duration (in seconds): 0.4082
Transformer throughput (in TFLOP/s): 200.319
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1357
Attention throughput (in TFLOP/s): 207.506
MLP duration (in seconds): 0.2185
MLP throughput (in TFLOP/s): 245.299
Transformer duration (in seconds): 0.3620
Transformer throughput (in TFLOP/s): 225.862
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0806
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 252.461
Elapsed time for attention_key_query_prob (512x2048x159x2048): 0.0112
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x159x2048): 60.830
Elapsed time for attention_prob_times_values (512x2048x2048x159): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x159): 63.419
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0263
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 258.007
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1073
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 253.070
Elapsed time for mlp_fused_gelu (2048x4x81408): 0.0022
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1097
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 247.428
Elapsed time for transformer_add_bias_dropout (2048x4x20352): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20352): 0.0008

Attention duration (in seconds): 0.1989
Attention throughput (in TFLOP/s): 143.360
MLP duration (in seconds): 0.2192
MLP throughput (in TFLOP/s): 247.663
Transformer duration (in seconds): 0.4227
Transformer throughput (in TFLOP/s): 195.880
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1477
Attention throughput (in TFLOP/s): 193.078
MLP duration (in seconds): 0.2228
MLP throughput (in TFLOP/s): 243.645
Transformer duration (in seconds): 0.3763
Transformer throughput (in TFLOP/s): 220.033
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 128, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0814
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 253.168
Elapsed time for attention_key_query_prob (512x2048x160x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x160x2048): 125.031
Elapsed time for attention_prob_times_values (512x2048x2048x160): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x160): 152.571
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 259.769
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1076
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 255.396
Elapsed time for mlp_fused_gelu (2048x4x81920): 0.0023
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1091
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 251.968
Elapsed time for transformer_add_bias_dropout (2048x4x20480): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20480): 0.0008

Attention duration (in seconds): 0.1878
Attention throughput (in TFLOP/s): 153.669
MLP duration (in seconds): 0.2190
MLP throughput (in TFLOP/s): 251.061
Transformer duration (in seconds): 0.4115
Transformer throughput (in TFLOP/s): 203.760
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1363
Attention throughput (in TFLOP/s): 211.741
MLP duration (in seconds): 0.2235
MLP throughput (in TFLOP/s): 245.945
Transformer duration (in seconds): 0.3672
Transformer throughput (in TFLOP/s): 228.286
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0824
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 253.191
Elapsed time for attention_key_query_prob (512x2048x161x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x161x2048): 58.123
Elapsed time for attention_prob_times_values (512x2048x2048x161): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x161): 63.752
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 260.298
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 254.026
Elapsed time for mlp_fused_gelu (2048x4x82432): 0.0023
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1751
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 158.981
Elapsed time for transformer_add_bias_dropout (2048x4x20608): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20608): 0.0008

Attention duration (in seconds): 0.2018
Attention throughput (in TFLOP/s): 144.743
MLP duration (in seconds): 0.2869
MLP throughput (in TFLOP/s): 194.021
Transformer duration (in seconds): 0.4934
Transformer throughput (in TFLOP/s): 172.023
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1534
Attention throughput (in TFLOP/s): 190.434
MLP duration (in seconds): 0.2838
MLP throughput (in TFLOP/s): 196.123
Transformer duration (in seconds): 0.4417
Transformer throughput (in TFLOP/s): 192.156
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 128, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0826
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 255.982
Elapsed time for attention_key_query_prob (512x2048x162x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x162x2048): 80.319
Elapsed time for attention_prob_times_values (512x2048x2048x162): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x162): 107.181
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0611
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0269
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 262.230
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1099
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 256.380
Elapsed time for mlp_fused_gelu (2048x4x82944): 0.0023
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1735
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 162.416
Elapsed time for transformer_add_bias_dropout (2048x4x20736): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20736): 0.0008

Attention duration (in seconds): 0.1947
Attention throughput (in TFLOP/s): 151.863
MLP duration (in seconds): 0.2857
MLP throughput (in TFLOP/s): 197.267
Transformer duration (in seconds): 0.4852
Transformer throughput (in TFLOP/s): 177.114
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1412
Attention throughput (in TFLOP/s): 209.375
MLP duration (in seconds): 0.2846
MLP throughput (in TFLOP/s): 198.039
Transformer duration (in seconds): 0.4288
Transformer throughput (in TFLOP/s): 200.409
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 128, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0838
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 255.182
Elapsed time for attention_key_query_prob (512x2048x163x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x163x2048): 58.144
Elapsed time for attention_prob_times_values (512x2048x2048x163): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x163): 65.710
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0275
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 259.532
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1124
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 253.745
Elapsed time for mlp_fused_gelu (2048x4x83456): 0.0023
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1799
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 158.567
Elapsed time for transformer_add_bias_dropout (2048x4x20864): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20864): 0.0008

Attention duration (in seconds): 0.2039
Attention throughput (in TFLOP/s): 146.744
MLP duration (in seconds): 0.2946
MLP throughput (in TFLOP/s): 193.651
Transformer duration (in seconds): 0.5033
Transformer throughput (in TFLOP/s): 172.820
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1539
Attention throughput (in TFLOP/s): 194.449
MLP duration (in seconds): 0.2905
MLP throughput (in TFLOP/s): 196.386
Transformer duration (in seconds): 0.4505
Transformer throughput (in TFLOP/s): 193.064
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0855
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 253.193
Elapsed time for attention_key_query_prob (512x2048x164x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x164x2048): 81.193
Elapsed time for attention_prob_times_values (512x2048x2048x164): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x164): 107.618
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 259.676
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1138
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 253.874
Elapsed time for mlp_fused_gelu (2048x4x83968): 0.0023
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 154.475
Elapsed time for transformer_add_bias_dropout (2048x4x20992): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20992): 0.0008

Attention duration (in seconds): 0.1985
Attention throughput (in TFLOP/s): 152.584
MLP duration (in seconds): 0.3030
MLP throughput (in TFLOP/s): 190.614
Transformer duration (in seconds): 0.5063
Transformer throughput (in TFLOP/s): 173.911
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1457
Attention throughput (in TFLOP/s): 207.936
MLP duration (in seconds): 0.3008
MLP throughput (in TFLOP/s): 192.016
Transformer duration (in seconds): 0.4523
Transformer throughput (in TFLOP/s): 194.671
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 128, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0857
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 255.754
Elapsed time for attention_key_query_prob (512x2048x165x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x165x2048): 58.750
Elapsed time for attention_prob_times_values (512x2048x2048x165): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x165): 66.253
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 261.709
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1150
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 254.295
Elapsed time for mlp_fused_gelu (2048x4x84480): 0.0023
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1927
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 151.677
Elapsed time for transformer_add_bias_dropout (2048x4x21120): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21120): 0.0009

Attention duration (in seconds): 0.2063
Attention throughput (in TFLOP/s): 148.549
MLP duration (in seconds): 0.3100
MLP throughput (in TFLOP/s): 188.593
Transformer duration (in seconds): 0.5211
Transformer throughput (in TFLOP/s): 170.999
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1567
Attention throughput (in TFLOP/s): 195.634
MLP duration (in seconds): 0.3078
MLP throughput (in TFLOP/s): 189.935
Transformer duration (in seconds): 0.4733
Transformer throughput (in TFLOP/s): 188.273
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
num_attention_heads: 128, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.0871
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 254.854
Elapsed time for attention_key_query_prob (512x2048x166x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x166x2048): 82.171
Elapsed time for attention_prob_times_values (512x2048x2048x166): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x166): 108.828
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 259.492
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1163
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 254.345
Elapsed time for mlp_fused_gelu (2048x4x84992): 0.0023
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1879
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 157.501
Elapsed time for transformer_add_bias_dropout (2048x4x21248): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21248): 0.0009

Attention duration (in seconds): 0.2007
Attention throughput (in TFLOP/s): 154.505
MLP duration (in seconds): 0.3065
MLP throughput (in TFLOP/s): 193.048
Transformer duration (in seconds): 0.5121
Transformer throughput (in TFLOP/s): 176.101
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1477
Attention throughput (in TFLOP/s): 209.958
MLP duration (in seconds): 0.3054
MLP throughput (in TFLOP/s): 193.796
Transformer duration (in seconds): 0.4554
Transformer throughput (in TFLOP/s): 198.032
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 128, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.0878
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 255.853
Elapsed time for attention_key_query_prob (512x2048x167x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x167x2048): 59.352
Elapsed time for attention_prob_times_values (512x2048x2048x167): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x167): 65.235
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 261.273
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 255.081
Elapsed time for mlp_fused_gelu (2048x4x85504): 0.0023
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1904
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 157.247
Elapsed time for transformer_add_bias_dropout (2048x4x21376): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21376): 0.0009

Attention duration (in seconds): 0.2094
Attention throughput (in TFLOP/s): 149.824
MLP duration (in seconds): 0.3102
MLP throughput (in TFLOP/s): 193.084
Transformer duration (in seconds): 0.5245
Transformer throughput (in TFLOP/s): 174.022
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1601
Attention throughput (in TFLOP/s): 196.062
MLP duration (in seconds): 0.3072
MLP throughput (in TFLOP/s): 194.953
Transformer duration (in seconds): 0.4774
Transformer throughput (in TFLOP/s): 191.190
Transformer - MLP - Attention (in seconds): 0.0101
========================================================================================================================
num_attention_heads: 128, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0895
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 254.042
Elapsed time for attention_key_query_prob (512x2048x168x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x168x2048): 111.143
Elapsed time for attention_prob_times_values (512x2048x2048x168): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x168): 157.890
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0290
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 261.218
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1188
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 255.160
Elapsed time for mlp_fused_gelu (2048x4x86016): 0.0024
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1927
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 157.287
Elapsed time for transformer_add_bias_dropout (2048x4x21504): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21504): 0.0009

Attention duration (in seconds): 0.1995
Attention throughput (in TFLOP/s): 159.171
MLP duration (in seconds): 0.3138
MLP throughput (in TFLOP/s): 193.145
Transformer duration (in seconds): 0.5181
Transformer throughput (in TFLOP/s): 178.248
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1497
Attention throughput (in TFLOP/s): 212.098
MLP duration (in seconds): 0.3077
MLP throughput (in TFLOP/s): 197.009
Transformer duration (in seconds): 0.4635
Transformer throughput (in TFLOP/s): 199.248
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.0904
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 254.547
Elapsed time for attention_key_query_prob (512x2048x169x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x169x2048): 59.875
Elapsed time for attention_prob_times_values (512x2048x2048x169): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x169): 65.711
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0296
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 259.046
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1206
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 254.349
Elapsed time for mlp_fused_gelu (2048x4x86528): 0.0024
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1929
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 159.018
Elapsed time for transformer_add_bias_dropout (2048x4x21632): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21632): 0.0009

Attention duration (in seconds): 0.2130
Attention throughput (in TFLOP/s): 150.758
MLP duration (in seconds): 0.3158
MLP throughput (in TFLOP/s): 194.217
Transformer duration (in seconds): 0.5338
Transformer throughput (in TFLOP/s): 175.085
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1630
Attention throughput (in TFLOP/s): 197.046
MLP duration (in seconds): 0.3149
MLP throughput (in TFLOP/s): 194.751
Transformer duration (in seconds): 0.4845
Transformer throughput (in TFLOP/s): 192.872
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.0922
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 252.289
Elapsed time for attention_key_query_prob (512x2048x170x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x170x2048): 84.292
Elapsed time for attention_prob_times_values (512x2048x2048x170): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x170): 111.009
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0300
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 258.736
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1223
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 253.747
Elapsed time for mlp_fused_gelu (2048x4x87040): 0.0024
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1928
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 160.987
Elapsed time for transformer_add_bias_dropout (2048x4x21760): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21760): 0.0009

Attention duration (in seconds): 0.2074
Attention throughput (in TFLOP/s): 156.665
MLP duration (in seconds): 0.3174
MLP throughput (in TFLOP/s): 195.508
Transformer duration (in seconds): 0.5298
Transformer throughput (in TFLOP/s): 178.480
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1543
Attention throughput (in TFLOP/s): 210.577
MLP duration (in seconds): 0.3141
MLP throughput (in TFLOP/s): 197.603
Transformer duration (in seconds): 0.4756
Transformer throughput (in TFLOP/s): 198.792
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.0934
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 252.016
Elapsed time for attention_key_query_prob (512x2048x171x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x171x2048): 60.390
Elapsed time for attention_prob_times_values (512x2048x2048x171): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x171): 68.872
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0304
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 258.483
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1244
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 252.361
Elapsed time for mlp_fused_gelu (2048x4x87552): 0.0024
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.2022
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 155.299
Elapsed time for transformer_add_bias_dropout (2048x4x21888): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21888): 0.0009

Attention duration (in seconds): 0.2166
Attention throughput (in TFLOP/s): 151.761
MLP duration (in seconds): 0.3290
MLP throughput (in TFLOP/s): 190.868
Transformer duration (in seconds): 0.5506
Transformer throughput (in TFLOP/s): 173.754
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1662
Attention throughput (in TFLOP/s): 197.713
MLP duration (in seconds): 0.3258
MLP throughput (in TFLOP/s): 192.743
Transformer duration (in seconds): 0.4994
Transformer throughput (in TFLOP/s): 191.534
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.0942
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 252.937
Elapsed time for attention_key_query_prob (512x2048x172x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x172x2048): 84.789
Elapsed time for attention_prob_times_values (512x2048x2048x172): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x172): 112.665
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0305
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 260.116
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1258
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 252.569
Elapsed time for mlp_fused_gelu (2048x4x88064): 0.0024
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.2052
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 154.803
Elapsed time for transformer_add_bias_dropout (2048x4x22016): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22016): 0.0009

Attention duration (in seconds): 0.2099
Attention throughput (in TFLOP/s): 158.362
MLP duration (in seconds): 0.3334
MLP throughput (in TFLOP/s): 190.560
Transformer duration (in seconds): 0.5483
Transformer throughput (in TFLOP/s): 176.496
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1572
Attention throughput (in TFLOP/s): 211.520
MLP duration (in seconds): 0.3285
MLP throughput (in TFLOP/s): 193.396
Transformer duration (in seconds): 0.4938
Transformer throughput (in TFLOP/s): 195.976
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 128, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.0954
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 252.564
Elapsed time for attention_key_query_prob (512x2048x173x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x173x2048): 60.768
Elapsed time for attention_prob_times_values (512x2048x2048x173): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x173): 69.217
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0311
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 258.469
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1268
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 253.350
Elapsed time for mlp_fused_gelu (2048x4x88576): 0.0024
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.2083
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 154.280
Elapsed time for transformer_add_bias_dropout (2048x4x22144): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22144): 0.0009

Attention duration (in seconds): 0.2194
Attention throughput (in TFLOP/s): 153.248
MLP duration (in seconds): 0.3376
MLP throughput (in TFLOP/s): 190.393
Transformer duration (in seconds): 0.5620
Transformer throughput (in TFLOP/s): 174.181
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1691
Attention throughput (in TFLOP/s): 198.861
MLP duration (in seconds): 0.3336
MLP throughput (in TFLOP/s): 192.651
Transformer duration (in seconds): 0.5119
Transformer throughput (in TFLOP/s): 191.246
Transformer - MLP - Attention (in seconds): 0.0092
========================================================================================================================
num_attention_heads: 128, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.0964
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 252.948
Elapsed time for attention_key_query_prob (512x2048x174x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x174x2048): 85.408
Elapsed time for attention_prob_times_values (512x2048x2048x174): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x174): 113.016
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 258.793
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1283
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 253.369
Elapsed time for mlp_fused_gelu (2048x4x89088): 0.0024
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.2065
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 157.448
Elapsed time for transformer_add_bias_dropout (2048x4x22272): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22272): 0.0009

Attention duration (in seconds): 0.2131
Attention throughput (in TFLOP/s): 159.585
MLP duration (in seconds): 0.3372
MLP throughput (in TFLOP/s): 192.801
Transformer duration (in seconds): 0.5554
Transformer throughput (in TFLOP/s): 178.300
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1602
Attention throughput (in TFLOP/s): 212.270
MLP duration (in seconds): 0.3313
MLP throughput (in TFLOP/s): 196.255
Transformer duration (in seconds): 0.4994
Transformer throughput (in TFLOP/s): 198.279
Transformer - MLP - Attention (in seconds): 0.0079
========================================================================================================================
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.0975
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 252.962
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 61.182
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 67.858
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 261.388
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1291
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 254.672
Elapsed time for mlp_fused_gelu (2048x4x89600): 0.0025
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1350
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 243.663
Elapsed time for transformer_add_bias_dropout (2048x4x22400): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22400): 0.0009

Attention duration (in seconds): 0.2222
Attention throughput (in TFLOP/s): 154.731
MLP duration (in seconds): 0.2665
MLP throughput (in TFLOP/s): 246.744
Transformer duration (in seconds): 0.4939
Transformer throughput (in TFLOP/s): 202.791
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1718
Attention throughput (in TFLOP/s): 200.167
MLP duration (in seconds): 0.2722
MLP throughput (in TFLOP/s): 241.629
Transformer duration (in seconds): 0.4531
Transformer throughput (in TFLOP/s): 221.063
Transformer - MLP - Attention (in seconds): 0.0091
========================================================================================================================
[2023-06-08 21:44:05,329] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-08 21:44:05,948] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.128.46, master_port=6000
[2023-06-08 21:44:05,948] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-08 21:44:09,387] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.0967
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 255.132
Elapsed time for attention_key_query_prob (512x2048x175x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x175x2048): 61.053
Elapsed time for attention_prob_times_values (512x2048x2048x175): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x175): 67.841
Elapsed time for attention_dropout (4x128x2048x2048): 0.0090
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 259.475
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1298
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 253.287
Elapsed time for mlp_fused_gelu (2048x4x89600): 0.0025
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1349
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 243.798
Elapsed time for transformer_add_bias_dropout (2048x4x22400): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22400): 0.0009

Attention duration (in seconds): 0.2217
Attention throughput (in TFLOP/s): 155.099
MLP duration (in seconds): 0.2672
MLP throughput (in TFLOP/s): 246.161
Transformer duration (in seconds): 0.4940
Transformer throughput (in TFLOP/s): 202.749
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1721
Attention throughput (in TFLOP/s): 199.811
MLP duration (in seconds): 0.2719
MLP throughput (in TFLOP/s): 241.899
Transformer duration (in seconds): 0.4504
Transformer throughput (in TFLOP/s): 222.350
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0984
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 253.420
Elapsed time for attention_key_query_prob (512x2048x176x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x176x2048): 124.124
Elapsed time for attention_prob_times_values (512x2048x2048x176): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x176): 165.591
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 258.743
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1310
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 253.859
Elapsed time for mlp_fused_gelu (2048x4x90112): 0.0025
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1364
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 243.784
Elapsed time for transformer_add_bias_dropout (2048x4x22528): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22528): 0.0009

Attention duration (in seconds): 0.2112
Attention throughput (in TFLOP/s): 164.615
MLP duration (in seconds): 0.2699
MLP throughput (in TFLOP/s): 246.436
Transformer duration (in seconds): 0.4863
Transformer throughput (in TFLOP/s): 208.293
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1618
Attention throughput (in TFLOP/s): 214.917
MLP duration (in seconds): 0.2752
MLP throughput (in TFLOP/s): 241.713
Transformer duration (in seconds): 0.4436
Transformer throughput (in TFLOP/s): 228.329
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.0997
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 253.082
Elapsed time for attention_key_query_prob (512x2048x177x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x177x2048): 61.484
Elapsed time for attention_prob_times_values (512x2048x2048x177): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x177): 68.255
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 258.356
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.1326
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 253.737
Elapsed time for mlp_fused_gelu (2048x4x90624): 0.0025
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1377
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 244.227
Elapsed time for transformer_add_bias_dropout (2048x4x22656): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22656): 0.0009

Attention duration (in seconds): 0.2257
Attention throughput (in TFLOP/s): 155.767
MLP duration (in seconds): 0.2728
MLP throughput (in TFLOP/s): 246.619
Transformer duration (in seconds): 0.5037
Transformer throughput (in TFLOP/s): 203.380
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1749
Attention throughput (in TFLOP/s): 201.012
MLP duration (in seconds): 0.2759
MLP throughput (in TFLOP/s): 243.854
Transformer duration (in seconds): 0.4569
Transformer throughput (in TFLOP/s): 224.205
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1005
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 253.952
Elapsed time for attention_key_query_prob (512x2048x178x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x178x2048): 86.653
Elapsed time for attention_prob_times_values (512x2048x2048x178): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x178): 114.752
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0327
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 259.814
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1342
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 253.559
Elapsed time for mlp_fused_gelu (2048x4x91136): 0.0025
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1396
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 243.662
Elapsed time for transformer_add_bias_dropout (2048x4x22784): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22784): 0.0009

Attention duration (in seconds): 0.2187
Attention throughput (in TFLOP/s): 162.565
MLP duration (in seconds): 0.2763
MLP throughput (in TFLOP/s): 246.261
Transformer duration (in seconds): 0.5001
Transformer throughput (in TFLOP/s): 207.123
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1663
Attention throughput (in TFLOP/s): 213.750
MLP duration (in seconds): 0.2810
MLP throughput (in TFLOP/s): 242.113
Transformer duration (in seconds): 0.4547
Transformer throughput (in TFLOP/s): 227.819
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1019
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 253.275
Elapsed time for attention_key_query_prob (512x2048x179x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x179x2048): 61.635
Elapsed time for attention_prob_times_values (512x2048x2048x179): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x179): 71.056
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 259.736
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.1359
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 253.201
Elapsed time for mlp_fused_gelu (2048x4x91648): 0.0025
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1410
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 243.975
Elapsed time for transformer_add_bias_dropout (2048x4x22912): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22912): 0.0009

Attention duration (in seconds): 0.2283
Attention throughput (in TFLOP/s): 157.445
MLP duration (in seconds): 0.2794
MLP throughput (in TFLOP/s): 246.262
Transformer duration (in seconds): 0.5129
Transformer throughput (in TFLOP/s): 204.235
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1789
Attention throughput (in TFLOP/s): 200.925
MLP duration (in seconds): 0.2838
MLP throughput (in TFLOP/s): 242.447
Transformer duration (in seconds): 0.4664
Transformer throughput (in TFLOP/s): 224.583
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 128, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1022
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 255.362
Elapsed time for attention_key_query_prob (512x2048x180x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x180x2048): 87.808
Elapsed time for attention_prob_times_values (512x2048x2048x180): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x180): 115.984
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 247.498
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.1377
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 252.696
Elapsed time for mlp_fused_gelu (2048x4x92160): 0.0025
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1428
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 243.657
Elapsed time for transformer_add_bias_dropout (2048x4x23040): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23040): 0.0009

Attention duration (in seconds): 0.2228
Attention throughput (in TFLOP/s): 163.090
MLP duration (in seconds): 0.2830
MLP throughput (in TFLOP/s): 245.873
Transformer duration (in seconds): 0.5110
Transformer throughput (in TFLOP/s): 207.259
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1708
Attention throughput (in TFLOP/s): 212.705
MLP duration (in seconds): 0.2866
MLP throughput (in TFLOP/s): 242.789
Transformer duration (in seconds): 0.4656
Transformer throughput (in TFLOP/s): 227.485
Transformer - MLP - Attention (in seconds): 0.0082
========================================================================================================================
num_attention_heads: 128, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1041
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 253.390
Elapsed time for attention_key_query_prob (512x2048x181x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x181x2048): 62.184
Elapsed time for attention_prob_times_values (512x2048x2048x181): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x181): 71.743
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0356
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 246.774
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.1389
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 253.242
Elapsed time for mlp_fused_gelu (2048x4x92672): 0.0025
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1447
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 243.113
Elapsed time for transformer_add_bias_dropout (2048x4x23168): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23168): 0.0009

Attention duration (in seconds): 0.2331
Attention throughput (in TFLOP/s): 157.593
MLP duration (in seconds): 0.2861
MLP throughput (in TFLOP/s): 245.866
Transformer duration (in seconds): 0.5245
Transformer throughput (in TFLOP/s): 204.170
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1832
Attention throughput (in TFLOP/s): 200.551
MLP duration (in seconds): 0.2905
MLP throughput (in TFLOP/s): 242.148
Transformer duration (in seconds): 0.4815
Transformer throughput (in TFLOP/s): 222.412
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 128, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1054
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 253.142
Elapsed time for attention_key_query_prob (512x2048x182x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x182x2048): 88.728
Elapsed time for attention_prob_times_values (512x2048x2048x182): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x182): 116.763
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 258.289
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.1405
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 253.119
Elapsed time for mlp_fused_gelu (2048x4x93184): 0.0026
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1460
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 243.576
Elapsed time for transformer_add_bias_dropout (2048x4x23296): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23296): 0.0009

Attention duration (in seconds): 0.2253
Attention throughput (in TFLOP/s): 164.801
MLP duration (in seconds): 0.2891
MLP throughput (in TFLOP/s): 246.058
Transformer duration (in seconds): 0.5197
Transformer throughput (in TFLOP/s): 208.319
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1713
Attention throughput (in TFLOP/s): 216.701
MLP duration (in seconds): 0.2943
MLP throughput (in TFLOP/s): 241.677
Transformer duration (in seconds): 0.4689
Transformer throughput (in TFLOP/s): 230.886
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 128, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 254.843
Elapsed time for attention_key_query_prob (512x2048x183x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x183x2048): 62.067
Elapsed time for attention_prob_times_values (512x2048x2048x183): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x183): 69.862
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0365
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 246.030
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.1419
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 253.325
Elapsed time for mlp_fused_gelu (2048x4x93696): 0.0026
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1474
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 244.035
Elapsed time for transformer_add_bias_dropout (2048x4x23424): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23424): 0.0009

Attention duration (in seconds): 0.2363
Attention throughput (in TFLOP/s): 158.848
MLP duration (in seconds): 0.2919
MLP throughput (in TFLOP/s): 246.402
Transformer duration (in seconds): 0.5335
Transformer throughput (in TFLOP/s): 205.166
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1865
Attention throughput (in TFLOP/s): 201.199
MLP duration (in seconds): 0.2947
MLP throughput (in TFLOP/s): 244.059
Transformer duration (in seconds): 0.4864
Transformer throughput (in TFLOP/s): 224.997
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
[2023-06-12 14:11:28,091] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-12 14:11:28,832] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.138.213, master_port=6000
[2023-06-12 14:11:28,832] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-12 14:11:31,830] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 128, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1068
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 255.277
Elapsed time for attention_key_query_prob (512x2048x184x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x184x2048): 113.691
Elapsed time for attention_prob_times_values (512x2048x2048x184): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x184): 169.462
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 262.381
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1411
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 257.570
Elapsed time for mlp_fused_gelu (2048x4x94208): 0.0026
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1502
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 242.032
Elapsed time for transformer_add_bias_dropout (2048x4x23552): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23552): 0.0009

Attention duration (in seconds): 0.2230
Attention throughput (in TFLOP/s): 170.103
MLP duration (in seconds): 0.2939
MLP throughput (in TFLOP/s): 247.360
Transformer duration (in seconds): 0.5223
Transformer throughput (in TFLOP/s): 211.836
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1070
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 257.538
Elapsed time for attention_key_query_prob (512x2048x185x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x185x2048): 62.702
Elapsed time for attention_prob_times_values (512x2048x2048x185): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x185): 70.387
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0353
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 259.983
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.1429
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 257.233
Elapsed time for mlp_fused_gelu (2048x4x94720): 0.0026
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1519
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 242.002
Elapsed time for transformer_add_bias_dropout (2048x4x23680): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23680): 0.0009

Attention duration (in seconds): 0.2363
Attention throughput (in TFLOP/s): 162.266
MLP duration (in seconds): 0.2973
MLP throughput (in TFLOP/s): 247.204
Transformer duration (in seconds): 0.5389
Transformer throughput (in TFLOP/s): 207.510
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1081
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 257.635
Elapsed time for attention_key_query_prob (512x2048x186x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x186x2048): 90.490
Elapsed time for attention_prob_times_values (512x2048x2048x186): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x186): 119.090
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0372
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 249.675
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.1445
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 257.097
Elapsed time for mlp_fused_gelu (2048x4x95232): 0.0026
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1533
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 242.370
Elapsed time for transformer_add_bias_dropout (2048x4x23808): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23808): 0.0010

Attention duration (in seconds): 0.2308
Attention throughput (in TFLOP/s): 167.856
MLP duration (in seconds): 0.3004
MLP throughput (in TFLOP/s): 247.345
Transformer duration (in seconds): 0.5366
Transformer throughput (in TFLOP/s): 210.663
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1093
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 257.604
Elapsed time for attention_key_query_prob (512x2048x187x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x187x2048): 63.364
Elapsed time for attention_prob_times_values (512x2048x2048x187): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x187): 74.449
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0360
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 260.817
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.1465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 256.350
Elapsed time for mlp_fused_gelu (2048x4x95744): 0.0026
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1555
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 241.388
Elapsed time for transformer_add_bias_dropout (2048x4x23936): 0.0018
Elapsed time for transformer_layer_norm (2048x4x23936): 0.0010

Attention duration (in seconds): 0.2387
Attention throughput (in TFLOP/s): 164.009
MLP duration (in seconds): 0.3047
MLP throughput (in TFLOP/s): 246.497
Transformer duration (in seconds): 0.5488
Transformer throughput (in TFLOP/s): 208.172
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 128, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1103
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 258.140
Elapsed time for attention_key_query_prob (512x2048x188x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x188x2048): 91.452
Elapsed time for attention_prob_times_values (512x2048x2048x188): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x188): 120.707
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0363
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 261.107
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.1477
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 256.958
Elapsed time for mlp_fused_gelu (2048x4x96256): 0.0026
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1568
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 242.080
Elapsed time for transformer_add_bias_dropout (2048x4x24064): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24064): 0.0010

Attention duration (in seconds): 0.2321
Attention throughput (in TFLOP/s): 170.484
MLP duration (in seconds): 0.3071
MLP throughput (in TFLOP/s): 247.151
Transformer duration (in seconds): 0.5447
Transformer throughput (in TFLOP/s): 211.999
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1115
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 257.986
Elapsed time for attention_key_query_prob (512x2048x189x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x189x2048): 63.976
Elapsed time for attention_prob_times_values (512x2048x2048x189): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x189): 75.179
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0367
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 261.172
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.1495
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 256.572
Elapsed time for mlp_fused_gelu (2048x4x96768): 0.0027
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 242.121
Elapsed time for transformer_add_bias_dropout (2048x4x24192): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24192): 0.0010

Attention duration (in seconds): 0.2417
Attention throughput (in TFLOP/s): 165.436
MLP duration (in seconds): 0.3106
MLP throughput (in TFLOP/s): 247.008
Transformer duration (in seconds): 0.5577
Transformer throughput (in TFLOP/s): 209.232
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1127
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 257.948
Elapsed time for attention_key_query_prob (512x2048x190x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x190x2048): 92.674
Elapsed time for attention_prob_times_values (512x2048x2048x190): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x190): 121.876
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0390
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 248.748
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.1512
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 256.391
Elapsed time for mlp_fused_gelu (2048x4x97280): 0.0027
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1600
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 242.209
Elapsed time for transformer_add_bias_dropout (2048x4x24320): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24320): 0.0010

Attention duration (in seconds): 0.2371
Attention throughput (in TFLOP/s): 170.359
MLP duration (in seconds): 0.3139
MLP throughput (in TFLOP/s): 246.982
Transformer duration (in seconds): 0.5565
Transformer throughput (in TFLOP/s): 211.887
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1138
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 258.159
Elapsed time for attention_key_query_prob (512x2048x191x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x191x2048): 64.623
Elapsed time for attention_prob_times_values (512x2048x2048x191): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x191): 74.026
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0376
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 260.213
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.1526
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 256.772
Elapsed time for mlp_fused_gelu (2048x4x97792): 0.0027
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1617
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 242.304
Elapsed time for transformer_add_bias_dropout (2048x4x24448): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24448): 0.0010

Attention duration (in seconds): 0.2452
Attention throughput (in TFLOP/s): 166.473
MLP duration (in seconds): 0.3169
MLP throughput (in TFLOP/s): 247.217
Transformer duration (in seconds): 0.5676
Transformer throughput (in TFLOP/s): 209.927
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 128, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1151
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 257.942
Elapsed time for attention_key_query_prob (512x2048x192x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x192x2048): 139.417
Elapsed time for attention_prob_times_values (512x2048x2048x192): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x192): 179.454
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0380
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 260.076
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.1541
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 256.878
Elapsed time for mlp_fused_gelu (2048x4x98304): 0.0027
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1636
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 241.903
Elapsed time for transformer_add_bias_dropout (2048x4x24576): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24576): 0.0010

Attention duration (in seconds): 0.2336
Attention throughput (in TFLOP/s): 176.506
MLP duration (in seconds): 0.3204
MLP throughput (in TFLOP/s): 247.068
Transformer duration (in seconds): 0.5596
Transformer throughput (in TFLOP/s): 215.146
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1165
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 257.451
Elapsed time for attention_key_query_prob (512x2048x193x2048): 0.0133
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x193x2048): 62.197
Elapsed time for attention_prob_times_values (512x2048x2048x193): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x193): 73.963
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0611
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0403
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 248.370
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.1563
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 255.916
Elapsed time for mlp_fused_gelu (2048x4x98816): 0.0027
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1656
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 241.575
Elapsed time for transformer_add_bias_dropout (2048x4x24704): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24704): 0.0010

Attention duration (in seconds): 0.2515
Attention throughput (in TFLOP/s): 165.641
MLP duration (in seconds): 0.3246
MLP throughput (in TFLOP/s): 246.462
Transformer duration (in seconds): 0.5817
Transformer throughput (in TFLOP/s): 209.134
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1179
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 257.098
Elapsed time for attention_key_query_prob (512x2048x194x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x194x2048): 88.408
Elapsed time for attention_prob_times_values (512x2048x2048x194): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x194): 122.202
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0386
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 261.720
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.1577
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 256.295
Elapsed time for mlp_fused_gelu (2048x4x99328): 0.0027
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1675
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 241.311
Elapsed time for transformer_add_bias_dropout (2048x4x24832): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24832): 0.0010

Attention duration (in seconds): 0.2427
Attention throughput (in TFLOP/s): 173.389
MLP duration (in seconds): 0.3279
MLP throughput (in TFLOP/s): 246.512
Transformer duration (in seconds): 0.5762
Transformer throughput (in TFLOP/s): 213.301
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 128, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1190
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 257.228
Elapsed time for attention_key_query_prob (512x2048x195x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x195x2048): 62.174
Elapsed time for attention_prob_times_values (512x2048x2048x195): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x195): 76.475
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0389
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 262.083
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.1592
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 256.532
Elapsed time for mlp_fused_gelu (2048x4x99840): 0.0027
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 241.675
Elapsed time for transformer_add_bias_dropout (2048x4x24960): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24960): 0.0010

Attention duration (in seconds): 0.2524
Attention throughput (in TFLOP/s): 168.428
MLP duration (in seconds): 0.3308
MLP throughput (in TFLOP/s): 246.823
Transformer duration (in seconds): 0.5888
Transformer throughput (in TFLOP/s): 210.856
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1201
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 257.615
Elapsed time for attention_key_query_prob (512x2048x196x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x196x2048): 89.089
Elapsed time for attention_prob_times_values (512x2048x2048x196): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x196): 121.511
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0413
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 249.485
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.1603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 257.356
Elapsed time for mlp_fused_gelu (2048x4x100352): 0.0028
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1702
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 242.391
Elapsed time for transformer_add_bias_dropout (2048x4x25088): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25088): 0.0010

Attention duration (in seconds): 0.2477
Attention throughput (in TFLOP/s): 173.293
MLP duration (in seconds): 0.3332
MLP throughput (in TFLOP/s): 247.587
Transformer duration (in seconds): 0.5866
Transformer throughput (in TFLOP/s): 213.809
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1212
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 257.808
Elapsed time for attention_key_query_prob (512x2048x197x2048): 0.0135
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x197x2048): 62.617
Elapsed time for attention_prob_times_values (512x2048x2048x197): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x197): 76.444
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0611
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 259.269
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.1624
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 256.648
Elapsed time for mlp_fused_gelu (2048x4x100864): 0.0028
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.1724
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 241.763
Elapsed time for transformer_add_bias_dropout (2048x4x25216): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25216): 0.0010

Attention duration (in seconds): 0.2562
Attention throughput (in TFLOP/s): 169.284
MLP duration (in seconds): 0.3375
MLP throughput (in TFLOP/s): 246.943
Transformer duration (in seconds): 0.5994
Transformer throughput (in TFLOP/s): 211.391
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1225
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 257.730
Elapsed time for attention_key_query_prob (512x2048x198x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x198x2048): 89.705
Elapsed time for attention_prob_times_values (512x2048x2048x198): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x198): 120.191
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 259.372
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.1646
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 255.726
Elapsed time for mlp_fused_gelu (2048x4x101376): 0.0028
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1743
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 241.465
Elapsed time for transformer_add_bias_dropout (2048x4x25344): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25344): 0.0010

Attention duration (in seconds): 0.2496
Attention throughput (in TFLOP/s): 175.485
MLP duration (in seconds): 0.3417
MLP throughput (in TFLOP/s): 246.372
Transformer duration (in seconds): 0.5970
Transformer throughput (in TFLOP/s): 214.369
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 128, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 256.793
Elapsed time for attention_key_query_prob (512x2048x199x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x199x2048): 62.620
Elapsed time for attention_prob_times_values (512x2048x2048x199): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x199): 74.228
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 260.884
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.1663
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 255.718
Elapsed time for mlp_fused_gelu (2048x4x101888): 0.0028
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1752
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 242.648
Elapsed time for transformer_add_bias_dropout (2048x4x25472): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25472): 0.0010

Attention duration (in seconds): 0.2600
Attention throughput (in TFLOP/s): 170.090
MLP duration (in seconds): 0.3443
MLP throughput (in TFLOP/s): 246.991
Transformer duration (in seconds): 0.6101
Transformer throughput (in TFLOP/s): 211.873
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 128, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1252
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 257.338
Elapsed time for attention_key_query_prob (512x2048x200x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x200x2048): 121.778
Elapsed time for attention_prob_times_values (512x2048x2048x200): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x200): 183.056
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 262.073
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.1681
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 255.530
Elapsed time for mlp_fused_gelu (2048x4x102400): 0.0028
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.1776
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 241.849
Elapsed time for transformer_add_bias_dropout (2048x4x25600): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25600): 0.0010

Attention duration (in seconds): 0.2478
Attention throughput (in TFLOP/s): 180.228
MLP duration (in seconds): 0.3485
MLP throughput (in TFLOP/s): 246.499
Transformer duration (in seconds): 0.6021
Transformer throughput (in TFLOP/s): 216.845
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 128, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.1265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 257.095
Elapsed time for attention_key_query_prob (512x2048x201x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x201x2048): 63.063
Elapsed time for attention_prob_times_values (512x2048x2048x201): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x201): 74.061
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 260.036
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.1696
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 255.797
Elapsed time for mlp_fused_gelu (2048x4x102912): 0.0028
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.1792
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 242.072
Elapsed time for transformer_add_bias_dropout (2048x4x25728): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25728): 0.0010

Attention duration (in seconds): 0.2636
Attention throughput (in TFLOP/s): 171.146
MLP duration (in seconds): 0.3516
MLP throughput (in TFLOP/s): 246.749
Transformer duration (in seconds): 0.6210
Transformer throughput (in TFLOP/s): 212.339
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1276
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 257.473
Elapsed time for attention_key_query_prob (512x2048x202x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x202x2048): 90.842
Elapsed time for attention_prob_times_values (512x2048x2048x202): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x202): 120.586
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0418
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 262.008
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1710
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 256.250
Elapsed time for mlp_fused_gelu (2048x4x103424): 0.0028
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1813
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 241.622
Elapsed time for transformer_add_bias_dropout (2048x4x25856): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25856): 0.0010

Attention duration (in seconds): 0.2561
Attention throughput (in TFLOP/s): 177.837
MLP duration (in seconds): 0.3551
MLP throughput (in TFLOP/s): 246.735
Transformer duration (in seconds): 0.6171
Transformer throughput (in TFLOP/s): 215.800
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1289
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 257.358
Elapsed time for attention_key_query_prob (512x2048x203x2048): 0.0136
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x203x2048): 64.047
Elapsed time for attention_prob_times_values (512x2048x2048x203): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x203): 76.777
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0426
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 259.409
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1728
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 255.992
Elapsed time for mlp_fused_gelu (2048x4x103936): 0.0029
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1826
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 242.313
Elapsed time for transformer_add_bias_dropout (2048x4x25984): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25984): 0.0010

Attention duration (in seconds): 0.2665
Attention throughput (in TFLOP/s): 172.568
MLP duration (in seconds): 0.3583
MLP throughput (in TFLOP/s): 246.984
Transformer duration (in seconds): 0.6307
Transformer throughput (in TFLOP/s): 213.230
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1303
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 257.170
Elapsed time for attention_key_query_prob (512x2048x204x2048): 0.0095
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x204x2048): 91.774
Elapsed time for attention_prob_times_values (512x2048x2048x204): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x204): 121.601
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0429
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 260.660
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.1749
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 255.549
Elapsed time for mlp_fused_gelu (2048x4x104448): 0.0029
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1851
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 241.465
Elapsed time for transformer_add_bias_dropout (2048x4x26112): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26112): 0.0010

Attention duration (in seconds): 0.2599
Attention throughput (in TFLOP/s): 178.680
MLP duration (in seconds): 0.3628
MLP throughput (in TFLOP/s): 246.348
Transformer duration (in seconds): 0.6286
Transformer throughput (in TFLOP/s): 216.050
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1316
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 257.234
Elapsed time for attention_key_query_prob (512x2048x205x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x205x2048): 64.415
Elapsed time for attention_prob_times_values (512x2048x2048x205): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x205): 77.015
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0433
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 260.290
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.1765
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 255.646
Elapsed time for mlp_fused_gelu (2048x4x104960): 0.0029
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1870
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 241.347
Elapsed time for transformer_add_bias_dropout (2048x4x26240): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26240): 0.0011

Attention duration (in seconds): 0.2700
Attention throughput (in TFLOP/s): 173.679
MLP duration (in seconds): 0.3664
MLP throughput (in TFLOP/s): 246.341
Transformer duration (in seconds): 0.6422
Transformer throughput (in TFLOP/s): 213.524
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 128, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1327
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 257.611
Elapsed time for attention_key_query_prob (512x2048x206x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x206x2048): 92.294
Elapsed time for attention_prob_times_values (512x2048x2048x206): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x206): 122.189
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0439
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 259.316
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1778
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 256.212
Elapsed time for mlp_fused_gelu (2048x4x105472): 0.0029
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1889
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 241.216
Elapsed time for transformer_add_bias_dropout (2048x4x26368): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26368): 0.0011

Attention duration (in seconds): 0.2634
Attention throughput (in TFLOP/s): 179.736
MLP duration (in seconds): 0.3696
MLP throughput (in TFLOP/s): 246.544
Transformer duration (in seconds): 0.6390
Transformer throughput (in TFLOP/s): 216.704
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1341
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 257.312
Elapsed time for attention_key_query_prob (512x2048x207x2048): 0.0137
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x207x2048): 64.795
Elapsed time for attention_prob_times_values (512x2048x2048x207): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x207): 76.408
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 258.159
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1801
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 255.527
Elapsed time for mlp_fused_gelu (2048x4x105984): 0.0029
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1910
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 240.924
Elapsed time for transformer_add_bias_dropout (2048x4x26496): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26496): 0.0011

Attention duration (in seconds): 0.2740
Attention throughput (in TFLOP/s): 174.432
MLP duration (in seconds): 0.3739
MLP throughput (in TFLOP/s): 246.084
Transformer duration (in seconds): 0.6539
Transformer throughput (in TFLOP/s): 213.807
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1353
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 257.476
Elapsed time for attention_key_query_prob (512x2048x208x2048): 0.0066
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x208x2048): 135.188
Elapsed time for attention_prob_times_values (512x2048x2048x208): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x208): 190.739
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0446
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 260.150
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.1815
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 255.979
Elapsed time for mlp_fused_gelu (2048x4x106496): 0.0029
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1923
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 241.561
Elapsed time for transformer_add_bias_dropout (2048x4x26624): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26624): 0.0011

Attention duration (in seconds): 0.2612
Attention throughput (in TFLOP/s): 184.693
MLP duration (in seconds): 0.3767
MLP throughput (in TFLOP/s): 246.636
Transformer duration (in seconds): 0.6439
Transformer throughput (in TFLOP/s): 219.203
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 128, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1368
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 257.058
Elapsed time for attention_key_query_prob (512x2048x209x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x209x2048): 65.117
Elapsed time for attention_prob_times_values (512x2048x2048x209): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x209): 76.283
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0449
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 260.873
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.1832
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 256.050
Elapsed time for mlp_fused_gelu (2048x4x107008): 0.0029
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1942
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 241.462
Elapsed time for transformer_add_bias_dropout (2048x4x26752): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26752): 0.0011

Attention duration (in seconds): 0.2773
Attention throughput (in TFLOP/s): 175.622
MLP duration (in seconds): 0.3804
MLP throughput (in TFLOP/s): 246.624
Transformer duration (in seconds): 0.6637
Transformer throughput (in TFLOP/s): 214.705
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 257.300
Elapsed time for attention_key_query_prob (512x2048x210x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x210x2048): 93.541
Elapsed time for attention_prob_times_values (512x2048x2048x210): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x210): 123.668
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0458
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 258.331
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.1845
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 256.594
Elapsed time for mlp_fused_gelu (2048x4x107520): 0.0029
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.1955
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 242.180
Elapsed time for transformer_add_bias_dropout (2048x4x26880): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26880): 0.0011

Attention duration (in seconds): 0.2707
Attention throughput (in TFLOP/s): 181.565
MLP duration (in seconds): 0.3830
MLP throughput (in TFLOP/s): 247.262
Transformer duration (in seconds): 0.6598
Transformer throughput (in TFLOP/s): 218.024
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 257.278
Elapsed time for attention_key_query_prob (512x2048x211x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x211x2048): 65.200
Elapsed time for attention_prob_times_values (512x2048x2048x211): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x211): 78.143
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0610
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 259.453
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.1869
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 255.831
Elapsed time for mlp_fused_gelu (2048x4x108032): 0.0030
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.1975
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 242.107
Elapsed time for transformer_add_bias_dropout (2048x4x27008): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27008): 0.0011

Attention duration (in seconds): 0.2809
Attention throughput (in TFLOP/s): 176.603
MLP duration (in seconds): 0.3873
MLP throughput (in TFLOP/s): 246.877
Transformer duration (in seconds): 0.6743
Transformer throughput (in TFLOP/s): 215.359
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1408
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 257.117
Elapsed time for attention_key_query_prob (512x2048x212x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x212x2048): 94.493
Elapsed time for attention_prob_times_values (512x2048x2048x212): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x212): 124.875
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 261.457
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.1886
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 255.826
Elapsed time for mlp_fused_gelu (2048x4x108544): 0.0030
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.1998
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 241.570
Elapsed time for transformer_add_bias_dropout (2048x4x27136): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27136): 0.0011

Attention duration (in seconds): 0.2738
Attention throughput (in TFLOP/s): 182.914
MLP duration (in seconds): 0.3914
MLP throughput (in TFLOP/s): 246.605
Transformer duration (in seconds): 0.6713
Transformer throughput (in TFLOP/s): 218.374
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 128, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1429
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 255.616
Elapsed time for attention_key_query_prob (512x2048x213x2048): 0.0139
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x213x2048): 65.591
Elapsed time for attention_prob_times_values (512x2048x2048x213): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x213): 78.740
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0471
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 258.678
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.1909
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 255.149
Elapsed time for mlp_fused_gelu (2048x4x109056): 0.0030
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.2020
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 241.138
Elapsed time for transformer_add_bias_dropout (2048x4x27264): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27264): 0.0011

Attention duration (in seconds): 0.2855
Attention throughput (in TFLOP/s): 177.023
MLP duration (in seconds): 0.3959
MLP throughput (in TFLOP/s): 246.074
Transformer duration (in seconds): 0.6876
Transformer throughput (in TFLOP/s): 215.190
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1432
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 257.466
Elapsed time for attention_key_query_prob (512x2048x214x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x214x2048): 95.925
Elapsed time for attention_prob_times_values (512x2048x2048x214): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x214): 125.500
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0611
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0473
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 259.898
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.1928
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 255.098
Elapsed time for mlp_fused_gelu (2048x4x109568): 0.0030
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2036
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 241.497
Elapsed time for transformer_add_bias_dropout (2048x4x27392): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27392): 0.0011

Attention duration (in seconds): 0.2776
Attention throughput (in TFLOP/s): 183.777
MLP duration (in seconds): 0.3994
MLP throughput (in TFLOP/s): 246.245
Transformer duration (in seconds): 0.6832
Transformer throughput (in TFLOP/s): 218.619
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.1445
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 257.594
Elapsed time for attention_key_query_prob (512x2048x215x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x215x2048): 65.975
Elapsed time for attention_prob_times_values (512x2048x2048x215): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x215): 77.826
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 259.792
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.1942
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 255.574
Elapsed time for mlp_fused_gelu (2048x4x110080): 0.0030
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.2052
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 241.825
Elapsed time for transformer_add_bias_dropout (2048x4x27520): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27520): 0.0011

Attention duration (in seconds): 0.2881
Attention throughput (in TFLOP/s): 178.697
MLP duration (in seconds): 0.4025
MLP throughput (in TFLOP/s): 246.646
Transformer duration (in seconds): 0.6968
Transformer throughput (in TFLOP/s): 216.346
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 128, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1457
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 257.806
Elapsed time for attention_key_query_prob (512x2048x216x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x216x2048): 123.924
Elapsed time for attention_prob_times_values (512x2048x2048x216): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x216): 195.056
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0480
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 260.926
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.1957
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 255.942
Elapsed time for mlp_fused_gelu (2048x4x110592): 0.0030
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.2071
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 241.837
Elapsed time for transformer_add_bias_dropout (2048x4x27648): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27648): 0.0011

Attention duration (in seconds): 0.2759
Attention throughput (in TFLOP/s): 188.281
MLP duration (in seconds): 0.4059
MLP throughput (in TFLOP/s): 246.834
Transformer duration (in seconds): 0.6881
Transformer throughput (in TFLOP/s): 221.111
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 257.219
Elapsed time for attention_key_query_prob (512x2048x217x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x217x2048): 66.475
Elapsed time for attention_prob_times_values (512x2048x2048x217): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x217): 78.127
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0489
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 258.430
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.1978
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 255.633
Elapsed time for mlp_fused_gelu (2048x4x111104): 0.0030
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.2109
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 239.760
Elapsed time for transformer_add_bias_dropout (2048x4x27776): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27776): 0.0011

Attention duration (in seconds): 0.2922
Attention throughput (in TFLOP/s): 179.394
MLP duration (in seconds): 0.4117
MLP throughput (in TFLOP/s): 245.613
Transformer duration (in seconds): 0.7103
Transformer throughput (in TFLOP/s): 216.184
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 128, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1490
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 256.777
Elapsed time for attention_key_query_prob (512x2048x218x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x218x2048): 97.217
Elapsed time for attention_prob_times_values (512x2048x2048x218): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x218): 127.232
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 258.505
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.1997
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 255.525
Elapsed time for mlp_fused_gelu (2048x4x111616): 0.0031
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.2110
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 241.809
Elapsed time for transformer_add_bias_dropout (2048x4x27904): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27904): 0.0011

Attention duration (in seconds): 0.2853
Attention throughput (in TFLOP/s): 185.394
MLP duration (in seconds): 0.4138
MLP throughput (in TFLOP/s): 246.642
Transformer duration (in seconds): 0.7055
Transformer throughput (in TFLOP/s): 219.646
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 257.014
Elapsed time for attention_key_query_prob (512x2048x219x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x219x2048): 67.190
Elapsed time for attention_prob_times_values (512x2048x2048x219): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x219): 81.267
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0494
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 260.446
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2018
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 255.226
Elapsed time for mlp_fused_gelu (2048x4x112128): 0.0031
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.2131
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 241.680
Elapsed time for transformer_add_bias_dropout (2048x4x28032): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28032): 0.0011

Attention duration (in seconds): 0.2952
Attention throughput (in TFLOP/s): 180.805
MLP duration (in seconds): 0.4179
MLP throughput (in TFLOP/s): 246.443
Transformer duration (in seconds): 0.7195
Transformer throughput (in TFLOP/s): 217.328
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.1513
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 257.660
Elapsed time for attention_key_query_prob (512x2048x220x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x220x2048): 97.546
Elapsed time for attention_prob_times_values (512x2048x2048x220): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x220): 129.038
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 258.116
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.2033
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 255.668
Elapsed time for mlp_fused_gelu (2048x4x112640): 0.0031
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.2154
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 241.315
Elapsed time for transformer_add_bias_dropout (2048x4x28160): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28160): 0.0011

Attention duration (in seconds): 0.2886
Attention throughput (in TFLOP/s): 186.627
MLP duration (in seconds): 0.4217
MLP throughput (in TFLOP/s): 246.468
Transformer duration (in seconds): 0.7167
Transformer throughput (in TFLOP/s): 220.169
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1534
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 256.474
Elapsed time for attention_key_query_prob (512x2048x221x2048): 0.0140
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x221x2048): 68.019
Elapsed time for attention_prob_times_values (512x2048x2048x221): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x221): 79.842
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0505
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 259.443
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.2055
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 255.186
Elapsed time for mlp_fused_gelu (2048x4x113152): 0.0031
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.2173
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 241.292
Elapsed time for transformer_add_bias_dropout (2048x4x28288): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28288): 0.0011

Attention duration (in seconds): 0.2997
Attention throughput (in TFLOP/s): 181.328
MLP duration (in seconds): 0.4259
MLP throughput (in TFLOP/s): 246.240
Transformer duration (in seconds): 0.7320
Transformer throughput (in TFLOP/s): 217.512
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1544
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 257.033
Elapsed time for attention_key_query_prob (512x2048x222x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x222x2048): 99.164
Elapsed time for attention_prob_times_values (512x2048x2048x222): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x222): 130.172
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 260.897
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.2069
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 255.774
Elapsed time for mlp_fused_gelu (2048x4x113664): 0.0031
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.2197
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 240.917
Elapsed time for transformer_add_bias_dropout (2048x4x28416): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28416): 0.0011

Attention duration (in seconds): 0.2920
Attention throughput (in TFLOP/s): 187.752
MLP duration (in seconds): 0.4297
MLP throughput (in TFLOP/s): 246.326
Transformer duration (in seconds): 0.7281
Transformer throughput (in TFLOP/s): 220.659
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 128, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1562
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 256.388
Elapsed time for attention_key_query_prob (512x2048x223x2048): 0.0138
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x223x2048): 69.224
Elapsed time for attention_prob_times_values (512x2048x2048x223): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x223): 81.612
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 258.649
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.2096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 254.729
Elapsed time for mlp_fused_gelu (2048x4x114176): 0.0031
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 241.009
Elapsed time for transformer_add_bias_dropout (2048x4x28544): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28544): 0.0011

Attention duration (in seconds): 0.3033
Attention throughput (in TFLOP/s): 182.356
MLP duration (in seconds): 0.4343
MLP throughput (in TFLOP/s): 245.894
Transformer duration (in seconds): 0.7441
Transformer throughput (in TFLOP/s): 217.857
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1570
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 257.370
Elapsed time for attention_key_query_prob (512x2048x224x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x224x2048): 149.955
Elapsed time for attention_prob_times_values (512x2048x2048x224): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x224): 203.825
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0519
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 259.671
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.2112
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 255.119
Elapsed time for mlp_fused_gelu (2048x4x114688): 0.0031
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2228
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 241.845
Elapsed time for transformer_add_bias_dropout (2048x4x28672): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28672): 0.0012

Attention duration (in seconds): 0.2900
Attention throughput (in TFLOP/s): 192.440
MLP duration (in seconds): 0.4371
MLP throughput (in TFLOP/s): 246.521
Transformer duration (in seconds): 0.7336
Transformer throughput (in TFLOP/s): 222.959
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1584
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 257.360
Elapsed time for attention_key_query_prob (512x2048x225x2048): 0.0144
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x225x2048): 66.951
Elapsed time for attention_prob_times_values (512x2048x2048x225): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x225): 82.031
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0523
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 259.618
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.2126
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 255.649
Elapsed time for mlp_fused_gelu (2048x4x115200): 0.0032
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 241.677
Elapsed time for transformer_add_bias_dropout (2048x4x28800): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28800): 0.0012

Attention duration (in seconds): 0.3069
Attention throughput (in TFLOP/s): 183.403
MLP duration (in seconds): 0.4407
MLP throughput (in TFLOP/s): 246.689
Transformer duration (in seconds): 0.7541
Transformer throughput (in TFLOP/s): 218.800
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.1597
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 257.493
Elapsed time for attention_key_query_prob (512x2048x226x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x226x2048): 95.111
Elapsed time for attention_prob_times_values (512x2048x2048x226): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x226): 131.935
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 259.503
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.2149
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 255.166
Elapsed time for mlp_fused_gelu (2048x4x115712): 0.0032
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2277
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 240.809
Elapsed time for transformer_add_bias_dropout (2048x4x28928): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28928): 0.0012

Attention duration (in seconds): 0.3001
Attention throughput (in TFLOP/s): 189.228
MLP duration (in seconds): 0.4458
MLP throughput (in TFLOP/s): 246.018
Transformer duration (in seconds): 0.7525
Transformer throughput (in TFLOP/s): 221.229
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 128, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1617
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 256.643
Elapsed time for attention_key_query_prob (512x2048x227x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x227x2048): 66.810
Elapsed time for attention_prob_times_values (512x2048x2048x227): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x227): 83.993
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0535
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 258.490
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.2174
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 254.454
Elapsed time for mlp_fused_gelu (2048x4x116224): 0.0032
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2294
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 241.162
Elapsed time for transformer_add_bias_dropout (2048x4x29056): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29056): 0.0012

Attention duration (in seconds): 0.3113
Attention throughput (in TFLOP/s): 183.971
MLP duration (in seconds): 0.4501
MLP throughput (in TFLOP/s): 245.877
Transformer duration (in seconds): 0.7680
Transformer throughput (in TFLOP/s): 218.676
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1634
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 256.206
Elapsed time for attention_key_query_prob (512x2048x228x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x228x2048): 95.414
Elapsed time for attention_prob_times_values (512x2048x2048x228): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x228): 132.811
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 258.798
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2201
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 253.586
Elapsed time for mlp_fused_gelu (2048x4x116736): 0.0032
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2321
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 240.515
Elapsed time for transformer_add_bias_dropout (2048x4x29184): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29184): 0.0012

Attention duration (in seconds): 0.3049
Attention throughput (in TFLOP/s): 189.494
MLP duration (in seconds): 0.4554
MLP throughput (in TFLOP/s): 245.146
Transformer duration (in seconds): 0.7669
Transformer throughput (in TFLOP/s): 220.902
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1650
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 255.885
Elapsed time for attention_key_query_prob (512x2048x229x2048): 0.0146
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x229x2048): 67.330
Elapsed time for attention_prob_times_values (512x2048x2048x229): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x229): 84.648
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0540
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 260.662
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.2218
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 253.839
Elapsed time for mlp_fused_gelu (2048x4x117248): 0.0032
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2340
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 240.650
Elapsed time for transformer_add_bias_dropout (2048x4x29312): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29312): 0.0012

Attention duration (in seconds): 0.3152
Attention throughput (in TFLOP/s): 184.874
MLP duration (in seconds): 0.4590
MLP throughput (in TFLOP/s): 245.341
Transformer duration (in seconds): 0.7809
Transformer throughput (in TFLOP/s): 218.847
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 128, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 255.988
Elapsed time for attention_key_query_prob (512x2048x230x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x230x2048): 96.586
Elapsed time for attention_prob_times_values (512x2048x2048x230): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x230): 133.550
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0549
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 258.682
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.2234
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 254.278
Elapsed time for mlp_fused_gelu (2048x4x117760): 0.0032
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2354
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 241.250
Elapsed time for transformer_add_bias_dropout (2048x4x29440): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29440): 0.0012

Attention duration (in seconds): 0.3089
Attention throughput (in TFLOP/s): 190.286
MLP duration (in seconds): 0.4620
MLP throughput (in TFLOP/s): 245.865
Transformer duration (in seconds): 0.7776
Transformer throughput (in TFLOP/s): 221.678
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 128, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1680
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 255.736
Elapsed time for attention_key_query_prob (512x2048x231x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x231x2048): 67.407
Elapsed time for attention_prob_times_values (512x2048x2048x231): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x231): 83.283
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0551
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 259.917
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.2270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 252.451
Elapsed time for mlp_fused_gelu (2048x4x118272): 0.0032
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 241.467
Elapsed time for transformer_add_bias_dropout (2048x4x29568): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29568): 0.0012

Attention duration (in seconds): 0.3197
Attention throughput (in TFLOP/s): 185.415
MLP duration (in seconds): 0.4675
MLP throughput (in TFLOP/s): 245.127
Transformer duration (in seconds): 0.7939
Transformer throughput (in TFLOP/s): 219.000
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 128, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1694
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 255.827
Elapsed time for attention_key_query_prob (512x2048x232x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x232x2048): 130.675
Elapsed time for attention_prob_times_values (512x2048x2048x232): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x232): 207.079
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0557
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 259.565
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.2281
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 253.329
Elapsed time for mlp_fused_gelu (2048x4x118784): 0.0033
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2398
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 241.021
Elapsed time for transformer_add_bias_dropout (2048x4x29696): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29696): 0.0012

Attention duration (in seconds): 0.3076
Attention throughput (in TFLOP/s): 194.385
MLP duration (in seconds): 0.4712
MLP throughput (in TFLOP/s): 245.317
Transformer duration (in seconds): 0.7855
Transformer throughput (in TFLOP/s): 223.271
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 128, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1706
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 256.210
Elapsed time for attention_key_query_prob (512x2048x233x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x233x2048): 67.786
Elapsed time for attention_prob_times_values (512x2048x2048x233): 0.0119
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x233): 83.825
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0566
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 257.585
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.2292
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 254.335
Elapsed time for mlp_fused_gelu (2048x4x119296): 0.0033
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2420
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 240.920
Elapsed time for transformer_add_bias_dropout (2048x4x29824): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29824): 0.0012

Attention duration (in seconds): 0.3239
Attention throughput (in TFLOP/s): 186.168
MLP duration (in seconds): 0.4744
MLP throughput (in TFLOP/s): 245.741
Transformer duration (in seconds): 0.8051
Transformer throughput (in TFLOP/s): 219.708
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1722
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 256.107
Elapsed time for attention_key_query_prob (512x2048x234x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x234x2048): 98.637
Elapsed time for attention_prob_times_values (512x2048x2048x234): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x234): 136.086
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0565
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 259.927
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.2318
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 253.652
Elapsed time for mlp_fused_gelu (2048x4x119808): 0.0033
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2444
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 240.611
Elapsed time for transformer_add_bias_dropout (2048x4x29952): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29952): 0.0012

Attention duration (in seconds): 0.3162
Attention throughput (in TFLOP/s): 192.270
MLP duration (in seconds): 0.4794
MLP throughput (in TFLOP/s): 245.271
Transformer duration (in seconds): 0.8024
Transformer throughput (in TFLOP/s): 222.313
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.1745
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 254.910
Elapsed time for attention_key_query_prob (512x2048x235x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x235x2048): 68.618
Elapsed time for attention_prob_times_values (512x2048x2048x235): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x235): 87.212
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0574
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 258.303
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.2338
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 253.669
Elapsed time for mlp_fused_gelu (2048x4x120320): 0.0033
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2465
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 240.570
Elapsed time for transformer_add_bias_dropout (2048x4x30080): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30080): 0.0012

Attention duration (in seconds): 0.3281
Attention throughput (in TFLOP/s): 186.889
MLP duration (in seconds): 0.4835
MLP throughput (in TFLOP/s): 245.265
Transformer duration (in seconds): 0.8184
Transformer throughput (in TFLOP/s): 219.825
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 128, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1748
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 256.572
Elapsed time for attention_key_query_prob (512x2048x236x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x236x2048): 98.918
Elapsed time for attention_prob_times_values (512x2048x2048x236): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x236): 138.024
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0575
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 260.151
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.2351
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 254.389
Elapsed time for mlp_fused_gelu (2048x4x120832): 0.0033
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2478
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 241.324
Elapsed time for transformer_add_bias_dropout (2048x4x30208): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30208): 0.0012

Attention duration (in seconds): 0.3198
Attention throughput (in TFLOP/s): 193.323
MLP duration (in seconds): 0.4862
MLP throughput (in TFLOP/s): 245.999
Transformer duration (in seconds): 0.8129
Transformer throughput (in TFLOP/s): 223.198
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.1768
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 255.795
Elapsed time for attention_key_query_prob (512x2048x237x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x237x2048): 69.192
Elapsed time for attention_prob_times_values (512x2048x2048x237): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x237): 87.785
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0610
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0581
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 259.362
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.2382
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 253.215
Elapsed time for mlp_fused_gelu (2048x4x121344): 0.0033
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2499
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 241.329
Elapsed time for transformer_add_bias_dropout (2048x4x30336): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30336): 0.0012

Attention duration (in seconds): 0.3314
Attention throughput (in TFLOP/s): 188.153
MLP duration (in seconds): 0.4914
MLP throughput (in TFLOP/s): 245.459
Transformer duration (in seconds): 0.8297
Transformer throughput (in TFLOP/s): 220.535
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1782
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 255.975
Elapsed time for attention_key_query_prob (512x2048x238x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x238x2048): 99.594
Elapsed time for attention_prob_times_values (512x2048x2048x238): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x238): 138.709
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0586
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 259.474
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.2396
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 253.867
Elapsed time for mlp_fused_gelu (2048x4x121856): 0.0033
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2520
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 241.344
Elapsed time for transformer_add_bias_dropout (2048x4x30464): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30464): 0.0012

Attention duration (in seconds): 0.3244
Attention throughput (in TFLOP/s): 193.798
MLP duration (in seconds): 0.4949
MLP throughput (in TFLOP/s): 245.780
Transformer duration (in seconds): 0.8262
Transformer throughput (in TFLOP/s): 223.315
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 128, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1799
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 255.648
Elapsed time for attention_key_query_prob (512x2048x239x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x239x2048): 69.217
Elapsed time for attention_prob_times_values (512x2048x2048x239): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x239): 86.882
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0592
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 259.038
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.2415
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 253.936
Elapsed time for mlp_fused_gelu (2048x4x122368): 0.0033
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2536
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 241.846
Elapsed time for transformer_add_bias_dropout (2048x4x30592): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30592): 0.0012

Attention duration (in seconds): 0.3358
Attention throughput (in TFLOP/s): 188.773
MLP duration (in seconds): 0.4985
MLP throughput (in TFLOP/s): 246.079
Transformer duration (in seconds): 0.8412
Transformer throughput (in TFLOP/s): 221.170
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1808
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 256.568
Elapsed time for attention_key_query_prob (512x2048x240x2048): 0.0071
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x240x2048): 144.856
Elapsed time for attention_prob_times_values (512x2048x2048x240): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x240): 215.528
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0596
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 259.243
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.2422
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 255.313
Elapsed time for mlp_fused_gelu (2048x4x122880): 0.0034
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2558
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 241.738
Elapsed time for transformer_add_bias_dropout (2048x4x30720): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30720): 0.0012

Attention duration (in seconds): 0.3223
Attention throughput (in TFLOP/s): 198.300
MLP duration (in seconds): 0.5015
MLP throughput (in TFLOP/s): 246.673
Transformer duration (in seconds): 0.8307
Transformer throughput (in TFLOP/s): 225.836
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 256.336
Elapsed time for attention_key_query_prob (512x2048x241x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x241x2048): 69.835
Elapsed time for attention_prob_times_values (512x2048x2048x241): 0.0118
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x241): 87.666
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0605
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 257.728
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.2457
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 253.815
Elapsed time for mlp_fused_gelu (2048x4x123392): 0.0034
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2579
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 241.815
Elapsed time for transformer_add_bias_dropout (2048x4x30848): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30848): 0.0012

Attention duration (in seconds): 0.3395
Attention throughput (in TFLOP/s): 189.772
MLP duration (in seconds): 0.5070
MLP throughput (in TFLOP/s): 246.020
Transformer duration (in seconds): 0.8535
Transformer throughput (in TFLOP/s): 221.635
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1842
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 256.074
Elapsed time for attention_key_query_prob (512x2048x242x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x242x2048): 101.175
Elapsed time for attention_prob_times_values (512x2048x2048x242): 0.0074
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x242): 141.290
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0608
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 258.763
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.2483
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 253.202
Elapsed time for mlp_fused_gelu (2048x4x123904): 0.0034
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 242.435
Elapsed time for transformer_add_bias_dropout (2048x4x30976): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30976): 0.0012

Attention duration (in seconds): 0.3325
Attention throughput (in TFLOP/s): 195.368
MLP duration (in seconds): 0.5111
MLP throughput (in TFLOP/s): 246.058
Transformer duration (in seconds): 0.8506
Transformer throughput (in TFLOP/s): 224.217
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 128, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.1862
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 255.426
Elapsed time for attention_key_query_prob (512x2048x243x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x243x2048): 70.705
Elapsed time for attention_prob_times_values (512x2048x2048x243): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x243): 90.328
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.0611
Throughput (in TFLOP/s) for attention_linear_projection (4x31104x31104, b=2048): 259.511
Elapsed time for mlp_h_to_4h (4x31104x124416, b=2048): 0.2504
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x124416, b=2048): 253.209
Elapsed time for mlp_fused_gelu (2048x4x124416): 0.0034
Elapsed time for mlp_4h_to_h (4x124416x31104, b=2048): 0.2635
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124416x31104, b=2048): 240.598
Elapsed time for transformer_add_bias_dropout (2048x4x31104): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31104): 0.0013

Attention duration (in seconds): 0.3435
Attention throughput (in TFLOP/s): 190.651
MLP duration (in seconds): 0.5173
MLP throughput (in TFLOP/s): 245.120
Transformer duration (in seconds): 0.8679
Transformer throughput (in TFLOP/s): 221.565
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x93696, b=2048): 0.1874
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x93696, b=2048): 255.894
Elapsed time for attention_key_query_prob (512x2048x244x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x244x2048): 102.570
Elapsed time for attention_prob_times_values (512x2048x2048x244): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x244): 142.607
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x31232x31232, b=2048): 0.0620
Throughput (in TFLOP/s) for attention_linear_projection (4x31232x31232, b=2048): 257.866
Elapsed time for mlp_h_to_4h (4x31232x124928, b=2048): 0.2529
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x124928, b=2048): 252.731
Elapsed time for mlp_fused_gelu (2048x4x124928): 0.0034
Elapsed time for mlp_4h_to_h (4x124928x31232, b=2048): 0.2648
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124928x31232, b=2048): 241.407
Elapsed time for transformer_add_bias_dropout (2048x4x31232): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31232): 0.0013

Attention duration (in seconds): 0.3369
Attention throughput (in TFLOP/s): 195.986
MLP duration (in seconds): 0.5212
MLP throughput (in TFLOP/s): 245.319
Transformer duration (in seconds): 0.8651
Transformer throughput (in TFLOP/s): 224.103
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x94080, b=2048): 0.1897
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x94080, b=2048): 254.761
Elapsed time for attention_key_query_prob (512x2048x245x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x245x2048): 71.487
Elapsed time for attention_prob_times_values (512x2048x2048x245): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x245): 91.056
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x31360x31360, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_linear_projection (4x31360x31360, b=2048): 258.073
Elapsed time for mlp_h_to_4h (4x31360x125440, b=2048): 0.2543
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x125440, b=2048): 253.478
Elapsed time for mlp_fused_gelu (2048x4x125440): 0.0034
Elapsed time for mlp_4h_to_h (4x125440x31360, b=2048): 0.2666
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125440x31360, b=2048): 241.749
Elapsed time for transformer_add_bias_dropout (2048x4x31360): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31360): 0.0013

Attention duration (in seconds): 0.3484
Attention throughput (in TFLOP/s): 191.030
MLP duration (in seconds): 0.5243
MLP throughput (in TFLOP/s): 245.853
Transformer duration (in seconds): 0.8798
Transformer throughput (in TFLOP/s): 222.158
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x94464, b=2048): 0.1907
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x94464, b=2048): 255.524
Elapsed time for attention_key_query_prob (512x2048x246x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x246x2048): 103.018
Elapsed time for attention_prob_times_values (512x2048x2048x246): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x246): 144.598
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x31488x31488, b=2048): 0.0626
Throughput (in TFLOP/s) for attention_linear_projection (4x31488x31488, b=2048): 259.689
Elapsed time for mlp_h_to_4h (4x31488x125952, b=2048): 0.2573
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x125952, b=2048): 252.537
Elapsed time for mlp_fused_gelu (2048x4x125952): 0.0034
Elapsed time for mlp_4h_to_h (4x125952x31488, b=2048): 0.2689
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125952x31488, b=2048): 241.650
Elapsed time for transformer_add_bias_dropout (2048x4x31488): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31488): 0.0013

Attention duration (in seconds): 0.3408
Attention throughput (in TFLOP/s): 196.862
MLP duration (in seconds): 0.5296
MLP throughput (in TFLOP/s): 245.368
Transformer duration (in seconds): 0.8775
Transformer throughput (in TFLOP/s): 224.547
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x94848, b=2048): 0.1920
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x94848, b=2048): 255.876
Elapsed time for attention_key_query_prob (512x2048x247x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x247x2048): 70.838
Elapsed time for attention_prob_times_values (512x2048x2048x247): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x247): 90.611
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x31616x31616, b=2048): 0.0635
Throughput (in TFLOP/s) for attention_linear_projection (4x31616x31616, b=2048): 257.826
Elapsed time for mlp_h_to_4h (4x31616x126464, b=2048): 0.2599
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x126464, b=2048): 252.091
Elapsed time for mlp_fused_gelu (2048x4x126464): 0.0035
Elapsed time for mlp_4h_to_h (4x126464x31616, b=2048): 0.2717
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126464x31616, b=2048): 241.102
Elapsed time for transformer_add_bias_dropout (2048x4x31616): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31616): 0.0013

Attention duration (in seconds): 0.3522
Attention throughput (in TFLOP/s): 192.044
MLP duration (in seconds): 0.5350
MLP throughput (in TFLOP/s): 244.880
Transformer duration (in seconds): 0.8943
Transformer throughput (in TFLOP/s): 222.118
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 128, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.1947
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 254.338
Elapsed time for attention_key_query_prob (512x2048x248x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x248x2048): 129.589
Elapsed time for attention_prob_times_values (512x2048x2048x248): 0.0049
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x248): 217.566
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.0641
Throughput (in TFLOP/s) for attention_linear_projection (4x31744x31744, b=2048): 257.751
Elapsed time for mlp_h_to_4h (4x31744x126976, b=2048): 0.2621
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x126976, b=2048): 251.961
Elapsed time for mlp_fused_gelu (2048x4x126976): 0.0035
Elapsed time for mlp_4h_to_h (4x126976x31744, b=2048): 0.2740
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126976x31744, b=2048): 241.057
Elapsed time for transformer_add_bias_dropout (2048x4x31744): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31744): 0.0013

Attention duration (in seconds): 0.3419
Attention throughput (in TFLOP/s): 199.408
MLP duration (in seconds): 0.5395
MLP throughput (in TFLOP/s): 244.803
Transformer duration (in seconds): 0.8886
Transformer throughput (in TFLOP/s): 225.360
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 128, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x95616, b=2048): 0.1957
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x95616, b=2048): 255.174
Elapsed time for attention_key_query_prob (512x2048x249x2048): 0.0150
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x249x2048): 71.486
Elapsed time for attention_prob_times_values (512x2048x2048x249): 0.0117
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x249): 91.402
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x31872x31872, b=2048): 0.0644
Throughput (in TFLOP/s) for attention_linear_projection (4x31872x31872, b=2048): 258.527
Elapsed time for mlp_h_to_4h (4x31872x127488, b=2048): 0.2638
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x127488, b=2048): 252.324
Elapsed time for mlp_fused_gelu (2048x4x127488): 0.0035
Elapsed time for mlp_4h_to_h (4x127488x31872, b=2048): 0.2758
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127488x31872, b=2048): 241.369
Elapsed time for transformer_add_bias_dropout (2048x4x31872): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31872): 0.0013

Attention duration (in seconds): 0.3567
Attention throughput (in TFLOP/s): 192.657
MLP duration (in seconds): 0.5431
MLP throughput (in TFLOP/s): 245.141
Transformer duration (in seconds): 0.9071
Transformer throughput (in TFLOP/s): 222.543
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x96000, b=2048): 0.1970
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x96000, b=2048): 255.500
Elapsed time for attention_key_query_prob (512x2048x250x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x250x2048): 105.366
Elapsed time for attention_prob_times_values (512x2048x2048x250): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x250): 147.108
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x32000x32000, b=2048): 0.0648
Throughput (in TFLOP/s) for attention_linear_projection (4x32000x32000, b=2048): 259.029
Elapsed time for mlp_h_to_4h (4x32000x128000, b=2048): 0.2649
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x128000, b=2048): 253.353
Elapsed time for mlp_fused_gelu (2048x4x128000): 0.0035
Elapsed time for mlp_4h_to_h (4x128000x32000, b=2048): 0.2777
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128000x32000, b=2048): 241.636
Elapsed time for transformer_add_bias_dropout (2048x4x32000): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32000): 0.0013

Attention duration (in seconds): 0.3492
Attention throughput (in TFLOP/s): 198.319
MLP duration (in seconds): 0.5461
MLP throughput (in TFLOP/s): 245.768
Transformer duration (in seconds): 0.9026
Transformer throughput (in TFLOP/s): 225.431
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x96384, b=2048): 0.1985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x96384, b=2048): 255.593
Elapsed time for attention_key_query_prob (512x2048x251x2048): 0.0148
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x251x2048): 72.886
Elapsed time for attention_prob_times_values (512x2048x2048x251): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x251): 94.431
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x32128x32128, b=2048): 0.0655
Throughput (in TFLOP/s) for attention_linear_projection (4x32128x32128, b=2048): 258.276
Elapsed time for mlp_h_to_4h (4x32128x128512, b=2048): 0.2682
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x128512, b=2048): 252.263
Elapsed time for mlp_fused_gelu (2048x4x128512): 0.0035
Elapsed time for mlp_4h_to_h (4x128512x32128, b=2048): 0.2810
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128512x32128, b=2048): 240.705
Elapsed time for transformer_add_bias_dropout (2048x4x32128): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32128): 0.0013

Attention duration (in seconds): 0.3601
Attention throughput (in TFLOP/s): 193.823
MLP duration (in seconds): 0.5527
MLP throughput (in TFLOP/s): 244.782
Transformer duration (in seconds): 0.9201
Transformer throughput (in TFLOP/s): 222.902
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x96768, b=2048): 0.1996
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x96768, b=2048): 256.239
Elapsed time for attention_key_query_prob (512x2048x252x2048): 0.0102
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x252x2048): 106.309
Elapsed time for attention_prob_times_values (512x2048x2048x252): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x252): 148.765
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x32256x32256, b=2048): 0.0656
Throughput (in TFLOP/s) for attention_linear_projection (4x32256x32256, b=2048): 259.740
Elapsed time for mlp_h_to_4h (4x32256x129024, b=2048): 0.2686
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x129024, b=2048): 253.877
Elapsed time for mlp_fused_gelu (2048x4x129024): 0.0035
Elapsed time for mlp_4h_to_h (4x129024x32256, b=2048): 0.2828
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129024x32256, b=2048): 241.153
Elapsed time for transformer_add_bias_dropout (2048x4x32256): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32256): 0.0013

Attention duration (in seconds): 0.3526
Attention throughput (in TFLOP/s): 199.507
MLP duration (in seconds): 0.5549
MLP throughput (in TFLOP/s): 245.777
Transformer duration (in seconds): 0.9148
Transformer throughput (in TFLOP/s): 225.987
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x97152, b=2048): 0.2020
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x97152, b=2048): 255.190
Elapsed time for attention_key_query_prob (512x2048x253x2048): 0.0147
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x253x2048): 73.694
Elapsed time for attention_prob_times_values (512x2048x2048x253): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x253): 95.342
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x32384x32384, b=2048): 0.0664
Throughput (in TFLOP/s) for attention_linear_projection (4x32384x32384, b=2048): 258.763
Elapsed time for mlp_h_to_4h (4x32384x129536, b=2048): 0.2726
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x129536, b=2048): 252.121
Elapsed time for mlp_fused_gelu (2048x4x129536): 0.0035
Elapsed time for mlp_4h_to_h (4x129536x32384, b=2048): 0.2846
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129536x32384, b=2048): 241.507
Elapsed time for transformer_add_bias_dropout (2048x4x32384): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32384): 0.0013

Attention duration (in seconds): 0.3645
Attention throughput (in TFLOP/s): 194.528
MLP duration (in seconds): 0.5607
MLP throughput (in TFLOP/s): 245.141
Transformer duration (in seconds): 0.9325
Transformer throughput (in TFLOP/s): 223.434
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x97536, b=2048): 0.2030
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x97536, b=2048): 255.926
Elapsed time for attention_key_query_prob (512x2048x254x2048): 0.0101
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x254x2048): 107.963
Elapsed time for attention_prob_times_values (512x2048x2048x254): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x254): 150.434
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x32512x32512, b=2048): 0.0673
Throughput (in TFLOP/s) for attention_linear_projection (4x32512x32512, b=2048): 257.391
Elapsed time for mlp_h_to_4h (4x32512x130048, b=2048): 0.2739
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x130048, b=2048): 252.900
Elapsed time for mlp_fused_gelu (2048x4x130048): 0.0036
Elapsed time for mlp_4h_to_h (4x130048x32512, b=2048): 0.2880
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130048x32512, b=2048): 240.565
Elapsed time for transformer_add_bias_dropout (2048x4x32512): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32512): 0.0013

Attention duration (in seconds): 0.3576
Attention throughput (in TFLOP/s): 199.820
MLP duration (in seconds): 0.5654
MLP throughput (in TFLOP/s): 245.027
Transformer duration (in seconds): 0.9304
Transformer throughput (in TFLOP/s): 225.719
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 128, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x97920, b=2048): 0.2062
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x97920, b=2048): 253.911
Elapsed time for attention_key_query_prob (512x2048x255x2048): 0.0149
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x255x2048): 73.626
Elapsed time for attention_prob_times_values (512x2048x2048x255): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x255): 95.095
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x32640x32640, b=2048): 0.0677
Throughput (in TFLOP/s) for attention_linear_projection (4x32640x32640, b=2048): 257.764
Elapsed time for mlp_h_to_4h (4x32640x130560, b=2048): 0.2753
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x130560, b=2048): 253.594
Elapsed time for mlp_fused_gelu (2048x4x130560): 0.0036
Elapsed time for mlp_4h_to_h (4x130560x32640, b=2048): 0.2909
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130560x32640, b=2048): 240.026
Elapsed time for transformer_add_bias_dropout (2048x4x32640): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32640): 0.0013

Attention duration (in seconds): 0.3703
Attention throughput (in TFLOP/s): 194.469
MLP duration (in seconds): 0.5698
MLP throughput (in TFLOP/s): 245.078
Transformer duration (in seconds): 0.9474
Transformer throughput (in TFLOP/s): 223.393
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 128, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32768x98304, b=2048): 0.2073
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32768x98304, b=2048): 254.651
Elapsed time for attention_key_query_prob (512x2048x256x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (512x2048x256x2048): 157.948
Elapsed time for attention_prob_times_values (512x2048x2048x256): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (512x2048x2048x256): 227.006
Elapsed time for attention_dropout (4x128x2048x2048): 0.0091
Elapsed time for attention_softmax (4x128x2048x2048): 0.0609
Elapsed time for attention_linear_projection (4x32768x32768, b=2048): 0.0680
Throughput (in TFLOP/s) for attention_linear_projection (4x32768x32768, b=2048): 258.823
Elapsed time for mlp_h_to_4h (4x32768x131072, b=2048): 0.2801
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32768x131072, b=2048): 251.223
Elapsed time for mlp_fused_gelu (2048x4x131072): 0.0036
Elapsed time for mlp_4h_to_h (4x131072x32768, b=2048): 0.2915
Throughput (in TFLOP/s) for mlp_4h_to_h (4x131072x32768, b=2048): 241.443
Elapsed time for transformer_add_bias_dropout (2048x4x32768): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32768): 0.0013

Attention duration (in seconds): 0.3570
Attention throughput (in TFLOP/s): 203.286
MLP duration (in seconds): 0.5751
MLP throughput (in TFLOP/s): 244.701
Transformer duration (in seconds): 0.9395
Transformer throughput (in TFLOP/s): 227.040
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
[2023-06-09 14:26:36,986] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-09 14:26:37,505] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.129.94, master_port=6000
[2023-06-09 14:26:37,505] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-09 14:26:38,941] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 9728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9728x29184, b=2048): 0.0185
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9728x29184, b=2048): 251.524
Elapsed time for attention_key_query_prob (256x2048x152x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x152x2048): 106.376
Elapsed time for attention_prob_times_values (256x2048x2048x152): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x152): 141.594
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x9728x9728, b=2048): 0.0063
Throughput (in TFLOP/s) for attention_linear_projection (4x9728x9728, b=2048): 245.539
Elapsed time for mlp_h_to_4h (4x9728x38912, b=2048): 0.0247
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9728x38912, b=2048): 251.369
Elapsed time for mlp_fused_gelu (2048x4x38912): 0.0011
Elapsed time for mlp_4h_to_h (4x38912x9728, b=2048): 0.0239
Throughput (in TFLOP/s) for mlp_4h_to_h (4x38912x9728, b=2048): 258.986
Elapsed time for transformer_add_bias_dropout (2048x4x9728): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9728): 0.0004

Attention duration (in seconds): 0.0652
Attention throughput (in TFLOP/s): 105.167
MLP duration (in seconds): 0.0497
MLP throughput (in TFLOP/s): 249.585
Transformer duration (in seconds): 0.1171
Transformer throughput (in TFLOP/s): 164.464
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0374
Attention throughput (in TFLOP/s): 183.433
MLP duration (in seconds): 0.0501
MLP throughput (in TFLOP/s): 247.627
Transformer duration (in seconds): 0.0903
Transformer throughput (in TFLOP/s): 213.377
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 9792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9792x29376, b=2048): 0.0189
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9792x29376, b=2048): 249.679
Elapsed time for attention_key_query_prob (256x2048x153x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x153x2048): 58.884
Elapsed time for attention_prob_times_values (256x2048x2048x153): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x153): 60.244
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x9792x9792, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9792x9792, b=2048): 242.975
Elapsed time for mlp_h_to_4h (4x9792x39168, b=2048): 0.0250
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9792x39168, b=2048): 251.048
Elapsed time for mlp_fused_gelu (2048x4x39168): 0.0011
Elapsed time for mlp_4h_to_h (4x39168x9792, b=2048): 0.0260
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39168x9792, b=2048): 241.796
Elapsed time for transformer_add_bias_dropout (2048x4x9792): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9792): 0.0004

Attention duration (in seconds): 0.0713
Attention throughput (in TFLOP/s): 97.283
MLP duration (in seconds): 0.0521
MLP throughput (in TFLOP/s): 241.206
Transformer duration (in seconds): 0.1257
Transformer throughput (in TFLOP/s): 155.202
Transformer - MLP - Attention (in seconds): 0.0022


Actual
------
Attention duration (in seconds): 0.0442
Attention throughput (in TFLOP/s): 157.124
MLP duration (in seconds): 0.0527
MLP throughput (in TFLOP/s): 238.546
Transformer duration (in seconds): 0.1004
Transformer throughput (in TFLOP/s): 194.291
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 9856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9856x29568, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9856x29568, b=2048): 250.133
Elapsed time for attention_key_query_prob (256x2048x154x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x154x2048): 82.627
Elapsed time for attention_prob_times_values (256x2048x2048x154): 0.0032
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x154): 102.043
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x9856x9856, b=2048): 0.0065
Throughput (in TFLOP/s) for attention_linear_projection (4x9856x9856, b=2048): 244.080
Elapsed time for mlp_h_to_4h (4x9856x39424, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9856x39424, b=2048): 250.726
Elapsed time for mlp_fused_gelu (2048x4x39424): 0.0011
Elapsed time for mlp_4h_to_h (4x39424x9856, b=2048): 0.0263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39424x9856, b=2048): 242.361
Elapsed time for transformer_add_bias_dropout (2048x4x9856): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9856): 0.0004

Attention duration (in seconds): 0.0678
Attention throughput (in TFLOP/s): 103.606
MLP duration (in seconds): 0.0528
MLP throughput (in TFLOP/s): 241.370
Transformer duration (in seconds): 0.1229
Transformer throughput (in TFLOP/s): 160.822
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0399
Attention throughput (in TFLOP/s): 176.141
MLP duration (in seconds): 0.0533
MLP throughput (in TFLOP/s): 238.991
Transformer duration (in seconds): 0.0963
Transformer throughput (in TFLOP/s): 205.257
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 9920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9920x29760, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9920x29760, b=2048): 248.546
Elapsed time for attention_key_query_prob (256x2048x155x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x155x2048): 59.397
Elapsed time for attention_prob_times_values (256x2048x2048x155): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x155): 63.006
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x9920x9920, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x9920x9920, b=2048): 240.928
Elapsed time for mlp_h_to_4h (4x9920x39680, b=2048): 0.0257
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9920x39680, b=2048): 250.588
Elapsed time for mlp_fused_gelu (2048x4x39680): 0.0011
Elapsed time for mlp_4h_to_h (4x39680x9920, b=2048): 0.0254
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39680x9920, b=2048): 254.078
Elapsed time for transformer_add_bias_dropout (2048x4x9920): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9920): 0.0004

Attention duration (in seconds): 0.0720
Attention throughput (in TFLOP/s): 98.791
MLP duration (in seconds): 0.0522
MLP throughput (in TFLOP/s): 247.011
Transformer duration (in seconds): 0.1265
Transformer throughput (in TFLOP/s): 158.181
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0451
Attention throughput (in TFLOP/s): 157.876
MLP duration (in seconds): 0.0528
MLP throughput (in TFLOP/s): 244.080
Transformer duration (in seconds): 0.1014
Transformer throughput (in TFLOP/s): 197.347
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 9984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x9984x29952, b=2048): 0.0197
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x9984x29952, b=2048): 248.334
Elapsed time for attention_key_query_prob (256x2048x156x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x156x2048): 83.194
Elapsed time for attention_prob_times_values (256x2048x2048x156): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x156): 103.056
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x9984x9984, b=2048): 0.0067
Throughput (in TFLOP/s) for attention_linear_projection (4x9984x9984, b=2048): 242.562
Elapsed time for mlp_h_to_4h (4x9984x39936, b=2048): 0.0261
Throughput (in TFLOP/s) for mlp_h_to_4h (4x9984x39936, b=2048): 250.486
Elapsed time for mlp_fused_gelu (2048x4x39936): 0.0011
Elapsed time for mlp_4h_to_h (4x39936x9984, b=2048): 0.0255
Throughput (in TFLOP/s) for mlp_4h_to_h (4x39936x9984, b=2048): 256.339
Elapsed time for transformer_add_bias_dropout (2048x4x9984): 0.0007
Elapsed time for transformer_layer_norm (2048x4x9984): 0.0004

Attention duration (in seconds): 0.0687
Attention throughput (in TFLOP/s): 104.813
MLP duration (in seconds): 0.0527
MLP throughput (in TFLOP/s): 248.061
Transformer duration (in seconds): 0.1237
Transformer throughput (in TFLOP/s): 163.889
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0407
Attention throughput (in TFLOP/s): 177.124
MLP duration (in seconds): 0.0534
MLP throughput (in TFLOP/s): 244.883
Transformer duration (in seconds): 0.0962
Transformer throughput (in TFLOP/s): 210.651
Transformer - MLP - Attention (in seconds): 0.0022
========================================================================================================================
num_attention_heads: 64, hidden_size: 10048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10048x30144, b=2048): 0.0198
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10048x30144, b=2048): 250.505
Elapsed time for attention_key_query_prob (256x2048x157x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x157x2048): 59.933
Elapsed time for attention_prob_times_values (256x2048x2048x157): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x157): 63.724
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10048x10048, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x10048x10048, b=2048): 243.334
Elapsed time for mlp_h_to_4h (4x10048x40192, b=2048): 0.0263
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10048x40192, b=2048): 251.355
Elapsed time for mlp_fused_gelu (2048x4x40192): 0.0011
Elapsed time for mlp_4h_to_h (4x40192x10048, b=2048): 0.0274
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40192x10048, b=2048): 241.548
Elapsed time for transformer_add_bias_dropout (2048x4x10048): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10048): 0.0004

Attention duration (in seconds): 0.0725
Attention throughput (in TFLOP/s): 100.569
MLP duration (in seconds): 0.0548
MLP throughput (in TFLOP/s): 241.355
Transformer duration (in seconds): 0.1296
Transformer throughput (in TFLOP/s): 158.326
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0454
Attention throughput (in TFLOP/s): 160.647
MLP duration (in seconds): 0.0554
MLP throughput (in TFLOP/s): 238.893
Transformer duration (in seconds): 0.1040
Transformer throughput (in TFLOP/s): 197.335
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 10112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10112x30336, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10112x30336, b=2048): 250.910
Elapsed time for attention_key_query_prob (256x2048x158x2048): 0.0040
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x158x2048): 84.384
Elapsed time for attention_prob_times_values (256x2048x2048x158): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x158): 103.877
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10112x10112, b=2048): 0.0068
Throughput (in TFLOP/s) for attention_linear_projection (4x10112x10112, b=2048): 244.979
Elapsed time for mlp_h_to_4h (4x10112x40448, b=2048): 0.0267
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10112x40448, b=2048): 251.420
Elapsed time for mlp_fused_gelu (2048x4x40448): 0.0011
Elapsed time for mlp_4h_to_h (4x40448x10112, b=2048): 0.0276
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40448x10112, b=2048): 242.435
Elapsed time for transformer_add_bias_dropout (2048x4x10112): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10112): 0.0004

Attention duration (in seconds): 0.0691
Attention throughput (in TFLOP/s): 106.738
MLP duration (in seconds): 0.0554
MLP throughput (in TFLOP/s): 241.856
Transformer duration (in seconds): 0.1269
Transformer throughput (in TFLOP/s): 163.823
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0406
Attention throughput (in TFLOP/s): 181.907
MLP duration (in seconds): 0.0560
MLP throughput (in TFLOP/s): 239.533
Transformer duration (in seconds): 0.1001
Transformer throughput (in TFLOP/s): 207.655
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 10176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10176x30528, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10176x30528, b=2048): 250.424
Elapsed time for attention_key_query_prob (256x2048x159x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x159x2048): 60.806
Elapsed time for attention_prob_times_values (256x2048x2048x159): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x159): 63.374
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10176x10176, b=2048): 0.0070
Throughput (in TFLOP/s) for attention_linear_projection (4x10176x10176, b=2048): 242.069
Elapsed time for mlp_h_to_4h (4x10176x40704, b=2048): 0.0270
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10176x40704, b=2048): 251.281
Elapsed time for mlp_fused_gelu (2048x4x40704): 0.0011
Elapsed time for mlp_4h_to_h (4x40704x10176, b=2048): 0.0265
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40704x10176, b=2048): 256.335
Elapsed time for transformer_add_bias_dropout (2048x4x10176): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10176): 0.0004

Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 101.885
MLP duration (in seconds): 0.0546
MLP throughput (in TFLOP/s): 248.551
Transformer duration (in seconds): 0.1303
Transformer throughput (in TFLOP/s): 161.547
Transformer - MLP - Attention (in seconds): 0.0023


Actual
------
Attention duration (in seconds): 0.0462
Attention throughput (in TFLOP/s): 161.636
MLP duration (in seconds): 0.0555
MLP throughput (in TFLOP/s): 244.769
Transformer duration (in seconds): 0.1049
Transformer throughput (in TFLOP/s): 200.676
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 10240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10240x30720, b=2048): 0.0207
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10240x30720, b=2048): 249.565
Elapsed time for attention_key_query_prob (256x2048x160x2048): 0.0028
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x160x2048): 124.642
Elapsed time for attention_prob_times_values (256x2048x2048x160): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x160): 150.288
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10240x10240, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x10240x10240, b=2048): 243.538
Elapsed time for mlp_h_to_4h (4x10240x40960, b=2048): 0.0273
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10240x40960, b=2048): 251.550
Elapsed time for mlp_fused_gelu (2048x4x40960): 0.0011
Elapsed time for mlp_4h_to_h (4x40960x10240, b=2048): 0.0269
Throughput (in TFLOP/s) for mlp_4h_to_h (4x40960x10240, b=2048): 255.672
Elapsed time for transformer_add_bias_dropout (2048x4x10240): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10240): 0.0004

Attention duration (in seconds): 0.0677
Attention throughput (in TFLOP/s): 111.609
MLP duration (in seconds): 0.0553
MLP throughput (in TFLOP/s): 248.394
Transformer duration (in seconds): 0.1254
Transformer throughput (in TFLOP/s): 169.857
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0405
Attention throughput (in TFLOP/s): 186.556
MLP duration (in seconds): 0.0560
MLP throughput (in TFLOP/s): 245.407
Transformer duration (in seconds): 0.0990
Transformer throughput (in TFLOP/s): 215.249
Transformer - MLP - Attention (in seconds): 0.0024
========================================================================================================================
num_attention_heads: 64, hidden_size: 10304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10304x30912, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10304x30912, b=2048): 248.915
Elapsed time for attention_key_query_prob (256x2048x161x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x161x2048): 58.133
Elapsed time for attention_prob_times_values (256x2048x2048x161): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x161): 63.678
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10304x10304, b=2048): 0.0071
Throughput (in TFLOP/s) for attention_linear_projection (4x10304x10304, b=2048): 243.317
Elapsed time for mlp_h_to_4h (4x10304x41216, b=2048): 0.0276
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10304x41216, b=2048): 251.772
Elapsed time for mlp_fused_gelu (2048x4x41216): 0.0011
Elapsed time for mlp_4h_to_h (4x41216x10304, b=2048): 0.0287
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41216x10304, b=2048): 242.771
Elapsed time for transformer_add_bias_dropout (2048x4x10304): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10304): 0.0004

Attention duration (in seconds): 0.0745
Attention throughput (in TFLOP/s): 102.716
MLP duration (in seconds): 0.0574
MLP throughput (in TFLOP/s): 242.280
Transformer duration (in seconds): 0.1343
Transformer throughput (in TFLOP/s): 160.601
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0473
Attention throughput (in TFLOP/s): 161.748
MLP duration (in seconds): 0.0580
MLP throughput (in TFLOP/s): 239.771
Transformer duration (in seconds): 0.1089
Transformer throughput (in TFLOP/s): 198.045
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 10368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10368x31104, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10368x31104, b=2048): 251.448
Elapsed time for attention_key_query_prob (256x2048x162x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x162x2048): 80.429
Elapsed time for attention_prob_times_values (256x2048x2048x162): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x162): 106.523
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x10368x10368, b=2048): 0.0072
Throughput (in TFLOP/s) for attention_linear_projection (4x10368x10368, b=2048): 244.712
Elapsed time for mlp_h_to_4h (4x10368x41472, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10368x41472, b=2048): 251.834
Elapsed time for mlp_fused_gelu (2048x4x41472): 0.0011
Elapsed time for mlp_4h_to_h (4x41472x10368, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41472x10368, b=2048): 243.760
Elapsed time for transformer_add_bias_dropout (2048x4x10368): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10368): 0.0004

Attention duration (in seconds): 0.0708
Attention throughput (in TFLOP/s): 109.369
MLP duration (in seconds): 0.0580
MLP throughput (in TFLOP/s): 242.828
Transformer duration (in seconds): 0.1312
Transformer throughput (in TFLOP/s): 166.414
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0427
Attention throughput (in TFLOP/s): 181.380
MLP duration (in seconds): 0.0587
MLP throughput (in TFLOP/s): 239.897
Transformer duration (in seconds): 0.1048
Transformer throughput (in TFLOP/s): 208.337
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 10432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10432x31296, b=2048): 0.0214
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10432x31296, b=2048): 249.482
Elapsed time for attention_key_query_prob (256x2048x163x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x163x2048): 58.197
Elapsed time for attention_prob_times_values (256x2048x2048x163): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x163): 65.637
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10432x10432, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10432x10432, b=2048): 242.308
Elapsed time for mlp_h_to_4h (4x10432x41728, b=2048): 0.0285
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10432x41728, b=2048): 250.305
Elapsed time for mlp_fused_gelu (2048x4x41728): 0.0012
Elapsed time for mlp_4h_to_h (4x41728x10432, b=2048): 0.0280
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41728x10432, b=2048): 254.522
Elapsed time for transformer_add_bias_dropout (2048x4x10432): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10432): 0.0004

Attention duration (in seconds): 0.0751
Attention throughput (in TFLOP/s): 104.264
MLP duration (in seconds): 0.0577
MLP throughput (in TFLOP/s): 247.341
Transformer duration (in seconds): 0.1352
Transformer throughput (in TFLOP/s): 163.439
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0483
Attention throughput (in TFLOP/s): 161.995
MLP duration (in seconds): 0.0583
MLP throughput (in TFLOP/s): 244.541
Transformer duration (in seconds): 0.1101
Transformer throughput (in TFLOP/s): 200.774
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 10496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10496x31488, b=2048): 0.0218
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10496x31488, b=2048): 248.660
Elapsed time for attention_key_query_prob (256x2048x164x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x164x2048): 81.147
Elapsed time for attention_prob_times_values (256x2048x2048x164): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x164): 107.197
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x10496x10496, b=2048): 0.0074
Throughput (in TFLOP/s) for attention_linear_projection (4x10496x10496, b=2048): 243.706
Elapsed time for mlp_h_to_4h (4x10496x41984, b=2048): 0.0289
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10496x41984, b=2048): 250.014
Elapsed time for mlp_fused_gelu (2048x4x41984): 0.0012
Elapsed time for mlp_4h_to_h (4x41984x10496, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x41984x10496, b=2048): 242.774
Elapsed time for transformer_add_bias_dropout (2048x4x10496): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10496): 0.0004

Attention duration (in seconds): 0.0718
Attention throughput (in TFLOP/s): 110.392
MLP duration (in seconds): 0.0598
MLP throughput (in TFLOP/s): 241.556
Transformer duration (in seconds): 0.1339
Transformer throughput (in TFLOP/s): 166.958
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0434
Attention throughput (in TFLOP/s): 182.459
MLP duration (in seconds): 0.0604
MLP throughput (in TFLOP/s): 239.048
Transformer duration (in seconds): 0.1074
Transformer throughput (in TFLOP/s): 208.210
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 10560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10560x31680, b=2048): 0.0220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10560x31680, b=2048): 249.253
Elapsed time for attention_key_query_prob (256x2048x165x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x165x2048): 58.785
Elapsed time for attention_prob_times_values (256x2048x2048x165): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x165): 66.198
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10560x10560, b=2048): 0.0075
Throughput (in TFLOP/s) for attention_linear_projection (4x10560x10560, b=2048): 242.823
Elapsed time for mlp_h_to_4h (4x10560x42240, b=2048): 0.0290
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10560x42240, b=2048): 251.861
Elapsed time for mlp_fused_gelu (2048x4x42240): 0.0012
Elapsed time for mlp_4h_to_h (4x42240x10560, b=2048): 0.0302
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42240x10560, b=2048): 242.328
Elapsed time for transformer_add_bias_dropout (2048x4x10560): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10560): 0.0004

Attention duration (in seconds): 0.0759
Attention throughput (in TFLOP/s): 105.657
MLP duration (in seconds): 0.0603
MLP throughput (in TFLOP/s): 242.220
Transformer duration (in seconds): 0.1387
Transformer throughput (in TFLOP/s): 163.230
Transformer - MLP - Attention (in seconds): 0.0024


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 163.279
MLP duration (in seconds): 0.0609
MLP throughput (in TFLOP/s): 240.009
Transformer duration (in seconds): 0.1144
Transformer throughput (in TFLOP/s): 197.816
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 10624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10624x31872, b=2048): 0.0222
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10624x31872, b=2048): 250.359
Elapsed time for attention_key_query_prob (256x2048x166x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x166x2048): 81.953
Elapsed time for attention_prob_times_values (256x2048x2048x166): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x166): 108.556
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10624x10624, b=2048): 0.0076
Throughput (in TFLOP/s) for attention_linear_projection (4x10624x10624, b=2048): 244.290
Elapsed time for mlp_h_to_4h (4x10624x42496, b=2048): 0.0294
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10624x42496, b=2048): 251.649
Elapsed time for mlp_fused_gelu (2048x4x42496): 0.0012
Elapsed time for mlp_4h_to_h (4x42496x10624, b=2048): 0.0303
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42496x10624, b=2048): 244.085
Elapsed time for transformer_add_bias_dropout (2048x4x10624): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10624): 0.0004

Attention duration (in seconds): 0.0723
Attention throughput (in TFLOP/s): 112.099
MLP duration (in seconds): 0.0609
MLP throughput (in TFLOP/s): 243.025
Transformer duration (in seconds): 0.1357
Transformer throughput (in TFLOP/s): 168.808
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0441
Attention throughput (in TFLOP/s): 183.918
MLP duration (in seconds): 0.0615
MLP throughput (in TFLOP/s): 240.447
Transformer duration (in seconds): 0.1093
Transformer throughput (in TFLOP/s): 209.606
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 10688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10688x32064, b=2048): 0.0224
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10688x32064, b=2048): 250.397
Elapsed time for attention_key_query_prob (256x2048x167x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x167x2048): 59.241
Elapsed time for attention_prob_times_values (256x2048x2048x167): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x167): 64.922
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10688x10688, b=2048): 0.0077
Throughput (in TFLOP/s) for attention_linear_projection (4x10688x10688, b=2048): 243.663
Elapsed time for mlp_h_to_4h (4x10688x42752, b=2048): 0.0298
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10688x42752, b=2048): 250.971
Elapsed time for mlp_fused_gelu (2048x4x42752): 0.0012
Elapsed time for mlp_4h_to_h (4x42752x10688, b=2048): 0.0293
Throughput (in TFLOP/s) for mlp_4h_to_h (4x42752x10688, b=2048): 255.301
Elapsed time for transformer_add_bias_dropout (2048x4x10688): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10688): 0.0004

Attention duration (in seconds): 0.0767
Attention throughput (in TFLOP/s): 107.018
MLP duration (in seconds): 0.0603
MLP throughput (in TFLOP/s): 248.159
Transformer duration (in seconds): 0.1394
Transformer throughput (in TFLOP/s): 166.200
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0501
Attention throughput (in TFLOP/s): 163.814
MLP duration (in seconds): 0.0611
MLP throughput (in TFLOP/s): 245.104
Transformer duration (in seconds): 0.1147
Transformer throughput (in TFLOP/s): 202.053
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 10752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10752x32256, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10752x32256, b=2048): 249.232
Elapsed time for attention_key_query_prob (256x2048x168x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x168x2048): 110.871
Elapsed time for attention_prob_times_values (256x2048x2048x168): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x168): 154.354
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x10752x10752, b=2048): 0.0078
Throughput (in TFLOP/s) for attention_linear_projection (4x10752x10752, b=2048): 244.175
Elapsed time for mlp_h_to_4h (4x10752x43008, b=2048): 0.0303
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10752x43008, b=2048): 249.797
Elapsed time for mlp_fused_gelu (2048x4x43008): 0.0012
Elapsed time for mlp_4h_to_h (4x43008x10752, b=2048): 0.0297
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43008x10752, b=2048): 255.250
Elapsed time for transformer_add_bias_dropout (2048x4x10752): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10752): 0.0004

Attention duration (in seconds): 0.0711
Attention throughput (in TFLOP/s): 116.658
MLP duration (in seconds): 0.0612
MLP throughput (in TFLOP/s): 247.588
Transformer duration (in seconds): 0.1348
Transformer throughput (in TFLOP/s): 173.976
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0437
Attention throughput (in TFLOP/s): 189.700
MLP duration (in seconds): 0.0615
MLP throughput (in TFLOP/s): 246.383
Transformer duration (in seconds): 0.1080
Transformer throughput (in TFLOP/s): 217.131
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 10816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10816x32448, b=2048): 0.0229
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10816x32448, b=2048): 250.808
Elapsed time for attention_key_query_prob (256x2048x169x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x169x2048): 59.779
Elapsed time for attention_prob_times_values (256x2048x2048x169): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x169): 65.538
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10816x10816, b=2048): 0.0079
Throughput (in TFLOP/s) for attention_linear_projection (4x10816x10816, b=2048): 243.522
Elapsed time for mlp_h_to_4h (4x10816x43264, b=2048): 0.0304
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10816x43264, b=2048): 251.873
Elapsed time for mlp_fused_gelu (2048x4x43264): 0.0012
Elapsed time for mlp_4h_to_h (4x43264x10816, b=2048): 0.0317
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43264x10816, b=2048): 242.066
Elapsed time for transformer_add_bias_dropout (2048x4x10816): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10816): 0.0004

Attention duration (in seconds): 0.0774
Attention throughput (in TFLOP/s): 108.447
MLP duration (in seconds): 0.0633
MLP throughput (in TFLOP/s): 242.207
Transformer duration (in seconds): 0.1432
Transformer throughput (in TFLOP/s): 165.703
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0508
Attention throughput (in TFLOP/s): 165.263
MLP duration (in seconds): 0.0641
MLP throughput (in TFLOP/s): 239.059
Transformer duration (in seconds): 0.1190
Transformer throughput (in TFLOP/s): 199.381
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 10880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10880x32640, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10880x32640, b=2048): 250.112
Elapsed time for attention_key_query_prob (256x2048x170x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x170x2048): 84.189
Elapsed time for attention_prob_times_values (256x2048x2048x170): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x170): 110.667
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x10880x10880, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10880x10880, b=2048): 243.858
Elapsed time for mlp_h_to_4h (4x10880x43520, b=2048): 0.0309
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10880x43520, b=2048): 250.855
Elapsed time for mlp_fused_gelu (2048x4x43520): 0.0012
Elapsed time for mlp_4h_to_h (4x43520x10880, b=2048): 0.0322
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43520x10880, b=2048): 240.784
Elapsed time for transformer_add_bias_dropout (2048x4x10880): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10880): 0.0004

Attention duration (in seconds): 0.0738
Attention throughput (in TFLOP/s): 114.967
MLP duration (in seconds): 0.0643
MLP throughput (in TFLOP/s): 241.124
Transformer duration (in seconds): 0.1407
Transformer throughput (in TFLOP/s): 170.646
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0461
Attention throughput (in TFLOP/s): 184.127
MLP duration (in seconds): 0.0649
MLP throughput (in TFLOP/s): 238.923
Transformer duration (in seconds): 0.1149
Transformer throughput (in TFLOP/s): 208.985
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 10944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x10944x32832, b=2048): 0.0237
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x10944x32832, b=2048): 248.490
Elapsed time for attention_key_query_prob (256x2048x171x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x171x2048): 60.339
Elapsed time for attention_prob_times_values (256x2048x2048x171): 0.0053
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x171): 68.740
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x10944x10944, b=2048): 0.0080
Throughput (in TFLOP/s) for attention_linear_projection (4x10944x10944, b=2048): 243.827
Elapsed time for mlp_h_to_4h (4x10944x43776, b=2048): 0.0315
Throughput (in TFLOP/s) for mlp_h_to_4h (4x10944x43776, b=2048): 249.384
Elapsed time for mlp_fused_gelu (2048x4x43776): 0.0012
Elapsed time for mlp_4h_to_h (4x43776x10944, b=2048): 0.0311
Throughput (in TFLOP/s) for mlp_4h_to_h (4x43776x10944, b=2048): 252.779
Elapsed time for transformer_add_bias_dropout (2048x4x10944): 0.0008
Elapsed time for transformer_layer_norm (2048x4x10944): 0.0004

Attention duration (in seconds): 0.0782
Attention throughput (in TFLOP/s): 109.824
MLP duration (in seconds): 0.0637
MLP throughput (in TFLOP/s): 246.302
Transformer duration (in seconds): 0.1444
Transformer throughput (in TFLOP/s): 168.148
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0517
Attention throughput (in TFLOP/s): 166.113
MLP duration (in seconds): 0.0644
MLP throughput (in TFLOP/s): 243.628
Transformer duration (in seconds): 0.1196
Transformer throughput (in TFLOP/s): 202.983
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 11008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11008x33024, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11008x33024, b=2048): 248.112
Elapsed time for attention_key_query_prob (256x2048x172x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x172x2048): 84.684
Elapsed time for attention_prob_times_values (256x2048x2048x172): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x172): 111.919
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x11008x11008, b=2048): 0.0081
Throughput (in TFLOP/s) for attention_linear_projection (4x11008x11008, b=2048): 244.306
Elapsed time for mlp_h_to_4h (4x11008x44032, b=2048): 0.0318
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11008x44032, b=2048): 249.976
Elapsed time for mlp_fused_gelu (2048x4x44032): 0.0012
Elapsed time for mlp_4h_to_h (4x44032x11008, b=2048): 0.0312
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44032x11008, b=2048): 254.299
Elapsed time for transformer_add_bias_dropout (2048x4x11008): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11008): 0.0004

Attention duration (in seconds): 0.0748
Attention throughput (in TFLOP/s): 116.079
MLP duration (in seconds): 0.0642
MLP throughput (in TFLOP/s): 247.340
Transformer duration (in seconds): 0.1415
Transformer throughput (in TFLOP/s): 173.568
Transformer - MLP - Attention (in seconds): 0.0025


Actual
------
Attention duration (in seconds): 0.0466
Attention throughput (in TFLOP/s): 186.282
MLP duration (in seconds): 0.0650
MLP throughput (in TFLOP/s): 244.525
Transformer duration (in seconds): 0.1149
Transformer throughput (in TFLOP/s): 213.799
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 11072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11072x33216, b=2048): 0.0243
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11072x33216, b=2048): 248.097
Elapsed time for attention_key_query_prob (256x2048x173x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x173x2048): 60.771
Elapsed time for attention_prob_times_values (256x2048x2048x173): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x173): 69.165
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11072x11072, b=2048): 0.0082
Throughput (in TFLOP/s) for attention_linear_projection (4x11072x11072, b=2048): 243.488
Elapsed time for mlp_h_to_4h (4x11072x44288, b=2048): 0.0322
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11072x44288, b=2048): 249.843
Elapsed time for mlp_fused_gelu (2048x4x44288): 0.0012
Elapsed time for mlp_4h_to_h (4x44288x11072, b=2048): 0.0333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44288x11072, b=2048): 241.243
Elapsed time for transformer_add_bias_dropout (2048x4x11072): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11072): 0.0005

Attention duration (in seconds): 0.0790
Attention throughput (in TFLOP/s): 111.090
MLP duration (in seconds): 0.0667
MLP throughput (in TFLOP/s): 240.963
Transformer duration (in seconds): 0.1483
Transformer throughput (in TFLOP/s): 167.589
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0524
Attention throughput (in TFLOP/s): 167.595
MLP duration (in seconds): 0.0673
MLP throughput (in TFLOP/s): 238.895
Transformer duration (in seconds): 0.1233
Transformer throughput (in TFLOP/s): 201.453
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 11136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11136x33408, b=2048): 0.0242
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11136x33408, b=2048): 251.668
Elapsed time for attention_key_query_prob (256x2048x174x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x174x2048): 85.273
Elapsed time for attention_prob_times_values (256x2048x2048x174): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x174): 112.864
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x11136x11136, b=2048): 0.0083
Throughput (in TFLOP/s) for attention_linear_projection (4x11136x11136, b=2048): 245.745
Elapsed time for mlp_h_to_4h (4x11136x44544, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11136x44544, b=2048): 251.952
Elapsed time for mlp_fused_gelu (2048x4x44544): 0.0012
Elapsed time for mlp_4h_to_h (4x44544x11136, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44544x11136, b=2048): 243.200
Elapsed time for transformer_add_bias_dropout (2048x4x11136): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11136): 0.0005

Attention duration (in seconds): 0.0752
Attention throughput (in TFLOP/s): 118.076
MLP duration (in seconds): 0.0669
MLP throughput (in TFLOP/s): 242.944
Transformer duration (in seconds): 0.1446
Transformer throughput (in TFLOP/s): 173.747
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0473
Attention throughput (in TFLOP/s): 187.718
MLP duration (in seconds): 0.0677
MLP throughput (in TFLOP/s): 240.219
Transformer duration (in seconds): 0.1187
Transformer throughput (in TFLOP/s): 211.714
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 11200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11200x33600, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11200x33600, b=2048): 251.257
Elapsed time for attention_key_query_prob (256x2048x175x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x175x2048): 61.054
Elapsed time for attention_prob_times_values (256x2048x2048x175): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x175): 67.774
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11200x11200, b=2048): 0.0084
Throughput (in TFLOP/s) for attention_linear_projection (4x11200x11200, b=2048): 243.945
Elapsed time for mlp_h_to_4h (4x11200x44800, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11200x44800, b=2048): 251.131
Elapsed time for mlp_fused_gelu (2048x4x44800): 0.0012
Elapsed time for mlp_4h_to_h (4x44800x11200, b=2048): 0.0323
Throughput (in TFLOP/s) for mlp_4h_to_h (4x44800x11200, b=2048): 254.165
Elapsed time for transformer_add_bias_dropout (2048x4x11200): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11200): 0.0005

Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 112.663
MLP duration (in seconds): 0.0663
MLP throughput (in TFLOP/s): 247.923
Transformer duration (in seconds): 0.1485
Transformer throughput (in TFLOP/s): 171.085
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0530
Attention throughput (in TFLOP/s): 169.145
MLP duration (in seconds): 0.0672
MLP throughput (in TFLOP/s): 244.772
Transformer duration (in seconds): 0.1237
Transformer throughput (in TFLOP/s): 205.488
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 11264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11264x33792, b=2048): 0.0250
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11264x33792, b=2048): 249.256
Elapsed time for attention_key_query_prob (256x2048x176x2048): 0.0031
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x176x2048): 123.062
Elapsed time for attention_prob_times_values (256x2048x2048x176): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x176): 162.767
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x11264x11264, b=2048): 0.0085
Throughput (in TFLOP/s) for attention_linear_projection (4x11264x11264, b=2048): 244.459
Elapsed time for mlp_h_to_4h (4x11264x45056, b=2048): 0.0332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11264x45056, b=2048): 250.211
Elapsed time for mlp_fused_gelu (2048x4x45056): 0.0012
Elapsed time for mlp_4h_to_h (4x45056x11264, b=2048): 0.0327
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45056x11264, b=2048): 254.638
Elapsed time for transformer_add_bias_dropout (2048x4x11264): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11264): 0.0005

Attention duration (in seconds): 0.0739
Attention throughput (in TFLOP/s): 122.752
MLP duration (in seconds): 0.0671
MLP throughput (in TFLOP/s): 247.721
Transformer duration (in seconds): 0.1436
Transformer throughput (in TFLOP/s): 178.954
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0470
Attention throughput (in TFLOP/s): 193.135
MLP duration (in seconds): 0.0679
MLP throughput (in TFLOP/s): 244.835
Transformer duration (in seconds): 0.1182
Transformer throughput (in TFLOP/s): 217.490
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 11328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11328x33984, b=2048): 0.0253
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11328x33984, b=2048): 248.922
Elapsed time for attention_key_query_prob (256x2048x177x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x177x2048): 61.420
Elapsed time for attention_prob_times_values (256x2048x2048x177): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x177): 68.168
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x11328x11328, b=2048): 0.0086
Throughput (in TFLOP/s) for attention_linear_projection (4x11328x11328, b=2048): 243.544
Elapsed time for mlp_h_to_4h (4x11328x45312, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11328x45312, b=2048): 250.552
Elapsed time for mlp_fused_gelu (2048x4x45312): 0.0013
Elapsed time for mlp_4h_to_h (4x45312x11328, b=2048): 0.0334
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45312x11328, b=2048): 251.435
Elapsed time for transformer_add_bias_dropout (2048x4x11328): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11328): 0.0005

Attention duration (in seconds): 0.0807
Attention throughput (in TFLOP/s): 113.611
MLP duration (in seconds): 0.0683
MLP throughput (in TFLOP/s): 246.385
Transformer duration (in seconds): 0.1516
Transformer throughput (in TFLOP/s): 171.446
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0538
Attention throughput (in TFLOP/s): 170.404
MLP duration (in seconds): 0.0690
MLP throughput (in TFLOP/s): 243.939
Transformer duration (in seconds): 0.1264
Transformer throughput (in TFLOP/s): 205.652
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 11392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11392x34176, b=2048): 0.0256
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11392x34176, b=2048): 248.859
Elapsed time for attention_key_query_prob (256x2048x178x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x178x2048): 86.602
Elapsed time for attention_prob_times_values (256x2048x2048x178): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x178): 114.168
Elapsed time for attention_dropout (4x64x2048x2048): 0.0047
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11392x11392, b=2048): 0.0087
Throughput (in TFLOP/s) for attention_linear_projection (4x11392x11392, b=2048): 245.125
Elapsed time for mlp_h_to_4h (4x11392x45568, b=2048): 0.0342
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11392x45568, b=2048): 248.557
Elapsed time for mlp_fused_gelu (2048x4x45568): 0.0013
Elapsed time for mlp_4h_to_h (4x45568x11392, b=2048): 0.0336
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45568x11392, b=2048): 253.135
Elapsed time for transformer_add_bias_dropout (2048x4x11392): 0.0008
Elapsed time for transformer_layer_norm (2048x4x11392): 0.0005

Attention duration (in seconds): 0.0773
Attention throughput (in TFLOP/s): 119.978
MLP duration (in seconds): 0.0691
MLP throughput (in TFLOP/s): 246.255
Transformer duration (in seconds): 0.1489
Transformer throughput (in TFLOP/s): 176.441
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0491
Attention throughput (in TFLOP/s): 188.696
MLP duration (in seconds): 0.0697
MLP throughput (in TFLOP/s): 243.883
Transformer duration (in seconds): 0.1220
Transformer throughput (in TFLOP/s): 215.349
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 11456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11456x34368, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11456x34368, b=2048): 247.958
Elapsed time for attention_key_query_prob (256x2048x179x2048): 0.0062
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x179x2048): 61.538
Elapsed time for attention_prob_times_values (256x2048x2048x179): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x179): 70.926
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11456x11456, b=2048): 0.0088
Throughput (in TFLOP/s) for attention_linear_projection (4x11456x11456, b=2048): 243.273
Elapsed time for mlp_h_to_4h (4x11456x45824, b=2048): 0.0345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11456x45824, b=2048): 249.642
Elapsed time for mlp_fused_gelu (2048x4x45824): 0.0013
Elapsed time for mlp_4h_to_h (4x45824x11456, b=2048): 0.0338
Throughput (in TFLOP/s) for mlp_4h_to_h (4x45824x11456, b=2048): 254.186
Elapsed time for transformer_add_bias_dropout (2048x4x11456): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11456): 0.0005

Attention duration (in seconds): 0.0815
Attention throughput (in TFLOP/s): 114.969
MLP duration (in seconds): 0.0696
MLP throughput (in TFLOP/s): 247.307
Transformer duration (in seconds): 0.1537
Transformer throughput (in TFLOP/s): 172.884
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0547
Attention throughput (in TFLOP/s): 171.224
MLP duration (in seconds): 0.0702
MLP throughput (in TFLOP/s): 245.162
Transformer duration (in seconds): 0.1279
Transformer throughput (in TFLOP/s): 207.762
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 64, hidden_size: 11520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11520x34560, b=2048): 0.0259
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11520x34560, b=2048): 251.715
Elapsed time for attention_key_query_prob (256x2048x180x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x180x2048): 87.766
Elapsed time for attention_prob_times_values (256x2048x2048x180): 0.0033
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x180): 116.200
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11520x11520, b=2048): 0.0089
Throughput (in TFLOP/s) for attention_linear_projection (4x11520x11520, b=2048): 244.986
Elapsed time for mlp_h_to_4h (4x11520x46080, b=2048): 0.0346
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11520x46080, b=2048): 251.196
Elapsed time for mlp_fused_gelu (2048x4x46080): 0.0013
Elapsed time for mlp_4h_to_h (4x46080x11520, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46080x11520, b=2048): 243.244
Elapsed time for transformer_add_bias_dropout (2048x4x11520): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11520): 0.0005

Attention duration (in seconds): 0.0775
Attention throughput (in TFLOP/s): 122.209
MLP duration (in seconds): 0.0717
MLP throughput (in TFLOP/s): 242.765
Transformer duration (in seconds): 0.1518
Transformer throughput (in TFLOP/s): 177.001
Transformer - MLP - Attention (in seconds): 0.0026


Actual
------
Attention duration (in seconds): 0.0496
Attention throughput (in TFLOP/s): 191.069
MLP duration (in seconds): 0.0730
MLP throughput (in TFLOP/s): 238.322
Transformer duration (in seconds): 0.1259
Transformer throughput (in TFLOP/s): 213.318
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 11584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11584x34752, b=2048): 0.0265
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11584x34752, b=2048): 249.194
Elapsed time for attention_key_query_prob (256x2048x181x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x181x2048): 62.119
Elapsed time for attention_prob_times_values (256x2048x2048x181): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x181): 71.641
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x11584x11584, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11584x11584, b=2048): 245.276
Elapsed time for mlp_h_to_4h (4x11584x46336, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11584x46336, b=2048): 249.912
Elapsed time for mlp_fused_gelu (2048x4x46336): 0.0013
Elapsed time for mlp_4h_to_h (4x46336x11584, b=2048): 0.0365
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46336x11584, b=2048): 241.224
Elapsed time for transformer_add_bias_dropout (2048x4x11584): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11584): 0.0005

Attention duration (in seconds): 0.0821
Attention throughput (in TFLOP/s): 116.582
MLP duration (in seconds): 0.0729
MLP throughput (in TFLOP/s): 241.177
Transformer duration (in seconds): 0.1577
Transformer throughput (in TFLOP/s): 172.219
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0555
Attention throughput (in TFLOP/s): 172.602
MLP duration (in seconds): 0.0738
MLP throughput (in TFLOP/s): 238.463
Transformer duration (in seconds): 0.1336
Transformer throughput (in TFLOP/s): 203.360
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 11648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11648x34944, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11648x34944, b=2048): 249.072
Elapsed time for attention_key_query_prob (256x2048x182x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x182x2048): 88.674
Elapsed time for attention_prob_times_values (256x2048x2048x182): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x182): 116.342
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11648x11648, b=2048): 0.0090
Throughput (in TFLOP/s) for attention_linear_projection (4x11648x11648, b=2048): 246.746
Elapsed time for mlp_h_to_4h (4x11648x46592, b=2048): 0.0355
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11648x46592, b=2048): 250.213
Elapsed time for mlp_fused_gelu (2048x4x46592): 0.0013
Elapsed time for mlp_4h_to_h (4x46592x11648, b=2048): 0.0366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46592x11648, b=2048): 242.728
Elapsed time for transformer_add_bias_dropout (2048x4x11648): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11648): 0.0005

Attention duration (in seconds): 0.0785
Attention throughput (in TFLOP/s): 123.178
MLP duration (in seconds): 0.0735
MLP throughput (in TFLOP/s): 242.093
Transformer duration (in seconds): 0.1547
Transformer throughput (in TFLOP/s): 177.533
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0503
Attention throughput (in TFLOP/s): 192.489
MLP duration (in seconds): 0.0741
MLP throughput (in TFLOP/s): 239.920
Transformer duration (in seconds): 0.1277
Transformer throughput (in TFLOP/s): 215.002
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 11712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11712x35136, b=2048): 0.0268
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11712x35136, b=2048): 251.647
Elapsed time for attention_key_query_prob (256x2048x183x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x183x2048): 62.046
Elapsed time for attention_prob_times_values (256x2048x2048x183): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x183): 69.802
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11712x11712, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11712x11712, b=2048): 245.175
Elapsed time for mlp_h_to_4h (4x11712x46848, b=2048): 0.0358
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11712x46848, b=2048): 250.968
Elapsed time for mlp_fused_gelu (2048x4x46848): 0.0013
Elapsed time for mlp_4h_to_h (4x46848x11712, b=2048): 0.0352
Throughput (in TFLOP/s) for mlp_4h_to_h (4x46848x11712, b=2048): 255.329
Elapsed time for transformer_add_bias_dropout (2048x4x11712): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11712): 0.0005

Attention duration (in seconds): 0.0829
Attention throughput (in TFLOP/s): 117.924
MLP duration (in seconds): 0.0723
MLP throughput (in TFLOP/s): 248.600
Transformer duration (in seconds): 0.1579
Transformer throughput (in TFLOP/s): 175.755
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0562
Attention throughput (in TFLOP/s): 174.015
MLP duration (in seconds): 0.0731
MLP throughput (in TFLOP/s): 245.982
Transformer duration (in seconds): 0.1329
Transformer throughput (in TFLOP/s): 208.896
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 11776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11776x35328, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11776x35328, b=2048): 250.756
Elapsed time for attention_key_query_prob (256x2048x184x2048): 0.0034
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x184x2048): 115.293
Elapsed time for attention_prob_times_values (256x2048x2048x184): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x184): 166.558
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11776x11776, b=2048): 0.0092
Throughput (in TFLOP/s) for attention_linear_projection (4x11776x11776, b=2048): 246.681
Elapsed time for mlp_h_to_4h (4x11776x47104, b=2048): 0.0361
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11776x47104, b=2048): 251.590
Elapsed time for mlp_fused_gelu (2048x4x47104): 0.0013
Elapsed time for mlp_4h_to_h (4x47104x11776, b=2048): 0.0354
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47104x11776, b=2048): 256.748
Elapsed time for transformer_add_bias_dropout (2048x4x11776): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11776): 0.0005

Attention duration (in seconds): 0.0772
Attention throughput (in TFLOP/s): 128.011
MLP duration (in seconds): 0.0728
MLP throughput (in TFLOP/s): 249.602
Transformer duration (in seconds): 0.1527
Transformer throughput (in TFLOP/s): 183.734
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0499
Attention throughput (in TFLOP/s): 197.929
MLP duration (in seconds): 0.0736
MLP throughput (in TFLOP/s): 246.807
Transformer duration (in seconds): 0.1267
Transformer throughput (in TFLOP/s): 221.378
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 11840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11840x35520, b=2048): 0.0274
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11840x35520, b=2048): 251.555
Elapsed time for attention_key_query_prob (256x2048x185x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x185x2048): 62.593
Elapsed time for attention_prob_times_values (256x2048x2048x185): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x185): 70.191
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11840x11840, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x11840x11840, b=2048): 245.320
Elapsed time for mlp_h_to_4h (4x11840x47360, b=2048): 0.0365
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11840x47360, b=2048): 251.565
Elapsed time for mlp_fused_gelu (2048x4x47360): 0.0013
Elapsed time for mlp_4h_to_h (4x47360x11840, b=2048): 0.0380
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47360x11840, b=2048): 241.930
Elapsed time for transformer_add_bias_dropout (2048x4x11840): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11840): 0.0005

Attention duration (in seconds): 0.0837
Attention throughput (in TFLOP/s): 119.207
MLP duration (in seconds): 0.0758
MLP throughput (in TFLOP/s): 242.397
Transformer duration (in seconds): 0.1623
Transformer throughput (in TFLOP/s): 174.754
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0571
Attention throughput (in TFLOP/s): 174.911
MLP duration (in seconds): 0.0768
MLP throughput (in TFLOP/s): 239.215
Transformer duration (in seconds): 0.1378
Transformer throughput (in TFLOP/s): 205.766
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 11904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11904x35712, b=2048): 0.0277
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11904x35712, b=2048): 251.477
Elapsed time for attention_key_query_prob (256x2048x186x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x186x2048): 90.371
Elapsed time for attention_prob_times_values (256x2048x2048x186): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x186): 119.034
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11904x11904, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x11904x11904, b=2048): 246.618
Elapsed time for mlp_h_to_4h (4x11904x47616, b=2048): 0.0370
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11904x47616, b=2048): 251.275
Elapsed time for mlp_fused_gelu (2048x4x47616): 0.0013
Elapsed time for mlp_4h_to_h (4x47616x11904, b=2048): 0.0382
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47616x11904, b=2048): 243.023
Elapsed time for transformer_add_bias_dropout (2048x4x11904): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11904): 0.0005

Attention duration (in seconds): 0.0799
Attention throughput (in TFLOP/s): 126.283
MLP duration (in seconds): 0.0765
MLP throughput (in TFLOP/s): 242.834
Transformer duration (in seconds): 0.1591
Transformer throughput (in TFLOP/s): 180.162
Transformer - MLP - Attention (in seconds): 0.0027


Actual
------
Attention duration (in seconds): 0.0520
Attention throughput (in TFLOP/s): 193.904
MLP duration (in seconds): 0.0775
MLP throughput (in TFLOP/s): 239.737
Transformer duration (in seconds): 0.1334
Transformer throughput (in TFLOP/s): 214.894
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 11968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x11968x35904, b=2048): 0.0275
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x11968x35904, b=2048): 256.344
Elapsed time for attention_key_query_prob (256x2048x187x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x187x2048): 63.255
Elapsed time for attention_prob_times_values (256x2048x2048x187): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x187): 74.263
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x11968x11968, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x11968x11968, b=2048): 258.292
Elapsed time for mlp_h_to_4h (4x11968x47872, b=2048): 0.0364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x11968x47872, b=2048): 258.045
Elapsed time for mlp_fused_gelu (2048x4x47872): 0.0013
Elapsed time for mlp_4h_to_h (4x47872x11968, b=2048): 0.0370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x47872x11968, b=2048): 254.015
Elapsed time for transformer_add_bias_dropout (2048x4x11968): 0.0009
Elapsed time for transformer_layer_norm (2048x4x11968): 0.0005

Attention duration (in seconds): 0.0833
Attention throughput (in TFLOP/s): 122.362
MLP duration (in seconds): 0.0747
MLP throughput (in TFLOP/s): 251.479
Transformer duration (in seconds): 0.1607
Transformer throughput (in TFLOP/s): 180.246
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0575
Attention throughput (in TFLOP/s): 177.216
MLP duration (in seconds): 0.0756
MLP throughput (in TFLOP/s): 248.292
Transformer duration (in seconds): 0.1371
Transformer throughput (in TFLOP/s): 211.309
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 12032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12032x36096, b=2048): 0.0280
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12032x36096, b=2048): 254.154
Elapsed time for attention_key_query_prob (256x2048x188x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x188x2048): 91.402
Elapsed time for attention_prob_times_values (256x2048x2048x188): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x188): 119.803
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x12032x12032, b=2048): 0.0091
Throughput (in TFLOP/s) for attention_linear_projection (4x12032x12032, b=2048): 259.653
Elapsed time for mlp_h_to_4h (4x12032x48128, b=2048): 0.0369
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12032x48128, b=2048): 257.377
Elapsed time for mlp_fused_gelu (2048x4x48128): 0.0013
Elapsed time for mlp_4h_to_h (4x48128x12032, b=2048): 0.0372
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48128x12032, b=2048): 254.817
Elapsed time for transformer_add_bias_dropout (2048x4x12032): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12032): 0.0005

Attention duration (in seconds): 0.0799
Attention throughput (in TFLOP/s): 128.832
MLP duration (in seconds): 0.0754
MLP throughput (in TFLOP/s): 251.579
Transformer duration (in seconds): 0.1581
Transformer throughput (in TFLOP/s): 185.146
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0524
Attention throughput (in TFLOP/s): 196.489
MLP duration (in seconds): 0.0763
MLP throughput (in TFLOP/s): 248.537
Transformer duration (in seconds): 0.1322
Transformer throughput (in TFLOP/s): 221.330
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 12096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12096x36288, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12096x36288, b=2048): 255.350
Elapsed time for attention_key_query_prob (256x2048x189x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x189x2048): 63.872
Elapsed time for attention_prob_times_values (256x2048x2048x189): 0.0054
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x189): 75.035
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12096x12096, b=2048): 0.0094
Throughput (in TFLOP/s) for attention_linear_projection (4x12096x12096, b=2048): 254.329
Elapsed time for mlp_h_to_4h (4x12096x48384, b=2048): 0.0371
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12096x48384, b=2048): 258.326
Elapsed time for mlp_fused_gelu (2048x4x48384): 0.0013
Elapsed time for mlp_4h_to_h (4x48384x12096, b=2048): 0.0397
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48384x12096, b=2048): 241.252
Elapsed time for transformer_add_bias_dropout (2048x4x12096): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12096): 0.0005

Attention duration (in seconds): 0.0843
Attention throughput (in TFLOP/s): 123.335
MLP duration (in seconds): 0.0782
MLP throughput (in TFLOP/s): 245.236
Transformer duration (in seconds): 0.1653
Transformer throughput (in TFLOP/s): 178.920
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0585
Attention throughput (in TFLOP/s): 177.842
MLP duration (in seconds): 0.0794
MLP throughput (in TFLOP/s): 241.663
Transformer duration (in seconds): 0.1421
Transformer throughput (in TFLOP/s): 208.104
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 12160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12160x36480, b=2048): 0.0285
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12160x36480, b=2048): 254.917
Elapsed time for attention_key_query_prob (256x2048x190x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x190x2048): 92.601
Elapsed time for attention_prob_times_values (256x2048x2048x190): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x190): 121.243
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12160x12160, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x12160x12160, b=2048): 250.139
Elapsed time for mlp_h_to_4h (4x12160x48640, b=2048): 0.0375
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12160x48640, b=2048): 258.092
Elapsed time for mlp_fused_gelu (2048x4x48640): 0.0013
Elapsed time for mlp_4h_to_h (4x48640x12160, b=2048): 0.0400
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48640x12160, b=2048): 242.344
Elapsed time for transformer_add_bias_dropout (2048x4x12160): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12160): 0.0005

Attention duration (in seconds): 0.0809
Attention throughput (in TFLOP/s): 129.810
MLP duration (in seconds): 0.0789
MLP throughput (in TFLOP/s): 245.714
Transformer duration (in seconds): 0.1626
Transformer throughput (in TFLOP/s): 183.812
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0536
Attention throughput (in TFLOP/s): 196.063
MLP duration (in seconds): 0.0801
MLP throughput (in TFLOP/s): 241.927
Transformer duration (in seconds): 0.1376
Transformer throughput (in TFLOP/s): 217.193
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 12224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12224x36672, b=2048): 0.0287
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12224x36672, b=2048): 255.476
Elapsed time for attention_key_query_prob (256x2048x191x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x191x2048): 64.513
Elapsed time for attention_prob_times_values (256x2048x2048x191): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x191): 73.851
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12224x12224, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x12224x12224, b=2048): 255.314
Elapsed time for mlp_h_to_4h (4x12224x48896, b=2048): 0.0381
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12224x48896, b=2048): 257.177
Elapsed time for mlp_fused_gelu (2048x4x48896): 0.0014
Elapsed time for mlp_4h_to_h (4x48896x12224, b=2048): 0.0387
Throughput (in TFLOP/s) for mlp_4h_to_h (4x48896x12224, b=2048): 253.108
Elapsed time for transformer_add_bias_dropout (2048x4x12224): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12224): 0.0005

Attention duration (in seconds): 0.0852
Attention throughput (in TFLOP/s): 124.537
MLP duration (in seconds): 0.0781
MLP throughput (in TFLOP/s): 250.716
Transformer duration (in seconds): 0.1662
Transformer throughput (in TFLOP/s): 181.753
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0593
Attention throughput (in TFLOP/s): 178.905
MLP duration (in seconds): 0.0792
MLP throughput (in TFLOP/s): 247.265
Transformer duration (in seconds): 0.1419
Transformer throughput (in TFLOP/s): 212.829
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 12288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12288x36864, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12288x36864, b=2048): 254.171
Elapsed time for attention_key_query_prob (256x2048x192x2048): 0.0030
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x192x2048): 136.723
Elapsed time for attention_prob_times_values (256x2048x2048x192): 0.0023
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x192): 176.031
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0306
Elapsed time for attention_linear_projection (4x12288x12288, b=2048): 0.0096
Throughput (in TFLOP/s) for attention_linear_projection (4x12288x12288, b=2048): 256.935
Elapsed time for mlp_h_to_4h (4x12288x49152, b=2048): 0.0384
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12288x49152, b=2048): 257.903
Elapsed time for mlp_fused_gelu (2048x4x49152): 0.0014
Elapsed time for mlp_4h_to_h (4x49152x12288, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49152x12288, b=2048): 253.780
Elapsed time for transformer_add_bias_dropout (2048x4x12288): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12288): 0.0005

Attention duration (in seconds): 0.0793
Attention throughput (in TFLOP/s): 135.119
MLP duration (in seconds): 0.0787
MLP throughput (in TFLOP/s): 251.414
Transformer duration (in seconds): 0.1609
Transformer throughput (in TFLOP/s): 189.662
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0530
Attention throughput (in TFLOP/s): 202.395
MLP duration (in seconds): 0.0801
MLP throughput (in TFLOP/s): 247.206
Transformer duration (in seconds): 0.1369
Transformer throughput (in TFLOP/s): 222.878
Transformer - MLP - Attention (in seconds): 0.0039
========================================================================================================================
num_attention_heads: 64, hidden_size: 12352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12352x37056, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12352x37056, b=2048): 254.349
Elapsed time for attention_key_query_prob (256x2048x193x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x193x2048): 62.079
Elapsed time for attention_prob_times_values (256x2048x2048x193): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x193): 73.884
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12352x12352, b=2048): 0.0098
Throughput (in TFLOP/s) for attention_linear_projection (4x12352x12352, b=2048): 255.871
Elapsed time for mlp_h_to_4h (4x12352x49408, b=2048): 0.0391
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12352x49408, b=2048): 256.002
Elapsed time for mlp_fused_gelu (2048x4x49408): 0.0014
Elapsed time for mlp_4h_to_h (4x49408x12352, b=2048): 0.0416
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49408x12352, b=2048): 240.313
Elapsed time for transformer_add_bias_dropout (2048x4x12352): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12352): 0.0005

Attention duration (in seconds): 0.0865
Attention throughput (in TFLOP/s): 125.157
MLP duration (in seconds): 0.0820
MLP throughput (in TFLOP/s): 243.786
Transformer duration (in seconds): 0.1714
Transformer throughput (in TFLOP/s): 179.857
Transformer - MLP - Attention (in seconds): 0.0028


Actual
------
Attention duration (in seconds): 0.0605
Attention throughput (in TFLOP/s): 178.836
MLP duration (in seconds): 0.0827
MLP throughput (in TFLOP/s): 241.828
Transformer duration (in seconds): 0.1466
Transformer throughput (in TFLOP/s): 210.318
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 12416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12416x37248, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12416x37248, b=2048): 257.068
Elapsed time for attention_key_query_prob (256x2048x194x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x194x2048): 88.447
Elapsed time for attention_prob_times_values (256x2048x2048x194): 0.0034
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x194): 122.000
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12416x12416, b=2048): 0.0097
Throughput (in TFLOP/s) for attention_linear_projection (4x12416x12416, b=2048): 259.540
Elapsed time for mlp_h_to_4h (4x12416x49664, b=2048): 0.0390
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12416x49664, b=2048): 259.040
Elapsed time for mlp_fused_gelu (2048x4x49664): 0.0014
Elapsed time for mlp_4h_to_h (4x49664x12416, b=2048): 0.0416
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49664x12416, b=2048): 243.130
Elapsed time for transformer_add_bias_dropout (2048x4x12416): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12416): 0.0005

Attention duration (in seconds): 0.0823
Attention throughput (in TFLOP/s): 132.861
MLP duration (in seconds): 0.0819
MLP throughput (in TFLOP/s): 246.639
Transformer duration (in seconds): 0.1671
Transformer throughput (in TFLOP/s): 186.380
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0555
Attention throughput (in TFLOP/s): 197.171
MLP duration (in seconds): 0.0833
MLP throughput (in TFLOP/s): 242.550
Transformer duration (in seconds): 0.1425
Transformer throughput (in TFLOP/s): 218.476
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 12480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12480x37440, b=2048): 0.0299
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12480x37440, b=2048): 256.271
Elapsed time for attention_key_query_prob (256x2048x195x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x195x2048): 62.067
Elapsed time for attention_prob_times_values (256x2048x2048x195): 0.0055
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x195): 76.333
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12480x12480, b=2048): 0.0101
Throughput (in TFLOP/s) for attention_linear_projection (4x12480x12480, b=2048): 253.839
Elapsed time for mlp_h_to_4h (4x12480x49920, b=2048): 0.0397
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12480x49920, b=2048): 257.130
Elapsed time for mlp_fused_gelu (2048x4x49920): 0.0014
Elapsed time for mlp_4h_to_h (4x49920x12480, b=2048): 0.0401
Throughput (in TFLOP/s) for mlp_4h_to_h (4x49920x12480, b=2048): 254.445
Elapsed time for transformer_add_bias_dropout (2048x4x12480): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12480): 0.0005

Attention duration (in seconds): 0.0871
Attention throughput (in TFLOP/s): 126.758
MLP duration (in seconds): 0.0812
MLP throughput (in TFLOP/s): 251.438
Transformer duration (in seconds): 0.1712
Transformer throughput (in TFLOP/s): 183.757
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0618
Attention throughput (in TFLOP/s): 178.594
MLP duration (in seconds): 0.0824
MLP throughput (in TFLOP/s): 247.844
Transformer duration (in seconds): 0.1474
Transformer throughput (in TFLOP/s): 213.360
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 12544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12544x37632, b=2048): 0.0302
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12544x37632, b=2048): 255.791
Elapsed time for attention_key_query_prob (256x2048x196x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x196x2048): 88.841
Elapsed time for attention_prob_times_values (256x2048x2048x196): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x196): 121.150
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12544x12544, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12544x12544, b=2048): 249.205
Elapsed time for mlp_h_to_4h (4x12544x50176, b=2048): 0.0400
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12544x50176, b=2048): 257.842
Elapsed time for mlp_fused_gelu (2048x4x50176): 0.0014
Elapsed time for mlp_4h_to_h (4x50176x12544, b=2048): 0.0404
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50176x12544, b=2048): 255.526
Elapsed time for transformer_add_bias_dropout (2048x4x12544): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12544): 0.0005

Attention duration (in seconds): 0.0838
Attention throughput (in TFLOP/s): 133.141
MLP duration (in seconds): 0.0817
MLP throughput (in TFLOP/s): 252.329
Transformer duration (in seconds): 0.1684
Transformer throughput (in TFLOP/s): 188.731
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0566
Attention throughput (in TFLOP/s): 196.898
MLP duration (in seconds): 0.0827
MLP throughput (in TFLOP/s): 249.424
Transformer duration (in seconds): 0.1429
Transformer throughput (in TFLOP/s): 222.444
Transformer - MLP - Attention (in seconds): 0.0035
========================================================================================================================
num_attention_heads: 64, hidden_size: 12608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12608x37824, b=2048): 0.0307
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12608x37824, b=2048): 254.666
Elapsed time for attention_key_query_prob (256x2048x197x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x197x2048): 62.521
Elapsed time for attention_prob_times_values (256x2048x2048x197): 0.0056
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x197): 76.197
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12608x12608, b=2048): 0.0102
Throughput (in TFLOP/s) for attention_linear_projection (4x12608x12608, b=2048): 254.867
Elapsed time for mlp_h_to_4h (4x12608x50432, b=2048): 0.0404
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12608x50432, b=2048): 257.775
Elapsed time for mlp_fused_gelu (2048x4x50432): 0.0014
Elapsed time for mlp_4h_to_h (4x50432x12608, b=2048): 0.0409
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50432x12608, b=2048): 254.879
Elapsed time for transformer_add_bias_dropout (2048x4x12608): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12608): 0.0005

Attention duration (in seconds): 0.0882
Attention throughput (in TFLOP/s): 127.717
MLP duration (in seconds): 0.0827
MLP throughput (in TFLOP/s): 252.003
Transformer duration (in seconds): 0.1738
Transformer throughput (in TFLOP/s): 184.717
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0625
Attention throughput (in TFLOP/s): 180.307
MLP duration (in seconds): 0.0838
MLP throughput (in TFLOP/s): 248.684
Transformer duration (in seconds): 0.1505
Transformer throughput (in TFLOP/s): 213.267
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 12672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12672x38016, b=2048): 0.0309
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12672x38016, b=2048): 255.746
Elapsed time for attention_key_query_prob (256x2048x198x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x198x2048): 89.494
Elapsed time for attention_prob_times_values (256x2048x2048x198): 0.0035
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x198): 119.883
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12672x12672, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12672x12672, b=2048): 255.823
Elapsed time for mlp_h_to_4h (4x12672x50688, b=2048): 0.0407
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12672x50688, b=2048): 258.856
Elapsed time for mlp_fused_gelu (2048x4x50688): 0.0014
Elapsed time for mlp_4h_to_h (4x50688x12672, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50688x12672, b=2048): 243.283
Elapsed time for transformer_add_bias_dropout (2048x4x12672): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12672): 0.0005

Attention duration (in seconds): 0.0844
Attention throughput (in TFLOP/s): 134.727
MLP duration (in seconds): 0.0853
MLP throughput (in TFLOP/s): 246.714
Transformer duration (in seconds): 0.1726
Transformer throughput (in TFLOP/s): 187.802
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0570
Attention throughput (in TFLOP/s): 199.570
MLP duration (in seconds): 0.0882
MLP throughput (in TFLOP/s): 238.651
Transformer duration (in seconds): 0.1472
Transformer throughput (in TFLOP/s): 220.228
Transformer - MLP - Attention (in seconds): 0.0020
========================================================================================================================
num_attention_heads: 64, hidden_size: 12736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12736x38208, b=2048): 0.0310
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12736x38208, b=2048): 257.251
Elapsed time for attention_key_query_prob (256x2048x199x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x199x2048): 62.529
Elapsed time for attention_prob_times_values (256x2048x2048x199): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x199): 73.991
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12736x12736, b=2048): 0.0103
Throughput (in TFLOP/s) for attention_linear_projection (4x12736x12736, b=2048): 257.908
Elapsed time for mlp_h_to_4h (4x12736x50944, b=2048): 0.0409
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12736x50944, b=2048): 259.965
Elapsed time for mlp_fused_gelu (2048x4x50944): 0.0014
Elapsed time for mlp_4h_to_h (4x50944x12736, b=2048): 0.0415
Throughput (in TFLOP/s) for mlp_4h_to_h (4x50944x12736, b=2048): 255.848
Elapsed time for transformer_add_bias_dropout (2048x4x12736): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12736): 0.0005

Attention duration (in seconds): 0.0889
Attention throughput (in TFLOP/s): 129.233
MLP duration (in seconds): 0.0838
MLP throughput (in TFLOP/s): 253.564
Transformer duration (in seconds): 0.1757
Transformer throughput (in TFLOP/s): 186.424
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0633
Attention throughput (in TFLOP/s): 181.512
MLP duration (in seconds): 0.0849
MLP throughput (in TFLOP/s): 250.295
Transformer duration (in seconds): 0.1522
Transformer throughput (in TFLOP/s): 215.188
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 12800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12800x38400, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12800x38400, b=2048): 257.603
Elapsed time for attention_key_query_prob (256x2048x200x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x200x2048): 122.330
Elapsed time for attention_prob_times_values (256x2048x2048x200): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x200): 179.820
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12800x12800, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12800x12800, b=2048): 258.973
Elapsed time for mlp_h_to_4h (4x12800x51200, b=2048): 0.0412
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12800x51200, b=2048): 260.638
Elapsed time for mlp_fused_gelu (2048x4x51200): 0.0014
Elapsed time for mlp_4h_to_h (4x51200x12800, b=2048): 0.0418
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51200x12800, b=2048): 256.936
Elapsed time for transformer_add_bias_dropout (2048x4x12800): 0.0009
Elapsed time for transformer_layer_norm (2048x4x12800): 0.0005

Attention duration (in seconds): 0.0825
Attention throughput (in TFLOP/s): 140.550
MLP duration (in seconds): 0.0844
MLP throughput (in TFLOP/s): 254.439
Transformer duration (in seconds): 0.1698
Transformer throughput (in TFLOP/s): 194.731
Transformer - MLP - Attention (in seconds): 0.0029


Actual
------
Attention duration (in seconds): 0.0568
Attention throughput (in TFLOP/s): 204.026
MLP duration (in seconds): 0.0857
MLP throughput (in TFLOP/s): 250.608
Transformer duration (in seconds): 0.1458
Transformer throughput (in TFLOP/s): 226.769
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 12864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12864x38592, b=2048): 0.0316
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12864x38592, b=2048): 257.665
Elapsed time for attention_key_query_prob (256x2048x201x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x201x2048): 62.951
Elapsed time for attention_prob_times_values (256x2048x2048x201): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x201): 73.766
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x12864x12864, b=2048): 0.0104
Throughput (in TFLOP/s) for attention_linear_projection (4x12864x12864, b=2048): 259.894
Elapsed time for mlp_h_to_4h (4x12864x51456, b=2048): 0.0418
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12864x51456, b=2048): 259.537
Elapsed time for mlp_fused_gelu (2048x4x51456): 0.0014
Elapsed time for mlp_4h_to_h (4x51456x12864, b=2048): 0.0426
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51456x12864, b=2048): 254.633
Elapsed time for transformer_add_bias_dropout (2048x4x12864): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12864): 0.0005

Attention duration (in seconds): 0.0897
Attention throughput (in TFLOP/s): 130.564
MLP duration (in seconds): 0.0858
MLP throughput (in TFLOP/s): 252.807
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 187.166
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0643
Attention throughput (in TFLOP/s): 182.064
MLP duration (in seconds): 0.0871
MLP throughput (in TFLOP/s): 249.147
Transformer duration (in seconds): 0.1558
Transformer throughput (in TFLOP/s): 214.388
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 12928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12928x38784, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12928x38784, b=2048): 255.132
Elapsed time for attention_key_query_prob (256x2048x202x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x202x2048): 90.712
Elapsed time for attention_prob_times_values (256x2048x2048x202): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x202): 120.137
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x12928x12928, b=2048): 0.0105
Throughput (in TFLOP/s) for attention_linear_projection (4x12928x12928, b=2048): 260.232
Elapsed time for mlp_h_to_4h (4x12928x51712, b=2048): 0.0424
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12928x51712, b=2048): 258.119
Elapsed time for mlp_fused_gelu (2048x4x51712): 0.0014
Elapsed time for mlp_4h_to_h (4x51712x12928, b=2048): 0.0429
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51712x12928, b=2048): 255.494
Elapsed time for transformer_add_bias_dropout (2048x4x12928): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12928): 0.0005

Attention duration (in seconds): 0.0861
Attention throughput (in TFLOP/s): 137.295
MLP duration (in seconds): 0.0867
MLP throughput (in TFLOP/s): 252.563
Transformer duration (in seconds): 0.1758
Transformer throughput (in TFLOP/s): 191.843
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0591
Attention throughput (in TFLOP/s): 199.923
MLP duration (in seconds): 0.0879
MLP throughput (in TFLOP/s): 249.220
Transformer duration (in seconds): 0.1511
Transformer throughput (in TFLOP/s): 223.211
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 12992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x12992x38976, b=2048): 0.0325
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x12992x38976, b=2048): 255.071
Elapsed time for attention_key_query_prob (256x2048x203x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x203x2048): 63.917
Elapsed time for attention_prob_times_values (256x2048x2048x203): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x203): 76.442
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x12992x12992, b=2048): 0.0108
Throughput (in TFLOP/s) for attention_linear_projection (4x12992x12992, b=2048): 255.421
Elapsed time for mlp_h_to_4h (4x12992x51968, b=2048): 0.0428
Throughput (in TFLOP/s) for mlp_h_to_4h (4x12992x51968, b=2048): 258.718
Elapsed time for mlp_fused_gelu (2048x4x51968): 0.0014
Elapsed time for mlp_4h_to_h (4x51968x12992, b=2048): 0.0434
Throughput (in TFLOP/s) for mlp_4h_to_h (4x51968x12992, b=2048): 254.861
Elapsed time for transformer_add_bias_dropout (2048x4x12992): 0.0010
Elapsed time for transformer_layer_norm (2048x4x12992): 0.0005

Attention duration (in seconds): 0.0909
Attention throughput (in TFLOP/s): 131.332
MLP duration (in seconds): 0.0876
MLP throughput (in TFLOP/s): 252.556
Transformer duration (in seconds): 0.1815
Transformer throughput (in TFLOP/s): 187.692
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0653
Attention throughput (in TFLOP/s): 182.675
MLP duration (in seconds): 0.0886
MLP throughput (in TFLOP/s): 249.658
Transformer duration (in seconds): 0.1579
Transformer throughput (in TFLOP/s): 215.635
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 13056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13056x39168, b=2048): 0.0326
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13056x39168, b=2048): 256.759
Elapsed time for attention_key_query_prob (256x2048x204x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x204x2048): 91.674
Elapsed time for attention_prob_times_values (256x2048x2048x204): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x204): 121.874
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13056x13056, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_linear_projection (4x13056x13056, b=2048): 233.110
Elapsed time for mlp_h_to_4h (4x13056x52224, b=2048): 0.0433
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13056x52224, b=2048): 258.116
Elapsed time for mlp_fused_gelu (2048x4x52224): 0.0014
Elapsed time for mlp_4h_to_h (4x52224x13056, b=2048): 0.0437
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52224x13056, b=2048): 255.649
Elapsed time for transformer_add_bias_dropout (2048x4x13056): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13056): 0.0005

Attention duration (in seconds): 0.0880
Attention throughput (in TFLOP/s): 136.950
MLP duration (in seconds): 0.0884
MLP throughput (in TFLOP/s): 252.689
Transformer duration (in seconds): 0.1794
Transformer throughput (in TFLOP/s): 191.715
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0600
Attention throughput (in TFLOP/s): 200.741
MLP duration (in seconds): 0.0895
MLP throughput (in TFLOP/s): 249.658
Transformer duration (in seconds): 0.1535
Transformer throughput (in TFLOP/s): 224.070
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 13120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13120x39360, b=2048): 0.0332
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13120x39360, b=2048): 254.832
Elapsed time for attention_key_query_prob (256x2048x205x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x205x2048): 64.319
Elapsed time for attention_prob_times_values (256x2048x2048x205): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x205): 76.758
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13120x13120, b=2048): 0.0110
Throughput (in TFLOP/s) for attention_linear_projection (4x13120x13120, b=2048): 256.544
Elapsed time for mlp_h_to_4h (4x13120x52480, b=2048): 0.0438
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13120x52480, b=2048): 257.339
Elapsed time for mlp_fused_gelu (2048x4x52480): 0.0014
Elapsed time for mlp_4h_to_h (4x52480x13120, b=2048): 0.0444
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52480x13120, b=2048): 254.094
Elapsed time for transformer_add_bias_dropout (2048x4x13120): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13120): 0.0005

Attention duration (in seconds): 0.0917
Attention throughput (in TFLOP/s): 132.558
MLP duration (in seconds): 0.0897
MLP throughput (in TFLOP/s): 251.577
Transformer duration (in seconds): 0.1844
Transformer throughput (in TFLOP/s): 188.254
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0664
Attention throughput (in TFLOP/s): 183.042
MLP duration (in seconds): 0.0909
MLP throughput (in TFLOP/s): 248.249
Transformer duration (in seconds): 0.1621
Transformer throughput (in TFLOP/s): 214.156
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 13184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13184x39552, b=2048): 0.0336
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13184x39552, b=2048): 254.620
Elapsed time for attention_key_query_prob (256x2048x206x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x206x2048): 92.080
Elapsed time for attention_prob_times_values (256x2048x2048x206): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x206): 121.660
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13184x13184, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x13184x13184, b=2048): 256.069
Elapsed time for mlp_h_to_4h (4x13184x52736, b=2048): 0.0443
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13184x52736, b=2048): 257.118
Elapsed time for mlp_fused_gelu (2048x4x52736): 0.0015
Elapsed time for mlp_4h_to_h (4x52736x13184, b=2048): 0.0447
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52736x13184, b=2048): 254.650
Elapsed time for transformer_add_bias_dropout (2048x4x13184): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13184): 0.0005

Attention duration (in seconds): 0.0881
Attention throughput (in TFLOP/s): 139.339
MLP duration (in seconds): 0.0905
MLP throughput (in TFLOP/s): 251.761
Transformer duration (in seconds): 0.1816
Transformer throughput (in TFLOP/s): 193.040
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0611
Attention throughput (in TFLOP/s): 200.805
MLP duration (in seconds): 0.0916
MLP throughput (in TFLOP/s): 248.705
Transformer duration (in seconds): 0.1573
Transformer throughput (in TFLOP/s): 222.920
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 13248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13248x39744, b=2048): 0.0338
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13248x39744, b=2048): 255.599
Elapsed time for attention_key_query_prob (256x2048x207x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x207x2048): 64.689
Elapsed time for attention_prob_times_values (256x2048x2048x207): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x207): 76.208
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13248x13248, b=2048): 0.0111
Throughput (in TFLOP/s) for attention_linear_projection (4x13248x13248, b=2048): 258.132
Elapsed time for mlp_h_to_4h (4x13248x52992, b=2048): 0.0447
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13248x52992, b=2048): 257.592
Elapsed time for mlp_fused_gelu (2048x4x52992): 0.0015
Elapsed time for mlp_4h_to_h (4x52992x13248, b=2048): 0.0452
Throughput (in TFLOP/s) for mlp_4h_to_h (4x52992x13248, b=2048): 254.243
Elapsed time for transformer_add_bias_dropout (2048x4x13248): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13248): 0.0005

Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 133.843
MLP duration (in seconds): 0.0914
MLP throughput (in TFLOP/s): 251.810
Transformer duration (in seconds): 0.1870
Transformer throughput (in TFLOP/s): 189.300
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0672
Attention throughput (in TFLOP/s): 184.457
MLP duration (in seconds): 0.0924
MLP throughput (in TFLOP/s): 248.889
Transformer duration (in seconds): 0.1635
Transformer throughput (in TFLOP/s): 216.552
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 13312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13312x39936, b=2048): 0.0341
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13312x39936, b=2048): 255.793
Elapsed time for attention_key_query_prob (256x2048x208x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x208x2048): 135.418
Elapsed time for attention_prob_times_values (256x2048x2048x208): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x208): 187.244
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13312x13312, b=2048): 0.0112
Throughput (in TFLOP/s) for attention_linear_projection (4x13312x13312, b=2048): 259.095
Elapsed time for mlp_h_to_4h (4x13312x53248, b=2048): 0.0451
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13312x53248, b=2048): 257.386
Elapsed time for mlp_fused_gelu (2048x4x53248): 0.0015
Elapsed time for mlp_4h_to_h (4x53248x13312, b=2048): 0.0455
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53248x13312, b=2048): 255.195
Elapsed time for transformer_add_bias_dropout (2048x4x13312): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13312): 0.0005

Attention duration (in seconds): 0.0859
Attention throughput (in TFLOP/s): 145.546
MLP duration (in seconds): 0.0921
MLP throughput (in TFLOP/s): 252.196
Transformer duration (in seconds): 0.1811
Transformer throughput (in TFLOP/s): 197.343
Transformer - MLP - Attention (in seconds): 0.0030


Actual
------
Attention duration (in seconds): 0.0609
Attention throughput (in TFLOP/s): 205.375
MLP duration (in seconds): 0.0932
MLP throughput (in TFLOP/s): 249.274
Transformer duration (in seconds): 0.1579
Transformer throughput (in TFLOP/s): 226.369
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 13376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13376x40128, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13376x40128, b=2048): 255.516
Elapsed time for attention_key_query_prob (256x2048x209x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x209x2048): 65.020
Elapsed time for attention_prob_times_values (256x2048x2048x209): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x209): 76.096
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13376x13376, b=2048): 0.0115
Throughput (in TFLOP/s) for attention_linear_projection (4x13376x13376, b=2048): 254.494
Elapsed time for mlp_h_to_4h (4x13376x53504, b=2048): 0.0455
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13376x53504, b=2048): 257.542
Elapsed time for mlp_fused_gelu (2048x4x53504): 0.0015
Elapsed time for mlp_4h_to_h (4x53504x13376, b=2048): 0.0464
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53504x13376, b=2048): 252.938
Elapsed time for transformer_add_bias_dropout (2048x4x13376): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13376): 0.0006

Attention duration (in seconds): 0.0937
Attention throughput (in TFLOP/s): 134.693
MLP duration (in seconds): 0.0934
MLP throughput (in TFLOP/s): 251.183
Transformer duration (in seconds): 0.1902
Transformer throughput (in TFLOP/s): 189.693
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0683
Attention throughput (in TFLOP/s): 184.758
MLP duration (in seconds): 0.0945
MLP throughput (in TFLOP/s): 248.260
Transformer duration (in seconds): 0.1670
Transformer throughput (in TFLOP/s): 216.051
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 13440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13440x40320, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13440x40320, b=2048): 254.564
Elapsed time for attention_key_query_prob (256x2048x210x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x210x2048): 93.378
Elapsed time for attention_prob_times_values (256x2048x2048x210): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x210): 123.596
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13440x13440, b=2048): 0.0116
Throughput (in TFLOP/s) for attention_linear_projection (4x13440x13440, b=2048): 255.648
Elapsed time for mlp_h_to_4h (4x13440x53760, b=2048): 0.0460
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13440x53760, b=2048): 257.586
Elapsed time for mlp_fused_gelu (2048x4x53760): 0.0015
Elapsed time for mlp_4h_to_h (4x53760x13440, b=2048): 0.0465
Throughput (in TFLOP/s) for mlp_4h_to_h (4x53760x13440, b=2048): 254.672
Elapsed time for transformer_add_bias_dropout (2048x4x13440): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13440): 0.0005

Attention duration (in seconds): 0.0899
Attention throughput (in TFLOP/s): 141.701
MLP duration (in seconds): 0.0939
MLP throughput (in TFLOP/s): 252.076
Transformer duration (in seconds): 0.1869
Transformer throughput (in TFLOP/s): 194.838
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0630
Attention throughput (in TFLOP/s): 202.363
MLP duration (in seconds): 0.0951
MLP throughput (in TFLOP/s): 248.910
Transformer duration (in seconds): 0.1627
Transformer throughput (in TFLOP/s): 223.883
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 13504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13504x40512, b=2048): 0.0351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13504x40512, b=2048): 255.414
Elapsed time for attention_key_query_prob (256x2048x211x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x211x2048): 65.109
Elapsed time for attention_prob_times_values (256x2048x2048x211): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x211): 78.006
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13504x13504, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13504x13504, b=2048): 256.292
Elapsed time for mlp_h_to_4h (4x13504x54016, b=2048): 0.0463
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13504x54016, b=2048): 258.234
Elapsed time for mlp_fused_gelu (2048x4x54016): 0.0015
Elapsed time for mlp_4h_to_h (4x54016x13504, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54016x13504, b=2048): 253.832
Elapsed time for transformer_add_bias_dropout (2048x4x13504): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13504): 0.0006

Attention duration (in seconds): 0.0945
Attention throughput (in TFLOP/s): 136.066
MLP duration (in seconds): 0.0949
MLP throughput (in TFLOP/s): 251.987
Transformer duration (in seconds): 0.1925
Transformer throughput (in TFLOP/s): 191.000
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0689
Attention throughput (in TFLOP/s): 186.640
MLP duration (in seconds): 0.0960
MLP throughput (in TFLOP/s): 249.007
Transformer duration (in seconds): 0.1691
Transformer throughput (in TFLOP/s): 217.401
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 13568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13568x40704, b=2048): 0.0354
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13568x40704, b=2048): 255.456
Elapsed time for attention_key_query_prob (256x2048x212x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x212x2048): 94.475
Elapsed time for attention_prob_times_values (256x2048x2048x212): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x212): 124.222
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13568x13568, b=2048): 0.0117
Throughput (in TFLOP/s) for attention_linear_projection (4x13568x13568, b=2048): 257.239
Elapsed time for mlp_h_to_4h (4x13568x54272, b=2048): 0.0468
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13568x54272, b=2048): 257.918
Elapsed time for mlp_fused_gelu (2048x4x54272): 0.0015
Elapsed time for mlp_4h_to_h (4x54272x13568, b=2048): 0.0473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54272x13568, b=2048): 255.103
Elapsed time for transformer_add_bias_dropout (2048x4x13568): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13568): 0.0005

Attention duration (in seconds): 0.0906
Attention throughput (in TFLOP/s): 143.200
MLP duration (in seconds): 0.0956
MLP throughput (in TFLOP/s): 252.481
Transformer duration (in seconds): 0.1893
Transformer throughput (in TFLOP/s): 196.026
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0636
Attention throughput (in TFLOP/s): 204.133
MLP duration (in seconds): 0.0969
MLP throughput (in TFLOP/s): 248.920
Transformer duration (in seconds): 0.1649
Transformer throughput (in TFLOP/s): 224.989
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 13632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13632x40896, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13632x40896, b=2048): 254.850
Elapsed time for attention_key_query_prob (256x2048x213x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x213x2048): 65.466
Elapsed time for attention_prob_times_values (256x2048x2048x213): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x213): 78.596
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13632x13632, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_linear_projection (4x13632x13632, b=2048): 258.659
Elapsed time for mlp_h_to_4h (4x13632x54528, b=2048): 0.0471
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13632x54528, b=2048): 258.617
Elapsed time for mlp_fused_gelu (2048x4x54528): 0.0015
Elapsed time for mlp_4h_to_h (4x54528x13632, b=2048): 0.0479
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54528x13632, b=2048): 254.457
Elapsed time for transformer_add_bias_dropout (2048x4x13632): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13632): 0.0006

Attention duration (in seconds): 0.0954
Attention throughput (in TFLOP/s): 137.256
MLP duration (in seconds): 0.0965
MLP throughput (in TFLOP/s): 252.520
Transformer duration (in seconds): 0.1950
Transformer throughput (in TFLOP/s): 192.070
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0696
Attention throughput (in TFLOP/s): 188.190
MLP duration (in seconds): 0.0976
MLP throughput (in TFLOP/s): 249.454
Transformer duration (in seconds): 0.1713
Transformer throughput (in TFLOP/s): 218.646
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 13696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13696x41088, b=2048): 0.0359
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13696x41088, b=2048): 257.162
Elapsed time for attention_key_query_prob (256x2048x214x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x214x2048): 95.843
Elapsed time for attention_prob_times_values (256x2048x2048x214): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x214): 125.335
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13696x13696, b=2048): 0.0118
Throughput (in TFLOP/s) for attention_linear_projection (4x13696x13696, b=2048): 259.734
Elapsed time for mlp_h_to_4h (4x13696x54784, b=2048): 0.0476
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13696x54784, b=2048): 258.445
Elapsed time for mlp_fused_gelu (2048x4x54784): 0.0015
Elapsed time for mlp_4h_to_h (4x54784x13696, b=2048): 0.0482
Throughput (in TFLOP/s) for mlp_4h_to_h (4x54784x13696, b=2048): 254.803
Elapsed time for transformer_add_bias_dropout (2048x4x13696): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13696): 0.0006

Attention duration (in seconds): 0.0911
Attention throughput (in TFLOP/s): 144.998
MLP duration (in seconds): 0.0973
MLP throughput (in TFLOP/s): 252.623
Transformer duration (in seconds): 0.1916
Transformer throughput (in TFLOP/s): 197.302
Transformer - MLP - Attention (in seconds): 0.0031


Actual
------
Attention duration (in seconds): 0.0643
Attention throughput (in TFLOP/s): 205.630
MLP duration (in seconds): 0.0986
MLP throughput (in TFLOP/s): 249.468
Transformer duration (in seconds): 0.1669
Transformer throughput (in TFLOP/s): 226.524
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 13760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13760x41280, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13760x41280, b=2048): 253.076
Elapsed time for attention_key_query_prob (256x2048x215x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x215x2048): 65.858
Elapsed time for attention_prob_times_values (256x2048x2048x215): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x215): 77.660
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13760x13760, b=2048): 0.0119
Throughput (in TFLOP/s) for attention_linear_projection (4x13760x13760, b=2048): 260.440
Elapsed time for mlp_h_to_4h (4x13760x55040, b=2048): 0.0482
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13760x55040, b=2048): 257.505
Elapsed time for mlp_fused_gelu (2048x4x55040): 0.0015
Elapsed time for mlp_4h_to_h (4x55040x13760, b=2048): 0.0485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55040x13760, b=2048): 255.891
Elapsed time for transformer_add_bias_dropout (2048x4x13760): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13760): 0.0006

Attention duration (in seconds): 0.0966
Attention throughput (in TFLOP/s): 137.977
MLP duration (in seconds): 0.0982
MLP throughput (in TFLOP/s): 252.725
Transformer duration (in seconds): 0.1980
Transformer throughput (in TFLOP/s): 192.686
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0707
Attention throughput (in TFLOP/s): 188.601
MLP duration (in seconds): 0.0994
MLP throughput (in TFLOP/s): 249.636
Transformer duration (in seconds): 0.1745
Transformer throughput (in TFLOP/s): 218.575
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 13824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13824x41472, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13824x41472, b=2048): 256.926
Elapsed time for attention_key_query_prob (256x2048x216x2048): 0.0037
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x216x2048): 123.884
Elapsed time for attention_prob_times_values (256x2048x2048x216): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x216): 191.162
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13824x13824, b=2048): 0.0120
Throughput (in TFLOP/s) for attention_linear_projection (4x13824x13824, b=2048): 261.235
Elapsed time for mlp_h_to_4h (4x13824x55296, b=2048): 0.0481
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13824x55296, b=2048): 260.340
Elapsed time for mlp_fused_gelu (2048x4x55296): 0.0015
Elapsed time for mlp_4h_to_h (4x55296x13824, b=2048): 0.0487
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55296x13824, b=2048): 257.271
Elapsed time for transformer_add_bias_dropout (2048x4x13824): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13824): 0.0006

Attention duration (in seconds): 0.0897
Attention throughput (in TFLOP/s): 149.974
MLP duration (in seconds): 0.0983
MLP throughput (in TFLOP/s): 254.776
Transformer duration (in seconds): 0.1912
Transformer throughput (in TFLOP/s): 201.390
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0639
Attention throughput (in TFLOP/s): 210.544
MLP duration (in seconds): 0.0997
MLP throughput (in TFLOP/s): 251.122
Transformer duration (in seconds): 0.1677
Transformer throughput (in TFLOP/s): 229.556
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 13888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13888x41664, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13888x41664, b=2048): 256.261
Elapsed time for attention_key_query_prob (256x2048x217x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x217x2048): 66.338
Elapsed time for attention_prob_times_values (256x2048x2048x217): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x217): 77.945
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x13888x13888, b=2048): 0.0123
Throughput (in TFLOP/s) for attention_linear_projection (4x13888x13888, b=2048): 256.728
Elapsed time for mlp_h_to_4h (4x13888x55552, b=2048): 0.0487
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13888x55552, b=2048): 259.670
Elapsed time for mlp_fused_gelu (2048x4x55552): 0.0015
Elapsed time for mlp_4h_to_h (4x55552x13888, b=2048): 0.0496
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55552x13888, b=2048): 254.953
Elapsed time for transformer_add_bias_dropout (2048x4x13888): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13888): 0.0006

Attention duration (in seconds): 0.0973
Attention throughput (in TFLOP/s): 139.508
MLP duration (in seconds): 0.0998
MLP throughput (in TFLOP/s): 253.333
Transformer duration (in seconds): 0.2003
Transformer throughput (in TFLOP/s): 193.999
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0715
Attention throughput (in TFLOP/s): 189.720
MLP duration (in seconds): 0.1010
MLP throughput (in TFLOP/s): 250.409
Transformer duration (in seconds): 0.1762
Transformer throughput (in TFLOP/s): 220.539
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 13952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x13952x41856, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x13952x41856, b=2048): 258.381
Elapsed time for attention_key_query_prob (256x2048x218x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x218x2048): 97.131
Elapsed time for attention_prob_times_values (256x2048x2048x218): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x218): 128.076
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x13952x13952, b=2048): 0.0124
Throughput (in TFLOP/s) for attention_linear_projection (4x13952x13952, b=2048): 257.868
Elapsed time for mlp_h_to_4h (4x13952x55808, b=2048): 0.0491
Throughput (in TFLOP/s) for mlp_h_to_4h (4x13952x55808, b=2048): 259.769
Elapsed time for mlp_fused_gelu (2048x4x55808): 0.0015
Elapsed time for mlp_4h_to_h (4x55808x13952, b=2048): 0.0497
Throughput (in TFLOP/s) for mlp_4h_to_h (4x55808x13952, b=2048): 256.457
Elapsed time for transformer_add_bias_dropout (2048x4x13952): 0.0010
Elapsed time for transformer_layer_norm (2048x4x13952): 0.0006

Attention duration (in seconds): 0.0929
Attention throughput (in TFLOP/s): 147.478
MLP duration (in seconds): 0.1004
MLP throughput (in TFLOP/s): 254.143
Transformer duration (in seconds): 0.1964
Transformer throughput (in TFLOP/s): 199.586
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0662
Attention throughput (in TFLOP/s): 206.988
MLP duration (in seconds): 0.1020
MLP throughput (in TFLOP/s): 250.031
Transformer duration (in seconds): 0.1723
Transformer throughput (in TFLOP/s): 227.545
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 14016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14016x42048, b=2048): 0.0376
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14016x42048, b=2048): 256.592
Elapsed time for attention_key_query_prob (256x2048x219x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x219x2048): 67.084
Elapsed time for attention_prob_times_values (256x2048x2048x219): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x219): 81.160
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14016x14016, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x14016x14016, b=2048): 258.259
Elapsed time for mlp_h_to_4h (4x14016x56064, b=2048): 0.0494
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14016x56064, b=2048): 260.464
Elapsed time for mlp_fused_gelu (2048x4x56064): 0.0015
Elapsed time for mlp_4h_to_h (4x56064x14016, b=2048): 0.0503
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56064x14016, b=2048): 255.914
Elapsed time for transformer_add_bias_dropout (2048x4x14016): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14016): 0.0006

Attention duration (in seconds): 0.0979
Attention throughput (in TFLOP/s): 141.148
MLP duration (in seconds): 0.1013
MLP throughput (in TFLOP/s): 254.226
Transformer duration (in seconds): 0.2024
Transformer throughput (in TFLOP/s): 195.487
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0724
Attention throughput (in TFLOP/s): 190.730
MLP duration (in seconds): 0.1028
MLP throughput (in TFLOP/s): 250.563
Transformer duration (in seconds): 0.1792
Transformer throughput (in TFLOP/s): 220.722
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 14080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14080x42240, b=2048): 0.0378
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14080x42240, b=2048): 257.695
Elapsed time for attention_key_query_prob (256x2048x220x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x220x2048): 97.618
Elapsed time for attention_prob_times_values (256x2048x2048x220): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x220): 129.378
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14080x14080, b=2048): 0.0125
Throughput (in TFLOP/s) for attention_linear_projection (4x14080x14080, b=2048): 259.466
Elapsed time for mlp_h_to_4h (4x14080x56320, b=2048): 0.0499
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14080x56320, b=2048): 260.546
Elapsed time for mlp_fused_gelu (2048x4x56320): 0.0016
Elapsed time for mlp_4h_to_h (4x56320x14080, b=2048): 0.0505
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56320x14080, b=2048): 257.066
Elapsed time for transformer_add_bias_dropout (2048x4x14080): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14080): 0.0006

Attention duration (in seconds): 0.0938
Attention throughput (in TFLOP/s): 148.617
MLP duration (in seconds): 0.1020
MLP throughput (in TFLOP/s): 254.852
Transformer duration (in seconds): 0.1990
Transformer throughput (in TFLOP/s): 200.651
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0668
Attention throughput (in TFLOP/s): 208.590
MLP duration (in seconds): 0.1036
MLP throughput (in TFLOP/s): 250.769
Transformer duration (in seconds): 0.1746
Transformer throughput (in TFLOP/s): 228.682
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 14144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14144x42432, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14144x42432, b=2048): 256.989
Elapsed time for attention_key_query_prob (256x2048x221x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x221x2048): 67.884
Elapsed time for attention_prob_times_values (256x2048x2048x221): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x221): 82.012
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14144x14144, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x14144x14144, b=2048): 259.897
Elapsed time for mlp_h_to_4h (4x14144x56576, b=2048): 0.0507
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14144x56576, b=2048): 258.778
Elapsed time for mlp_fused_gelu (2048x4x56576): 0.0016
Elapsed time for mlp_4h_to_h (4x56576x14144, b=2048): 0.0514
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56576x14144, b=2048): 254.852
Elapsed time for transformer_add_bias_dropout (2048x4x14144): 0.0010
Elapsed time for transformer_layer_norm (2048x4x14144): 0.0006

Attention duration (in seconds): 0.0986
Attention throughput (in TFLOP/s): 142.585
MLP duration (in seconds): 0.1037
MLP throughput (in TFLOP/s): 252.933
Transformer duration (in seconds): 0.2055
Transformer throughput (in TFLOP/s): 195.981
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0733
Attention throughput (in TFLOP/s): 191.798
MLP duration (in seconds): 0.1046
MLP throughput (in TFLOP/s): 250.575
Transformer duration (in seconds): 0.1824
Transformer throughput (in TFLOP/s): 220.868
Transformer - MLP - Attention (in seconds): 0.0044
========================================================================================================================
num_attention_heads: 64, hidden_size: 14208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14208x42624, b=2048): 0.0384
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14208x42624, b=2048): 258.578
Elapsed time for attention_key_query_prob (256x2048x222x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x222x2048): 99.114
Elapsed time for attention_prob_times_values (256x2048x2048x222): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x222): 130.617
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14208x14208, b=2048): 0.0126
Throughput (in TFLOP/s) for attention_linear_projection (4x14208x14208, b=2048): 261.573
Elapsed time for mlp_h_to_4h (4x14208x56832, b=2048): 0.0511
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14208x56832, b=2048): 259.105
Elapsed time for mlp_fused_gelu (2048x4x56832): 0.0016
Elapsed time for mlp_4h_to_h (4x56832x14208, b=2048): 0.0518
Throughput (in TFLOP/s) for mlp_4h_to_h (4x56832x14208, b=2048): 255.476
Elapsed time for transformer_add_bias_dropout (2048x4x14208): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14208): 0.0006

Attention duration (in seconds): 0.0944
Attention throughput (in TFLOP/s): 150.184
MLP duration (in seconds): 0.1044
MLP throughput (in TFLOP/s): 253.418
Transformer duration (in seconds): 0.2021
Transformer throughput (in TFLOP/s): 201.107
Transformer - MLP - Attention (in seconds): 0.0032


Actual
------
Attention duration (in seconds): 0.0679
Attention throughput (in TFLOP/s): 208.945
MLP duration (in seconds): 0.1061
MLP throughput (in TFLOP/s): 249.439
Transformer duration (in seconds): 0.1784
Transformer throughput (in TFLOP/s): 227.788
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 14272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14272x42816, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14272x42816, b=2048): 255.027
Elapsed time for attention_key_query_prob (256x2048x223x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x223x2048): 69.074
Elapsed time for attention_prob_times_values (256x2048x2048x223): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x223): 81.446
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14272x14272, b=2048): 0.0130
Throughput (in TFLOP/s) for attention_linear_projection (4x14272x14272, b=2048): 255.819
Elapsed time for mlp_h_to_4h (4x14272x57088, b=2048): 0.0517
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14272x57088, b=2048): 258.354
Elapsed time for mlp_fused_gelu (2048x4x57088): 0.0016
Elapsed time for mlp_4h_to_h (4x57088x14272, b=2048): 0.0524
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57088x14272, b=2048): 254.850
Elapsed time for transformer_add_bias_dropout (2048x4x14272): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14272): 0.0006

Attention duration (in seconds): 0.1001
Attention throughput (in TFLOP/s): 142.959
MLP duration (in seconds): 0.1056
MLP throughput (in TFLOP/s): 252.767
Transformer duration (in seconds): 0.2090
Transformer throughput (in TFLOP/s): 196.214
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0748
Attention throughput (in TFLOP/s): 191.188
MLP duration (in seconds): 0.1079
MLP throughput (in TFLOP/s): 247.378
Transformer duration (in seconds): 0.1870
Transformer throughput (in TFLOP/s): 219.335
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 14336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14336x43008, b=2048): 0.0397
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14336x43008, b=2048): 254.423
Elapsed time for attention_key_query_prob (256x2048x224x2048): 0.0033
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x224x2048): 147.684
Elapsed time for attention_prob_times_values (256x2048x2048x224): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x224): 199.366
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14336x14336, b=2048): 0.0131
Throughput (in TFLOP/s) for attention_linear_projection (4x14336x14336, b=2048): 256.604
Elapsed time for mlp_h_to_4h (4x14336x57344, b=2048): 0.0520
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14336x57344, b=2048): 259.176
Elapsed time for mlp_fused_gelu (2048x4x57344): 0.0016
Elapsed time for mlp_4h_to_h (4x57344x14336, b=2048): 0.0525
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57344x14336, b=2048): 256.642
Elapsed time for transformer_add_bias_dropout (2048x4x14336): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14336): 0.0006

Attention duration (in seconds): 0.0935
Attention throughput (in TFLOP/s): 154.404
MLP duration (in seconds): 0.1060
MLP throughput (in TFLOP/s): 254.060
Transformer duration (in seconds): 0.2028
Transformer throughput (in TFLOP/s): 204.016
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0677
Attention throughput (in TFLOP/s): 213.277
MLP duration (in seconds): 0.1072
MLP throughput (in TFLOP/s): 251.356
Transformer duration (in seconds): 0.1789
Transformer throughput (in TFLOP/s): 231.263
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 14400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14400x43200, b=2048): 0.0394
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14400x43200, b=2048): 258.447
Elapsed time for attention_key_query_prob (256x2048x225x2048): 0.0072
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x225x2048): 66.846
Elapsed time for attention_prob_times_values (256x2048x2048x225): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x225): 81.898
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14400x14400, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14400x14400, b=2048): 257.562
Elapsed time for mlp_h_to_4h (4x14400x57600, b=2048): 0.0526
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14400x57600, b=2048): 258.429
Elapsed time for mlp_fused_gelu (2048x4x57600): 0.0016
Elapsed time for mlp_4h_to_h (4x57600x14400, b=2048): 0.0534
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57600x14400, b=2048): 254.334
Elapsed time for transformer_add_bias_dropout (2048x4x14400): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14400): 0.0006

Attention duration (in seconds): 0.1007
Attention throughput (in TFLOP/s): 144.518
MLP duration (in seconds): 0.1076
MLP throughput (in TFLOP/s): 252.582
Transformer duration (in seconds): 0.2116
Transformer throughput (in TFLOP/s): 197.193
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0758
Attention throughput (in TFLOP/s): 191.979
MLP duration (in seconds): 0.1086
MLP throughput (in TFLOP/s): 250.176
Transformer duration (in seconds): 0.1881
Transformer throughput (in TFLOP/s): 221.876
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 14464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14464x43392, b=2048): 0.0398
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14464x43392, b=2048): 258.324
Elapsed time for attention_key_query_prob (256x2048x226x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x226x2048): 88.792
Elapsed time for attention_prob_times_values (256x2048x2048x226): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x226): 132.471
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14464x14464, b=2048): 0.0132
Throughput (in TFLOP/s) for attention_linear_projection (4x14464x14464, b=2048): 259.170
Elapsed time for mlp_h_to_4h (4x14464x57856, b=2048): 0.0528
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14464x57856, b=2048): 259.649
Elapsed time for mlp_fused_gelu (2048x4x57856): 0.0016
Elapsed time for mlp_4h_to_h (4x57856x14464, b=2048): 0.0537
Throughput (in TFLOP/s) for mlp_4h_to_h (4x57856x14464, b=2048): 255.434
Elapsed time for transformer_add_bias_dropout (2048x4x14464): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14464): 0.0006

Attention duration (in seconds): 0.0971
Attention throughput (in TFLOP/s): 151.166
MLP duration (in seconds): 0.1081
MLP throughput (in TFLOP/s): 253.725
Transformer duration (in seconds): 0.2085
Transformer throughput (in TFLOP/s): 201.933
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0705
Attention throughput (in TFLOP/s): 208.165
MLP duration (in seconds): 0.1098
MLP throughput (in TFLOP/s): 249.637
Transformer duration (in seconds): 0.1855
Transformer throughput (in TFLOP/s): 226.945
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 14528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14528x43584, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14528x43584, b=2048): 255.599
Elapsed time for attention_key_query_prob (256x2048x227x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x227x2048): 66.714
Elapsed time for attention_prob_times_values (256x2048x2048x227): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x227): 83.979
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14528x14528, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x14528x14528, b=2048): 258.639
Elapsed time for mlp_h_to_4h (4x14528x58112, b=2048): 0.0536
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14528x58112, b=2048): 257.990
Elapsed time for mlp_fused_gelu (2048x4x58112): 0.0016
Elapsed time for mlp_4h_to_h (4x58112x14528, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58112x14528, b=2048): 253.730
Elapsed time for transformer_add_bias_dropout (2048x4x14528): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14528): 0.0006

Attention duration (in seconds): 0.1020
Attention throughput (in TFLOP/s): 145.125
MLP duration (in seconds): 0.1097
MLP throughput (in TFLOP/s): 252.107
Transformer duration (in seconds): 0.2151
Transformer throughput (in TFLOP/s): 197.451
Transformer - MLP - Attention (in seconds): 0.0033


Actual
------
Attention duration (in seconds): 0.0772
Attention throughput (in TFLOP/s): 191.730
MLP duration (in seconds): 0.1112
MLP throughput (in TFLOP/s): 248.857
Transformer duration (in seconds): 0.1924
Transformer throughput (in TFLOP/s): 220.729
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 14592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14592x43776, b=2048): 0.0405
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14592x43776, b=2048): 258.314
Elapsed time for attention_key_query_prob (256x2048x228x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x228x2048): 95.234
Elapsed time for attention_prob_times_values (256x2048x2048x228): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x228): 133.816
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14592x14592, b=2048): 0.0134
Throughput (in TFLOP/s) for attention_linear_projection (4x14592x14592, b=2048): 260.881
Elapsed time for mlp_h_to_4h (4x14592x58368, b=2048): 0.0535
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14592x58368, b=2048): 261.008
Elapsed time for mlp_fused_gelu (2048x4x58368): 0.0016
Elapsed time for mlp_4h_to_h (4x58368x14592, b=2048): 0.0545
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58368x14592, b=2048): 256.020
Elapsed time for transformer_add_bias_dropout (2048x4x14592): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14592): 0.0006

Attention duration (in seconds): 0.0977
Attention throughput (in TFLOP/s): 152.926
MLP duration (in seconds): 0.1096
MLP throughput (in TFLOP/s): 254.697
Transformer duration (in seconds): 0.2106
Transformer throughput (in TFLOP/s): 203.437
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0710
Attention throughput (in TFLOP/s): 210.272
MLP duration (in seconds): 0.1111
MLP throughput (in TFLOP/s): 251.309
Transformer duration (in seconds): 0.1863
Transformer throughput (in TFLOP/s): 229.972
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 14656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14656x43968, b=2048): 0.0408
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14656x43968, b=2048): 258.536
Elapsed time for attention_key_query_prob (256x2048x229x2048): 0.0073
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x229x2048): 67.206
Elapsed time for attention_prob_times_values (256x2048x2048x229): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x229): 84.510
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14656x14656, b=2048): 0.0137
Throughput (in TFLOP/s) for attention_linear_projection (4x14656x14656, b=2048): 256.209
Elapsed time for mlp_h_to_4h (4x14656x58624, b=2048): 0.0542
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14656x58624, b=2048): 259.491
Elapsed time for mlp_fused_gelu (2048x4x58624): 0.0016
Elapsed time for mlp_4h_to_h (4x58624x14656, b=2048): 0.0556
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58624x14656, b=2048): 253.029
Elapsed time for transformer_add_bias_dropout (2048x4x14656): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14656): 0.0006

Attention duration (in seconds): 0.1027
Attention throughput (in TFLOP/s): 146.694
MLP duration (in seconds): 0.1115
MLP throughput (in TFLOP/s): 252.507
Transformer duration (in seconds): 0.2175
Transformer throughput (in TFLOP/s): 198.661
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0781
Attention throughput (in TFLOP/s): 192.842
MLP duration (in seconds): 0.1132
MLP throughput (in TFLOP/s): 248.697
Transformer duration (in seconds): 0.1963
Transformer throughput (in TFLOP/s): 220.198
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 14720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14720x44160, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14720x44160, b=2048): 255.317
Elapsed time for attention_key_query_prob (256x2048x230x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x230x2048): 96.360
Elapsed time for attention_prob_times_values (256x2048x2048x230): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x230): 133.760
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14720x14720, b=2048): 0.0138
Throughput (in TFLOP/s) for attention_linear_projection (4x14720x14720, b=2048): 256.431
Elapsed time for mlp_h_to_4h (4x14720x58880, b=2048): 0.0550
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14720x58880, b=2048): 258.360
Elapsed time for mlp_fused_gelu (2048x4x58880): 0.0016
Elapsed time for mlp_4h_to_h (4x58880x14720, b=2048): 0.0557
Throughput (in TFLOP/s) for mlp_4h_to_h (4x58880x14720, b=2048): 255.056
Elapsed time for transformer_add_bias_dropout (2048x4x14720): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14720): 0.0006

Attention duration (in seconds): 0.0993
Attention throughput (in TFLOP/s): 152.896
MLP duration (in seconds): 0.1123
MLP throughput (in TFLOP/s): 252.986
Transformer duration (in seconds): 0.2150
Transformer throughput (in TFLOP/s): 202.772
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0726
Attention throughput (in TFLOP/s): 209.171
MLP duration (in seconds): 0.1139
MLP throughput (in TFLOP/s): 249.288
Transformer duration (in seconds): 0.1914
Transformer throughput (in TFLOP/s): 227.700
Transformer - MLP - Attention (in seconds): 0.0049
========================================================================================================================
num_attention_heads: 64, hidden_size: 14784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14784x44352, b=2048): 0.0420
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14784x44352, b=2048): 255.747
Elapsed time for attention_key_query_prob (256x2048x231x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x231x2048): 67.320
Elapsed time for attention_prob_times_values (256x2048x2048x231): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x231): 83.141
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14784x14784, b=2048): 0.0139
Throughput (in TFLOP/s) for attention_linear_projection (4x14784x14784, b=2048): 257.174
Elapsed time for mlp_h_to_4h (4x14784x59136, b=2048): 0.0553
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14784x59136, b=2048): 258.999
Elapsed time for mlp_fused_gelu (2048x4x59136): 0.0016
Elapsed time for mlp_4h_to_h (4x59136x14784, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59136x14784, b=2048): 254.870
Elapsed time for transformer_add_bias_dropout (2048x4x14784): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14784): 0.0006

Attention duration (in seconds): 0.1042
Attention throughput (in TFLOP/s): 146.939
MLP duration (in seconds): 0.1131
MLP throughput (in TFLOP/s): 253.216
Transformer duration (in seconds): 0.2208
Transformer throughput (in TFLOP/s): 199.141
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0791
Attention throughput (in TFLOP/s): 193.647
MLP duration (in seconds): 0.1143
MLP throughput (in TFLOP/s): 250.720
Transformer duration (in seconds): 0.1975
Transformer throughput (in TFLOP/s): 222.580
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 14848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14848x44544, b=2048): 0.0418
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14848x44544, b=2048): 259.044
Elapsed time for attention_key_query_prob (256x2048x232x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x232x2048): 130.867
Elapsed time for attention_prob_times_values (256x2048x2048x232): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x232): 203.153
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x14848x14848, b=2048): 0.0140
Throughput (in TFLOP/s) for attention_linear_projection (4x14848x14848, b=2048): 258.702
Elapsed time for mlp_h_to_4h (4x14848x59392, b=2048): 0.0557
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14848x59392, b=2048): 259.447
Elapsed time for mlp_fused_gelu (2048x4x59392): 0.0016
Elapsed time for mlp_4h_to_h (4x59392x14848, b=2048): 0.0564
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59392x14848, b=2048): 256.080
Elapsed time for transformer_add_bias_dropout (2048x4x14848): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14848): 0.0006

Attention duration (in seconds): 0.0970
Attention throughput (in TFLOP/s): 159.191
MLP duration (in seconds): 0.1137
MLP throughput (in TFLOP/s): 254.045
Transformer duration (in seconds): 0.2142
Transformer throughput (in TFLOP/s): 207.035
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0727
Attention throughput (in TFLOP/s): 212.335
MLP duration (in seconds): 0.1158
MLP throughput (in TFLOP/s): 249.596
Transformer duration (in seconds): 0.1936
Transformer throughput (in TFLOP/s): 229.087
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 64, hidden_size: 14912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14912x44736, b=2048): 0.0428
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14912x44736, b=2048): 255.382
Elapsed time for attention_key_query_prob (256x2048x233x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x233x2048): 67.635
Elapsed time for attention_prob_times_values (256x2048x2048x233): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x233): 83.595
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x14912x14912, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x14912x14912, b=2048): 258.603
Elapsed time for mlp_h_to_4h (4x14912x59648, b=2048): 0.0562
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14912x59648, b=2048): 259.399
Elapsed time for mlp_fused_gelu (2048x4x59648): 0.0016
Elapsed time for mlp_4h_to_h (4x59648x14912, b=2048): 0.0570
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59648x14912, b=2048): 255.768
Elapsed time for transformer_add_bias_dropout (2048x4x14912): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14912): 0.0006

Attention duration (in seconds): 0.1053
Attention throughput (in TFLOP/s): 147.965
MLP duration (in seconds): 0.1148
MLP throughput (in TFLOP/s): 253.874
Transformer duration (in seconds): 0.2235
Transformer throughput (in TFLOP/s): 200.090
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0800
Attention throughput (in TFLOP/s): 194.779
MLP duration (in seconds): 0.1161
MLP throughput (in TFLOP/s): 250.962
Transformer duration (in seconds): 0.2002
Transformer throughput (in TFLOP/s): 223.334
Transformer - MLP - Attention (in seconds): 0.0041
========================================================================================================================
num_attention_heads: 64, hidden_size: 14976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x14976x44928, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x14976x44928, b=2048): 259.887
Elapsed time for attention_key_query_prob (256x2048x234x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x234x2048): 98.350
Elapsed time for attention_prob_times_values (256x2048x2048x234): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x234): 136.862
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x14976x14976, b=2048): 0.0141
Throughput (in TFLOP/s) for attention_linear_projection (4x14976x14976, b=2048): 260.398
Elapsed time for mlp_h_to_4h (4x14976x59904, b=2048): 0.0564
Throughput (in TFLOP/s) for mlp_h_to_4h (4x14976x59904, b=2048): 260.787
Elapsed time for mlp_fused_gelu (2048x4x59904): 0.0017
Elapsed time for mlp_4h_to_h (4x59904x14976, b=2048): 0.0574
Throughput (in TFLOP/s) for mlp_4h_to_h (4x59904x14976, b=2048): 256.079
Elapsed time for transformer_add_bias_dropout (2048x4x14976): 0.0011
Elapsed time for transformer_layer_norm (2048x4x14976): 0.0006

Attention duration (in seconds): 0.1003
Attention throughput (in TFLOP/s): 156.591
MLP duration (in seconds): 0.1154
MLP throughput (in TFLOP/s): 254.706
Transformer duration (in seconds): 0.2191
Transformer throughput (in TFLOP/s): 205.812
Transformer - MLP - Attention (in seconds): 0.0034


Actual
------
Attention duration (in seconds): 0.0743
Attention throughput (in TFLOP/s): 211.235
MLP duration (in seconds): 0.1175
MLP throughput (in TFLOP/s): 250.085
Transformer duration (in seconds): 0.1970
Transformer throughput (in TFLOP/s): 228.967
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 15040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15040x45120, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15040x45120, b=2048): 255.684
Elapsed time for attention_key_query_prob (256x2048x235x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x235x2048): 68.496
Elapsed time for attention_prob_times_values (256x2048x2048x235): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x235): 87.051
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x15040x15040, b=2048): 0.0143
Throughput (in TFLOP/s) for attention_linear_projection (4x15040x15040, b=2048): 259.873
Elapsed time for mlp_h_to_4h (4x15040x60160, b=2048): 0.0574
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15040x60160, b=2048): 258.338
Elapsed time for mlp_fused_gelu (2048x4x60160): 0.0017
Elapsed time for mlp_4h_to_h (4x60160x15040, b=2048): 0.0582
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60160x15040, b=2048): 254.666
Elapsed time for transformer_add_bias_dropout (2048x4x15040): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15040): 0.0006

Attention duration (in seconds): 0.1059
Attention throughput (in TFLOP/s): 149.536
MLP duration (in seconds): 0.1173
MLP throughput (in TFLOP/s): 252.862
Transformer duration (in seconds): 0.2266
Transformer throughput (in TFLOP/s): 200.715
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0812
Attention throughput (in TFLOP/s): 195.044
MLP duration (in seconds): 0.1192
MLP throughput (in TFLOP/s): 248.753
Transformer duration (in seconds): 0.2055
Transformer throughput (in TFLOP/s): 221.331
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 15104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15104x45312, b=2048): 0.0438
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15104x45312, b=2048): 256.104
Elapsed time for attention_key_query_prob (256x2048x236x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x236x2048): 98.817
Elapsed time for attention_prob_times_values (256x2048x2048x236): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x236): 137.941
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15104x15104, b=2048): 0.0142
Throughput (in TFLOP/s) for attention_linear_projection (4x15104x15104, b=2048): 262.322
Elapsed time for mlp_h_to_4h (4x15104x60416, b=2048): 0.0575
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15104x60416, b=2048): 260.126
Elapsed time for mlp_fused_gelu (2048x4x60416): 0.0017
Elapsed time for mlp_4h_to_h (4x60416x15104, b=2048): 0.0584
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60416x15104, b=2048): 256.199
Elapsed time for transformer_add_bias_dropout (2048x4x15104): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15104): 0.0006

Attention duration (in seconds): 0.1018
Attention throughput (in TFLOP/s): 156.826
MLP duration (in seconds): 0.1175
MLP throughput (in TFLOP/s): 254.486
Transformer duration (in seconds): 0.2227
Transformer throughput (in TFLOP/s): 205.909
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0749
Attention throughput (in TFLOP/s): 213.183
MLP duration (in seconds): 0.1191
MLP throughput (in TFLOP/s): 251.095
Transformer duration (in seconds): 0.1983
Transformer throughput (in TFLOP/s): 231.299
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 15168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15168x45504, b=2048): 0.0438
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15168x45504, b=2048): 258.186
Elapsed time for attention_key_query_prob (256x2048x237x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x237x2048): 69.083
Elapsed time for attention_prob_times_values (256x2048x2048x237): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x237): 87.653
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15168x15168, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x15168x15168, b=2048): 257.255
Elapsed time for mlp_h_to_4h (4x15168x60672, b=2048): 0.0579
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15168x60672, b=2048): 260.493
Elapsed time for mlp_fused_gelu (2048x4x60672): 0.0017
Elapsed time for mlp_4h_to_h (4x60672x15168, b=2048): 0.0591
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60672x15168, b=2048): 255.299
Elapsed time for transformer_add_bias_dropout (2048x4x15168): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15168): 0.0006

Attention duration (in seconds): 0.1066
Attention throughput (in TFLOP/s): 151.014
MLP duration (in seconds): 0.1186
MLP throughput (in TFLOP/s): 254.238
Transformer duration (in seconds): 0.2287
Transformer throughput (in TFLOP/s): 202.251
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0820
Attention throughput (in TFLOP/s): 196.369
MLP duration (in seconds): 0.1204
MLP throughput (in TFLOP/s): 250.382
Transformer duration (in seconds): 0.2067
Transformer throughput (in TFLOP/s): 223.799
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 15232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15232x45696, b=2048): 0.0440
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15232x45696, b=2048): 259.263
Elapsed time for attention_key_query_prob (256x2048x238x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x238x2048): 93.841
Elapsed time for attention_prob_times_values (256x2048x2048x238): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x238): 138.869
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15232x15232, b=2048): 0.0147
Throughput (in TFLOP/s) for attention_linear_projection (4x15232x15232, b=2048): 258.192
Elapsed time for mlp_h_to_4h (4x15232x60928, b=2048): 0.0585
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15232x60928, b=2048): 260.093
Elapsed time for mlp_fused_gelu (2048x4x60928): 0.0017
Elapsed time for mlp_4h_to_h (4x60928x15232, b=2048): 0.0594
Throughput (in TFLOP/s) for mlp_4h_to_h (4x60928x15232, b=2048): 255.872
Elapsed time for transformer_add_bias_dropout (2048x4x15232): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15232): 0.0006

Attention duration (in seconds): 0.1028
Attention throughput (in TFLOP/s): 157.863
MLP duration (in seconds): 0.1196
MLP throughput (in TFLOP/s): 254.344
Transformer duration (in seconds): 0.2258
Transformer throughput (in TFLOP/s): 206.511
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0766
Attention throughput (in TFLOP/s): 211.831
MLP duration (in seconds): 0.1215
MLP throughput (in TFLOP/s): 250.277
Transformer duration (in seconds): 0.2063
Transformer throughput (in TFLOP/s): 226.026
Transformer - MLP - Attention (in seconds): 0.0082
========================================================================================================================
num_attention_heads: 64, hidden_size: 15296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15296x45888, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15296x45888, b=2048): 255.143
Elapsed time for attention_key_query_prob (256x2048x239x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x239x2048): 69.099
Elapsed time for attention_prob_times_values (256x2048x2048x239): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x239): 86.791
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15296x15296, b=2048): 0.0148
Throughput (in TFLOP/s) for attention_linear_projection (4x15296x15296, b=2048): 258.507
Elapsed time for mlp_h_to_4h (4x15296x61184, b=2048): 0.0593
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15296x61184, b=2048): 258.471
Elapsed time for mlp_fused_gelu (2048x4x61184): 0.0017
Elapsed time for mlp_4h_to_h (4x61184x15296, b=2048): 0.0604
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61184x15296, b=2048): 253.700
Elapsed time for transformer_add_bias_dropout (2048x4x15296): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15296): 0.0006

Attention duration (in seconds): 0.1082
Attention throughput (in TFLOP/s): 151.191
MLP duration (in seconds): 0.1214
MLP throughput (in TFLOP/s): 252.511
Transformer duration (in seconds): 0.2332
Transformer throughput (in TFLOP/s): 201.684
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0835
Attention throughput (in TFLOP/s): 196.024
MLP duration (in seconds): 0.1231
MLP throughput (in TFLOP/s): 249.065
Transformer duration (in seconds): 0.2119
Transformer throughput (in TFLOP/s): 221.968
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 15360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15360x46080, b=2048): 0.0454
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15360x46080, b=2048): 255.619
Elapsed time for attention_key_query_prob (256x2048x240x2048): 0.0036
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x240x2048): 144.205
Elapsed time for attention_prob_times_values (256x2048x2048x240): 0.0024
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x240): 210.588
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15360x15360, b=2048): 0.0150
Throughput (in TFLOP/s) for attention_linear_projection (4x15360x15360, b=2048): 258.037
Elapsed time for mlp_h_to_4h (4x15360x61440, b=2048): 0.0600
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15360x61440, b=2048): 257.849
Elapsed time for mlp_fused_gelu (2048x4x61440): 0.0017
Elapsed time for mlp_4h_to_h (4x61440x15360, b=2048): 0.0605
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61440x15360, b=2048): 255.367
Elapsed time for transformer_add_bias_dropout (2048x4x15360): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15360): 0.0006

Attention duration (in seconds): 0.1013
Attention throughput (in TFLOP/s): 162.762
MLP duration (in seconds): 0.1222
MLP throughput (in TFLOP/s): 253.049
Transformer duration (in seconds): 0.2270
Transformer throughput (in TFLOP/s): 208.852
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0767
Attention throughput (in TFLOP/s): 215.004
MLP duration (in seconds): 0.1244
MLP throughput (in TFLOP/s): 248.568
Transformer duration (in seconds): 0.2043
Transformer throughput (in TFLOP/s): 232.127
Transformer - MLP - Attention (in seconds): 0.0032
========================================================================================================================
num_attention_heads: 64, hidden_size: 15424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15424x46272, b=2048): 0.0452
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15424x46272, b=2048): 258.554
Elapsed time for attention_key_query_prob (256x2048x241x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x241x2048): 69.719
Elapsed time for attention_prob_times_values (256x2048x2048x241): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x241): 87.333
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15424x15424, b=2048): 0.0149
Throughput (in TFLOP/s) for attention_linear_projection (4x15424x15424, b=2048): 260.851
Elapsed time for mlp_h_to_4h (4x15424x61696, b=2048): 0.0601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15424x61696, b=2048): 259.611
Elapsed time for mlp_fused_gelu (2048x4x61696): 0.0017
Elapsed time for mlp_4h_to_h (4x61696x15424, b=2048): 0.0612
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61696x15424, b=2048): 254.687
Elapsed time for transformer_add_bias_dropout (2048x4x15424): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15424): 0.0006

Attention duration (in seconds): 0.1085
Attention throughput (in TFLOP/s): 153.268
MLP duration (in seconds): 0.1230
MLP throughput (in TFLOP/s): 253.571
Transformer duration (in seconds): 0.2350
Transformer throughput (in TFLOP/s): 203.443
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0843
Attention throughput (in TFLOP/s): 197.144
MLP duration (in seconds): 0.1249
MLP throughput (in TFLOP/s): 249.695
Transformer duration (in seconds): 0.2144
Transformer throughput (in TFLOP/s): 222.996
Transformer - MLP - Attention (in seconds): 0.0052
========================================================================================================================
num_attention_heads: 64, hidden_size: 15488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15488x46464, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15488x46464, b=2048): 255.954
Elapsed time for attention_key_query_prob (256x2048x242x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x242x2048): 101.071
Elapsed time for attention_prob_times_values (256x2048x2048x242): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x242): 140.796
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15488x15488, b=2048): 0.0152
Throughput (in TFLOP/s) for attention_linear_projection (4x15488x15488, b=2048): 259.327
Elapsed time for mlp_h_to_4h (4x15488x61952, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15488x61952, b=2048): 257.567
Elapsed time for mlp_fused_gelu (2048x4x61952): 0.0017
Elapsed time for mlp_4h_to_h (4x61952x15488, b=2048): 0.0619
Throughput (in TFLOP/s) for mlp_4h_to_h (4x61952x15488, b=2048): 253.970
Elapsed time for transformer_add_bias_dropout (2048x4x15488): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15488): 0.0006

Attention duration (in seconds): 0.1050
Attention throughput (in TFLOP/s): 159.598
MLP duration (in seconds): 0.1246
MLP throughput (in TFLOP/s): 252.255
Transformer duration (in seconds): 0.2332
Transformer throughput (in TFLOP/s): 206.707
Transformer - MLP - Attention (in seconds): 0.0035


Actual
------
Attention duration (in seconds): 0.0785
Attention throughput (in TFLOP/s): 213.586
MLP duration (in seconds): 0.1255
MLP throughput (in TFLOP/s): 250.600
Transformer duration (in seconds): 0.2083
Transformer throughput (in TFLOP/s): 231.432
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 15552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15552x46656, b=2048): 0.0459
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15552x46656, b=2048): 259.182
Elapsed time for attention_key_query_prob (256x2048x243x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x243x2048): 70.631
Elapsed time for attention_prob_times_values (256x2048x2048x243): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x243): 90.143
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15552x15552, b=2048): 0.0154
Throughput (in TFLOP/s) for attention_linear_projection (4x15552x15552, b=2048): 257.572
Elapsed time for mlp_h_to_4h (4x15552x62208, b=2048): 0.0610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15552x62208, b=2048): 259.876
Elapsed time for mlp_fused_gelu (2048x4x62208): 0.0017
Elapsed time for mlp_4h_to_h (4x62208x15552, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62208x15552, b=2048): 253.429
Elapsed time for transformer_add_bias_dropout (2048x4x15552): 0.0011
Elapsed time for transformer_layer_norm (2048x4x15552): 0.0007

Attention duration (in seconds): 0.1094
Attention throughput (in TFLOP/s): 154.440
MLP duration (in seconds): 0.1253
MLP throughput (in TFLOP/s): 253.100
Transformer duration (in seconds): 0.2382
Transformer throughput (in TFLOP/s): 203.973
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0853
Attention throughput (in TFLOP/s): 197.956
MLP duration (in seconds): 0.1270
MLP throughput (in TFLOP/s): 249.686
Transformer duration (in seconds): 0.2168
Transformer throughput (in TFLOP/s): 224.105
Transformer - MLP - Attention (in seconds): 0.0045
========================================================================================================================
num_attention_heads: 64, hidden_size: 15616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15616x46848, b=2048): 0.0466
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15616x46848, b=2048): 257.012
Elapsed time for attention_key_query_prob (256x2048x244x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x244x2048): 102.499
Elapsed time for attention_prob_times_values (256x2048x2048x244): 0.0037
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x244): 142.753
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15616x15616, b=2048): 0.0155
Throughput (in TFLOP/s) for attention_linear_projection (4x15616x15616, b=2048): 257.536
Elapsed time for mlp_h_to_4h (4x15616x62464, b=2048): 0.0618
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15616x62464, b=2048): 258.476
Elapsed time for mlp_fused_gelu (2048x4x62464): 0.0017
Elapsed time for mlp_4h_to_h (4x62464x15616, b=2048): 0.0625
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62464x15616, b=2048): 255.717
Elapsed time for transformer_add_bias_dropout (2048x4x15616): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15616): 0.0006

Attention duration (in seconds): 0.1059
Attention throughput (in TFLOP/s): 160.817
MLP duration (in seconds): 0.1260
MLP throughput (in TFLOP/s): 253.579
Transformer duration (in seconds): 0.2355
Transformer throughput (in TFLOP/s): 208.037
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0796
Attention throughput (in TFLOP/s): 214.062
MLP duration (in seconds): 0.1282
MLP throughput (in TFLOP/s): 249.263
Transformer duration (in seconds): 0.2128
Transformer throughput (in TFLOP/s): 230.269
Transformer - MLP - Attention (in seconds): 0.0050
========================================================================================================================
num_attention_heads: 64, hidden_size: 15680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15680x47040, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15680x47040, b=2048): 258.223
Elapsed time for attention_key_query_prob (256x2048x245x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x245x2048): 71.427
Elapsed time for attention_prob_times_values (256x2048x2048x245): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x245): 90.901
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0310
Elapsed time for attention_linear_projection (4x15680x15680, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_linear_projection (4x15680x15680, b=2048): 258.430
Elapsed time for mlp_h_to_4h (4x15680x62720, b=2048): 0.0621
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15680x62720, b=2048): 259.469
Elapsed time for mlp_fused_gelu (2048x4x62720): 0.0017
Elapsed time for mlp_4h_to_h (4x62720x15680, b=2048): 0.0635
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62720x15680, b=2048): 253.856
Elapsed time for transformer_add_bias_dropout (2048x4x15680): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15680): 0.0006

Attention duration (in seconds): 0.1111
Attention throughput (in TFLOP/s): 154.536
MLP duration (in seconds): 0.1273
MLP throughput (in TFLOP/s): 253.149
Transformer duration (in seconds): 0.2420
Transformer throughput (in TFLOP/s): 204.121
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0864
Attention throughput (in TFLOP/s): 198.575
MLP duration (in seconds): 0.1292
MLP throughput (in TFLOP/s): 249.397
Transformer duration (in seconds): 0.2203
Transformer throughput (in TFLOP/s): 224.153
Transformer - MLP - Attention (in seconds): 0.0047
========================================================================================================================
num_attention_heads: 64, hidden_size: 15744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15744x47232, b=2048): 0.0474
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15744x47232, b=2048): 257.181
Elapsed time for attention_key_query_prob (256x2048x246x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x246x2048): 103.037
Elapsed time for attention_prob_times_values (256x2048x2048x246): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x246): 144.776
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15744x15744, b=2048): 0.0156
Throughput (in TFLOP/s) for attention_linear_projection (4x15744x15744, b=2048): 259.884
Elapsed time for mlp_h_to_4h (4x15744x62976, b=2048): 0.0624
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15744x62976, b=2048): 260.255
Elapsed time for mlp_fused_gelu (2048x4x62976): 0.0017
Elapsed time for mlp_4h_to_h (4x62976x15744, b=2048): 0.0638
Throughput (in TFLOP/s) for mlp_4h_to_h (4x62976x15744, b=2048): 254.444
Elapsed time for transformer_add_bias_dropout (2048x4x15744): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15744): 0.0006

Attention duration (in seconds): 0.1067
Attention throughput (in TFLOP/s): 162.094
MLP duration (in seconds): 0.1280
MLP throughput (in TFLOP/s): 253.829
Transformer duration (in seconds): 0.2383
Transformer throughput (in TFLOP/s): 208.922
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0803
Attention throughput (in TFLOP/s): 215.428
MLP duration (in seconds): 0.1299
MLP throughput (in TFLOP/s): 250.149
Transformer duration (in seconds): 0.2158
Transformer throughput (in TFLOP/s): 230.757
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 64, hidden_size: 15808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15808x47424, b=2048): 0.0476
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15808x47424, b=2048): 257.863
Elapsed time for attention_key_query_prob (256x2048x247x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x247x2048): 70.749
Elapsed time for attention_prob_times_values (256x2048x2048x247): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x247): 90.461
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15808x15808, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x15808x15808, b=2048): 258.872
Elapsed time for mlp_h_to_4h (4x15808x63232, b=2048): 0.0635
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15808x63232, b=2048): 257.797
Elapsed time for mlp_fused_gelu (2048x4x63232): 0.0017
Elapsed time for mlp_4h_to_h (4x63232x15808, b=2048): 0.0648
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63232x15808, b=2048): 252.792
Elapsed time for transformer_add_bias_dropout (2048x4x15808): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15808): 0.0006

Attention duration (in seconds): 0.1118
Attention throughput (in TFLOP/s): 156.019
MLP duration (in seconds): 0.1301
MLP throughput (in TFLOP/s): 251.852
Transformer duration (in seconds): 0.2454
Transformer throughput (in TFLOP/s): 204.491
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0874
Attention throughput (in TFLOP/s): 199.583
MLP duration (in seconds): 0.1311
MLP throughput (in TFLOP/s): 249.863
Transformer duration (in seconds): 0.2226
Transformer throughput (in TFLOP/s): 225.454
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 15872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15872x47616, b=2048): 0.0479
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15872x47616, b=2048): 258.325
Elapsed time for attention_key_query_prob (256x2048x248x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x248x2048): 130.106
Elapsed time for attention_prob_times_values (256x2048x2048x248): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x248): 213.860
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x15872x15872, b=2048): 0.0158
Throughput (in TFLOP/s) for attention_linear_projection (4x15872x15872, b=2048): 261.121
Elapsed time for mlp_h_to_4h (4x15872x63488, b=2048): 0.0636
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15872x63488, b=2048): 259.693
Elapsed time for mlp_fused_gelu (2048x4x63488): 0.0017
Elapsed time for mlp_4h_to_h (4x63488x15872, b=2048): 0.0648
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63488x15872, b=2048): 254.614
Elapsed time for transformer_add_bias_dropout (2048x4x15872): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15872): 0.0006

Attention duration (in seconds): 0.1053
Attention throughput (in TFLOP/s): 166.920
MLP duration (in seconds): 0.1302
MLP throughput (in TFLOP/s): 253.676
Transformer duration (in seconds): 0.2391
Transformer throughput (in TFLOP/s): 211.631
Transformer - MLP - Attention (in seconds): 0.0036


Actual
------
Attention duration (in seconds): 0.0810
Attention throughput (in TFLOP/s): 216.985
MLP duration (in seconds): 0.1325
MLP throughput (in TFLOP/s): 249.262
Transformer duration (in seconds): 0.2188
Transformer throughput (in TFLOP/s): 231.231
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 15936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x15936x47808, b=2048): 0.0488
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x15936x47808, b=2048): 255.819
Elapsed time for attention_key_query_prob (256x2048x249x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x249x2048): 71.377
Elapsed time for attention_prob_times_values (256x2048x2048x249): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x249): 91.325
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x15936x15936, b=2048): 0.0162
Throughput (in TFLOP/s) for attention_linear_projection (4x15936x15936, b=2048): 256.220
Elapsed time for mlp_h_to_4h (4x15936x63744, b=2048): 0.0645
Throughput (in TFLOP/s) for mlp_h_to_4h (4x15936x63744, b=2048): 257.963
Elapsed time for mlp_fused_gelu (2048x4x63744): 0.0018
Elapsed time for mlp_4h_to_h (4x63744x15936, b=2048): 0.0651
Throughput (in TFLOP/s) for mlp_4h_to_h (4x63744x15936, b=2048): 255.527
Elapsed time for transformer_add_bias_dropout (2048x4x15936): 0.0012
Elapsed time for transformer_layer_norm (2048x4x15936): 0.0007

Attention duration (in seconds): 0.1133
Attention throughput (in TFLOP/s): 156.279
MLP duration (in seconds): 0.1314
MLP throughput (in TFLOP/s): 253.308
Transformer duration (in seconds): 0.2484
Transformer throughput (in TFLOP/s): 205.304
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0887
Attention throughput (in TFLOP/s): 199.779
MLP duration (in seconds): 0.1338
MLP throughput (in TFLOP/s): 248.795
Transformer duration (in seconds): 0.2280
Transformer throughput (in TFLOP/s): 223.730
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 64, hidden_size: 16000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16000x48000, b=2048): 0.0492
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16000x48000, b=2048): 255.968
Elapsed time for attention_key_query_prob (256x2048x250x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x250x2048): 105.222
Elapsed time for attention_prob_times_values (256x2048x2048x250): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x250): 147.336
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16000x16000, b=2048): 0.0163
Throughput (in TFLOP/s) for attention_linear_projection (4x16000x16000, b=2048): 256.601
Elapsed time for mlp_h_to_4h (4x16000x64000, b=2048): 0.0650
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16000x64000, b=2048): 258.223
Elapsed time for mlp_fused_gelu (2048x4x64000): 0.0018
Elapsed time for mlp_4h_to_h (4x64000x16000, b=2048): 0.0668
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64000x16000, b=2048): 251.059
Elapsed time for transformer_add_bias_dropout (2048x4x16000): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16000): 0.0006

Attention duration (in seconds): 0.1092
Attention throughput (in TFLOP/s): 163.466
MLP duration (in seconds): 0.1336
MLP throughput (in TFLOP/s): 251.231
Transformer duration (in seconds): 0.2464
Transformer throughput (in TFLOP/s): 208.607
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0832
Attention throughput (in TFLOP/s): 214.578
MLP duration (in seconds): 0.1347
MLP throughput (in TFLOP/s): 249.079
Transformer duration (in seconds): 0.2236
Transformer throughput (in TFLOP/s): 229.871
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 16064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16064x48192, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16064x48192, b=2048): 257.318
Elapsed time for attention_key_query_prob (256x2048x251x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x251x2048): 72.816
Elapsed time for attention_prob_times_values (256x2048x2048x251): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x251): 94.364
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16064x16064, b=2048): 0.0164
Throughput (in TFLOP/s) for attention_linear_projection (4x16064x16064, b=2048): 257.932
Elapsed time for mlp_h_to_4h (4x16064x64256, b=2048): 0.0651
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16064x64256, b=2048): 259.652
Elapsed time for mlp_fused_gelu (2048x4x64256): 0.0018
Elapsed time for mlp_4h_to_h (4x64256x16064, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64256x16064, b=2048): 254.812
Elapsed time for transformer_add_bias_dropout (2048x4x16064): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16064): 0.0007

Attention duration (in seconds): 0.1138
Attention throughput (in TFLOP/s): 158.137
MLP duration (in seconds): 0.1333
MLP throughput (in TFLOP/s): 253.795
Transformer duration (in seconds): 0.2507
Transformer throughput (in TFLOP/s): 206.655
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0894
Attention throughput (in TFLOP/s): 201.202
MLP duration (in seconds): 0.1353
MLP throughput (in TFLOP/s): 249.952
Transformer duration (in seconds): 0.2300
Transformer throughput (in TFLOP/s): 225.264
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 16128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16128x48384, b=2048): 0.0496
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16128x48384, b=2048): 257.645
Elapsed time for attention_key_query_prob (256x2048x252x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x252x2048): 106.285
Elapsed time for attention_prob_times_values (256x2048x2048x252): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x252): 149.647
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16128x16128, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x16128x16128, b=2048): 259.019
Elapsed time for mlp_h_to_4h (4x16128x64512, b=2048): 0.0661
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16128x64512, b=2048): 257.711
Elapsed time for mlp_fused_gelu (2048x4x64512): 0.0018
Elapsed time for mlp_4h_to_h (4x64512x16128, b=2048): 0.0664
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64512x16128, b=2048): 256.639
Elapsed time for transformer_add_bias_dropout (2048x4x16128): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16128): 0.0006

Attention duration (in seconds): 0.1097
Attention throughput (in TFLOP/s): 165.199
MLP duration (in seconds): 0.1343
MLP throughput (in TFLOP/s): 253.772
Transformer duration (in seconds): 0.2478
Transformer throughput (in TFLOP/s): 210.782
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0834
Attention throughput (in TFLOP/s): 217.397
MLP duration (in seconds): 0.1360
MLP throughput (in TFLOP/s): 250.656
Transformer duration (in seconds): 0.2234
Transformer throughput (in TFLOP/s): 233.780
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 16192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16192x48576, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16192x48576, b=2048): 258.914
Elapsed time for attention_key_query_prob (256x2048x253x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x253x2048): 73.563
Elapsed time for attention_prob_times_values (256x2048x2048x253): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x253): 95.140
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x16192x16192, b=2048): 0.0165
Throughput (in TFLOP/s) for attention_linear_projection (4x16192x16192, b=2048): 259.602
Elapsed time for mlp_h_to_4h (4x16192x64768, b=2048): 0.0660
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16192x64768, b=2048): 260.444
Elapsed time for mlp_fused_gelu (2048x4x64768): 0.0018
Elapsed time for mlp_4h_to_h (4x64768x16192, b=2048): 0.0674
Throughput (in TFLOP/s) for mlp_4h_to_h (4x64768x16192, b=2048): 254.765
Elapsed time for transformer_add_bias_dropout (2048x4x16192): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16192): 0.0007

Attention duration (in seconds): 0.1144
Attention throughput (in TFLOP/s): 159.725
MLP duration (in seconds): 0.1352
MLP throughput (in TFLOP/s): 254.174
Transformer duration (in seconds): 0.2533
Transformer throughput (in TFLOP/s): 207.805
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0904
Attention throughput (in TFLOP/s): 202.122
MLP duration (in seconds): 0.1372
MLP throughput (in TFLOP/s): 250.508
Transformer duration (in seconds): 0.2323
Transformer throughput (in TFLOP/s): 226.537
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 16256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16256x48768, b=2048): 0.0503
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16256x48768, b=2048): 258.403
Elapsed time for attention_key_query_prob (256x2048x254x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x254x2048): 107.754
Elapsed time for attention_prob_times_values (256x2048x2048x254): 0.0036
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x254): 150.948
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16256x16256, b=2048): 0.0166
Throughput (in TFLOP/s) for attention_linear_projection (4x16256x16256, b=2048): 260.896
Elapsed time for mlp_h_to_4h (4x16256x65024, b=2048): 0.0669
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16256x65024, b=2048): 258.968
Elapsed time for mlp_fused_gelu (2048x4x65024): 0.0018
Elapsed time for mlp_4h_to_h (4x65024x16256, b=2048): 0.0677
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65024x16256, b=2048): 255.768
Elapsed time for transformer_add_bias_dropout (2048x4x16256): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16256): 0.0007

Attention duration (in seconds): 0.1105
Attention throughput (in TFLOP/s): 166.558
MLP duration (in seconds): 0.1364
MLP throughput (in TFLOP/s): 253.977
Transformer duration (in seconds): 0.2506
Transformer throughput (in TFLOP/s): 211.669
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0848
Attention throughput (in TFLOP/s): 216.988
MLP duration (in seconds): 0.1388
MLP throughput (in TFLOP/s): 249.521
Transformer duration (in seconds): 0.2292
Transformer throughput (in TFLOP/s): 231.439
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 64, hidden_size: 16320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16320x48960, b=2048): 0.0513
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16320x48960, b=2048): 255.379
Elapsed time for attention_key_query_prob (256x2048x255x2048): 0.0074
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x255x2048): 73.553
Elapsed time for attention_prob_times_values (256x2048x2048x255): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x255): 94.860
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x16320x16320, b=2048): 0.0170
Throughput (in TFLOP/s) for attention_linear_projection (4x16320x16320, b=2048): 257.206
Elapsed time for mlp_h_to_4h (4x16320x65280, b=2048): 0.0676
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16320x65280, b=2048): 258.045
Elapsed time for mlp_fused_gelu (2048x4x65280): 0.0018
Elapsed time for mlp_4h_to_h (4x65280x16320, b=2048): 0.0685
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65280x16320, b=2048): 254.794
Elapsed time for transformer_add_bias_dropout (2048x4x16320): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16320): 0.0007

Attention duration (in seconds): 0.1164
Attention throughput (in TFLOP/s): 159.303
MLP duration (in seconds): 0.1379
MLP throughput (in TFLOP/s): 253.068
Transformer duration (in seconds): 0.2581
Transformer throughput (in TFLOP/s): 207.105
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0917
Attention throughput (in TFLOP/s): 202.215
MLP duration (in seconds): 0.1401
MLP throughput (in TFLOP/s): 249.225
Transformer duration (in seconds): 0.2375
Transformer throughput (in TFLOP/s): 225.117
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 16384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16384x49152, b=2048): 0.0515
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16384x49152, b=2048): 256.350
Elapsed time for attention_key_query_prob (256x2048x256x2048): 0.0035
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x256x2048): 158.319
Elapsed time for attention_prob_times_values (256x2048x2048x256): 0.0025
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x256): 224.387
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16384x16384, b=2048): 0.0169
Throughput (in TFLOP/s) for attention_linear_projection (4x16384x16384, b=2048): 260.691
Elapsed time for mlp_h_to_4h (4x16384x65536, b=2048): 0.0680
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16384x65536, b=2048): 258.894
Elapsed time for mlp_fused_gelu (2048x4x65536): 0.0018
Elapsed time for mlp_4h_to_h (4x65536x16384, b=2048): 0.0690
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65536x16384, b=2048): 255.114
Elapsed time for transformer_add_bias_dropout (2048x4x16384): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16384): 0.0007

Attention duration (in seconds): 0.1092
Attention throughput (in TFLOP/s): 171.093
MLP duration (in seconds): 0.1387
MLP throughput (in TFLOP/s): 253.620
Transformer duration (in seconds): 0.2517
Transformer throughput (in TFLOP/s): 214.043
Transformer - MLP - Attention (in seconds): 0.0037


Actual
------
Attention duration (in seconds): 0.0850
Attention throughput (in TFLOP/s): 219.871
MLP duration (in seconds): 0.1416
MLP throughput (in TFLOP/s): 248.455
Transformer duration (in seconds): 0.2323
Transformer throughput (in TFLOP/s): 231.891
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 16448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16448x49344, b=2048): 0.0516
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16448x49344, b=2048): 257.728
Elapsed time for attention_key_query_prob (256x2048x257x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x257x2048): 71.055
Elapsed time for attention_prob_times_values (256x2048x2048x257): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x257): 56.133
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16448x16448, b=2048): 0.0172
Throughput (in TFLOP/s) for attention_linear_projection (4x16448x16448, b=2048): 257.641
Elapsed time for mlp_h_to_4h (4x16448x65792, b=2048): 0.0684
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16448x65792, b=2048): 259.207
Elapsed time for mlp_fused_gelu (2048x4x65792): 0.0018
Elapsed time for mlp_4h_to_h (4x65792x16448, b=2048): 0.0695
Throughput (in TFLOP/s) for mlp_4h_to_h (4x65792x16448, b=2048): 255.103
Elapsed time for transformer_add_bias_dropout (2048x4x16448): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16448): 0.0007

Attention duration (in seconds): 0.1214
Attention throughput (in TFLOP/s): 155.158
MLP duration (in seconds): 0.1397
MLP throughput (in TFLOP/s): 253.804
Transformer duration (in seconds): 0.2649
Transformer throughput (in TFLOP/s): 204.984
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0976
Attention throughput (in TFLOP/s): 193.026
MLP duration (in seconds): 0.1422
MLP throughput (in TFLOP/s): 249.435
Transformer duration (in seconds): 0.2451
Transformer throughput (in TFLOP/s): 221.484
Transformer - MLP - Attention (in seconds): 0.0054
========================================================================================================================
num_attention_heads: 64, hidden_size: 16512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16512x49536, b=2048): 0.0522
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16512x49536, b=2048): 256.646
Elapsed time for attention_key_query_prob (256x2048x258x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x258x2048): 103.016
Elapsed time for attention_prob_times_values (256x2048x2048x258): 0.0057
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x258): 96.844
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16512x16512, b=2048): 0.0173
Throughput (in TFLOP/s) for attention_linear_projection (4x16512x16512, b=2048): 258.465
Elapsed time for mlp_h_to_4h (4x16512x66048, b=2048): 0.0691
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16512x66048, b=2048): 258.591
Elapsed time for mlp_fused_gelu (2048x4x66048): 0.0018
Elapsed time for mlp_4h_to_h (4x66048x16512, b=2048): 0.0700
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66048x16512, b=2048): 255.156
Elapsed time for transformer_add_bias_dropout (2048x4x16512): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16512): 0.0007

Attention duration (in seconds): 0.1156
Attention throughput (in TFLOP/s): 164.179
MLP duration (in seconds): 0.1409
MLP throughput (in TFLOP/s): 253.548
Transformer duration (in seconds): 0.2603
Transformer throughput (in TFLOP/s): 210.194
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0900
Attention throughput (in TFLOP/s): 210.874
MLP duration (in seconds): 0.1433
MLP throughput (in TFLOP/s): 249.318
Transformer duration (in seconds): 0.2391
Transformer throughput (in TFLOP/s): 228.789
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 16576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16576x49728, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16576x49728, b=2048): 255.686
Elapsed time for attention_key_query_prob (256x2048x259x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x259x2048): 71.374
Elapsed time for attention_prob_times_values (256x2048x2048x259): 0.0097
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x259): 57.074
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16576x16576, b=2048): 0.0175
Throughput (in TFLOP/s) for attention_linear_projection (4x16576x16576, b=2048): 257.327
Elapsed time for mlp_h_to_4h (4x16576x66304, b=2048): 0.0700
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16576x66304, b=2048): 257.166
Elapsed time for mlp_fused_gelu (2048x4x66304): 0.0018
Elapsed time for mlp_4h_to_h (4x66304x16576, b=2048): 0.0712
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66304x16576, b=2048): 252.754
Elapsed time for transformer_add_bias_dropout (2048x4x16576): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16576): 0.0007

Attention duration (in seconds): 0.1228
Attention throughput (in TFLOP/s): 155.639
MLP duration (in seconds): 0.1431
MLP throughput (in TFLOP/s): 251.689
Transformer duration (in seconds): 0.2697
Transformer throughput (in TFLOP/s): 204.403
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0993
Attention throughput (in TFLOP/s): 192.590
MLP duration (in seconds): 0.1452
MLP throughput (in TFLOP/s): 247.979
Transformer duration (in seconds): 0.2506
Transformer throughput (in TFLOP/s): 220.033
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 16640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16640x49920, b=2048): 0.0534
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16640x49920, b=2048): 254.869
Elapsed time for attention_key_query_prob (256x2048x260x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x260x2048): 103.399
Elapsed time for attention_prob_times_values (256x2048x2048x260): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x260): 96.992
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x16640x16640, b=2048): 0.0174
Throughput (in TFLOP/s) for attention_linear_projection (4x16640x16640, b=2048): 260.115
Elapsed time for mlp_h_to_4h (4x16640x66560, b=2048): 0.0699
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16640x66560, b=2048): 259.784
Elapsed time for mlp_fused_gelu (2048x4x66560): 0.0018
Elapsed time for mlp_4h_to_h (4x66560x16640, b=2048): 0.0709
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66560x16640, b=2048): 255.932
Elapsed time for transformer_add_bias_dropout (2048x4x16640): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16640): 0.0007

Attention duration (in seconds): 0.1170
Attention throughput (in TFLOP/s): 164.647
MLP duration (in seconds): 0.1426
MLP throughput (in TFLOP/s): 254.527
Transformer duration (in seconds): 0.2634
Transformer throughput (in TFLOP/s): 210.941
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0905
Attention throughput (in TFLOP/s): 212.821
MLP duration (in seconds): 0.1447
MLP throughput (in TFLOP/s): 250.823
Transformer duration (in seconds): 0.2409
Transformer throughput (in TFLOP/s): 230.617
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 16704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16704x50112, b=2048): 0.0532
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16704x50112, b=2048): 257.884
Elapsed time for attention_key_query_prob (256x2048x261x2048): 0.0079
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x261x2048): 71.342
Elapsed time for attention_prob_times_values (256x2048x2048x261): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x261): 57.277
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x16704x16704, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_linear_projection (4x16704x16704, b=2048): 259.673
Elapsed time for mlp_h_to_4h (4x16704x66816, b=2048): 0.0710
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16704x66816, b=2048): 257.670
Elapsed time for mlp_fused_gelu (2048x4x66816): 0.0018
Elapsed time for mlp_4h_to_h (4x66816x16704, b=2048): 0.0724
Throughput (in TFLOP/s) for mlp_4h_to_h (4x66816x16704, b=2048): 252.607
Elapsed time for transformer_add_bias_dropout (2048x4x16704): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16704): 0.0007

Attention duration (in seconds): 0.1234
Attention throughput (in TFLOP/s): 157.249
MLP duration (in seconds): 0.1452
MLP throughput (in TFLOP/s): 251.881
Transformer duration (in seconds): 0.2724
Transformer throughput (in TFLOP/s): 205.475
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.1004
Attention throughput (in TFLOP/s): 193.282
MLP duration (in seconds): 0.1469
MLP throughput (in TFLOP/s): 248.900
Transformer duration (in seconds): 0.2501
Transformer throughput (in TFLOP/s): 223.830
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 16768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16768x50304, b=2048): 0.0534
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16768x50304, b=2048): 258.645
Elapsed time for attention_key_query_prob (256x2048x262x2048): 0.0054
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x262x2048): 103.519
Elapsed time for attention_prob_times_values (256x2048x2048x262): 0.0058
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x262): 97.344
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x16768x16768, b=2048): 0.0176
Throughput (in TFLOP/s) for attention_linear_projection (4x16768x16768, b=2048): 261.457
Elapsed time for mlp_h_to_4h (4x16768x67072, b=2048): 0.0712
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16768x67072, b=2048): 258.944
Elapsed time for mlp_fused_gelu (2048x4x67072): 0.0018
Elapsed time for mlp_4h_to_h (4x67072x16768, b=2048): 0.0722
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67072x16768, b=2048): 255.303
Elapsed time for transformer_add_bias_dropout (2048x4x16768): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16768): 0.0007

Attention duration (in seconds): 0.1172
Attention throughput (in TFLOP/s): 166.753
MLP duration (in seconds): 0.1452
MLP throughput (in TFLOP/s): 253.842
Transformer duration (in seconds): 0.2663
Transformer throughput (in TFLOP/s): 211.828
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0917
Attention throughput (in TFLOP/s): 213.294
MLP duration (in seconds): 0.1472
MLP throughput (in TFLOP/s): 250.342
Transformer duration (in seconds): 0.2441
Transformer throughput (in TFLOP/s): 231.040
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 16832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16832x50496, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16832x50496, b=2048): 258.566
Elapsed time for attention_key_query_prob (256x2048x263x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x263x2048): 70.455
Elapsed time for attention_prob_times_values (256x2048x2048x263): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x263): 56.727
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x16832x16832, b=2048): 0.0180
Throughput (in TFLOP/s) for attention_linear_projection (4x16832x16832, b=2048): 258.066
Elapsed time for mlp_h_to_4h (4x16832x67328, b=2048): 0.0715
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16832x67328, b=2048): 259.848
Elapsed time for mlp_fused_gelu (2048x4x67328): 0.0019
Elapsed time for mlp_4h_to_h (4x67328x16832, b=2048): 0.0730
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67328x16832, b=2048): 254.189
Elapsed time for transformer_add_bias_dropout (2048x4x16832): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16832): 0.0007

Attention duration (in seconds): 0.1248
Attention throughput (in TFLOP/s): 157.838
MLP duration (in seconds): 0.1464
MLP throughput (in TFLOP/s): 253.733
Transformer duration (in seconds): 0.2750
Transformer throughput (in TFLOP/s): 206.644
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1014
Attention throughput (in TFLOP/s): 194.220
MLP duration (in seconds): 0.1493
MLP throughput (in TFLOP/s): 248.805
Transformer duration (in seconds): 0.2553
Transformer throughput (in TFLOP/s): 222.645
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 16896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16896x50688, b=2048): 0.0545
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16896x50688, b=2048): 257.418
Elapsed time for attention_key_query_prob (256x2048x264x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x264x2048): 136.164
Elapsed time for attention_prob_times_values (256x2048x2048x264): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x264): 133.325
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16896x16896, b=2048): 0.0181
Throughput (in TFLOP/s) for attention_linear_projection (4x16896x16896, b=2048): 257.959
Elapsed time for mlp_h_to_4h (4x16896x67584, b=2048): 0.0725
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16896x67584, b=2048): 258.097
Elapsed time for mlp_fused_gelu (2048x4x67584): 0.0019
Elapsed time for mlp_4h_to_h (4x67584x16896, b=2048): 0.0735
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67584x16896, b=2048): 254.536
Elapsed time for transformer_add_bias_dropout (2048x4x16896): 0.0012
Elapsed time for transformer_layer_norm (2048x4x16896): 0.0007

Attention duration (in seconds): 0.1160
Attention throughput (in TFLOP/s): 171.014
MLP duration (in seconds): 0.1478
MLP throughput (in TFLOP/s): 253.081
Transformer duration (in seconds): 0.2677
Transformer throughput (in TFLOP/s): 213.876
Transformer - MLP - Attention (in seconds): 0.0038


Actual
------
Attention duration (in seconds): 0.0926
Attention throughput (in TFLOP/s): 214.338
MLP duration (in seconds): 0.1508
MLP throughput (in TFLOP/s): 248.110
Transformer duration (in seconds): 0.2496
Transformer throughput (in TFLOP/s): 229.397
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 16960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x16960x50880, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x16960x50880, b=2048): 255.132
Elapsed time for attention_key_query_prob (256x2048x265x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x265x2048): 70.457
Elapsed time for attention_prob_times_values (256x2048x2048x265): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x265): 56.870
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x16960x16960, b=2048): 0.0184
Throughput (in TFLOP/s) for attention_linear_projection (4x16960x16960, b=2048): 256.571
Elapsed time for mlp_h_to_4h (4x16960x67840, b=2048): 0.0732
Throughput (in TFLOP/s) for mlp_h_to_4h (4x16960x67840, b=2048): 257.535
Elapsed time for mlp_fused_gelu (2048x4x67840): 0.0019
Elapsed time for mlp_4h_to_h (4x67840x16960, b=2048): 0.0748
Throughput (in TFLOP/s) for mlp_4h_to_h (4x67840x16960, b=2048): 251.912
Elapsed time for transformer_add_bias_dropout (2048x4x16960): 0.0013
Elapsed time for transformer_layer_norm (2048x4x16960): 0.0007

Attention duration (in seconds): 0.1268
Attention throughput (in TFLOP/s): 157.598
MLP duration (in seconds): 0.1499
MLP throughput (in TFLOP/s): 251.521
Transformer duration (in seconds): 0.2806
Transformer throughput (in TFLOP/s): 205.584
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1028
Attention throughput (in TFLOP/s): 194.483
MLP duration (in seconds): 0.1519
MLP throughput (in TFLOP/s): 248.255
Transformer duration (in seconds): 0.2606
Transformer throughput (in TFLOP/s): 221.397
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 17024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17024x51072, b=2048): 0.0557
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17024x51072, b=2048): 255.527
Elapsed time for attention_key_query_prob (256x2048x266x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x266x2048): 103.911
Elapsed time for attention_prob_times_values (256x2048x2048x266): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x266): 97.122
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x17024x17024, b=2048): 0.0183
Throughput (in TFLOP/s) for attention_linear_projection (4x17024x17024, b=2048): 259.498
Elapsed time for mlp_h_to_4h (4x17024x68096, b=2048): 0.0737
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17024x68096, b=2048): 257.813
Elapsed time for mlp_fused_gelu (2048x4x68096): 0.0019
Elapsed time for mlp_4h_to_h (4x68096x17024, b=2048): 0.0747
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68096x17024, b=2048): 254.340
Elapsed time for transformer_add_bias_dropout (2048x4x17024): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17024): 0.0007

Attention duration (in seconds): 0.1204
Attention throughput (in TFLOP/s): 167.216
MLP duration (in seconds): 0.1502
MLP throughput (in TFLOP/s): 252.871
Transformer duration (in seconds): 0.2745
Transformer throughput (in TFLOP/s): 211.728
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0947
Attention throughput (in TFLOP/s): 212.640
MLP duration (in seconds): 0.1526
MLP throughput (in TFLOP/s): 248.944
Transformer duration (in seconds): 0.2532
Transformer throughput (in TFLOP/s): 229.534
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 17088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17088x51264, b=2048): 0.0561
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17088x51264, b=2048): 255.764
Elapsed time for attention_key_query_prob (256x2048x267x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x267x2048): 71.796
Elapsed time for attention_prob_times_values (256x2048x2048x267): 0.0098
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x267): 58.305
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x17088x17088, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17088x17088, b=2048): 256.695
Elapsed time for mlp_h_to_4h (4x17088x68352, b=2048): 0.0740
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17088x68352, b=2048): 258.428
Elapsed time for mlp_fused_gelu (2048x4x68352): 0.0019
Elapsed time for mlp_4h_to_h (4x68352x17088, b=2048): 0.0752
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68352x17088, b=2048): 254.466
Elapsed time for transformer_add_bias_dropout (2048x4x17088): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17088): 0.0007

Attention duration (in seconds): 0.1276
Attention throughput (in TFLOP/s): 158.986
MLP duration (in seconds): 0.1511
MLP throughput (in TFLOP/s): 253.241
Transformer duration (in seconds): 0.2826
Transformer throughput (in TFLOP/s): 207.188
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1033
Attention throughput (in TFLOP/s): 196.355
MLP duration (in seconds): 0.1539
MLP throughput (in TFLOP/s): 248.618
Transformer duration (in seconds): 0.2623
Transformer throughput (in TFLOP/s): 223.241
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 17152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17152x51456, b=2048): 0.0560
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17152x51456, b=2048): 258.282
Elapsed time for attention_key_query_prob (256x2048x268x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x268x2048): 104.989
Elapsed time for attention_prob_times_values (256x2048x2048x268): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x268): 97.726
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17152x17152, b=2048): 0.0186
Throughput (in TFLOP/s) for attention_linear_projection (4x17152x17152, b=2048): 259.475
Elapsed time for mlp_h_to_4h (4x17152x68608, b=2048): 0.0746
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17152x68608, b=2048): 258.306
Elapsed time for mlp_fused_gelu (2048x4x68608): 0.0019
Elapsed time for mlp_4h_to_h (4x68608x17152, b=2048): 0.0757
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68608x17152, b=2048): 254.807
Elapsed time for transformer_add_bias_dropout (2048x4x17152): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17152): 0.0007

Attention duration (in seconds): 0.1209
Attention throughput (in TFLOP/s): 168.935
MLP duration (in seconds): 0.1522
MLP throughput (in TFLOP/s): 253.361
Transformer duration (in seconds): 0.2770
Transformer throughput (in TFLOP/s): 212.940
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0956
Attention throughput (in TFLOP/s): 213.670
MLP duration (in seconds): 0.1555
MLP throughput (in TFLOP/s): 247.920
Transformer duration (in seconds): 0.2575
Transformer throughput (in TFLOP/s): 229.072
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 17216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17216x51648, b=2048): 0.0571
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17216x51648, b=2048): 254.994
Elapsed time for attention_key_query_prob (256x2048x269x2048): 0.0080
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x269x2048): 71.819
Elapsed time for attention_prob_times_values (256x2048x2048x269): 0.0099
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x269): 58.492
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17216x17216, b=2048): 0.0187
Throughput (in TFLOP/s) for attention_linear_projection (4x17216x17216, b=2048): 260.202
Elapsed time for mlp_h_to_4h (4x17216x68864, b=2048): 0.0753
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17216x68864, b=2048): 258.088
Elapsed time for mlp_fused_gelu (2048x4x68864): 0.0019
Elapsed time for mlp_4h_to_h (4x68864x17216, b=2048): 0.0761
Throughput (in TFLOP/s) for mlp_4h_to_h (4x68864x17216, b=2048): 255.204
Elapsed time for transformer_add_bias_dropout (2048x4x17216): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17216): 0.0007

Attention duration (in seconds): 0.1287
Attention throughput (in TFLOP/s): 159.889
MLP duration (in seconds): 0.1533
MLP throughput (in TFLOP/s): 253.465
Transformer duration (in seconds): 0.2859
Transformer throughput (in TFLOP/s): 207.846
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.1045
Attention throughput (in TFLOP/s): 196.950
MLP duration (in seconds): 0.1556
MLP throughput (in TFLOP/s): 249.711
Transformer duration (in seconds): 0.2646
Transformer throughput (in TFLOP/s): 224.566
Transformer - MLP - Attention (in seconds): 0.0046
========================================================================================================================
num_attention_heads: 64, hidden_size: 17280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17280x51840, b=2048): 0.0569
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17280x51840, b=2048): 258.004
Elapsed time for attention_key_query_prob (256x2048x270x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x270x2048): 104.890
Elapsed time for attention_prob_times_values (256x2048x2048x270): 0.0059
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x270): 97.739
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17280x17280, b=2048): 0.0188
Throughput (in TFLOP/s) for attention_linear_projection (4x17280x17280, b=2048): 260.639
Elapsed time for mlp_h_to_4h (4x17280x69120, b=2048): 0.0755
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17280x69120, b=2048): 259.282
Elapsed time for mlp_fused_gelu (2048x4x69120): 0.0019
Elapsed time for mlp_4h_to_h (4x69120x17280, b=2048): 0.0764
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69120x17280, b=2048): 256.119
Elapsed time for transformer_add_bias_dropout (2048x4x17280): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17280): 0.0007

Attention duration (in seconds): 0.1221
Attention throughput (in TFLOP/s): 169.779
MLP duration (in seconds): 0.1538
MLP throughput (in TFLOP/s): 254.505
Transformer duration (in seconds): 0.2798
Transformer throughput (in TFLOP/s): 213.957
Transformer - MLP - Attention (in seconds): 0.0039


Actual
------
Attention duration (in seconds): 0.0983
Attention throughput (in TFLOP/s): 210.917
MLP duration (in seconds): 0.1560
MLP throughput (in TFLOP/s): 250.923
Transformer duration (in seconds): 0.2575
Transformer throughput (in TFLOP/s): 232.470
Transformer - MLP - Attention (in seconds): 0.0033
========================================================================================================================
num_attention_heads: 64, hidden_size: 17344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17344x52032, b=2048): 0.0574
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17344x52032, b=2048): 257.720
Elapsed time for attention_key_query_prob (256x2048x271x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x271x2048): 71.631
Elapsed time for attention_prob_times_values (256x2048x2048x271): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x271): 57.828
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17344x17344, b=2048): 0.0191
Throughput (in TFLOP/s) for attention_linear_projection (4x17344x17344, b=2048): 258.206
Elapsed time for mlp_h_to_4h (4x17344x69376, b=2048): 0.0763
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17344x69376, b=2048): 258.258
Elapsed time for mlp_fused_gelu (2048x4x69376): 0.0019
Elapsed time for mlp_4h_to_h (4x69376x17344, b=2048): 0.0776
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69376x17344, b=2048): 254.159
Elapsed time for transformer_add_bias_dropout (2048x4x17344): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17344): 0.0007

Attention duration (in seconds): 0.1296
Attention throughput (in TFLOP/s): 161.070
MLP duration (in seconds): 0.1558
MLP throughput (in TFLOP/s): 253.053
Transformer duration (in seconds): 0.2894
Transformer throughput (in TFLOP/s): 208.352
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1062
Attention throughput (in TFLOP/s): 196.554
MLP duration (in seconds): 0.1596
MLP throughput (in TFLOP/s): 247.102
Transformer duration (in seconds): 0.2713
Transformer throughput (in TFLOP/s): 222.284
Transformer - MLP - Attention (in seconds): 0.0055
========================================================================================================================
num_attention_heads: 64, hidden_size: 17408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17408x52224, b=2048): 0.0583
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17408x52224, b=2048): 255.367
Elapsed time for attention_key_query_prob (256x2048x272x2048): 0.0039
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x272x2048): 149.551
Elapsed time for attention_prob_times_values (256x2048x2048x272): 0.0042
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x272): 138.211
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17408x17408, b=2048): 0.0192
Throughput (in TFLOP/s) for attention_linear_projection (4x17408x17408, b=2048): 258.246
Elapsed time for mlp_h_to_4h (4x17408x69632, b=2048): 0.0771
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17408x69632, b=2048): 257.707
Elapsed time for mlp_fused_gelu (2048x4x69632): 0.0019
Elapsed time for mlp_4h_to_h (4x69632x17408, b=2048): 0.0780
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69632x17408, b=2048): 254.661
Elapsed time for transformer_add_bias_dropout (2048x4x17408): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17408): 0.0007

Attention duration (in seconds): 0.1207
Attention throughput (in TFLOP/s): 174.280
MLP duration (in seconds): 0.1570
MLP throughput (in TFLOP/s): 253.050
Transformer duration (in seconds): 0.2816
Transformer throughput (in TFLOP/s): 215.724
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.0968
Attention throughput (in TFLOP/s): 217.236
MLP duration (in seconds): 0.1595
MLP throughput (in TFLOP/s): 249.015
Transformer duration (in seconds): 0.2629
Transformer throughput (in TFLOP/s): 231.088
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 17472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17472x52416, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17472x52416, b=2048): 254.815
Elapsed time for attention_key_query_prob (256x2048x273x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x273x2048): 72.109
Elapsed time for attention_prob_times_values (256x2048x2048x273): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x273): 57.885
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x17472x17472, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_linear_projection (4x17472x17472, b=2048): 256.483
Elapsed time for mlp_h_to_4h (4x17472x69888, b=2048): 0.0778
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17472x69888, b=2048): 257.311
Elapsed time for mlp_fused_gelu (2048x4x69888): 0.0019
Elapsed time for mlp_4h_to_h (4x69888x17472, b=2048): 0.0792
Throughput (in TFLOP/s) for mlp_4h_to_h (4x69888x17472, b=2048): 252.620
Elapsed time for transformer_add_bias_dropout (2048x4x17472): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17472): 0.0007

Attention duration (in seconds): 0.1316
Attention throughput (in TFLOP/s): 160.909
MLP duration (in seconds): 0.1589
MLP throughput (in TFLOP/s): 251.859
Transformer duration (in seconds): 0.2945
Transformer throughput (in TFLOP/s): 207.781
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1071
Attention throughput (in TFLOP/s): 197.665
MLP duration (in seconds): 0.1601
MLP throughput (in TFLOP/s): 249.973
Transformer duration (in seconds): 0.2715
Transformer throughput (in TFLOP/s): 225.360
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 17536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17536x52608, b=2048): 0.0585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17536x52608, b=2048): 258.497
Elapsed time for attention_key_query_prob (256x2048x274x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x274x2048): 105.965
Elapsed time for attention_prob_times_values (256x2048x2048x274): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x274): 98.856
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17536x17536, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_linear_projection (4x17536x17536, b=2048): 260.129
Elapsed time for mlp_h_to_4h (4x17536x70144, b=2048): 0.0781
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17536x70144, b=2048): 257.897
Elapsed time for mlp_fused_gelu (2048x4x70144): 0.0019
Elapsed time for mlp_4h_to_h (4x70144x17536, b=2048): 0.0792
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70144x17536, b=2048): 254.454
Elapsed time for transformer_add_bias_dropout (2048x4x17536): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17536): 0.0007

Attention duration (in seconds): 0.1243
Attention throughput (in TFLOP/s): 171.544
MLP duration (in seconds): 0.1593
MLP throughput (in TFLOP/s): 253.059
Transformer duration (in seconds): 0.2876
Transformer throughput (in TFLOP/s): 214.311
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.0994
Attention throughput (in TFLOP/s): 214.494
MLP duration (in seconds): 0.1620
MLP throughput (in TFLOP/s): 248.824
Transformer duration (in seconds): 0.2641
Transformer throughput (in TFLOP/s): 233.379
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 64, hidden_size: 17600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17600x52800, b=2048): 0.0589
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17600x52800, b=2048): 258.433
Elapsed time for attention_key_query_prob (256x2048x275x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x275x2048): 72.742
Elapsed time for attention_prob_times_values (256x2048x2048x275): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x275): 59.161
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x17600x17600, b=2048): 0.0194
Throughput (in TFLOP/s) for attention_linear_projection (4x17600x17600, b=2048): 261.393
Elapsed time for mlp_h_to_4h (4x17600x70400, b=2048): 0.0782
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17600x70400, b=2048): 259.613
Elapsed time for mlp_fused_gelu (2048x4x70400): 0.0019
Elapsed time for mlp_4h_to_h (4x70400x17600, b=2048): 0.0798
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70400x17600, b=2048): 254.309
Elapsed time for transformer_add_bias_dropout (2048x4x17600): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17600): 0.0007

Attention duration (in seconds): 0.1315
Attention throughput (in TFLOP/s): 163.397
MLP duration (in seconds): 0.1600
MLP throughput (in TFLOP/s): 253.823
Transformer duration (in seconds): 0.2955
Transformer throughput (in TFLOP/s): 210.119
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1089
Attention throughput (in TFLOP/s): 197.228
MLP duration (in seconds): 0.1623
MLP throughput (in TFLOP/s): 250.095
Transformer duration (in seconds): 0.2749
Transformer throughput (in TFLOP/s): 225.861
Transformer - MLP - Attention (in seconds): 0.0036
========================================================================================================================
num_attention_heads: 64, hidden_size: 17664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17664x52992, b=2048): 0.0592
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17664x52992, b=2048): 258.930
Elapsed time for attention_key_query_prob (256x2048x276x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x276x2048): 106.678
Elapsed time for attention_prob_times_values (256x2048x2048x276): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x276): 99.066
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17664x17664, b=2048): 0.0195
Throughput (in TFLOP/s) for attention_linear_projection (4x17664x17664, b=2048): 261.556
Elapsed time for mlp_h_to_4h (4x17664x70656, b=2048): 0.0791
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17664x70656, b=2048): 258.518
Elapsed time for mlp_fused_gelu (2048x4x70656): 0.0019
Elapsed time for mlp_4h_to_h (4x70656x17664, b=2048): 0.0803
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70656x17664, b=2048): 254.705
Elapsed time for transformer_add_bias_dropout (2048x4x17664): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17664): 0.0007

Attention duration (in seconds): 0.1253
Attention throughput (in TFLOP/s): 172.627
MLP duration (in seconds): 0.1613
MLP throughput (in TFLOP/s): 253.506
Transformer duration (in seconds): 0.2907
Transformer throughput (in TFLOP/s): 215.125
Transformer - MLP - Attention (in seconds): 0.0040


Actual
------
Attention duration (in seconds): 0.1000
Attention throughput (in TFLOP/s): 216.281
MLP duration (in seconds): 0.1645
MLP throughput (in TFLOP/s): 248.549
Transformer duration (in seconds): 0.2714
Transformer throughput (in TFLOP/s): 230.423
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 17728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17728x53184, b=2048): 0.0618
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17728x53184, b=2048): 249.831
Elapsed time for attention_key_query_prob (256x2048x277x2048): 0.0081
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x277x2048): 73.120
Elapsed time for attention_prob_times_values (256x2048x2048x277): 0.0100
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x277): 59.346
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17728x17728, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17728x17728, b=2048): 254.513
Elapsed time for mlp_h_to_4h (4x17728x70912, b=2048): 0.0799
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17728x70912, b=2048): 257.766
Elapsed time for mlp_fused_gelu (2048x4x70912): 0.0020
Elapsed time for mlp_4h_to_h (4x70912x17728, b=2048): 0.0815
Throughput (in TFLOP/s) for mlp_4h_to_h (4x70912x17728, b=2048): 252.806
Elapsed time for transformer_add_bias_dropout (2048x4x17728): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17728): 0.0007

Attention duration (in seconds): 0.1352
Attention throughput (in TFLOP/s): 161.109
MLP duration (in seconds): 0.1633
MLP throughput (in TFLOP/s): 252.212
Transformer duration (in seconds): 0.3026
Transformer throughput (in TFLOP/s): 208.117
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1103
Attention throughput (in TFLOP/s): 197.546
MLP duration (in seconds): 0.1665
MLP throughput (in TFLOP/s): 247.462
Transformer duration (in seconds): 0.2826
Transformer throughput (in TFLOP/s): 222.841
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 17792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17792x53376, b=2048): 0.0603
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17792x53376, b=2048): 258.204
Elapsed time for attention_key_query_prob (256x2048x278x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x278x2048): 108.222
Elapsed time for attention_prob_times_values (256x2048x2048x278): 0.0060
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x278): 99.439
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17792x17792, b=2048): 0.0200
Throughput (in TFLOP/s) for attention_linear_projection (4x17792x17792, b=2048): 259.350
Elapsed time for mlp_h_to_4h (4x17792x71168, b=2048): 0.0800
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17792x71168, b=2048): 259.431
Elapsed time for mlp_fused_gelu (2048x4x71168): 0.0020
Elapsed time for mlp_4h_to_h (4x71168x17792, b=2048): 0.0815
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71168x17792, b=2048): 254.622
Elapsed time for transformer_add_bias_dropout (2048x4x17792): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17792): 0.0007

Attention duration (in seconds): 0.1268
Attention throughput (in TFLOP/s): 173.043
MLP duration (in seconds): 0.1634
MLP throughput (in TFLOP/s): 253.922
Transformer duration (in seconds): 0.2942
Transformer throughput (in TFLOP/s): 215.575
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1012
Attention throughput (in TFLOP/s): 216.693
MLP duration (in seconds): 0.1666
MLP throughput (in TFLOP/s): 249.004
Transformer duration (in seconds): 0.2722
Transformer throughput (in TFLOP/s): 233.014
Transformer - MLP - Attention (in seconds): 0.0043
========================================================================================================================
num_attention_heads: 64, hidden_size: 17856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17856x53568, b=2048): 0.0605
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17856x53568, b=2048): 259.155
Elapsed time for attention_key_query_prob (256x2048x279x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x279x2048): 73.110
Elapsed time for attention_prob_times_values (256x2048x2048x279): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x279): 58.640
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17856x17856, b=2048): 0.0201
Throughput (in TFLOP/s) for attention_linear_projection (4x17856x17856, b=2048): 260.016
Elapsed time for mlp_h_to_4h (4x17856x71424, b=2048): 0.0808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17856x71424, b=2048): 258.641
Elapsed time for mlp_fused_gelu (2048x4x71424): 0.0020
Elapsed time for mlp_4h_to_h (4x71424x17856, b=2048): 0.0830
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71424x17856, b=2048): 251.650
Elapsed time for transformer_add_bias_dropout (2048x4x17856): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17856): 0.0007

Attention duration (in seconds): 0.1340
Attention throughput (in TFLOP/s): 164.909
MLP duration (in seconds): 0.1658
MLP throughput (in TFLOP/s): 252.072
Transformer duration (in seconds): 0.3039
Transformer throughput (in TFLOP/s): 210.229
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1107
Attention throughput (in TFLOP/s): 199.514
MLP duration (in seconds): 0.1692
MLP throughput (in TFLOP/s): 246.995
Transformer duration (in seconds): 0.2833
Transformer throughput (in TFLOP/s): 225.481
Transformer - MLP - Attention (in seconds): 0.0034
========================================================================================================================
num_attention_heads: 64, hidden_size: 17920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17920x53760, b=2048): 0.0610
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17920x53760, b=2048): 258.606
Elapsed time for attention_key_query_prob (256x2048x280x2048): 0.0045
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x280x2048): 134.955
Elapsed time for attention_prob_times_values (256x2048x2048x280): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x280): 140.861
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x17920x17920, b=2048): 0.0202
Throughput (in TFLOP/s) for attention_linear_projection (4x17920x17920, b=2048): 260.970
Elapsed time for mlp_h_to_4h (4x17920x71680, b=2048): 0.0813
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17920x71680, b=2048): 258.968
Elapsed time for mlp_fused_gelu (2048x4x71680): 0.0020
Elapsed time for mlp_4h_to_h (4x71680x17920, b=2048): 0.0826
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71680x17920, b=2048): 254.668
Elapsed time for transformer_add_bias_dropout (2048x4x17920): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17920): 0.0007

Attention duration (in seconds): 0.1249
Attention throughput (in TFLOP/s): 178.102
MLP duration (in seconds): 0.1659
MLP throughput (in TFLOP/s): 253.745
Transformer duration (in seconds): 0.2949
Transformer throughput (in TFLOP/s): 218.187
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1020
Attention throughput (in TFLOP/s): 218.214
MLP duration (in seconds): 0.1694
MLP throughput (in TFLOP/s): 248.529
Transformer duration (in seconds): 0.2744
Transformer throughput (in TFLOP/s): 234.467
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 17984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x17984x53952, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x17984x53952, b=2048): 258.563
Elapsed time for attention_key_query_prob (256x2048x281x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x281x2048): 73.576
Elapsed time for attention_prob_times_values (256x2048x2048x281): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x281): 58.779
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0307
Elapsed time for attention_linear_projection (4x17984x17984, b=2048): 0.0203
Throughput (in TFLOP/s) for attention_linear_projection (4x17984x17984, b=2048): 260.859
Elapsed time for mlp_h_to_4h (4x17984x71936, b=2048): 0.0821
Throughput (in TFLOP/s) for mlp_h_to_4h (4x17984x71936, b=2048): 258.314
Elapsed time for mlp_fused_gelu (2048x4x71936): 0.0020
Elapsed time for mlp_4h_to_h (4x71936x17984, b=2048): 0.0837
Throughput (in TFLOP/s) for mlp_4h_to_h (4x71936x17984, b=2048): 253.367
Elapsed time for transformer_add_bias_dropout (2048x4x17984): 0.0013
Elapsed time for transformer_layer_norm (2048x4x17984): 0.0007

Attention duration (in seconds): 0.1356
Attention throughput (in TFLOP/s): 165.264
MLP duration (in seconds): 0.1677
MLP throughput (in TFLOP/s): 252.795
Transformer duration (in seconds): 0.3074
Transformer throughput (in TFLOP/s): 210.793
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1123
Attention throughput (in TFLOP/s): 199.518
MLP duration (in seconds): 0.1714
MLP throughput (in TFLOP/s): 247.286
Transformer duration (in seconds): 0.2903
Transformer throughput (in TFLOP/s): 223.178
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 18048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18048x54144, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18048x54144, b=2048): 254.599
Elapsed time for attention_key_query_prob (256x2048x282x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x282x2048): 108.955
Elapsed time for attention_prob_times_values (256x2048x2048x282): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x282): 99.129
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x18048x18048, b=2048): 0.0206
Throughput (in TFLOP/s) for attention_linear_projection (4x18048x18048, b=2048): 258.646
Elapsed time for mlp_h_to_4h (4x18048x72192, b=2048): 0.0828
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18048x72192, b=2048): 257.758
Elapsed time for mlp_fused_gelu (2048x4x72192): 0.0020
Elapsed time for mlp_4h_to_h (4x72192x18048, b=2048): 0.0846
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72192x18048, b=2048): 252.342
Elapsed time for transformer_add_bias_dropout (2048x4x18048): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18048): 0.0007

Attention duration (in seconds): 0.1302
Attention throughput (in TFLOP/s): 173.300
MLP duration (in seconds): 0.1694
MLP throughput (in TFLOP/s): 252.022
Transformer duration (in seconds): 0.3037
Transformer throughput (in TFLOP/s): 214.860
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1040
Attention throughput (in TFLOP/s): 216.805
MLP duration (in seconds): 0.1723
MLP throughput (in TFLOP/s): 247.740
Transformer duration (in seconds): 0.2852
Transformer throughput (in TFLOP/s): 228.765
Transformer - MLP - Attention (in seconds): 0.0089
========================================================================================================================
num_attention_heads: 64, hidden_size: 18112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18112x54336, b=2048): 0.0632
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18112x54336, b=2048): 255.132
Elapsed time for attention_key_query_prob (256x2048x283x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x283x2048): 74.029
Elapsed time for attention_prob_times_values (256x2048x2048x283): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x283): 60.411
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x18112x18112, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x18112x18112, b=2048): 255.770
Elapsed time for mlp_h_to_4h (4x18112x72448, b=2048): 0.0836
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18112x72448, b=2048): 257.112
Elapsed time for mlp_fused_gelu (2048x4x72448): 0.0020
Elapsed time for mlp_4h_to_h (4x72448x18112, b=2048): 0.0854
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72448x18112, b=2048): 251.674
Elapsed time for transformer_add_bias_dropout (2048x4x18112): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18112): 0.0007

Attention duration (in seconds): 0.1375
Attention throughput (in TFLOP/s): 165.228
MLP duration (in seconds): 0.1710
MLP throughput (in TFLOP/s): 251.393
Transformer duration (in seconds): 0.3127
Transformer throughput (in TFLOP/s): 210.171
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1135
Attention throughput (in TFLOP/s): 200.116
MLP duration (in seconds): 0.1735
MLP throughput (in TFLOP/s): 247.765
Transformer duration (in seconds): 0.2933
Transformer throughput (in TFLOP/s): 224.042
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 18176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18176x54528, b=2048): 0.0637
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18176x54528, b=2048): 255.015
Elapsed time for attention_key_query_prob (256x2048x284x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x284x2048): 109.080
Elapsed time for attention_prob_times_values (256x2048x2048x284): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x284): 99.871
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x18176x18176, b=2048): 0.0210
Throughput (in TFLOP/s) for attention_linear_projection (4x18176x18176, b=2048): 257.608
Elapsed time for mlp_h_to_4h (4x18176x72704, b=2048): 0.0839
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18176x72704, b=2048): 257.999
Elapsed time for mlp_fused_gelu (2048x4x72704): 0.0020
Elapsed time for mlp_4h_to_h (4x72704x18176, b=2048): 0.0854
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72704x18176, b=2048): 253.655
Elapsed time for transformer_add_bias_dropout (2048x4x18176): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18176): 0.0007

Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 174.105
MLP duration (in seconds): 0.1713
MLP throughput (in TFLOP/s): 252.821
Transformer duration (in seconds): 0.3068
Transformer throughput (in TFLOP/s): 215.706
Transformer - MLP - Attention (in seconds): 0.0041


Actual
------
Attention duration (in seconds): 0.1053
Attention throughput (in TFLOP/s): 217.151
MLP duration (in seconds): 0.1743
MLP throughput (in TFLOP/s): 248.431
Transformer duration (in seconds): 0.2863
Transformer throughput (in TFLOP/s): 231.145
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 18240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18240x54720, b=2048): 0.0641
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18240x54720, b=2048): 254.966
Elapsed time for attention_key_query_prob (256x2048x285x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x285x2048): 74.527
Elapsed time for attention_prob_times_values (256x2048x2048x285): 0.0101
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x285): 60.830
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18240x18240, b=2048): 0.0213
Throughput (in TFLOP/s) for attention_linear_projection (4x18240x18240, b=2048): 256.141
Elapsed time for mlp_h_to_4h (4x18240x72960, b=2048): 0.0850
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18240x72960, b=2048): 256.602
Elapsed time for mlp_fused_gelu (2048x4x72960): 0.0020
Elapsed time for mlp_4h_to_h (4x72960x18240, b=2048): 0.0867
Throughput (in TFLOP/s) for mlp_4h_to_h (4x72960x18240, b=2048): 251.342
Elapsed time for transformer_add_bias_dropout (2048x4x18240): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18240): 0.0007

Attention duration (in seconds): 0.1387
Attention throughput (in TFLOP/s): 166.069
MLP duration (in seconds): 0.1737
MLP throughput (in TFLOP/s): 251.013
Transformer duration (in seconds): 0.3166
Transformer throughput (in TFLOP/s): 210.495
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1149
Attention throughput (in TFLOP/s): 200.496
MLP duration (in seconds): 0.1764
MLP throughput (in TFLOP/s): 247.255
Transformer duration (in seconds): 0.2943
Transformer throughput (in TFLOP/s): 226.387
Transformer - MLP - Attention (in seconds): 0.0031
========================================================================================================================
num_attention_heads: 64, hidden_size: 18304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18304x54912, b=2048): 0.0637
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18304x54912, b=2048): 258.348
Elapsed time for attention_key_query_prob (256x2048x286x2048): 0.0056
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x286x2048): 110.595
Elapsed time for attention_prob_times_values (256x2048x2048x286): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x286): 101.427
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18304x18304, b=2048): 0.0211
Throughput (in TFLOP/s) for attention_linear_projection (4x18304x18304, b=2048): 259.979
Elapsed time for mlp_h_to_4h (4x18304x73216, b=2048): 0.0850
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18304x73216, b=2048): 258.349
Elapsed time for mlp_fused_gelu (2048x4x73216): 0.0020
Elapsed time for mlp_4h_to_h (4x73216x18304, b=2048): 0.0862
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73216x18304, b=2048): 254.732
Elapsed time for transformer_add_bias_dropout (2048x4x18304): 0.0013
Elapsed time for transformer_layer_norm (2048x4x18304): 0.0007

Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 176.394
MLP duration (in seconds): 0.1732
MLP throughput (in TFLOP/s): 253.546
Transformer duration (in seconds): 0.3088
Transformer throughput (in TFLOP/s): 217.284
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1062
Attention throughput (in TFLOP/s): 218.238
MLP duration (in seconds): 0.1760
MLP throughput (in TFLOP/s): 249.464
Transformer duration (in seconds): 0.2884
Transformer throughput (in TFLOP/s): 232.693
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 18368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18368x55104, b=2048): 0.0647
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18368x55104, b=2048): 256.125
Elapsed time for attention_key_query_prob (256x2048x287x2048): 0.0082
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x287x2048): 75.484
Elapsed time for attention_prob_times_values (256x2048x2048x287): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x287): 60.298
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18368x18368, b=2048): 0.0215
Throughput (in TFLOP/s) for attention_linear_projection (4x18368x18368, b=2048): 257.095
Elapsed time for mlp_h_to_4h (4x18368x73472, b=2048): 0.0860
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18368x73472, b=2048): 257.010
Elapsed time for mlp_fused_gelu (2048x4x73472): 0.0020
Elapsed time for mlp_4h_to_h (4x73472x18368, b=2048): 0.0881
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73472x18368, b=2048): 251.115
Elapsed time for transformer_add_bias_dropout (2048x4x18368): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18368): 0.0008

Attention duration (in seconds): 0.1396
Attention throughput (in TFLOP/s): 167.212
MLP duration (in seconds): 0.1761
MLP throughput (in TFLOP/s): 251.113
Transformer duration (in seconds): 0.3199
Transformer throughput (in TFLOP/s): 211.196
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1163
Attention throughput (in TFLOP/s): 200.768
MLP duration (in seconds): 0.1798
MLP throughput (in TFLOP/s): 245.883
Transformer duration (in seconds): 0.3027
Transformer throughput (in TFLOP/s): 223.244
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 18432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18432x55296, b=2048): 0.0655
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18432x55296, b=2048): 254.844
Elapsed time for attention_key_query_prob (256x2048x288x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x288x2048): 161.398
Elapsed time for attention_prob_times_values (256x2048x2048x288): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x288): 144.886
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x18432x18432, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x18432x18432, b=2048): 256.791
Elapsed time for mlp_h_to_4h (4x18432x73728, b=2048): 0.0868
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18432x73728, b=2048): 256.602
Elapsed time for mlp_fused_gelu (2048x4x73728): 0.0020
Elapsed time for mlp_4h_to_h (4x73728x18432, b=2048): 0.0883
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73728x18432, b=2048): 252.189
Elapsed time for transformer_add_bias_dropout (2048x4x18432): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18432): 0.0007

Attention duration (in seconds): 0.1303
Attention throughput (in TFLOP/s): 180.395
MLP duration (in seconds): 0.1771
MLP throughput (in TFLOP/s): 251.463
Transformer duration (in seconds): 0.3116
Transformer throughput (in TFLOP/s): 218.353
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1063
Attention throughput (in TFLOP/s): 220.989
MLP duration (in seconds): 0.1784
MLP throughput (in TFLOP/s): 249.574
Transformer duration (in seconds): 0.2888
Transformer throughput (in TFLOP/s): 235.591
Transformer - MLP - Attention (in seconds): 0.0040
========================================================================================================================
num_attention_heads: 64, hidden_size: 18496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18496x55488, b=2048): 0.0652
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18496x55488, b=2048): 257.860
Elapsed time for attention_key_query_prob (256x2048x289x2048): 0.0085
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x289x2048): 73.077
Elapsed time for attention_prob_times_values (256x2048x2048x289): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x289): 60.247
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18496x18496, b=2048): 0.0216
Throughput (in TFLOP/s) for attention_linear_projection (4x18496x18496, b=2048): 259.921
Elapsed time for mlp_h_to_4h (4x18496x73984, b=2048): 0.0868
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18496x73984, b=2048): 258.381
Elapsed time for mlp_fused_gelu (2048x4x73984): 0.0020
Elapsed time for mlp_4h_to_h (4x73984x18496, b=2048): 0.0882
Throughput (in TFLOP/s) for mlp_4h_to_h (4x73984x18496, b=2048): 254.297
Elapsed time for transformer_add_bias_dropout (2048x4x18496): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18496): 0.0008

Attention duration (in seconds): 0.1405
Attention throughput (in TFLOP/s): 168.361
MLP duration (in seconds): 0.1770
MLP throughput (in TFLOP/s): 253.376
Transformer duration (in seconds): 0.3218
Transformer throughput (in TFLOP/s): 212.898
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1170
Attention throughput (in TFLOP/s): 202.232
MLP duration (in seconds): 0.1795
MLP throughput (in TFLOP/s): 249.804
Transformer duration (in seconds): 0.3026
Transformer throughput (in TFLOP/s): 226.357
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 18560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18560x55680, b=2048): 0.0657
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18560x55680, b=2048): 257.705
Elapsed time for attention_key_query_prob (256x2048x290x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x290x2048): 106.021
Elapsed time for attention_prob_times_values (256x2048x2048x290): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x290): 102.054
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18560x18560, b=2048): 0.0217
Throughput (in TFLOP/s) for attention_linear_projection (4x18560x18560, b=2048): 259.954
Elapsed time for mlp_h_to_4h (4x18560x74240, b=2048): 0.0876
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18560x74240, b=2048): 257.847
Elapsed time for mlp_fused_gelu (2048x4x74240): 0.0021
Elapsed time for mlp_4h_to_h (4x74240x18560, b=2048): 0.0888
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74240x18560, b=2048): 254.357
Elapsed time for transformer_add_bias_dropout (2048x4x18560): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18560): 0.0007

Attention duration (in seconds): 0.1344
Attention throughput (in TFLOP/s): 177.290
MLP duration (in seconds): 0.1784
MLP throughput (in TFLOP/s): 253.142
Transformer duration (in seconds): 0.3169
Transformer throughput (in TFLOP/s): 217.616
Transformer - MLP - Attention (in seconds): 0.0042


Actual
------
Attention duration (in seconds): 0.1093
Attention throughput (in TFLOP/s): 217.892
MLP duration (in seconds): 0.1821
MLP throughput (in TFLOP/s): 247.999
Transformer duration (in seconds): 0.2986
Transformer throughput (in TFLOP/s): 231.013
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 18624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18624x55872, b=2048): 0.0671
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18624x55872, b=2048): 253.977
Elapsed time for attention_key_query_prob (256x2048x291x2048): 0.0086
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x291x2048): 72.265
Elapsed time for attention_prob_times_values (256x2048x2048x291): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x291): 61.463
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18624x18624, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x18624x18624, b=2048): 257.253
Elapsed time for mlp_h_to_4h (4x18624x74496, b=2048): 0.0884
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18624x74496, b=2048): 257.238
Elapsed time for mlp_fused_gelu (2048x4x74496): 0.0020
Elapsed time for mlp_4h_to_h (4x74496x18624, b=2048): 0.0902
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74496x18624, b=2048): 251.964
Elapsed time for transformer_add_bias_dropout (2048x4x18624): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18624): 0.0008

Attention duration (in seconds): 0.1430
Attention throughput (in TFLOP/s): 167.697
MLP duration (in seconds): 0.1806
MLP throughput (in TFLOP/s): 251.687
Transformer duration (in seconds): 0.3279
Transformer throughput (in TFLOP/s): 211.779
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1193
Attention throughput (in TFLOP/s): 200.942
MLP duration (in seconds): 0.1837
MLP throughput (in TFLOP/s): 247.459
Transformer duration (in seconds): 0.3096
Transformer throughput (in TFLOP/s): 224.272
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 18688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18688x56064, b=2048): 0.0673
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18688x56064, b=2048): 254.922
Elapsed time for attention_key_query_prob (256x2048x292x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x292x2048): 106.177
Elapsed time for attention_prob_times_values (256x2048x2048x292): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x292): 101.558
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18688x18688, b=2048): 0.0222
Throughput (in TFLOP/s) for attention_linear_projection (4x18688x18688, b=2048): 257.541
Elapsed time for mlp_h_to_4h (4x18688x74752, b=2048): 0.0890
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18688x74752, b=2048): 257.057
Elapsed time for mlp_fused_gelu (2048x4x74752): 0.0021
Elapsed time for mlp_4h_to_h (4x74752x18688, b=2048): 0.0905
Throughput (in TFLOP/s) for mlp_4h_to_h (4x74752x18688, b=2048): 252.821
Elapsed time for transformer_add_bias_dropout (2048x4x18688): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18688): 0.0008

Attention duration (in seconds): 0.1366
Attention throughput (in TFLOP/s): 176.729
MLP duration (in seconds): 0.1816
MLP throughput (in TFLOP/s): 252.037
Transformer duration (in seconds): 0.3225
Transformer throughput (in TFLOP/s): 216.810
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1105
Attention throughput (in TFLOP/s): 218.482
MLP duration (in seconds): 0.1839
MLP throughput (in TFLOP/s): 248.867
Transformer duration (in seconds): 0.2982
Transformer throughput (in TFLOP/s): 234.456
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 18752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18752x56256, b=2048): 0.0671
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18752x56256, b=2048): 257.535
Elapsed time for attention_key_query_prob (256x2048x293x2048): 0.0087
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x293x2048): 72.195
Elapsed time for attention_prob_times_values (256x2048x2048x293): 0.0102
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x293): 61.552
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18752x18752, b=2048): 0.0221
Throughput (in TFLOP/s) for attention_linear_projection (4x18752x18752, b=2048): 260.423
Elapsed time for mlp_h_to_4h (4x18752x75008, b=2048): 0.0893
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18752x75008, b=2048): 257.995
Elapsed time for mlp_fused_gelu (2048x4x75008): 0.0021
Elapsed time for mlp_4h_to_h (4x75008x18752, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75008x18752, b=2048): 251.597
Elapsed time for transformer_add_bias_dropout (2048x4x18752): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18752): 0.0008

Attention duration (in seconds): 0.1431
Attention throughput (in TFLOP/s): 169.783
MLP duration (in seconds): 0.1830
MLP throughput (in TFLOP/s): 251.885
Transformer duration (in seconds): 0.3305
Transformer throughput (in TFLOP/s): 213.022
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1206
Attention throughput (in TFLOP/s): 201.578
MLP duration (in seconds): 0.1865
MLP throughput (in TFLOP/s): 247.102
Transformer duration (in seconds): 0.3134
Transformer throughput (in TFLOP/s): 224.599
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 18816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18816x56448, b=2048): 0.0683
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18816x56448, b=2048): 254.943
Elapsed time for attention_key_query_prob (256x2048x294x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x294x2048): 106.909
Elapsed time for attention_prob_times_values (256x2048x2048x294): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x294): 102.061
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x18816x18816, b=2048): 0.0226
Throughput (in TFLOP/s) for attention_linear_projection (4x18816x18816, b=2048): 256.998
Elapsed time for mlp_h_to_4h (4x18816x75264, b=2048): 0.0905
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18816x75264, b=2048): 256.247
Elapsed time for mlp_fused_gelu (2048x4x75264): 0.0021
Elapsed time for mlp_4h_to_h (4x75264x18816, b=2048): 0.0920
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75264x18816, b=2048): 252.082
Elapsed time for transformer_add_bias_dropout (2048x4x18816): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18816): 0.0008

Attention duration (in seconds): 0.1379
Attention throughput (in TFLOP/s): 177.420
MLP duration (in seconds): 0.1847
MLP throughput (in TFLOP/s): 251.299
Transformer duration (in seconds): 0.3268
Transformer throughput (in TFLOP/s): 216.842
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1119
Attention throughput (in TFLOP/s): 218.574
MLP duration (in seconds): 0.1874
MLP throughput (in TFLOP/s): 247.686
Transformer duration (in seconds): 0.3067
Transformer throughput (in TFLOP/s): 231.066
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 18880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18880x56640, b=2048): 0.0688
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18880x56640, b=2048): 254.611
Elapsed time for attention_key_query_prob (256x2048x295x2048): 0.0088
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x295x2048): 71.944
Elapsed time for attention_prob_times_values (256x2048x2048x295): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x295): 60.643
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18880x18880, b=2048): 0.0225
Throughput (in TFLOP/s) for attention_linear_projection (4x18880x18880, b=2048): 259.931
Elapsed time for mlp_h_to_4h (4x18880x75520, b=2048): 0.0905
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18880x75520, b=2048): 258.174
Elapsed time for mlp_fused_gelu (2048x4x75520): 0.0021
Elapsed time for mlp_4h_to_h (4x75520x18880, b=2048): 0.0909
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75520x18880, b=2048): 256.897
Elapsed time for transformer_add_bias_dropout (2048x4x18880): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18880): 0.0008

Attention duration (in seconds): 0.1455
Attention throughput (in TFLOP/s): 169.257
MLP duration (in seconds): 0.1835
MLP throughput (in TFLOP/s): 254.619
Transformer duration (in seconds): 0.3333
Transformer throughput (in TFLOP/s): 214.054
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1214
Attention throughput (in TFLOP/s): 202.789
MLP duration (in seconds): 0.1868
MLP throughput (in TFLOP/s): 250.079
Transformer duration (in seconds): 0.3152
Transformer throughput (in TFLOP/s): 226.336
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 18944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x18944x56832, b=2048): 0.0690
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x18944x56832, b=2048): 255.510
Elapsed time for attention_key_query_prob (256x2048x296x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x296x2048): 136.813
Elapsed time for attention_prob_times_values (256x2048x2048x296): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x296): 146.295
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x18944x18944, b=2048): 0.0228
Throughput (in TFLOP/s) for attention_linear_projection (4x18944x18944, b=2048): 258.154
Elapsed time for mlp_h_to_4h (4x18944x75776, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_h_to_4h (4x18944x75776, b=2048): 256.663
Elapsed time for mlp_fused_gelu (2048x4x75776): 0.0021
Elapsed time for mlp_4h_to_h (4x75776x18944, b=2048): 0.0916
Throughput (in TFLOP/s) for mlp_4h_to_h (4x75776x18944, b=2048): 256.625
Elapsed time for transformer_add_bias_dropout (2048x4x18944): 0.0014
Elapsed time for transformer_layer_norm (2048x4x18944): 0.0008

Attention duration (in seconds): 0.1358
Attention throughput (in TFLOP/s): 182.590
MLP duration (in seconds): 0.1854
MLP throughput (in TFLOP/s): 253.759
Transformer duration (in seconds): 0.3254
Transformer throughput (in TFLOP/s): 220.708
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1122
Attention throughput (in TFLOP/s): 220.874
MLP duration (in seconds): 0.1877
MLP throughput (in TFLOP/s): 250.659
Transformer duration (in seconds): 0.3173
Transformer throughput (in TFLOP/s): 226.341
Transformer - MLP - Attention (in seconds): 0.0175
========================================================================================================================
num_attention_heads: 64, hidden_size: 19008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19008x57024, b=2048): 0.0690
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19008x57024, b=2048): 257.385
Elapsed time for attention_key_query_prob (256x2048x297x2048): 0.0089
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x297x2048): 71.817
Elapsed time for attention_prob_times_values (256x2048x2048x297): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x297): 60.860
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19008x19008, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_linear_projection (4x19008x19008, b=2048): 257.277
Elapsed time for mlp_h_to_4h (4x19008x76032, b=2048): 0.0919
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19008x76032, b=2048): 257.749
Elapsed time for mlp_fused_gelu (2048x4x76032): 0.0021
Elapsed time for mlp_4h_to_h (4x76032x19008, b=2048): 0.0930
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76032x19008, b=2048): 254.556
Elapsed time for transformer_add_bias_dropout (2048x4x19008): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19008): 0.0008

Attention duration (in seconds): 0.1464
Attention throughput (in TFLOP/s): 170.487
MLP duration (in seconds): 0.1870
MLP throughput (in TFLOP/s): 253.278
Transformer duration (in seconds): 0.3377
Transformer throughput (in TFLOP/s): 214.118
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1232
Attention throughput (in TFLOP/s): 202.542
MLP duration (in seconds): 0.1906
MLP throughput (in TFLOP/s): 248.513
Transformer duration (in seconds): 0.3168
Transformer throughput (in TFLOP/s): 228.253
Transformer - MLP - Attention (in seconds): 0.0030
========================================================================================================================
num_attention_heads: 64, hidden_size: 19072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19072x57216, b=2048): 0.0694
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19072x57216, b=2048): 257.621
Elapsed time for attention_key_query_prob (256x2048x298x2048): 0.0059
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x298x2048): 108.132
Elapsed time for attention_prob_times_values (256x2048x2048x298): 0.0061
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x298): 104.278
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x19072x19072, b=2048): 0.0230
Throughput (in TFLOP/s) for attention_linear_projection (4x19072x19072, b=2048): 259.245
Elapsed time for mlp_h_to_4h (4x19072x76288, b=2048): 0.0925
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19072x76288, b=2048): 257.784
Elapsed time for mlp_fused_gelu (2048x4x76288): 0.0021
Elapsed time for mlp_4h_to_h (4x76288x19072, b=2048): 0.0935
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76288x19072, b=2048): 254.823
Elapsed time for transformer_add_bias_dropout (2048x4x19072): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19072): 0.0008

Attention duration (in seconds): 0.1394
Attention throughput (in TFLOP/s): 180.127
MLP duration (in seconds): 0.1881
MLP throughput (in TFLOP/s): 253.434
Transformer duration (in seconds): 0.3319
Transformer throughput (in TFLOP/s): 219.318
Transformer - MLP - Attention (in seconds): 0.0043


Actual
------
Attention duration (in seconds): 0.1148
Attention throughput (in TFLOP/s): 218.806
MLP duration (in seconds): 0.1902
MLP throughput (in TFLOP/s): 250.649
Transformer duration (in seconds): 0.3098
Transformer throughput (in TFLOP/s): 234.961
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 19136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19136x57408, b=2048): 0.0698
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19136x57408, b=2048): 257.946
Elapsed time for attention_key_query_prob (256x2048x299x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x299x2048): 71.541
Elapsed time for attention_prob_times_values (256x2048x2048x299): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x299): 62.494
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x19136x19136, b=2048): 0.0233
Throughput (in TFLOP/s) for attention_linear_projection (4x19136x19136, b=2048): 257.481
Elapsed time for mlp_h_to_4h (4x19136x76544, b=2048): 0.0929
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19136x76544, b=2048): 258.222
Elapsed time for mlp_fused_gelu (2048x4x76544): 0.0021
Elapsed time for mlp_4h_to_h (4x76544x19136, b=2048): 0.0938
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76544x19136, b=2048): 255.861
Elapsed time for transformer_add_bias_dropout (2048x4x19136): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19136): 0.0008

Attention duration (in seconds): 0.1473
Attention throughput (in TFLOP/s): 171.614
MLP duration (in seconds): 0.1888
MLP throughput (in TFLOP/s): 254.171
Transformer duration (in seconds): 0.3406
Transformer throughput (in TFLOP/s): 215.179
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1241
Attention throughput (in TFLOP/s): 203.760
MLP duration (in seconds): 0.1923
MLP throughput (in TFLOP/s): 249.638
Transformer duration (in seconds): 0.3228
Transformer throughput (in TFLOP/s): 227.002
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 19200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19200x57600, b=2048): 0.0709
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19200x57600, b=2048): 255.492
Elapsed time for attention_key_query_prob (256x2048x300x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x300x2048): 108.268
Elapsed time for attention_prob_times_values (256x2048x2048x300): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x300): 104.623
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x19200x19200, b=2048): 0.0232
Throughput (in TFLOP/s) for attention_linear_projection (4x19200x19200, b=2048): 260.466
Elapsed time for mlp_h_to_4h (4x19200x76800, b=2048): 0.0935
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19200x76800, b=2048): 258.497
Elapsed time for mlp_fused_gelu (2048x4x76800): 0.0021
Elapsed time for mlp_4h_to_h (4x76800x19200, b=2048): 0.0939
Throughput (in TFLOP/s) for mlp_4h_to_h (4x76800x19200, b=2048): 257.389
Elapsed time for transformer_add_bias_dropout (2048x4x19200): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19200): 0.0008

Attention duration (in seconds): 0.1412
Attention throughput (in TFLOP/s): 180.226
MLP duration (in seconds): 0.1894
MLP throughput (in TFLOP/s): 255.063
Transformer duration (in seconds): 0.3350
Transformer throughput (in TFLOP/s): 220.190
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1158
Attention throughput (in TFLOP/s): 219.768
MLP duration (in seconds): 0.1933
MLP throughput (in TFLOP/s): 249.954
Transformer duration (in seconds): 0.3163
Transformer throughput (in TFLOP/s): 233.229
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 19264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19264x57792, b=2048): 0.0716
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19264x57792, b=2048): 254.670
Elapsed time for attention_key_query_prob (256x2048x301x2048): 0.0090
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x301x2048): 71.585
Elapsed time for attention_prob_times_values (256x2048x2048x301): 0.0103
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x301): 62.644
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x19264x19264, b=2048): 0.0236
Throughput (in TFLOP/s) for attention_linear_projection (4x19264x19264, b=2048): 257.626
Elapsed time for mlp_h_to_4h (4x19264x77056, b=2048): 0.0948
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19264x77056, b=2048): 256.422
Elapsed time for mlp_fused_gelu (2048x4x77056): 0.0021
Elapsed time for mlp_4h_to_h (4x77056x19264, b=2048): 0.0961
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77056x19264, b=2048): 253.118
Elapsed time for transformer_add_bias_dropout (2048x4x19264): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19264): 0.0008

Attention duration (in seconds): 0.1496
Attention throughput (in TFLOP/s): 171.267
MLP duration (in seconds): 0.1930
MLP throughput (in TFLOP/s): 251.962
Transformer duration (in seconds): 0.3470
Transformer throughput (in TFLOP/s): 213.982
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1256
Attention throughput (in TFLOP/s): 203.932
MLP duration (in seconds): 0.1960
MLP throughput (in TFLOP/s): 248.220
Transformer duration (in seconds): 0.3294
Transformer throughput (in TFLOP/s): 225.454
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 64, hidden_size: 19328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19328x57984, b=2048): 0.0721
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19328x57984, b=2048): 254.670
Elapsed time for attention_key_query_prob (256x2048x302x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x302x2048): 108.279
Elapsed time for attention_prob_times_values (256x2048x2048x302): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x302): 104.728
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19328x19328, b=2048): 0.0235
Throughput (in TFLOP/s) for attention_linear_projection (4x19328x19328, b=2048): 260.951
Elapsed time for mlp_h_to_4h (4x19328x77312, b=2048): 0.0948
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19328x77312, b=2048): 258.353
Elapsed time for mlp_fused_gelu (2048x4x77312): 0.0021
Elapsed time for mlp_4h_to_h (4x77312x19328, b=2048): 0.0956
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77312x19328, b=2048): 256.190
Elapsed time for transformer_add_bias_dropout (2048x4x19328): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19328): 0.0008

Attention duration (in seconds): 0.1427
Attention throughput (in TFLOP/s): 180.622
MLP duration (in seconds): 0.1925
MLP throughput (in TFLOP/s): 254.423
Transformer duration (in seconds): 0.3396
Transformer throughput (in TFLOP/s): 220.107
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1172
Attention throughput (in TFLOP/s): 220.037
MLP duration (in seconds): 0.1961
MLP throughput (in TFLOP/s): 249.704
Transformer duration (in seconds): 0.3201
Transformer throughput (in TFLOP/s): 233.501
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 19392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19392x58176, b=2048): 0.0728
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19392x58176, b=2048): 253.939
Elapsed time for attention_key_query_prob (256x2048x303x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x303x2048): 71.652
Elapsed time for attention_prob_times_values (256x2048x2048x303): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x303): 62.021
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19392x19392, b=2048): 0.0238
Throughput (in TFLOP/s) for attention_linear_projection (4x19392x19392, b=2048): 258.487
Elapsed time for mlp_h_to_4h (4x19392x77568, b=2048): 0.0956
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19392x77568, b=2048): 257.768
Elapsed time for mlp_fused_gelu (2048x4x77568): 0.0021
Elapsed time for mlp_4h_to_h (4x77568x19392, b=2048): 0.0961
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77568x19392, b=2048): 256.342
Elapsed time for transformer_add_bias_dropout (2048x4x19392): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19392): 0.0008

Attention duration (in seconds): 0.1512
Attention throughput (in TFLOP/s): 171.629
MLP duration (in seconds): 0.1939
MLP throughput (in TFLOP/s): 254.222
Transformer duration (in seconds): 0.3495
Transformer throughput (in TFLOP/s): 215.268
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1272
Attention throughput (in TFLOP/s): 204.016
MLP duration (in seconds): 0.1983
MLP throughput (in TFLOP/s): 248.598
Transformer duration (in seconds): 0.3331
Transformer throughput (in TFLOP/s): 225.884
Transformer - MLP - Attention (in seconds): 0.0076
========================================================================================================================
num_attention_heads: 64, hidden_size: 19456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19456x58368, b=2048): 0.0729
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19456x58368, b=2048): 255.227
Elapsed time for attention_key_query_prob (256x2048x304x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x304x2048): 153.214
Elapsed time for attention_prob_times_values (256x2048x2048x304): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x304): 152.838
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19456x19456, b=2048): 0.0240
Throughput (in TFLOP/s) for attention_linear_projection (4x19456x19456, b=2048): 258.079
Elapsed time for mlp_h_to_4h (4x19456x77824, b=2048): 0.0963
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19456x77824, b=2048): 257.515
Elapsed time for mlp_fused_gelu (2048x4x77824): 0.0021
Elapsed time for mlp_4h_to_h (4x77824x19456, b=2048): 0.0969
Throughput (in TFLOP/s) for mlp_4h_to_h (4x77824x19456, b=2048): 255.917
Elapsed time for transformer_add_bias_dropout (2048x4x19456): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19456): 0.0008

Attention duration (in seconds): 0.1404
Attention throughput (in TFLOP/s): 185.937
MLP duration (in seconds): 0.1954
MLP throughput (in TFLOP/s): 253.904
Transformer duration (in seconds): 0.3403
Transformer throughput (in TFLOP/s): 222.550
Transformer - MLP - Attention (in seconds): 0.0044


Actual
------
Attention duration (in seconds): 0.1178
Attention throughput (in TFLOP/s): 221.642
MLP duration (in seconds): 0.1992
MLP throughput (in TFLOP/s): 249.022
Transformer duration (in seconds): 0.3244
Transformer throughput (in TFLOP/s): 233.472
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 19520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19520x58560, b=2048): 0.0736
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19520x58560, b=2048): 254.345
Elapsed time for attention_key_query_prob (256x2048x305x2048): 0.0091
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x305x2048): 71.807
Elapsed time for attention_prob_times_values (256x2048x2048x305): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x305): 62.242
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x19520x19520, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19520x19520, b=2048): 256.241
Elapsed time for mlp_h_to_4h (4x19520x78080, b=2048): 0.0972
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19520x78080, b=2048): 257.018
Elapsed time for mlp_fused_gelu (2048x4x78080): 0.0021
Elapsed time for mlp_4h_to_h (4x78080x19520, b=2048): 0.0979
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78080x19520, b=2048): 254.940
Elapsed time for transformer_add_bias_dropout (2048x4x19520): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19520): 0.0008

Attention duration (in seconds): 0.1526
Attention throughput (in TFLOP/s): 172.204
MLP duration (in seconds): 0.1973
MLP throughput (in TFLOP/s): 253.190
Transformer duration (in seconds): 0.3544
Transformer throughput (in TFLOP/s): 215.099
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1287
Attention throughput (in TFLOP/s): 204.199
MLP duration (in seconds): 0.2020
MLP throughput (in TFLOP/s): 247.185
Transformer duration (in seconds): 0.3376
Transformer throughput (in TFLOP/s): 225.784
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 19584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19584x58752, b=2048): 0.0740
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19584x58752, b=2048): 254.748
Elapsed time for attention_key_query_prob (256x2048x306x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x306x2048): 108.818
Elapsed time for attention_prob_times_values (256x2048x2048x306): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x306): 105.695
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x19584x19584, b=2048): 0.0244
Throughput (in TFLOP/s) for attention_linear_projection (4x19584x19584, b=2048): 258.043
Elapsed time for mlp_h_to_4h (4x19584x78336, b=2048): 0.0979
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19584x78336, b=2048): 256.684
Elapsed time for mlp_fused_gelu (2048x4x78336): 0.0022
Elapsed time for mlp_4h_to_h (4x78336x19584, b=2048): 0.0983
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78336x19584, b=2048): 255.619
Elapsed time for transformer_add_bias_dropout (2048x4x19584): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19584): 0.0008

Attention duration (in seconds): 0.1456
Attention throughput (in TFLOP/s): 181.679
MLP duration (in seconds): 0.1984
MLP throughput (in TFLOP/s): 253.370
Transformer duration (in seconds): 0.3485
Transformer throughput (in TFLOP/s): 220.173
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1200
Attention throughput (in TFLOP/s): 220.483
MLP duration (in seconds): 0.2022
MLP throughput (in TFLOP/s): 248.616
Transformer duration (in seconds): 0.3300
Transformer throughput (in TFLOP/s): 232.476
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 64, hidden_size: 19648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19648x58944, b=2048): 0.0746
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19648x58944, b=2048): 254.410
Elapsed time for attention_key_query_prob (256x2048x307x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x307x2048): 71.271
Elapsed time for attention_prob_times_values (256x2048x2048x307): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x307): 63.523
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19648x19648, b=2048): 0.0245
Throughput (in TFLOP/s) for attention_linear_projection (4x19648x19648, b=2048): 258.466
Elapsed time for mlp_h_to_4h (4x19648x78592, b=2048): 0.0984
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19648x78592, b=2048): 257.170
Elapsed time for mlp_fused_gelu (2048x4x78592): 0.0022
Elapsed time for mlp_4h_to_h (4x78592x19648, b=2048): 0.1001
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78592x19648, b=2048): 252.822
Elapsed time for transformer_add_bias_dropout (2048x4x19648): 0.0014
Elapsed time for transformer_layer_norm (2048x4x19648): 0.0008

Attention duration (in seconds): 0.1537
Attention throughput (in TFLOP/s): 173.222
MLP duration (in seconds): 0.2006
MLP throughput (in TFLOP/s): 252.231
Transformer duration (in seconds): 0.3588
Transformer throughput (in TFLOP/s): 215.223
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1296
Attention throughput (in TFLOP/s): 205.368
MLP duration (in seconds): 0.2039
MLP throughput (in TFLOP/s): 248.176
Transformer duration (in seconds): 0.3415
Transformer throughput (in TFLOP/s): 226.114
Transformer - MLP - Attention (in seconds): 0.0080
========================================================================================================================
num_attention_heads: 64, hidden_size: 19712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19712x59136, b=2048): 0.0749
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19712x59136, b=2048): 254.900
Elapsed time for attention_key_query_prob (256x2048x308x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x308x2048): 109.958
Elapsed time for attention_prob_times_values (256x2048x2048x308): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x308): 105.876
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x19712x19712, b=2048): 0.0246
Throughput (in TFLOP/s) for attention_linear_projection (4x19712x19712, b=2048): 258.889
Elapsed time for mlp_h_to_4h (4x19712x78848, b=2048): 0.0992
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19712x78848, b=2048): 256.780
Elapsed time for mlp_fused_gelu (2048x4x78848): 0.0022
Elapsed time for mlp_4h_to_h (4x78848x19712, b=2048): 0.0997
Throughput (in TFLOP/s) for mlp_4h_to_h (4x78848x19712, b=2048): 255.365
Elapsed time for transformer_add_bias_dropout (2048x4x19712): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19712): 0.0008

Attention duration (in seconds): 0.1468
Attention throughput (in TFLOP/s): 182.536
MLP duration (in seconds): 0.2011
MLP throughput (in TFLOP/s): 253.308
Transformer duration (in seconds): 0.3523
Transformer throughput (in TFLOP/s): 220.573
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1204
Attention throughput (in TFLOP/s): 222.497
MLP duration (in seconds): 0.2038
MLP throughput (in TFLOP/s): 249.844
Transformer duration (in seconds): 0.3324
Transformer throughput (in TFLOP/s): 233.792
Transformer - MLP - Attention (in seconds): 0.0082
========================================================================================================================
num_attention_heads: 64, hidden_size: 19776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19776x59328, b=2048): 0.0754
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19776x59328, b=2048): 254.917
Elapsed time for attention_key_query_prob (256x2048x309x2048): 0.0093
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x309x2048): 71.691
Elapsed time for attention_prob_times_values (256x2048x2048x309): 0.0104
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x309): 63.690
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19776x19776, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_linear_projection (4x19776x19776, b=2048): 259.818
Elapsed time for mlp_h_to_4h (4x19776x79104, b=2048): 0.0996
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19776x79104, b=2048): 257.433
Elapsed time for mlp_fused_gelu (2048x4x79104): 0.0022
Elapsed time for mlp_4h_to_h (4x79104x19776, b=2048): 0.1010
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79104x19776, b=2048): 253.710
Elapsed time for transformer_add_bias_dropout (2048x4x19776): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19776): 0.0008

Attention duration (in seconds): 0.1547
Attention throughput (in TFLOP/s): 174.223
MLP duration (in seconds): 0.2028
MLP throughput (in TFLOP/s): 252.815
Transformer duration (in seconds): 0.3620
Transformer throughput (in TFLOP/s): 216.059
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1314
Attention throughput (in TFLOP/s): 205.230
MLP duration (in seconds): 0.2060
MLP throughput (in TFLOP/s): 248.834
Transformer duration (in seconds): 0.3447
Transformer throughput (in TFLOP/s): 226.921
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 19840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19840x59520, b=2048): 0.0760
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19840x59520, b=2048): 254.572
Elapsed time for attention_key_query_prob (256x2048x310x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x310x2048): 110.436
Elapsed time for attention_prob_times_values (256x2048x2048x310): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x310): 106.548
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19840x19840, b=2048): 0.0247
Throughput (in TFLOP/s) for attention_linear_projection (4x19840x19840, b=2048): 260.783
Elapsed time for mlp_h_to_4h (4x19840x79360, b=2048): 0.1000
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19840x79360, b=2048): 257.996
Elapsed time for mlp_fused_gelu (2048x4x79360): 0.0022
Elapsed time for mlp_4h_to_h (4x79360x19840, b=2048): 0.1009
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79360x19840, b=2048): 255.661
Elapsed time for transformer_add_bias_dropout (2048x4x19840): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19840): 0.0008

Attention duration (in seconds): 0.1480
Attention throughput (in TFLOP/s): 183.315
MLP duration (in seconds): 0.2031
MLP throughput (in TFLOP/s): 254.062
Transformer duration (in seconds): 0.3556
Transformer throughput (in TFLOP/s): 221.390
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1218
Attention throughput (in TFLOP/s): 222.774
MLP duration (in seconds): 0.2065
MLP throughput (in TFLOP/s): 249.807
Transformer duration (in seconds): 0.3371
Transformer throughput (in TFLOP/s): 233.543
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
num_attention_heads: 64, hidden_size: 19904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19904x59712, b=2048): 0.0767
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19904x59712, b=2048): 254.006
Elapsed time for attention_key_query_prob (256x2048x311x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x311x2048): 71.162
Elapsed time for attention_prob_times_values (256x2048x2048x311): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x311): 62.914
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19904x19904, b=2048): 0.0252
Throughput (in TFLOP/s) for attention_linear_projection (4x19904x19904, b=2048): 257.436
Elapsed time for mlp_h_to_4h (4x19904x79616, b=2048): 0.1008
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19904x79616, b=2048): 257.530
Elapsed time for mlp_fused_gelu (2048x4x79616): 0.0022
Elapsed time for mlp_4h_to_h (4x79616x19904, b=2048): 0.1020
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79616x19904, b=2048): 254.572
Elapsed time for transformer_add_bias_dropout (2048x4x19904): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19904): 0.0008

Attention duration (in seconds): 0.1569
Attention throughput (in TFLOP/s): 174.040
MLP duration (in seconds): 0.2050
MLP throughput (in TFLOP/s): 253.307
Transformer duration (in seconds): 0.3664
Transformer throughput (in TFLOP/s): 216.211
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1323
Attention throughput (in TFLOP/s): 206.271
MLP duration (in seconds): 0.2081
MLP throughput (in TFLOP/s): 249.512
Transformer duration (in seconds): 0.3479
Transformer throughput (in TFLOP/s): 227.702
Transformer - MLP - Attention (in seconds): 0.0075
========================================================================================================================
num_attention_heads: 64, hidden_size: 19968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x19968x59904, b=2048): 0.0770
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x19968x59904, b=2048): 254.665
Elapsed time for attention_key_query_prob (256x2048x312x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x312x2048): 135.004
Elapsed time for attention_prob_times_values (256x2048x2048x312): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x312): 153.155
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x19968x19968, b=2048): 0.0253
Throughput (in TFLOP/s) for attention_linear_projection (4x19968x19968, b=2048): 257.862
Elapsed time for mlp_h_to_4h (4x19968x79872, b=2048): 0.1014
Throughput (in TFLOP/s) for mlp_h_to_4h (4x19968x79872, b=2048): 257.677
Elapsed time for mlp_fused_gelu (2048x4x79872): 0.0022
Elapsed time for mlp_4h_to_h (4x79872x19968, b=2048): 0.1024
Throughput (in TFLOP/s) for mlp_4h_to_h (4x79872x19968, b=2048): 255.251
Elapsed time for transformer_add_bias_dropout (2048x4x19968): 0.0015
Elapsed time for transformer_layer_norm (2048x4x19968): 0.0008

Attention duration (in seconds): 0.1466
Attention throughput (in TFLOP/s): 187.373
MLP duration (in seconds): 0.2060
MLP throughput (in TFLOP/s): 253.722
Transformer duration (in seconds): 0.3571
Transformer throughput (in TFLOP/s): 223.260
Transformer - MLP - Attention (in seconds): 0.0045


Actual
------
Attention duration (in seconds): 0.1226
Attention throughput (in TFLOP/s): 224.105
MLP duration (in seconds): 0.2082
MLP throughput (in TFLOP/s): 250.964
Transformer duration (in seconds): 0.3368
Transformer throughput (in TFLOP/s): 236.753
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 20032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20032x60096, b=2048): 0.0764
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20032x60096, b=2048): 258.224
Elapsed time for attention_key_query_prob (256x2048x313x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x313x2048): 71.331
Elapsed time for attention_prob_times_values (256x2048x2048x313): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x313): 63.175
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20032x20032, b=2048): 0.0253
Throughput (in TFLOP/s) for attention_linear_projection (4x20032x20032, b=2048): 259.749
Elapsed time for mlp_h_to_4h (4x20032x80128, b=2048): 0.1022
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20032x80128, b=2048): 257.415
Elapsed time for mlp_fused_gelu (2048x4x80128): 0.0022
Elapsed time for mlp_4h_to_h (4x80128x20032, b=2048): 0.1029
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80128x20032, b=2048): 255.608
Elapsed time for transformer_add_bias_dropout (2048x4x20032): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20032): 0.0008

Attention duration (in seconds): 0.1567
Attention throughput (in TFLOP/s): 176.367
MLP duration (in seconds): 0.2073
MLP throughput (in TFLOP/s): 253.782
Transformer duration (in seconds): 0.3686
Transformer throughput (in TFLOP/s): 217.708
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1330
Attention throughput (in TFLOP/s): 207.787
MLP duration (in seconds): 0.2106
MLP throughput (in TFLOP/s): 249.788
Transformer duration (in seconds): 0.3478
Transformer throughput (in TFLOP/s): 230.719
Transformer - MLP - Attention (in seconds): 0.0042
========================================================================================================================
num_attention_heads: 64, hidden_size: 20096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20096x60288, b=2048): 0.0769
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20096x60288, b=2048): 258.238
Elapsed time for attention_key_query_prob (256x2048x314x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x314x2048): 111.705
Elapsed time for attention_prob_times_values (256x2048x2048x314): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x314): 108.869
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20096x20096, b=2048): 0.0254
Throughput (in TFLOP/s) for attention_linear_projection (4x20096x20096, b=2048): 260.276
Elapsed time for mlp_h_to_4h (4x20096x80384, b=2048): 0.1024
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20096x80384, b=2048): 258.459
Elapsed time for mlp_fused_gelu (2048x4x80384): 0.0022
Elapsed time for mlp_4h_to_h (4x80384x20096, b=2048): 0.1039
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80384x20096, b=2048): 254.825
Elapsed time for transformer_add_bias_dropout (2048x4x20096): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20096): 0.0008

Attention duration (in seconds): 0.1495
Attention throughput (in TFLOP/s): 186.049
MLP duration (in seconds): 0.2085
MLP throughput (in TFLOP/s): 253.908
Transformer duration (in seconds): 0.3626
Transformer throughput (in TFLOP/s): 222.723
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1251
Attention throughput (in TFLOP/s): 222.406
MLP duration (in seconds): 0.2121
MLP throughput (in TFLOP/s): 249.512
Transformer duration (in seconds): 0.3446
Transformer throughput (in TFLOP/s): 234.335
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 20160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20160x60480, b=2048): 0.0783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20160x60480, b=2048): 255.192
Elapsed time for attention_key_query_prob (256x2048x315x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x315x2048): 71.712
Elapsed time for attention_prob_times_values (256x2048x2048x315): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x315): 63.930
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x20160x20160, b=2048): 0.0255
Throughput (in TFLOP/s) for attention_linear_projection (4x20160x20160, b=2048): 260.776
Elapsed time for mlp_h_to_4h (4x20160x80640, b=2048): 0.1037
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20160x80640, b=2048): 256.858
Elapsed time for mlp_fused_gelu (2048x4x80640): 0.0022
Elapsed time for mlp_4h_to_h (4x80640x20160, b=2048): 0.1047
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80640x20160, b=2048): 254.359
Elapsed time for transformer_add_bias_dropout (2048x4x20160): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20160): 0.0008

Attention duration (in seconds): 0.1588
Attention throughput (in TFLOP/s): 176.247
MLP duration (in seconds): 0.2106
MLP throughput (in TFLOP/s): 252.913
Transformer duration (in seconds): 0.3740
Transformer throughput (in TFLOP/s): 217.244
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1349
Attention throughput (in TFLOP/s): 207.505
MLP duration (in seconds): 0.2136
MLP throughput (in TFLOP/s): 249.409
Transformer duration (in seconds): 0.3541
Transformer throughput (in TFLOP/s): 229.513
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 64, hidden_size: 20224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20224x60672, b=2048): 0.0780
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20224x60672, b=2048): 257.885
Elapsed time for attention_key_query_prob (256x2048x316x2048): 0.0061
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x316x2048): 112.136
Elapsed time for attention_prob_times_values (256x2048x2048x316): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x316): 109.270
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x20224x20224, b=2048): 0.0257
Throughput (in TFLOP/s) for attention_linear_projection (4x20224x20224, b=2048): 260.803
Elapsed time for mlp_h_to_4h (4x20224x80896, b=2048): 0.1040
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20224x80896, b=2048): 257.740
Elapsed time for mlp_fused_gelu (2048x4x80896): 0.0022
Elapsed time for mlp_4h_to_h (4x80896x20224, b=2048): 0.1054
Throughput (in TFLOP/s) for mlp_4h_to_h (4x80896x20224, b=2048): 254.310
Elapsed time for transformer_add_bias_dropout (2048x4x20224): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20224): 0.0008

Attention duration (in seconds): 0.1509
Attention throughput (in TFLOP/s): 186.633
MLP duration (in seconds): 0.2116
MLP throughput (in TFLOP/s): 253.323
Transformer duration (in seconds): 0.3671
Transformer throughput (in TFLOP/s): 222.736
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1262
Attention throughput (in TFLOP/s): 223.146
MLP duration (in seconds): 0.2140
MLP throughput (in TFLOP/s): 250.558
Transformer duration (in seconds): 0.3449
Transformer throughput (in TFLOP/s): 237.059
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 20288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20288x60864, b=2048): 0.0783
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20288x60864, b=2048): 258.411
Elapsed time for attention_key_query_prob (256x2048x317x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x317x2048): 72.147
Elapsed time for attention_prob_times_values (256x2048x2048x317): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x317): 65.120
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x20288x20288, b=2048): 0.0260
Throughput (in TFLOP/s) for attention_linear_projection (4x20288x20288, b=2048): 259.240
Elapsed time for mlp_h_to_4h (4x20288x81152, b=2048): 0.1045
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20288x81152, b=2048): 258.126
Elapsed time for mlp_fused_gelu (2048x4x81152): 0.0022
Elapsed time for mlp_4h_to_h (4x81152x20288, b=2048): 0.1061
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81152x20288, b=2048): 254.221
Elapsed time for transformer_add_bias_dropout (2048x4x20288): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20288): 0.0008

Attention duration (in seconds): 0.1592
Attention throughput (in TFLOP/s): 178.018
MLP duration (in seconds): 0.2128
MLP throughput (in TFLOP/s): 253.472
Transformer duration (in seconds): 0.3767
Transformer throughput (in TFLOP/s): 218.461
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1364
Attention throughput (in TFLOP/s): 207.819
MLP duration (in seconds): 0.2170
MLP throughput (in TFLOP/s): 248.668
Transformer duration (in seconds): 0.3610
Transformer throughput (in TFLOP/s): 227.918
Transformer - MLP - Attention (in seconds): 0.0077
========================================================================================================================

[2023-06-12 13:58:54,051] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-12 13:58:54,943] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.130.134, master_port=6000
[2023-06-12 13:58:54,943] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-12 13:58:58,256] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 20352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20352x61056, b=2048): 0.0801
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20352x61056, b=2048): 254.230
Elapsed time for attention_key_query_prob (256x2048x318x2048): 0.0060
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x318x2048): 113.592
Elapsed time for attention_prob_times_values (256x2048x2048x318): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x318): 109.082
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20352x20352, b=2048): 0.0264
Throughput (in TFLOP/s) for attention_linear_projection (4x20352x20352, b=2048): 256.612
Elapsed time for mlp_h_to_4h (4x20352x81408, b=2048): 0.1065
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20352x81408, b=2048): 254.857
Elapsed time for mlp_fused_gelu (2048x4x81408): 0.0022
Elapsed time for mlp_4h_to_h (4x81408x20352, b=2048): 0.1075
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81408x20352, b=2048): 252.627
Elapsed time for transformer_add_bias_dropout (2048x4x20352): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20352): 0.0008

Attention duration (in seconds): 0.1538
Attention throughput (in TFLOP/s): 185.350
MLP duration (in seconds): 0.2162
MLP throughput (in TFLOP/s): 251.111
Transformer duration (in seconds): 0.3747
Transformer throughput (in TFLOP/s): 221.005
Transformer - MLP - Attention (in seconds): 0.0046


Actual
------
Attention duration (in seconds): 0.1284
Attention throughput (in TFLOP/s): 222.009
MLP duration (in seconds): 0.2194
MLP throughput (in TFLOP/s): 247.407
Transformer duration (in seconds): 0.3547
Transformer throughput (in TFLOP/s): 233.409
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 20416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20416x61248, b=2048): 0.0800
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20416x61248, b=2048): 255.989
Elapsed time for attention_key_query_prob (256x2048x319x2048): 0.0094
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x319x2048): 72.875
Elapsed time for attention_prob_times_values (256x2048x2048x319): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x319): 65.047
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20416x20416, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20416x20416, b=2048): 255.627
Elapsed time for mlp_h_to_4h (4x20416x81664, b=2048): 0.1072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20416x81664, b=2048): 254.701
Elapsed time for mlp_fused_gelu (2048x4x81664): 0.0022
Elapsed time for mlp_4h_to_h (4x81664x20416, b=2048): 0.1084
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81664x20416, b=2048): 252.066
Elapsed time for transformer_add_bias_dropout (2048x4x20416): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20416): 0.0008

Attention duration (in seconds): 0.1617
Attention throughput (in TFLOP/s): 177.405
MLP duration (in seconds): 0.2179
MLP throughput (in TFLOP/s): 250.765
Transformer duration (in seconds): 0.3842
Transformer throughput (in TFLOP/s): 216.841
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1383
Attention throughput (in TFLOP/s): 207.383
MLP duration (in seconds): 0.2202
MLP throughput (in TFLOP/s): 248.148
Transformer duration (in seconds): 0.3641
Transformer throughput (in TFLOP/s): 228.865
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 64, hidden_size: 20480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20480x61440, b=2048): 0.0802
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20480x61440, b=2048): 257.064
Elapsed time for attention_key_query_prob (256x2048x320x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x320x2048): 166.356
Elapsed time for attention_prob_times_values (256x2048x2048x320): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x320): 160.797
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20480x20480, b=2048): 0.0267
Throughput (in TFLOP/s) for attention_linear_projection (4x20480x20480, b=2048): 257.180
Elapsed time for mlp_h_to_4h (4x20480x81920, b=2048): 0.1078
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20480x81920, b=2048): 254.885
Elapsed time for mlp_fused_gelu (2048x4x81920): 0.0023
Elapsed time for mlp_4h_to_h (4x81920x20480, b=2048): 0.1086
Throughput (in TFLOP/s) for mlp_4h_to_h (4x81920x20480, b=2048): 253.142
Elapsed time for transformer_add_bias_dropout (2048x4x20480): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20480): 0.0008

Attention duration (in seconds): 0.1504
Attention throughput (in TFLOP/s): 191.961
MLP duration (in seconds): 0.2187
MLP throughput (in TFLOP/s): 251.374
Transformer duration (in seconds): 0.3737
Transformer throughput (in TFLOP/s): 224.332
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1291
Attention throughput (in TFLOP/s): 223.579
MLP duration (in seconds): 0.2211
MLP throughput (in TFLOP/s): 248.621
Transformer duration (in seconds): 0.3574
Transformer throughput (in TFLOP/s): 234.609
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 20544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20544x61632, b=2048): 0.0817
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20544x61632, b=2048): 253.999
Elapsed time for attention_key_query_prob (256x2048x321x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x321x2048): 71.161
Elapsed time for attention_prob_times_values (256x2048x2048x321): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x321): 64.913
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20544x20544, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_linear_projection (4x20544x20544, b=2048): 256.377
Elapsed time for mlp_h_to_4h (4x20544x82176, b=2048): 0.1080
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20544x82176, b=2048): 256.226
Elapsed time for mlp_fused_gelu (2048x4x82176): 0.0023
Elapsed time for mlp_4h_to_h (4x82176x20544, b=2048): 0.1095
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82176x20544, b=2048): 252.490
Elapsed time for transformer_add_bias_dropout (2048x4x20544): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20544): 0.0008

Attention duration (in seconds): 0.1640
Attention throughput (in TFLOP/s): 177.108
MLP duration (in seconds): 0.2198
MLP throughput (in TFLOP/s): 251.728
Transformer duration (in seconds): 0.3884
Transformer throughput (in TFLOP/s): 217.177
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1403
Attention throughput (in TFLOP/s): 207.004
MLP duration (in seconds): 0.2230
MLP throughput (in TFLOP/s): 248.096
Transformer duration (in seconds): 0.3692
Transformer throughput (in TFLOP/s): 228.502
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 20608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20608x61824, b=2048): 0.0818
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20608x61824, b=2048): 255.299
Elapsed time for attention_key_query_prob (256x2048x322x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x322x2048): 109.522
Elapsed time for attention_prob_times_values (256x2048x2048x322): 0.0062
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x322): 110.901
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20608x20608, b=2048): 0.0270
Throughput (in TFLOP/s) for attention_linear_projection (4x20608x20608, b=2048): 257.868
Elapsed time for mlp_h_to_4h (4x20608x82432, b=2048): 0.1093
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20608x82432, b=2048): 254.649
Elapsed time for mlp_fused_gelu (2048x4x82432): 0.0023
Elapsed time for mlp_4h_to_h (4x82432x20608, b=2048): 0.1753
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82432x20608, b=2048): 158.727
Elapsed time for transformer_add_bias_dropout (2048x4x20608): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20608): 0.0008

Attention duration (in seconds): 0.1563
Attention throughput (in TFLOP/s): 186.902
MLP duration (in seconds): 0.2869
MLP throughput (in TFLOP/s): 194.014
Transformer duration (in seconds): 0.4479
Transformer throughput (in TFLOP/s): 189.504
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1324
Attention throughput (in TFLOP/s): 220.600
MLP duration (in seconds): 0.2832
MLP throughput (in TFLOP/s): 196.534
Transformer duration (in seconds): 0.4185
Transformer throughput (in TFLOP/s): 202.840
Transformer - MLP - Attention (in seconds): 0.0028
========================================================================================================================
num_attention_heads: 64, hidden_size: 20672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20672x62016, b=2048): 0.0819
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20672x62016, b=2048): 256.415
Elapsed time for attention_key_query_prob (256x2048x323x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x323x2048): 70.545
Elapsed time for attention_prob_times_values (256x2048x2048x323): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x323): 65.567
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20672x20672, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x20672x20672, b=2048): 257.692
Elapsed time for mlp_h_to_4h (4x20672x82688, b=2048): 0.1096
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20672x82688, b=2048): 255.628
Elapsed time for mlp_fused_gelu (2048x4x82688): 0.0023
Elapsed time for mlp_4h_to_h (4x82688x20672, b=2048): 0.1776
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82688x20672, b=2048): 157.731
Elapsed time for transformer_add_bias_dropout (2048x4x20672): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20672): 0.0009

Attention duration (in seconds): 0.1645
Attention throughput (in TFLOP/s): 178.691
MLP duration (in seconds): 0.2894
MLP throughput (in TFLOP/s): 193.554
Transformer duration (in seconds): 0.4586
Transformer throughput (in TFLOP/s): 186.214
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1416
Attention throughput (in TFLOP/s): 207.566
MLP duration (in seconds): 0.2895
MLP throughput (in TFLOP/s): 193.475
Transformer duration (in seconds): 0.4348
Transformer throughput (in TFLOP/s): 196.435
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 20736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20736x62208, b=2048): 0.0825
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20736x62208, b=2048): 256.188
Elapsed time for attention_key_query_prob (256x2048x324x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x324x2048): 109.846
Elapsed time for attention_prob_times_values (256x2048x2048x324): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x324): 110.890
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20736x20736, b=2048): 0.0272
Throughput (in TFLOP/s) for attention_linear_projection (4x20736x20736, b=2048): 259.124
Elapsed time for mlp_h_to_4h (4x20736x82944, b=2048): 0.1105
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20736x82944, b=2048): 255.040
Elapsed time for mlp_fused_gelu (2048x4x82944): 0.0023
Elapsed time for mlp_4h_to_h (4x82944x20736, b=2048): 0.1752
Throughput (in TFLOP/s) for mlp_4h_to_h (4x82944x20736, b=2048): 160.878
Elapsed time for transformer_add_bias_dropout (2048x4x20736): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20736): 0.0008

Attention duration (in seconds): 0.1573
Attention throughput (in TFLOP/s): 188.000
MLP duration (in seconds): 0.2879
MLP throughput (in TFLOP/s): 195.735
Transformer duration (in seconds): 0.4500
Transformer throughput (in TFLOP/s): 190.973
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1330
Attention throughput (in TFLOP/s): 222.287
MLP duration (in seconds): 0.2875
MLP throughput (in TFLOP/s): 196.059
Transformer duration (in seconds): 0.4260
Transformer throughput (in TFLOP/s): 201.692
Transformer - MLP - Attention (in seconds): 0.0056
========================================================================================================================
num_attention_heads: 64, hidden_size: 20800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20800x62400, b=2048): 0.0840
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20800x62400, b=2048): 253.177
Elapsed time for attention_key_query_prob (256x2048x325x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x325x2048): 71.224
Elapsed time for attention_prob_times_values (256x2048x2048x325): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x325): 65.639
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20800x20800, b=2048): 0.0278
Throughput (in TFLOP/s) for attention_linear_projection (4x20800x20800, b=2048): 254.830
Elapsed time for mlp_h_to_4h (4x20800x83200, b=2048): 0.1113
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20800x83200, b=2048): 254.698
Elapsed time for mlp_fused_gelu (2048x4x83200): 0.0023
Elapsed time for mlp_4h_to_h (4x83200x20800, b=2048): 0.1788
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83200x20800, b=2048): 158.556
Elapsed time for transformer_add_bias_dropout (2048x4x20800): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20800): 0.0009

Attention duration (in seconds): 0.1673
Attention throughput (in TFLOP/s): 177.840
MLP duration (in seconds): 0.2924
MLP throughput (in TFLOP/s): 193.913
Transformer duration (in seconds): 0.4645
Transformer throughput (in TFLOP/s): 186.139
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1435
Attention throughput (in TFLOP/s): 207.279
MLP duration (in seconds): 0.2903
MLP throughput (in TFLOP/s): 195.310
Transformer duration (in seconds): 0.4469
Transformer throughput (in TFLOP/s): 193.462
Transformer - MLP - Attention (in seconds): 0.0130
========================================================================================================================
num_attention_heads: 64, hidden_size: 20864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20864x62592, b=2048): 0.0839
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20864x62592, b=2048): 254.961
Elapsed time for attention_key_query_prob (256x2048x326x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x326x2048): 110.446
Elapsed time for attention_prob_times_values (256x2048x2048x326): 0.0063
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x326): 110.385
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20864x20864, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x20864x20864, b=2048): 255.847
Elapsed time for mlp_h_to_4h (4x20864x83456, b=2048): 0.1121
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20864x83456, b=2048): 254.510
Elapsed time for mlp_fused_gelu (2048x4x83456): 0.0023
Elapsed time for mlp_4h_to_h (4x83456x20864, b=2048): 0.1778
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83456x20864, b=2048): 160.490
Elapsed time for transformer_add_bias_dropout (2048x4x20864): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20864): 0.0008

Attention duration (in seconds): 0.1595
Attention throughput (in TFLOP/s): 187.634
MLP duration (in seconds): 0.2921
MLP throughput (in TFLOP/s): 195.300
Transformer duration (in seconds): 0.4564
Transformer throughput (in TFLOP/s): 190.593
Transformer - MLP - Attention (in seconds): 0.0047


Actual
------
Attention duration (in seconds): 0.1354
Attention throughput (in TFLOP/s): 221.110
MLP duration (in seconds): 0.2932
MLP throughput (in TFLOP/s): 194.606
Transformer duration (in seconds): 0.4337
Transformer throughput (in TFLOP/s): 200.586
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 20928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20928x62784, b=2048): 0.0847
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20928x62784, b=2048): 254.040
Elapsed time for attention_key_query_prob (256x2048x327x2048): 0.0099
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x327x2048): 71.201
Elapsed time for attention_prob_times_values (256x2048x2048x327): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x327): 64.771
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20928x20928, b=2048): 0.0279
Throughput (in TFLOP/s) for attention_linear_projection (4x20928x20928, b=2048): 257.049
Elapsed time for mlp_h_to_4h (4x20928x83712, b=2048): 0.1129
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20928x83712, b=2048): 254.307
Elapsed time for mlp_fused_gelu (2048x4x83712): 0.0023
Elapsed time for mlp_4h_to_h (4x83712x20928, b=2048): 0.1854
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83712x20928, b=2048): 154.800
Elapsed time for transformer_add_bias_dropout (2048x4x20928): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20928): 0.0009

Attention duration (in seconds): 0.1684
Attention throughput (in TFLOP/s): 178.795
MLP duration (in seconds): 0.3006
MLP throughput (in TFLOP/s): 190.977
Transformer duration (in seconds): 0.4738
Transformer throughput (in TFLOP/s): 184.716
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1453
Attention throughput (in TFLOP/s): 207.152
MLP duration (in seconds): 0.2992
MLP throughput (in TFLOP/s): 191.873
Transformer duration (in seconds): 0.4483
Transformer throughput (in TFLOP/s): 195.215
Transformer - MLP - Attention (in seconds): 0.0038
========================================================================================================================
num_attention_heads: 64, hidden_size: 20992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x20992x62976, b=2048): 0.0851
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x20992x62976, b=2048): 254.392
Elapsed time for attention_key_query_prob (256x2048x328x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x328x2048): 140.549
Elapsed time for attention_prob_times_values (256x2048x2048x328): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x328): 160.268
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x20992x20992, b=2048): 0.0282
Throughput (in TFLOP/s) for attention_linear_projection (4x20992x20992, b=2048): 256.403
Elapsed time for mlp_h_to_4h (4x20992x83968, b=2048): 0.1138
Throughput (in TFLOP/s) for mlp_h_to_4h (4x20992x83968, b=2048): 253.751
Elapsed time for mlp_fused_gelu (2048x4x83968): 0.0023
Elapsed time for mlp_4h_to_h (4x83968x20992, b=2048): 0.1854
Throughput (in TFLOP/s) for mlp_4h_to_h (4x83968x20992, b=2048): 155.764
Elapsed time for transformer_add_bias_dropout (2048x4x20992): 0.0015
Elapsed time for transformer_layer_norm (2048x4x20992): 0.0008

Attention duration (in seconds): 0.1577
Attention throughput (in TFLOP/s): 192.018
MLP duration (in seconds): 0.3015
MLP throughput (in TFLOP/s): 191.556
Transformer duration (in seconds): 0.4640
Transformer throughput (in TFLOP/s): 189.746
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1363
Attention throughput (in TFLOP/s): 222.271
MLP duration (in seconds): 0.2980
MLP throughput (in TFLOP/s): 193.853
Transformer duration (in seconds): 0.4421
Transformer throughput (in TFLOP/s): 199.152
Transformer - MLP - Attention (in seconds): 0.0079
========================================================================================================================
num_attention_heads: 64, hidden_size: 21056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21056x63168, b=2048): 0.0862
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21056x63168, b=2048): 252.841
Elapsed time for attention_key_query_prob (256x2048x329x2048): 0.0098
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x329x2048): 71.889
Elapsed time for attention_prob_times_values (256x2048x2048x329): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x329): 65.550
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21056x21056, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_linear_projection (4x21056x21056, b=2048): 255.370
Elapsed time for mlp_h_to_4h (4x21056x84224, b=2048): 0.1141
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21056x84224, b=2048): 254.598
Elapsed time for mlp_fused_gelu (2048x4x84224): 0.0023
Elapsed time for mlp_4h_to_h (4x84224x21056, b=2048): 0.1867
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84224x21056, b=2048): 155.665
Elapsed time for transformer_add_bias_dropout (2048x4x21056): 0.0015
Elapsed time for transformer_layer_norm (2048x4x21056): 0.0009

Attention duration (in seconds): 0.1703
Attention throughput (in TFLOP/s): 178.950
MLP duration (in seconds): 0.3031
MLP throughput (in TFLOP/s): 191.726
Transformer duration (in seconds): 0.4782
Transformer throughput (in TFLOP/s): 185.236
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1472
Attention throughput (in TFLOP/s): 206.935
MLP duration (in seconds): 0.2995
MLP throughput (in TFLOP/s): 194.032
Transformer duration (in seconds): 0.4569
Transformer throughput (in TFLOP/s): 193.858
Transformer - MLP - Attention (in seconds): 0.0102
========================================================================================================================
num_attention_heads: 64, hidden_size: 21120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21120x63360, b=2048): 0.0862
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21120x63360, b=2048): 254.325
Elapsed time for attention_key_query_prob (256x2048x330x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x330x2048): 111.364
Elapsed time for attention_prob_times_values (256x2048x2048x330): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x330): 110.358
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0306
Elapsed time for attention_linear_projection (4x21120x21120, b=2048): 0.0284
Throughput (in TFLOP/s) for attention_linear_projection (4x21120x21120, b=2048): 257.382
Elapsed time for mlp_h_to_4h (4x21120x84480, b=2048): 0.1146
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21120x84480, b=2048): 255.124
Elapsed time for mlp_fused_gelu (2048x4x84480): 0.0023
Elapsed time for mlp_4h_to_h (4x84480x21120, b=2048): 0.1926
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84480x21120, b=2048): 151.793
Elapsed time for transformer_add_bias_dropout (2048x4x21120): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21120): 0.0009

Attention duration (in seconds): 0.1625
Attention throughput (in TFLOP/s): 188.561
MLP duration (in seconds): 0.3095
MLP throughput (in TFLOP/s): 188.910
Transformer duration (in seconds): 0.4768
Transformer throughput (in TFLOP/s): 186.886
Transformer - MLP - Attention (in seconds): 0.0048


Actual
------
Attention duration (in seconds): 0.1376
Attention throughput (in TFLOP/s): 222.770
MLP duration (in seconds): 0.3066
MLP throughput (in TFLOP/s): 190.691
Transformer duration (in seconds): 0.4512
Transformer throughput (in TFLOP/s): 197.517
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 21184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21184x63552, b=2048): 0.0868
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21184x63552, b=2048): 254.210
Elapsed time for attention_key_query_prob (256x2048x331x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x331x2048): 73.086
Elapsed time for attention_prob_times_values (256x2048x2048x331): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x331): 67.188
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0306
Elapsed time for attention_linear_projection (4x21184x21184, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_linear_projection (4x21184x21184, b=2048): 244.312
Elapsed time for mlp_h_to_4h (4x21184x84736, b=2048): 0.1157
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21184x84736, b=2048): 254.136
Elapsed time for mlp_fused_gelu (2048x4x84736): 0.0023
Elapsed time for mlp_4h_to_h (4x84736x21184, b=2048): 0.1850
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84736x21184, b=2048): 158.945
Elapsed time for transformer_add_bias_dropout (2048x4x21184): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21184): 0.0009

Attention duration (in seconds): 0.1723
Attention throughput (in TFLOP/s): 178.958
MLP duration (in seconds): 0.3031
MLP throughput (in TFLOP/s): 194.067
Transformer duration (in seconds): 0.4802
Transformer throughput (in TFLOP/s): 186.680
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1486
Attention throughput (in TFLOP/s): 207.507
MLP duration (in seconds): 0.2979
MLP throughput (in TFLOP/s): 197.448
Transformer duration (in seconds): 0.4538
Transformer throughput (in TFLOP/s): 197.569
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 21248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21248x63744, b=2048): 0.0870
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21248x63744, b=2048): 255.194
Elapsed time for attention_key_query_prob (256x2048x332x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x332x2048): 112.361
Elapsed time for attention_prob_times_values (256x2048x2048x332): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x332): 111.171
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21248x21248, b=2048): 0.0292
Throughput (in TFLOP/s) for attention_linear_projection (4x21248x21248, b=2048): 253.210
Elapsed time for mlp_h_to_4h (4x21248x84992, b=2048): 0.1162
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21248x84992, b=2048): 254.556
Elapsed time for mlp_fused_gelu (2048x4x84992): 0.0023
Elapsed time for mlp_4h_to_h (4x84992x21248, b=2048): 0.1895
Throughput (in TFLOP/s) for mlp_4h_to_h (4x84992x21248, b=2048): 156.132
Elapsed time for transformer_add_bias_dropout (2048x4x21248): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21248): 0.0009

Attention duration (in seconds): 0.1640
Attention throughput (in TFLOP/s): 189.152
MLP duration (in seconds): 0.3081
MLP throughput (in TFLOP/s): 192.080
Transformer duration (in seconds): 0.4769
Transformer throughput (in TFLOP/s): 189.105
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1393
Attention throughput (in TFLOP/s): 222.572
MLP duration (in seconds): 0.3006
MLP throughput (in TFLOP/s): 196.842
Transformer duration (in seconds): 0.4518
Transformer throughput (in TFLOP/s): 199.620
Transformer - MLP - Attention (in seconds): 0.0118
========================================================================================================================
num_attention_heads: 64, hidden_size: 21312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21312x63936, b=2048): 0.0880
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21312x63936, b=2048): 253.559
Elapsed time for attention_key_query_prob (256x2048x333x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x333x2048): 73.628
Elapsed time for attention_prob_times_values (256x2048x2048x333): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x333): 67.189
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21312x21312, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_linear_projection (4x21312x21312, b=2048): 255.477
Elapsed time for mlp_h_to_4h (4x21312x85248, b=2048): 0.1177
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21312x85248, b=2048): 252.901
Elapsed time for mlp_fused_gelu (2048x4x85248): 0.0023
Elapsed time for mlp_4h_to_h (4x85248x21312, b=2048): 0.1872
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85248x21312, b=2048): 159.021
Elapsed time for transformer_add_bias_dropout (2048x4x21312): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21312): 0.0009

Attention duration (in seconds): 0.1725
Attention throughput (in TFLOP/s): 180.802
MLP duration (in seconds): 0.3072
MLP throughput (in TFLOP/s): 193.773
Transformer duration (in seconds): 0.4847
Transformer throughput (in TFLOP/s): 187.203
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1495
Attention throughput (in TFLOP/s): 208.725
MLP duration (in seconds): 0.3065
MLP throughput (in TFLOP/s): 194.218
Transformer duration (in seconds): 0.4630
Transformer throughput (in TFLOP/s): 195.961
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 21376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21376x64128, b=2048): 0.0883
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21376x64128, b=2048): 254.468
Elapsed time for attention_key_query_prob (256x2048x334x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x334x2048): 112.346
Elapsed time for attention_prob_times_values (256x2048x2048x334): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x334): 111.916
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21376x21376, b=2048): 0.0291
Throughput (in TFLOP/s) for attention_linear_projection (4x21376x21376, b=2048): 257.173
Elapsed time for mlp_h_to_4h (4x21376x85504, b=2048): 0.1176
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21376x85504, b=2048): 254.626
Elapsed time for mlp_fused_gelu (2048x4x85504): 0.0024
Elapsed time for mlp_4h_to_h (4x85504x21376, b=2048): 0.1892
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85504x21376, b=2048): 158.315
Elapsed time for transformer_add_bias_dropout (2048x4x21376): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21376): 0.0009

Attention duration (in seconds): 0.1652
Attention throughput (in TFLOP/s): 189.977
MLP duration (in seconds): 0.3091
MLP throughput (in TFLOP/s): 193.754
Transformer duration (in seconds): 0.4791
Transformer throughput (in TFLOP/s): 190.487
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1408
Attention throughput (in TFLOP/s): 222.810
MLP duration (in seconds): 0.3068
MLP throughput (in TFLOP/s): 195.206
Transformer duration (in seconds): 0.4564
Transformer throughput (in TFLOP/s): 199.966
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
num_attention_heads: 64, hidden_size: 21440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21440x64320, b=2048): 0.0889
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21440x64320, b=2048): 254.126
Elapsed time for attention_key_query_prob (256x2048x335x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x335x2048): 74.656
Elapsed time for attention_prob_times_values (256x2048x2048x335): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x335): 66.804
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0307
Elapsed time for attention_linear_projection (4x21440x21440, b=2048): 0.0294
Throughput (in TFLOP/s) for attention_linear_projection (4x21440x21440, b=2048): 256.016
Elapsed time for mlp_h_to_4h (4x21440x85760, b=2048): 0.1187
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21440x85760, b=2048): 253.741
Elapsed time for mlp_fused_gelu (2048x4x85760): 0.0024
Elapsed time for mlp_4h_to_h (4x85760x21440, b=2048): 0.1917
Throughput (in TFLOP/s) for mlp_4h_to_h (4x85760x21440, b=2048): 157.147
Elapsed time for transformer_add_bias_dropout (2048x4x21440): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21440): 0.0009

Attention duration (in seconds): 0.1740
Attention throughput (in TFLOP/s): 181.420
MLP duration (in seconds): 0.3128
MLP throughput (in TFLOP/s): 192.624
Transformer duration (in seconds): 0.4917
Transformer throughput (in TFLOP/s): 186.740
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1518
Attention throughput (in TFLOP/s): 207.969
MLP duration (in seconds): 0.3085
MLP throughput (in TFLOP/s): 195.322
Transformer duration (in seconds): 0.4673
Transformer throughput (in TFLOP/s): 196.459
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 21504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21504x64512, b=2048): 0.0897
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21504x64512, b=2048): 253.462
Elapsed time for attention_key_query_prob (256x2048x336x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x336x2048): 154.764
Elapsed time for attention_prob_times_values (256x2048x2048x336): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x336): 167.068
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21504x21504, b=2048): 0.0295
Throughput (in TFLOP/s) for attention_linear_projection (4x21504x21504, b=2048): 257.057
Elapsed time for mlp_h_to_4h (4x21504x86016, b=2048): 0.1195
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21504x86016, b=2048): 253.684
Elapsed time for mlp_fused_gelu (2048x4x86016): 0.0024
Elapsed time for mlp_4h_to_h (4x86016x21504, b=2048): 0.1910
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86016x21504, b=2048): 158.695
Elapsed time for transformer_add_bias_dropout (2048x4x21504): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21504): 0.0009

Attention duration (in seconds): 0.1631
Attention throughput (in TFLOP/s): 194.613
MLP duration (in seconds): 0.3128
MLP throughput (in TFLOP/s): 193.774
Transformer duration (in seconds): 0.4808
Transformer throughput (in TFLOP/s): 192.089
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1417
Attention throughput (in TFLOP/s): 224.047
MLP duration (in seconds): 0.3112
MLP throughput (in TFLOP/s): 194.777
Transformer duration (in seconds): 0.4577
Transformer throughput (in TFLOP/s): 201.787
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 21568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21568x64704, b=2048): 0.0899
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21568x64704, b=2048): 254.403
Elapsed time for attention_key_query_prob (256x2048x337x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x337x2048): 75.610
Elapsed time for attention_prob_times_values (256x2048x2048x337): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x337): 67.709
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21568x21568, b=2048): 0.0298
Throughput (in TFLOP/s) for attention_linear_projection (4x21568x21568, b=2048): 256.121
Elapsed time for mlp_h_to_4h (4x21568x86272, b=2048): 0.1197
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21568x86272, b=2048): 254.676
Elapsed time for mlp_fused_gelu (2048x4x86272): 0.0024
Elapsed time for mlp_4h_to_h (4x86272x21568, b=2048): 0.1966
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86272x21568, b=2048): 155.054
Elapsed time for transformer_add_bias_dropout (2048x4x21568): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21568): 0.0009

Attention duration (in seconds): 0.1749
Attention throughput (in TFLOP/s): 182.569
MLP duration (in seconds): 0.3187
MLP throughput (in TFLOP/s): 191.318
Transformer duration (in seconds): 0.4986
Transformer throughput (in TFLOP/s): 186.348
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1531
Attention throughput (in TFLOP/s): 208.540
MLP duration (in seconds): 0.3168
MLP throughput (in TFLOP/s): 192.483
Transformer duration (in seconds): 0.4761
Transformer throughput (in TFLOP/s): 195.152
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 21632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21632x64896, b=2048): 0.0897
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21632x64896, b=2048): 256.323
Elapsed time for attention_key_query_prob (256x2048x338x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x338x2048): 113.542
Elapsed time for attention_prob_times_values (256x2048x2048x338): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x338): 113.766
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21632x21632, b=2048): 0.0297
Throughput (in TFLOP/s) for attention_linear_projection (4x21632x21632, b=2048): 258.169
Elapsed time for mlp_h_to_4h (4x21632x86528, b=2048): 0.1197
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21632x86528, b=2048): 256.156
Elapsed time for mlp_fused_gelu (2048x4x86528): 0.0024
Elapsed time for mlp_4h_to_h (4x86528x21632, b=2048): 0.1951
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86528x21632, b=2048): 157.225
Elapsed time for transformer_add_bias_dropout (2048x4x21632): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21632): 0.0009

Attention duration (in seconds): 0.1672
Attention throughput (in TFLOP/s): 192.059
MLP duration (in seconds): 0.3172
MLP throughput (in TFLOP/s): 193.387
Transformer duration (in seconds): 0.4893
Transformer throughput (in TFLOP/s): 190.986
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1446
Attention throughput (in TFLOP/s): 222.113
MLP duration (in seconds): 0.3132
MLP throughput (in TFLOP/s): 195.808
Transformer duration (in seconds): 0.4639
Transformer throughput (in TFLOP/s): 201.434
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 21696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21696x65088, b=2048): 0.0906
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21696x65088, b=2048): 255.395
Elapsed time for attention_key_query_prob (256x2048x339x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x339x2048): 75.776
Elapsed time for attention_prob_times_values (256x2048x2048x339): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x339): 68.911
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21696x21696, b=2048): 0.0301
Throughput (in TFLOP/s) for attention_linear_projection (4x21696x21696, b=2048): 256.446
Elapsed time for mlp_h_to_4h (4x21696x86784, b=2048): 0.1202
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21696x86784, b=2048): 256.698
Elapsed time for mlp_fused_gelu (2048x4x86784): 0.0024
Elapsed time for mlp_4h_to_h (4x86784x21696, b=2048): 0.1973
Throughput (in TFLOP/s) for mlp_4h_to_h (4x86784x21696, b=2048): 156.389
Elapsed time for transformer_add_bias_dropout (2048x4x21696): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21696): 0.0009

Attention duration (in seconds): 0.1758
Attention throughput (in TFLOP/s): 183.711
MLP duration (in seconds): 0.3198
MLP throughput (in TFLOP/s): 192.915
Transformer duration (in seconds): 0.5006
Transformer throughput (in TFLOP/s): 187.768
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1532
Attention throughput (in TFLOP/s): 210.930
MLP duration (in seconds): 0.3156
MLP throughput (in TFLOP/s): 195.520
Transformer duration (in seconds): 0.4785
Transformer throughput (in TFLOP/s): 196.442
Transformer - MLP - Attention (in seconds): 0.0098
========================================================================================================================
num_attention_heads: 64, hidden_size: 21760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21760x65280, b=2048): 0.0912
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21760x65280, b=2048): 255.141
Elapsed time for attention_key_query_prob (256x2048x340x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x340x2048): 114.619
Elapsed time for attention_prob_times_values (256x2048x2048x340): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x340): 113.942
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21760x21760, b=2048): 0.0303
Throughput (in TFLOP/s) for attention_linear_projection (4x21760x21760, b=2048): 256.165
Elapsed time for mlp_h_to_4h (4x21760x87040, b=2048): 0.1216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21760x87040, b=2048): 255.277
Elapsed time for mlp_fused_gelu (2048x4x87040): 0.0024
Elapsed time for mlp_4h_to_h (4x87040x21760, b=2048): 0.1910
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87040x21760, b=2048): 162.489
Elapsed time for transformer_add_bias_dropout (2048x4x21760): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21760): 0.0009

Attention duration (in seconds): 0.1693
Attention throughput (in TFLOP/s): 191.935
MLP duration (in seconds): 0.3149
MLP throughput (in TFLOP/s): 197.071
Transformer duration (in seconds): 0.4892
Transformer throughput (in TFLOP/s): 193.301
Transformer - MLP - Attention (in seconds): 0.0049


Actual
------
Attention duration (in seconds): 0.1442
Attention throughput (in TFLOP/s): 225.247
MLP duration (in seconds): 0.3145
MLP throughput (in TFLOP/s): 197.305
Transformer duration (in seconds): 0.4625
Transformer throughput (in TFLOP/s): 204.434
Transformer - MLP - Attention (in seconds): 0.0037
========================================================================================================================
num_attention_heads: 64, hidden_size: 21824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21824x65472, b=2048): 0.0918
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21824x65472, b=2048): 255.144
Elapsed time for attention_key_query_prob (256x2048x341x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x341x2048): 76.451
Elapsed time for attention_prob_times_values (256x2048x2048x341): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x341): 69.000
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21824x21824, b=2048): 0.0304
Throughput (in TFLOP/s) for attention_linear_projection (4x21824x21824, b=2048): 256.493
Elapsed time for mlp_h_to_4h (4x21824x87296, b=2048): 0.1225
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21824x87296, b=2048): 254.807
Elapsed time for mlp_fused_gelu (2048x4x87296): 0.0024
Elapsed time for mlp_4h_to_h (4x87296x21824, b=2048): 0.2056
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87296x21824, b=2048): 151.803
Elapsed time for transformer_add_bias_dropout (2048x4x21824): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21824): 0.0009

Attention duration (in seconds): 0.1774
Attention throughput (in TFLOP/s): 184.222
MLP duration (in seconds): 0.3305
MLP throughput (in TFLOP/s): 188.877
Transformer duration (in seconds): 0.5129
Transformer throughput (in TFLOP/s): 185.424
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1554
Attention throughput (in TFLOP/s): 210.238
MLP duration (in seconds): 0.3284
MLP throughput (in TFLOP/s): 190.081
Transformer duration (in seconds): 0.4907
Transformer throughput (in TFLOP/s): 193.832
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 21888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21888x65664, b=2048): 0.0920
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21888x65664, b=2048): 256.011
Elapsed time for attention_key_query_prob (256x2048x342x2048): 0.0063
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x342x2048): 116.314
Elapsed time for attention_prob_times_values (256x2048x2048x342): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x342): 114.637
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21888x21888, b=2048): 0.0304
Throughput (in TFLOP/s) for attention_linear_projection (4x21888x21888, b=2048): 258.038
Elapsed time for mlp_h_to_4h (4x21888x87552, b=2048): 0.1237
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21888x87552, b=2048): 253.755
Elapsed time for mlp_fused_gelu (2048x4x87552): 0.0024
Elapsed time for mlp_4h_to_h (4x87552x21888, b=2048): 0.2019
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87552x21888, b=2048): 155.522
Elapsed time for transformer_add_bias_dropout (2048x4x21888): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21888): 0.0009

Attention duration (in seconds): 0.1701
Attention throughput (in TFLOP/s): 193.172
MLP duration (in seconds): 0.3280
MLP throughput (in TFLOP/s): 191.434
Transformer duration (in seconds): 0.5032
Transformer throughput (in TFLOP/s): 190.118
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1459
Attention throughput (in TFLOP/s): 225.302
MLP duration (in seconds): 0.3255
MLP throughput (in TFLOP/s): 192.934
Transformer duration (in seconds): 0.4774
Transformer throughput (in TFLOP/s): 200.390
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 21952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x21952x65856, b=2048): 0.0931
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x21952x65856, b=2048): 254.400
Elapsed time for attention_key_query_prob (256x2048x343x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x343x2048): 76.790
Elapsed time for attention_prob_times_values (256x2048x2048x343): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x343): 68.127
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x21952x21952, b=2048): 0.0306
Throughput (in TFLOP/s) for attention_linear_projection (4x21952x21952, b=2048): 257.686
Elapsed time for mlp_h_to_4h (4x21952x87808, b=2048): 0.1239
Throughput (in TFLOP/s) for mlp_h_to_4h (4x21952x87808, b=2048): 254.919
Elapsed time for mlp_fused_gelu (2048x4x87808): 0.0024
Elapsed time for mlp_4h_to_h (4x87808x21952, b=2048): 0.1979
Throughput (in TFLOP/s) for mlp_4h_to_h (4x87808x21952, b=2048): 159.603
Elapsed time for transformer_add_bias_dropout (2048x4x21952): 0.0016
Elapsed time for transformer_layer_norm (2048x4x21952): 0.0009

Attention duration (in seconds): 0.1792
Attention throughput (in TFLOP/s): 184.489
MLP duration (in seconds): 0.3242
MLP throughput (in TFLOP/s): 194.842
Transformer duration (in seconds): 0.5084
Transformer throughput (in TFLOP/s): 189.268
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1568
Attention throughput (in TFLOP/s): 210.750
MLP duration (in seconds): 0.3246
MLP throughput (in TFLOP/s): 194.608
Transformer duration (in seconds): 0.4865
Transformer throughput (in TFLOP/s): 197.753
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 22016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22016x66048, b=2048): 0.0938
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22016x66048, b=2048): 253.966
Elapsed time for attention_key_query_prob (256x2048x344x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x344x2048): 179.130
Elapsed time for attention_prob_times_values (256x2048x2048x344): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x344): 167.874
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22016x22016, b=2048): 0.0308
Throughput (in TFLOP/s) for attention_linear_projection (4x22016x22016, b=2048): 257.871
Elapsed time for mlp_h_to_4h (4x22016x88064, b=2048): 0.1243
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22016x88064, b=2048): 255.508
Elapsed time for mlp_fused_gelu (2048x4x88064): 0.0024
Elapsed time for mlp_4h_to_h (4x88064x22016, b=2048): 0.2058
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88064x22016, b=2048): 154.341
Elapsed time for transformer_add_bias_dropout (2048x4x22016): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22016): 0.0009

Attention duration (in seconds): 0.1681
Attention throughput (in TFLOP/s): 197.710
MLP duration (in seconds): 0.3326
MLP throughput (in TFLOP/s): 191.037
Transformer duration (in seconds): 0.5057
Transformer throughput (in TFLOP/s): 191.367
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1462
Attention throughput (in TFLOP/s): 227.433
MLP duration (in seconds): 0.3301
MLP throughput (in TFLOP/s): 192.464
Transformer duration (in seconds): 0.4825
Transformer throughput (in TFLOP/s): 200.583
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 22080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22080x66240, b=2048): 0.0947
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22080x66240, b=2048): 253.152
Elapsed time for attention_key_query_prob (256x2048x345x2048): 0.0096
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x345x2048): 76.845
Elapsed time for attention_prob_times_values (256x2048x2048x345): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x345): 69.011
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22080x22080, b=2048): 0.0313
Throughput (in TFLOP/s) for attention_linear_projection (4x22080x22080, b=2048): 255.236
Elapsed time for mlp_h_to_4h (4x22080x88320, b=2048): 0.1253
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22080x88320, b=2048): 254.995
Elapsed time for mlp_fused_gelu (2048x4x88320): 0.0024
Elapsed time for mlp_4h_to_h (4x88320x22080, b=2048): 0.2038
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88320x22080, b=2048): 156.735
Elapsed time for transformer_add_bias_dropout (2048x4x22080): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22080): 0.0009

Attention duration (in seconds): 0.1813
Attention throughput (in TFLOP/s): 184.367
MLP duration (in seconds): 0.3316
MLP throughput (in TFLOP/s): 192.718
Transformer duration (in seconds): 0.5180
Transformer throughput (in TFLOP/s): 187.917
Transformer - MLP - Attention (in seconds): 0.0050


Actual
------
Attention duration (in seconds): 0.1586
Attention throughput (in TFLOP/s): 210.780
MLP duration (in seconds): 0.3303
MLP throughput (in TFLOP/s): 193.477
Transformer duration (in seconds): 0.4951
Transformer throughput (in TFLOP/s): 196.584
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 22144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22144x66432, b=2048): 0.0950
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22144x66432, b=2048): 253.737
Elapsed time for attention_key_query_prob (256x2048x346x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x346x2048): 116.578
Elapsed time for attention_prob_times_values (256x2048x2048x346): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x346): 115.154
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22144x22144, b=2048): 0.0314
Throughput (in TFLOP/s) for attention_linear_projection (4x22144x22144, b=2048): 255.819
Elapsed time for mlp_h_to_4h (4x22144x88576, b=2048): 0.1260
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22144x88576, b=2048): 255.108
Elapsed time for mlp_fused_gelu (2048x4x88576): 0.0024
Elapsed time for mlp_4h_to_h (4x88576x22144, b=2048): 0.2098
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88576x22144, b=2048): 153.197
Elapsed time for transformer_add_bias_dropout (2048x4x22144): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22144): 0.0009

Attention duration (in seconds): 0.1742
Attention throughput (in TFLOP/s): 192.981
MLP duration (in seconds): 0.3382
MLP throughput (in TFLOP/s): 190.055
Transformer duration (in seconds): 0.5175
Transformer throughput (in TFLOP/s): 189.185
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1498
Attention throughput (in TFLOP/s): 224.482
MLP duration (in seconds): 0.3350
MLP throughput (in TFLOP/s): 191.861
Transformer duration (in seconds): 0.4907
Transformer throughput (in TFLOP/s): 199.501
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 22208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22208x66624, b=2048): 0.0950
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22208x66624, b=2048): 255.275
Elapsed time for attention_key_query_prob (256x2048x347x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x347x2048): 76.513
Elapsed time for attention_prob_times_values (256x2048x2048x347): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x347): 70.474
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22208x22208, b=2048): 0.0315
Throughput (in TFLOP/s) for attention_linear_projection (4x22208x22208, b=2048): 256.479
Elapsed time for mlp_h_to_4h (4x22208x88832, b=2048): 0.1268
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22208x88832, b=2048): 254.889
Elapsed time for mlp_fused_gelu (2048x4x88832): 0.0024
Elapsed time for mlp_4h_to_h (4x88832x22208, b=2048): 0.2045
Throughput (in TFLOP/s) for mlp_4h_to_h (4x88832x22208, b=2048): 158.088
Elapsed time for transformer_add_bias_dropout (2048x4x22208): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22208): 0.0009

Attention duration (in seconds): 0.1818
Attention throughput (in TFLOP/s): 185.994
MLP duration (in seconds): 0.3337
MLP throughput (in TFLOP/s): 193.716
Transformer duration (in seconds): 0.5206
Transformer throughput (in TFLOP/s): 189.128
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1602
Attention throughput (in TFLOP/s): 211.118
MLP duration (in seconds): 0.3291
MLP throughput (in TFLOP/s): 196.414
Transformer duration (in seconds): 0.4978
Transformer throughput (in TFLOP/s): 197.765
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 64, hidden_size: 22272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22272x66816, b=2048): 0.0964
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22272x66816, b=2048): 252.795
Elapsed time for attention_key_query_prob (256x2048x348x2048): 0.0065
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x348x2048): 115.504
Elapsed time for attention_prob_times_values (256x2048x2048x348): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x348): 115.232
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22272x22272, b=2048): 0.0317
Throughput (in TFLOP/s) for attention_linear_projection (4x22272x22272, b=2048): 256.755
Elapsed time for mlp_h_to_4h (4x22272x89088, b=2048): 0.1272
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22272x89088, b=2048): 255.556
Elapsed time for mlp_fused_gelu (2048x4x89088): 0.0025
Elapsed time for mlp_4h_to_h (4x89088x22272, b=2048): 0.2060
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89088x22272, b=2048): 157.787
Elapsed time for transformer_add_bias_dropout (2048x4x22272): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22272): 0.0009

Attention duration (in seconds): 0.1761
Attention throughput (in TFLOP/s): 193.120
MLP duration (in seconds): 0.3357
MLP throughput (in TFLOP/s): 193.684
Transformer duration (in seconds): 0.5168
Transformer throughput (in TFLOP/s): 191.587
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1511
Attention throughput (in TFLOP/s): 224.998
MLP duration (in seconds): 0.3318
MLP throughput (in TFLOP/s): 195.957
Transformer duration (in seconds): 0.4905
Transformer throughput (in TFLOP/s): 201.886
Transformer - MLP - Attention (in seconds): 0.0076
========================================================================================================================
num_attention_heads: 64, hidden_size: 22336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22336x67008, b=2048): 0.0969
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22336x67008, b=2048): 253.100
Elapsed time for attention_key_query_prob (256x2048x349x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x349x2048): 76.879
Elapsed time for attention_prob_times_values (256x2048x2048x349): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x349): 70.635
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22336x22336, b=2048): 0.0319
Throughput (in TFLOP/s) for attention_linear_projection (4x22336x22336, b=2048): 256.459
Elapsed time for mlp_h_to_4h (4x22336x89344, b=2048): 0.1287
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22336x89344, b=2048): 254.018
Elapsed time for mlp_fused_gelu (2048x4x89344): 0.0025
Elapsed time for mlp_4h_to_h (4x89344x22336, b=2048): 0.1355
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89344x22336, b=2048): 241.360
Elapsed time for transformer_add_bias_dropout (2048x4x22336): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22336): 0.0009

Attention duration (in seconds): 0.1841
Attention throughput (in TFLOP/s): 185.702
MLP duration (in seconds): 0.2666
MLP throughput (in TFLOP/s): 245.243
Transformer duration (in seconds): 0.4559
Transformer throughput (in TFLOP/s): 218.442
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1616
Attention throughput (in TFLOP/s): 211.566
MLP duration (in seconds): 0.2712
MLP throughput (in TFLOP/s): 241.141
Transformer duration (in seconds): 0.4420
Transformer throughput (in TFLOP/s): 225.317
Transformer - MLP - Attention (in seconds): 0.0092
========================================================================================================================
num_attention_heads: 64, hidden_size: 22400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22400x67200, b=2048): 0.0975
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22400x67200, b=2048): 252.923
Elapsed time for attention_key_query_prob (256x2048x350x2048): 0.0064
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x350x2048): 116.963
Elapsed time for attention_prob_times_values (256x2048x2048x350): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x350): 116.162
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22400x22400, b=2048): 0.0321
Throughput (in TFLOP/s) for attention_linear_projection (4x22400x22400, b=2048): 256.158
Elapsed time for mlp_h_to_4h (4x22400x89600, b=2048): 0.1293
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22400x89600, b=2048): 254.257
Elapsed time for mlp_fused_gelu (2048x4x89600): 0.0025
Elapsed time for mlp_4h_to_h (4x89600x22400, b=2048): 0.1358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89600x22400, b=2048): 242.107
Elapsed time for transformer_add_bias_dropout (2048x4x22400): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22400): 0.0009

Attention duration (in seconds): 0.1775
Attention throughput (in TFLOP/s): 193.715
MLP duration (in seconds): 0.2676
MLP throughput (in TFLOP/s): 245.747
Transformer duration (in seconds): 0.4502
Transformer throughput (in TFLOP/s): 222.441
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1527
Attention throughput (in TFLOP/s): 225.174
MLP duration (in seconds): 0.2722
MLP throughput (in TFLOP/s): 241.598
Transformer duration (in seconds): 0.4326
Transformer throughput (in TFLOP/s): 231.526
Transformer - MLP - Attention (in seconds): 0.0077
========================================================================================================================
num_attention_heads: 64, hidden_size: 22464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22464x67392, b=2048): 0.0973
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22464x67392, b=2048): 254.937
Elapsed time for attention_key_query_prob (256x2048x351x2048): 0.0097
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x351x2048): 77.549
Elapsed time for attention_prob_times_values (256x2048x2048x351): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x351): 70.499
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22464x22464, b=2048): 0.0320
Throughput (in TFLOP/s) for attention_linear_projection (4x22464x22464, b=2048): 258.003
Elapsed time for mlp_h_to_4h (4x22464x89856, b=2048): 0.1307
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22464x89856, b=2048): 253.051
Elapsed time for mlp_fused_gelu (2048x4x89856): 0.0025
Elapsed time for mlp_4h_to_h (4x89856x22464, b=2048): 0.1366
Throughput (in TFLOP/s) for mlp_4h_to_h (4x89856x22464, b=2048): 242.132
Elapsed time for transformer_add_bias_dropout (2048x4x22464): 0.0016
Elapsed time for transformer_layer_norm (2048x4x22464): 0.0009

Attention duration (in seconds): 0.1848
Attention throughput (in TFLOP/s): 187.142
MLP duration (in seconds): 0.2697
MLP throughput (in TFLOP/s): 245.204
Transformer duration (in seconds): 0.4597
Transformer throughput (in TFLOP/s): 219.121
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1627
Attention throughput (in TFLOP/s): 212.596
MLP duration (in seconds): 0.2726
MLP throughput (in TFLOP/s): 242.682
Transformer duration (in seconds): 0.4405
Transformer throughput (in TFLOP/s): 228.648
Transformer - MLP - Attention (in seconds): 0.0053
========================================================================================================================
num_attention_heads: 64, hidden_size: 22528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22528x67584, b=2048): 0.0978
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22528x67584, b=2048): 255.134
Elapsed time for attention_key_query_prob (256x2048x352x2048): 0.0038
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x352x2048): 196.997
Elapsed time for attention_prob_times_values (256x2048x2048x352): 0.0043
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x352): 174.957
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22528x22528, b=2048): 0.0322
Throughput (in TFLOP/s) for attention_linear_projection (4x22528x22528, b=2048): 258.398
Elapsed time for mlp_h_to_4h (4x22528x90112, b=2048): 0.1306
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22528x90112, b=2048): 254.758
Elapsed time for mlp_fused_gelu (2048x4x90112): 0.0025
Elapsed time for mlp_4h_to_h (4x90112x22528, b=2048): 0.1373
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90112x22528, b=2048): 242.300
Elapsed time for transformer_add_bias_dropout (2048x4x22528): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22528): 0.0009

Attention duration (in seconds): 0.1731
Attention throughput (in TFLOP/s): 200.846
MLP duration (in seconds): 0.2703
MLP throughput (in TFLOP/s): 246.092
Transformer duration (in seconds): 0.4486
Transformer throughput (in TFLOP/s): 225.812
Transformer - MLP - Attention (in seconds): 0.0051


Actual
------
Attention duration (in seconds): 0.1524
Attention throughput (in TFLOP/s): 228.189
MLP duration (in seconds): 0.2767
MLP throughput (in TFLOP/s): 240.390
Transformer duration (in seconds): 0.4358
Transformer throughput (in TFLOP/s): 232.404
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 22592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22592x67776, b=2048): 0.0989
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22592x67776, b=2048): 253.621
Elapsed time for attention_key_query_prob (256x2048x353x2048): 0.0100
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x353x2048): 75.439
Elapsed time for attention_prob_times_values (256x2048x2048x353): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x353): 70.970
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22592x22592, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_linear_projection (4x22592x22592, b=2048): 253.830
Elapsed time for mlp_h_to_4h (4x22592x90368, b=2048): 0.1318
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22592x90368, b=2048): 253.838
Elapsed time for mlp_fused_gelu (2048x4x90368): 0.0025
Elapsed time for mlp_4h_to_h (4x90368x22592, b=2048): 0.1388
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90368x22592, b=2048): 241.015
Elapsed time for transformer_add_bias_dropout (2048x4x22592): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22592): 0.0009

Attention duration (in seconds): 0.1876
Attention throughput (in TFLOP/s): 186.388
MLP duration (in seconds): 0.2730
MLP throughput (in TFLOP/s): 245.010
Transformer duration (in seconds): 0.4658
Transformer throughput (in TFLOP/s): 218.678
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1650
Attention throughput (in TFLOP/s): 211.943
MLP duration (in seconds): 0.2777
MLP throughput (in TFLOP/s): 240.896
Transformer duration (in seconds): 0.4505
Transformer throughput (in TFLOP/s): 226.125
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 64, hidden_size: 22656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22656x67968, b=2048): 0.0996
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22656x67968, b=2048): 253.384
Elapsed time for attention_key_query_prob (256x2048x354x2048): 0.0067
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x354x2048): 112.711
Elapsed time for attention_prob_times_values (256x2048x2048x354): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x354): 117.757
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0307
Elapsed time for attention_linear_projection (4x22656x22656, b=2048): 0.0329
Throughput (in TFLOP/s) for attention_linear_projection (4x22656x22656, b=2048): 255.550
Elapsed time for mlp_h_to_4h (4x22656x90624, b=2048): 0.1318
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22656x90624, b=2048): 255.220
Elapsed time for mlp_fused_gelu (2048x4x90624): 0.0025
Elapsed time for mlp_4h_to_h (4x90624x22656, b=2048): 0.1392
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90624x22656, b=2048): 241.644
Elapsed time for transformer_add_bias_dropout (2048x4x22656): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22656): 0.0009

Attention duration (in seconds): 0.1809
Attention throughput (in TFLOP/s): 194.380
MLP duration (in seconds): 0.2735
MLP throughput (in TFLOP/s): 245.984
Transformer duration (in seconds): 0.4595
Transformer throughput (in TFLOP/s): 222.913
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1570
Attention throughput (in TFLOP/s): 223.893
MLP duration (in seconds): 0.2779
MLP throughput (in TFLOP/s): 242.097
Transformer duration (in seconds): 0.4400
Transformer throughput (in TFLOP/s): 232.792
Transformer - MLP - Attention (in seconds): 0.0051
========================================================================================================================
num_attention_heads: 64, hidden_size: 22720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22720x68160, b=2048): 0.0993
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22720x68160, b=2048): 255.438
Elapsed time for attention_key_query_prob (256x2048x355x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x355x2048): 74.227
Elapsed time for attention_prob_times_values (256x2048x2048x355): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x355): 71.697
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22720x22720, b=2048): 0.0330
Throughput (in TFLOP/s) for attention_linear_projection (4x22720x22720, b=2048): 256.620
Elapsed time for mlp_h_to_4h (4x22720x90880, b=2048): 0.1330
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22720x90880, b=2048): 254.295
Elapsed time for mlp_fused_gelu (2048x4x90880): 0.0025
Elapsed time for mlp_4h_to_h (4x90880x22720, b=2048): 0.1405
Throughput (in TFLOP/s) for mlp_4h_to_h (4x90880x22720, b=2048): 240.829
Elapsed time for transformer_add_bias_dropout (2048x4x22720): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22720): 0.0009

Attention duration (in seconds): 0.1882
Attention throughput (in TFLOP/s): 187.862
MLP duration (in seconds): 0.2760
MLP throughput (in TFLOP/s): 245.141
Transformer duration (in seconds): 0.4694
Transformer throughput (in TFLOP/s): 219.461
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1669
Attention throughput (in TFLOP/s): 211.829
MLP duration (in seconds): 0.2815
MLP throughput (in TFLOP/s): 240.323
Transformer duration (in seconds): 0.4564
Transformer throughput (in TFLOP/s): 225.702
Transformer - MLP - Attention (in seconds): 0.0080
========================================================================================================================
num_attention_heads: 64, hidden_size: 22784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22784x68352, b=2048): 0.1004
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22784x68352, b=2048): 254.197
Elapsed time for attention_key_query_prob (256x2048x356x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x356x2048): 113.054
Elapsed time for attention_prob_times_values (256x2048x2048x356): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x356): 118.316
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22784x22784, b=2048): 0.0331
Throughput (in TFLOP/s) for attention_linear_projection (4x22784x22784, b=2048): 257.300
Elapsed time for mlp_h_to_4h (4x22784x91136, b=2048): 0.1340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22784x91136, b=2048): 253.885
Elapsed time for mlp_fused_gelu (2048x4x91136): 0.0025
Elapsed time for mlp_4h_to_h (4x91136x22784, b=2048): 0.1406
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91136x22784, b=2048): 241.927
Elapsed time for transformer_add_bias_dropout (2048x4x22784): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22784): 0.0009

Attention duration (in seconds): 0.1817
Attention throughput (in TFLOP/s): 195.693
MLP duration (in seconds): 0.2771
MLP throughput (in TFLOP/s): 245.524
Transformer duration (in seconds): 0.4640
Transformer throughput (in TFLOP/s): 223.278
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1577
Attention throughput (in TFLOP/s): 225.439
MLP duration (in seconds): 0.2826
MLP throughput (in TFLOP/s): 240.736
Transformer duration (in seconds): 0.4480
Transformer throughput (in TFLOP/s): 231.252
Transformer - MLP - Attention (in seconds): 0.0076
========================================================================================================================
num_attention_heads: 64, hidden_size: 22848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22848x68544, b=2048): 0.1014
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22848x68544, b=2048): 253.058
Elapsed time for attention_key_query_prob (256x2048x357x2048): 0.0103
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x357x2048): 74.208
Elapsed time for attention_prob_times_values (256x2048x2048x357): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x357): 71.501
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22848x22848, b=2048): 0.0334
Throughput (in TFLOP/s) for attention_linear_projection (4x22848x22848, b=2048): 255.984
Elapsed time for mlp_h_to_4h (4x22848x91392, b=2048): 0.1349
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22848x91392, b=2048): 253.521
Elapsed time for mlp_fused_gelu (2048x4x91392): 0.0025
Elapsed time for mlp_4h_to_h (4x91392x22848, b=2048): 0.1419
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91392x22848, b=2048): 241.079
Elapsed time for transformer_add_bias_dropout (2048x4x22848): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22848): 0.0009

Attention duration (in seconds): 0.1909
Attention throughput (in TFLOP/s): 187.291
MLP duration (in seconds): 0.2794
MLP throughput (in TFLOP/s): 244.920
Transformer duration (in seconds): 0.4754
Transformer throughput (in TFLOP/s): 219.099
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1685
Attention throughput (in TFLOP/s): 212.151
MLP duration (in seconds): 0.2827
MLP throughput (in TFLOP/s): 242.068
Transformer duration (in seconds): 0.4578
Transformer throughput (in TFLOP/s): 227.527
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 22912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22912x68736, b=2048): 0.1010
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22912x68736, b=2048): 255.360
Elapsed time for attention_key_query_prob (256x2048x358x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x358x2048): 113.644
Elapsed time for attention_prob_times_values (256x2048x2048x358): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x358): 118.939
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22912x22912, b=2048): 0.0335
Throughput (in TFLOP/s) for attention_linear_projection (4x22912x22912, b=2048): 256.611
Elapsed time for mlp_h_to_4h (4x22912x91648, b=2048): 0.1360
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22912x91648, b=2048): 253.016
Elapsed time for mlp_fused_gelu (2048x4x91648): 0.0025
Elapsed time for mlp_4h_to_h (4x91648x22912, b=2048): 0.1432
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91648x22912, b=2048): 240.211
Elapsed time for transformer_add_bias_dropout (2048x4x22912): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22912): 0.0009

Attention duration (in seconds): 0.1828
Attention throughput (in TFLOP/s): 196.604
MLP duration (in seconds): 0.2817
MLP throughput (in TFLOP/s): 244.244
Transformer duration (in seconds): 0.4697
Transformer throughput (in TFLOP/s): 222.998
Transformer - MLP - Attention (in seconds): 0.0052


Actual
------
Attention duration (in seconds): 0.1605
Attention throughput (in TFLOP/s): 223.880
MLP duration (in seconds): 0.2868
MLP throughput (in TFLOP/s): 239.884
Transformer duration (in seconds): 0.4542
Transformer throughput (in TFLOP/s): 230.620
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 22976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x22976x68928, b=2048): 0.1027
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x22976x68928, b=2048): 252.704
Elapsed time for attention_key_query_prob (256x2048x359x2048): 0.0104
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x359x2048): 73.846
Elapsed time for attention_prob_times_values (256x2048x2048x359): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x359): 70.519
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x22976x22976, b=2048): 0.0350
Throughput (in TFLOP/s) for attention_linear_projection (4x22976x22976, b=2048): 247.152
Elapsed time for mlp_h_to_4h (4x22976x91904, b=2048): 0.1364
Throughput (in TFLOP/s) for mlp_h_to_4h (4x22976x91904, b=2048): 253.578
Elapsed time for mlp_fused_gelu (2048x4x91904): 0.0025
Elapsed time for mlp_4h_to_h (4x91904x22976, b=2048): 0.1435
Throughput (in TFLOP/s) for mlp_4h_to_h (4x91904x22976, b=2048): 241.036
Elapsed time for transformer_add_bias_dropout (2048x4x22976): 0.0017
Elapsed time for transformer_layer_norm (2048x4x22976): 0.0010

Attention duration (in seconds): 0.1941
Attention throughput (in TFLOP/s): 186.225
MLP duration (in seconds): 0.2825
MLP throughput (in TFLOP/s): 244.928
Transformer duration (in seconds): 0.4819
Transformer throughput (in TFLOP/s): 218.596
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1714
Attention throughput (in TFLOP/s): 210.860
MLP duration (in seconds): 0.2867
MLP throughput (in TFLOP/s): 241.365
Transformer duration (in seconds): 0.4638
Transformer throughput (in TFLOP/s): 227.088
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 23040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23040x69120, b=2048): 0.1023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23040x69120, b=2048): 255.047
Elapsed time for attention_key_query_prob (256x2048x360x2048): 0.0043
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x360x2048): 180.932
Elapsed time for attention_prob_times_values (256x2048x2048x360): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x360): 174.041
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23040x23040, b=2048): 0.0349
Throughput (in TFLOP/s) for attention_linear_projection (4x23040x23040, b=2048): 248.888
Elapsed time for mlp_h_to_4h (4x23040x92160, b=2048): 0.1376
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23040x92160, b=2048): 252.821
Elapsed time for mlp_fused_gelu (2048x4x92160): 0.0025
Elapsed time for mlp_4h_to_h (4x92160x23040, b=2048): 0.1441
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92160x23040, b=2048): 241.360
Elapsed time for transformer_add_bias_dropout (2048x4x23040): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23040): 0.0009

Attention duration (in seconds): 0.1810
Attention throughput (in TFLOP/s): 200.776
MLP duration (in seconds): 0.2843
MLP throughput (in TFLOP/s): 244.752
Transformer duration (in seconds): 0.4705
Transformer throughput (in TFLOP/s): 225.104
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1607
Attention throughput (in TFLOP/s): 226.168
MLP duration (in seconds): 0.2869
MLP throughput (in TFLOP/s): 242.509
Transformer duration (in seconds): 0.4537
Transformer throughput (in TFLOP/s): 233.456
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 23104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23104x69312, b=2048): 0.1032
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23104x69312, b=2048): 254.297
Elapsed time for attention_key_query_prob (256x2048x361x2048): 0.0105
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x361x2048): 73.791
Elapsed time for attention_prob_times_values (256x2048x2048x361): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x361): 71.082
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23104x23104, b=2048): 0.0343
Throughput (in TFLOP/s) for attention_linear_projection (4x23104x23104, b=2048): 255.337
Elapsed time for mlp_h_to_4h (4x23104x92416, b=2048): 0.1379
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23104x92416, b=2048): 253.665
Elapsed time for mlp_fused_gelu (2048x4x92416): 0.0025
Elapsed time for mlp_4h_to_h (4x92416x23104, b=2048): 0.1448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92416x23104, b=2048): 241.631
Elapsed time for transformer_add_bias_dropout (2048x4x23104): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23104): 0.0010

Attention duration (in seconds): 0.1939
Attention throughput (in TFLOP/s): 188.459
MLP duration (in seconds): 0.2852
MLP throughput (in TFLOP/s): 245.293
Transformer duration (in seconds): 0.4844
Transformer throughput (in TFLOP/s): 219.849
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1713
Attention throughput (in TFLOP/s): 213.267
MLP duration (in seconds): 0.2888
MLP throughput (in TFLOP/s): 242.301
Transformer duration (in seconds): 0.4673
Transformer throughput (in TFLOP/s): 227.912
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 23168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23168x69504, b=2048): 0.1030
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23168x69504, b=2048): 256.106
Elapsed time for attention_key_query_prob (256x2048x362x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x362x2048): 114.863
Elapsed time for attention_prob_times_values (256x2048x2048x362): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x362): 120.176
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23168x23168, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23168x23168, b=2048): 247.855
Elapsed time for mlp_h_to_4h (4x23168x92672, b=2048): 0.1387
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23168x92672, b=2048): 253.707
Elapsed time for mlp_fused_gelu (2048x4x92672): 0.0026
Elapsed time for mlp_4h_to_h (4x92672x23168, b=2048): 0.1450
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92672x23168, b=2048): 242.593
Elapsed time for transformer_add_bias_dropout (2048x4x23168): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23168): 0.0009

Attention duration (in seconds): 0.1868
Attention throughput (in TFLOP/s): 196.682
MLP duration (in seconds): 0.2862
MLP throughput (in TFLOP/s): 245.815
Transformer duration (in seconds): 0.4782
Transformer throughput (in TFLOP/s): 223.921
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1639
Attention throughput (in TFLOP/s): 224.110
MLP duration (in seconds): 0.2903
MLP throughput (in TFLOP/s): 242.363
Transformer duration (in seconds): 0.4608
Transformer throughput (in TFLOP/s): 232.388
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 23232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23232x69696, b=2048): 0.1044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23232x69696, b=2048): 254.202
Elapsed time for attention_key_query_prob (256x2048x363x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x363x2048): 73.780
Elapsed time for attention_prob_times_values (256x2048x2048x363): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x363): 72.906
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x23232x23232, b=2048): 0.0344
Throughput (in TFLOP/s) for attention_linear_projection (4x23232x23232, b=2048): 257.395
Elapsed time for mlp_h_to_4h (4x23232x92928, b=2048): 0.1396
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23232x92928, b=2048): 253.402
Elapsed time for mlp_fused_gelu (2048x4x92928): 0.0026
Elapsed time for mlp_4h_to_h (4x92928x23232, b=2048): 0.1462
Throughput (in TFLOP/s) for mlp_4h_to_h (4x92928x23232, b=2048): 241.863
Elapsed time for transformer_add_bias_dropout (2048x4x23232): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23232): 0.0010

Attention duration (in seconds): 0.1950
Attention throughput (in TFLOP/s): 189.436
MLP duration (in seconds): 0.2884
MLP throughput (in TFLOP/s): 245.305
Transformer duration (in seconds): 0.4887
Transformer throughput (in TFLOP/s): 220.344
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1735
Attention throughput (in TFLOP/s): 212.810
MLP duration (in seconds): 0.2939
MLP throughput (in TFLOP/s): 240.666
Transformer duration (in seconds): 0.4765
Transformer throughput (in TFLOP/s): 225.975
Transformer - MLP - Attention (in seconds): 0.0090
========================================================================================================================
num_attention_heads: 64, hidden_size: 23296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23296x69888, b=2048): 0.1054
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23296x69888, b=2048): 253.032
Elapsed time for attention_key_query_prob (256x2048x364x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x364x2048): 114.312
Elapsed time for attention_prob_times_values (256x2048x2048x364): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x364): 120.381
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23296x23296, b=2048): 0.0346
Throughput (in TFLOP/s) for attention_linear_projection (4x23296x23296, b=2048): 256.989
Elapsed time for mlp_h_to_4h (4x23296x93184, b=2048): 0.1398
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23296x93184, b=2048): 254.477
Elapsed time for mlp_fused_gelu (2048x4x93184): 0.0026
Elapsed time for mlp_4h_to_h (4x93184x23296, b=2048): 0.1473
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93184x23296, b=2048): 241.452
Elapsed time for transformer_add_bias_dropout (2048x4x23296): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23296): 0.0009

Attention duration (in seconds): 0.1884
Attention throughput (in TFLOP/s): 197.101
MLP duration (in seconds): 0.2896
MLP throughput (in TFLOP/s): 245.601
Transformer duration (in seconds): 0.4833
Transformer throughput (in TFLOP/s): 223.997
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1638
Attention throughput (in TFLOP/s): 226.648
MLP duration (in seconds): 0.2952
MLP throughput (in TFLOP/s): 240.966
Transformer duration (in seconds): 0.4678
Transformer throughput (in TFLOP/s): 231.425
Transformer - MLP - Attention (in seconds): 0.0088
========================================================================================================================
num_attention_heads: 64, hidden_size: 23360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23360x70080, b=2048): 0.1059
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23360x70080, b=2048): 253.304
Elapsed time for attention_key_query_prob (256x2048x365x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x365x2048): 74.225
Elapsed time for attention_prob_times_values (256x2048x2048x365): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x365): 72.818
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23360x23360, b=2048): 0.0362
Throughput (in TFLOP/s) for attention_linear_projection (4x23360x23360, b=2048): 246.717
Elapsed time for mlp_h_to_4h (4x23360x93440, b=2048): 0.1402
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23360x93440, b=2048): 255.130
Elapsed time for mlp_fused_gelu (2048x4x93440): 0.0026
Elapsed time for mlp_4h_to_h (4x93440x23360, b=2048): 0.1485
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93440x23360, b=2048): 240.796
Elapsed time for transformer_add_bias_dropout (2048x4x23360): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23360): 0.0010

Attention duration (in seconds): 0.1985
Attention throughput (in TFLOP/s): 188.069
MLP duration (in seconds): 0.2913
MLP throughput (in TFLOP/s): 245.570
Transformer duration (in seconds): 0.4951
Transformer throughput (in TFLOP/s): 219.865
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1771
Attention throughput (in TFLOP/s): 210.802
MLP duration (in seconds): 0.2974
MLP throughput (in TFLOP/s): 240.487
Transformer duration (in seconds): 0.4772
Transformer throughput (in TFLOP/s): 228.120
Transformer - MLP - Attention (in seconds): 0.0027
========================================================================================================================
num_attention_heads: 64, hidden_size: 23424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23424x70272, b=2048): 0.1058
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23424x70272, b=2048): 254.980
Elapsed time for attention_key_query_prob (256x2048x366x2048): 0.0068
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x366x2048): 115.081
Elapsed time for attention_prob_times_values (256x2048x2048x366): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x366): 121.757
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23424x23424, b=2048): 0.0361
Throughput (in TFLOP/s) for attention_linear_projection (4x23424x23424, b=2048): 249.086
Elapsed time for mlp_h_to_4h (4x23424x93696, b=2048): 0.1406
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23424x93696, b=2048): 255.803
Elapsed time for mlp_fused_gelu (2048x4x93696): 0.0026
Elapsed time for mlp_4h_to_h (4x93696x23424, b=2048): 0.1492
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93696x23424, b=2048): 241.061
Elapsed time for transformer_add_bias_dropout (2048x4x23424): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23424): 0.0009

Attention duration (in seconds): 0.1901
Attention throughput (in TFLOP/s): 197.397
MLP duration (in seconds): 0.2923
MLP throughput (in TFLOP/s): 246.030
Transformer duration (in seconds): 0.4878
Transformer throughput (in TFLOP/s): 224.388
Transformer - MLP - Attention (in seconds): 0.0053


Actual
------
Attention duration (in seconds): 0.1680
Attention throughput (in TFLOP/s): 223.436
MLP duration (in seconds): 0.2987
MLP throughput (in TFLOP/s): 240.739
Transformer duration (in seconds): 0.4747
Transformer throughput (in TFLOP/s): 230.554
Transformer - MLP - Attention (in seconds): 0.0080
========================================================================================================================
num_attention_heads: 64, hidden_size: 23488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23488x70464, b=2048): 0.1072
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23488x70464, b=2048): 252.981
Elapsed time for attention_key_query_prob (256x2048x367x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x367x2048): 74.506
Elapsed time for attention_prob_times_values (256x2048x2048x367): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x367): 72.594
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23488x23488, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23488x23488, b=2048): 254.923
Elapsed time for mlp_h_to_4h (4x23488x93952, b=2048): 0.1429
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23488x93952, b=2048): 252.990
Elapsed time for mlp_fused_gelu (2048x4x93952): 0.0026
Elapsed time for mlp_4h_to_h (4x93952x23488, b=2048): 0.1502
Throughput (in TFLOP/s) for mlp_4h_to_h (4x93952x23488, b=2048): 240.791
Elapsed time for transformer_add_bias_dropout (2048x4x23488): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23488): 0.0010

Attention duration (in seconds): 0.1991
Attention throughput (in TFLOP/s): 189.538
MLP duration (in seconds): 0.2956
MLP throughput (in TFLOP/s): 244.586
Transformer duration (in seconds): 0.5001
Transformer throughput (in TFLOP/s): 220.045
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1767
Attention throughput (in TFLOP/s): 213.489
MLP duration (in seconds): 0.3007
MLP throughput (in TFLOP/s): 240.440
Transformer duration (in seconds): 0.4880
Transformer throughput (in TFLOP/s): 225.499
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 64, hidden_size: 23552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23552x70656, b=2048): 0.1083
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23552x70656, b=2048): 251.641
Elapsed time for attention_key_query_prob (256x2048x368x2048): 0.0042
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x368x2048): 189.612
Elapsed time for attention_prob_times_values (256x2048x2048x368): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x368): 180.425
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x23552x23552, b=2048): 0.0354
Throughput (in TFLOP/s) for attention_linear_projection (4x23552x23552, b=2048): 256.455
Elapsed time for mlp_h_to_4h (4x23552x94208, b=2048): 0.1432
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23552x94208, b=2048): 253.878
Elapsed time for mlp_fused_gelu (2048x4x94208): 0.0026
Elapsed time for mlp_4h_to_h (4x94208x23552, b=2048): 0.1498
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94208x23552, b=2048): 242.612
Elapsed time for transformer_add_bias_dropout (2048x4x23552): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23552): 0.0009

Attention duration (in seconds): 0.1873
Attention throughput (in TFLOP/s): 202.510
MLP duration (in seconds): 0.2956
MLP throughput (in TFLOP/s): 245.946
Transformer duration (in seconds): 0.4883
Transformer throughput (in TFLOP/s): 226.587
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1652
Attention throughput (in TFLOP/s): 229.555
MLP duration (in seconds): 0.2998
MLP throughput (in TFLOP/s): 242.476
Transformer duration (in seconds): 0.4723
Transformer throughput (in TFLOP/s): 234.258
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 23616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23616x70848, b=2048): 0.1077
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23616x70848, b=2048): 254.487
Elapsed time for attention_key_query_prob (256x2048x369x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x369x2048): 74.911
Elapsed time for attention_prob_times_values (256x2048x2048x369): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x369): 73.496
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23616x23616, b=2048): 0.0355
Throughput (in TFLOP/s) for attention_linear_projection (4x23616x23616, b=2048): 257.307
Elapsed time for mlp_h_to_4h (4x23616x94464, b=2048): 0.1444
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23616x94464, b=2048): 253.047
Elapsed time for mlp_fused_gelu (2048x4x94464): 0.0026
Elapsed time for mlp_4h_to_h (4x94464x23616, b=2048): 0.1516
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94464x23616, b=2048): 241.141
Elapsed time for transformer_add_bias_dropout (2048x4x23616): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23616): 0.0010

Attention duration (in seconds): 0.1996
Attention throughput (in TFLOP/s): 191.072
MLP duration (in seconds): 0.2986
MLP throughput (in TFLOP/s): 244.803
Transformer duration (in seconds): 0.5036
Transformer throughput (in TFLOP/s): 220.883
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1783
Attention throughput (in TFLOP/s): 213.856
MLP duration (in seconds): 0.3041
MLP throughput (in TFLOP/s): 240.391
Transformer duration (in seconds): 0.4909
Transformer throughput (in TFLOP/s): 226.589
Transformer - MLP - Attention (in seconds): 0.0085
========================================================================================================================
num_attention_heads: 64, hidden_size: 23680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23680x71040, b=2048): 0.1093
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23680x71040, b=2048): 252.202
Elapsed time for attention_key_query_prob (256x2048x370x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x370x2048): 114.072
Elapsed time for attention_prob_times_values (256x2048x2048x370): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x370): 123.180
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23680x23680, b=2048): 0.0358
Throughput (in TFLOP/s) for attention_linear_projection (4x23680x23680, b=2048): 256.708
Elapsed time for mlp_h_to_4h (4x23680x94720, b=2048): 0.1460
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23680x94720, b=2048): 251.788
Elapsed time for mlp_fused_gelu (2048x4x94720): 0.0026
Elapsed time for mlp_4h_to_h (4x94720x23680, b=2048): 0.1521
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94720x23680, b=2048): 241.616
Elapsed time for transformer_add_bias_dropout (2048x4x23680): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23680): 0.0009

Attention duration (in seconds): 0.1935
Attention throughput (in TFLOP/s): 198.153
MLP duration (in seconds): 0.3007
MLP throughput (in TFLOP/s): 244.461
Transformer duration (in seconds): 0.4995
Transformer throughput (in TFLOP/s): 223.897
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1680
Attention throughput (in TFLOP/s): 228.263
MLP duration (in seconds): 0.3051
MLP throughput (in TFLOP/s): 240.871
Transformer duration (in seconds): 0.4823
Transformer throughput (in TFLOP/s): 231.864
Transformer - MLP - Attention (in seconds): 0.0092
========================================================================================================================
num_attention_heads: 64, hidden_size: 23744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23744x71232, b=2048): 0.1095
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23744x71232, b=2048): 253.074
Elapsed time for attention_key_query_prob (256x2048x371x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x371x2048): 74.872
Elapsed time for attention_prob_times_values (256x2048x2048x371): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x371): 74.850
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23744x23744, b=2048): 0.0373
Throughput (in TFLOP/s) for attention_linear_projection (4x23744x23744, b=2048): 247.541
Elapsed time for mlp_h_to_4h (4x23744x94976, b=2048): 0.1465
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23744x94976, b=2048): 252.199
Elapsed time for mlp_fused_gelu (2048x4x94976): 0.0026
Elapsed time for mlp_4h_to_h (4x94976x23744, b=2048): 0.1538
Throughput (in TFLOP/s) for mlp_4h_to_h (4x94976x23744, b=2048): 240.291
Elapsed time for transformer_add_bias_dropout (2048x4x23744): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23744): 0.0010

Attention duration (in seconds): 0.2031
Attention throughput (in TFLOP/s): 189.758
MLP duration (in seconds): 0.3029
MLP throughput (in TFLOP/s): 243.979
Transformer duration (in seconds): 0.5114
Transformer throughput (in TFLOP/s): 219.855
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1827
Attention throughput (in TFLOP/s): 210.913
MLP duration (in seconds): 0.3083
MLP throughput (in TFLOP/s): 239.706
Transformer duration (in seconds): 0.4996
Transformer throughput (in TFLOP/s): 225.054
Transformer - MLP - Attention (in seconds): 0.0086
========================================================================================================================
num_attention_heads: 64, hidden_size: 23808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23808x71424, b=2048): 0.1099
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23808x71424, b=2048): 253.539
Elapsed time for attention_key_query_prob (256x2048x372x2048): 0.0069
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x372x2048): 115.256
Elapsed time for attention_prob_times_values (256x2048x2048x372): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x372): 123.905
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23808x23808, b=2048): 0.0375
Throughput (in TFLOP/s) for attention_linear_projection (4x23808x23808, b=2048): 247.645
Elapsed time for mlp_h_to_4h (4x23808x95232, b=2048): 0.1469
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23808x95232, b=2048): 252.929
Elapsed time for mlp_fused_gelu (2048x4x95232): 0.0026
Elapsed time for mlp_4h_to_h (4x95232x23808, b=2048): 0.1541
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95232x23808, b=2048): 240.999
Elapsed time for transformer_add_bias_dropout (2048x4x23808): 0.0017
Elapsed time for transformer_layer_norm (2048x4x23808): 0.0010

Attention duration (in seconds): 0.1958
Attention throughput (in TFLOP/s): 197.904
MLP duration (in seconds): 0.3036
MLP throughput (in TFLOP/s): 244.694
Transformer duration (in seconds): 0.5048
Transformer throughput (in TFLOP/s): 223.930
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1704
Attention throughput (in TFLOP/s): 227.313
MLP duration (in seconds): 0.3080
MLP throughput (in TFLOP/s): 241.210
Transformer duration (in seconds): 0.4876
Transformer throughput (in TFLOP/s): 231.815
Transformer - MLP - Attention (in seconds): 0.0092
========================================================================================================================
num_attention_heads: 64, hidden_size: 23872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23872x71616, b=2048): 0.1106
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23872x71616, b=2048): 253.211
Elapsed time for attention_key_query_prob (256x2048x373x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x373x2048): 75.545
Elapsed time for attention_prob_times_values (256x2048x2048x373): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x373): 74.994
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x23872x23872, b=2048): 0.0366
Throughput (in TFLOP/s) for attention_linear_projection (4x23872x23872, b=2048): 255.124
Elapsed time for mlp_h_to_4h (4x23872x95488, b=2048): 0.1464
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23872x95488, b=2048): 255.093
Elapsed time for mlp_fused_gelu (2048x4x95488): 0.0026
Elapsed time for mlp_4h_to_h (4x95488x23872, b=2048): 0.1547
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95488x23872, b=2048): 241.421
Elapsed time for transformer_add_bias_dropout (2048x4x23872): 0.0018
Elapsed time for transformer_layer_norm (2048x4x23872): 0.0010

Attention duration (in seconds): 0.2035
Attention throughput (in TFLOP/s): 191.400
MLP duration (in seconds): 0.3037
MLP throughput (in TFLOP/s): 245.927
Transformer duration (in seconds): 0.5127
Transformer throughput (in TFLOP/s): 221.665
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1816
Attention throughput (in TFLOP/s): 214.528
MLP duration (in seconds): 0.3098
MLP throughput (in TFLOP/s): 241.103
Transformer duration (in seconds): 0.5010
Transformer throughput (in TFLOP/s): 226.835
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 64, hidden_size: 23936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x23936x71808, b=2048): 0.1111
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x23936x71808, b=2048): 253.407
Elapsed time for attention_key_query_prob (256x2048x374x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x374x2048): 115.329
Elapsed time for attention_prob_times_values (256x2048x2048x374): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x374): 124.281
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x23936x23936, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_linear_projection (4x23936x23936, b=2048): 255.230
Elapsed time for mlp_h_to_4h (4x23936x95744, b=2048): 0.1487
Throughput (in TFLOP/s) for mlp_h_to_4h (4x23936x95744, b=2048): 252.494
Elapsed time for mlp_fused_gelu (2048x4x95744): 0.0026
Elapsed time for mlp_4h_to_h (4x95744x23936, b=2048): 0.1550
Throughput (in TFLOP/s) for mlp_4h_to_h (4x95744x23936, b=2048): 242.244
Elapsed time for transformer_add_bias_dropout (2048x4x23936): 0.0018
Elapsed time for transformer_layer_norm (2048x4x23936): 0.0010

Attention duration (in seconds): 0.1963
Attention throughput (in TFLOP/s): 199.470
MLP duration (in seconds): 0.3063
MLP throughput (in TFLOP/s): 245.139
Transformer duration (in seconds): 0.5081
Transformer throughput (in TFLOP/s): 224.875
Transformer - MLP - Attention (in seconds): 0.0054


Actual
------
Attention duration (in seconds): 0.1702
Attention throughput (in TFLOP/s): 230.035
MLP duration (in seconds): 0.3099
MLP throughput (in TFLOP/s): 242.348
Transformer duration (in seconds): 0.4885
Transformer throughput (in TFLOP/s): 233.891
Transformer - MLP - Attention (in seconds): 0.0084
========================================================================================================================
num_attention_heads: 64, hidden_size: 24000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24000x72000, b=2048): 0.1116
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24000x72000, b=2048): 253.760
Elapsed time for attention_key_query_prob (256x2048x375x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x375x2048): 75.242
Elapsed time for attention_prob_times_values (256x2048x2048x375): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x375): 74.275
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24000x24000, b=2048): 0.0368
Throughput (in TFLOP/s) for attention_linear_projection (4x24000x24000, b=2048): 256.322
Elapsed time for mlp_h_to_4h (4x24000x96000, b=2048): 0.1483
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24000x96000, b=2048): 254.573
Elapsed time for mlp_fused_gelu (2048x4x96000): 0.0026
Elapsed time for mlp_4h_to_h (4x96000x24000, b=2048): 0.1562
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96000x24000, b=2048): 241.651
Elapsed time for transformer_add_bias_dropout (2048x4x24000): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24000): 0.0010

Attention duration (in seconds): 0.2049
Attention throughput (in TFLOP/s): 192.054
MLP duration (in seconds): 0.3071
MLP throughput (in TFLOP/s): 245.814
Transformer duration (in seconds): 0.5176
Transformer throughput (in TFLOP/s): 221.920
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1825
Attention throughput (in TFLOP/s): 215.612
MLP duration (in seconds): 0.3121
MLP throughput (in TFLOP/s): 241.907
Transformer duration (in seconds): 0.5058
Transformer throughput (in TFLOP/s): 227.093
Transformer - MLP - Attention (in seconds): 0.0111
========================================================================================================================
num_attention_heads: 64, hidden_size: 24064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24064x72192, b=2048): 0.1125
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24064x72192, b=2048): 252.996
Elapsed time for attention_key_query_prob (256x2048x376x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x376x2048): 182.445
Elapsed time for attention_prob_times_values (256x2048x2048x376): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x376): 180.523
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24064x24064, b=2048): 0.0370
Throughput (in TFLOP/s) for attention_linear_projection (4x24064x24064, b=2048): 256.365
Elapsed time for mlp_h_to_4h (4x24064x96256, b=2048): 0.1503
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24064x96256, b=2048): 252.536
Elapsed time for mlp_fused_gelu (2048x4x96256): 0.0026
Elapsed time for mlp_4h_to_h (4x96256x24064, b=2048): 0.1572
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96256x24064, b=2048): 241.367
Elapsed time for transformer_add_bias_dropout (2048x4x24064): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24064): 0.0010

Attention duration (in seconds): 0.1934
Attention throughput (in TFLOP/s): 204.588
MLP duration (in seconds): 0.3102
MLP throughput (in TFLOP/s): 244.722
Transformer duration (in seconds): 0.5090
Transformer throughput (in TFLOP/s): 226.848
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1724
Attention throughput (in TFLOP/s): 229.459
MLP duration (in seconds): 0.3168
MLP throughput (in TFLOP/s): 239.592
Transformer duration (in seconds): 0.4979
Transformer throughput (in TFLOP/s): 231.888
Transformer - MLP - Attention (in seconds): 0.0087
========================================================================================================================
num_attention_heads: 64, hidden_size: 24128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24128x72384, b=2048): 0.1136
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24128x72384, b=2048): 251.823
Elapsed time for attention_key_query_prob (256x2048x377x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x377x2048): 75.591
Elapsed time for attention_prob_times_values (256x2048x2048x377): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x377): 75.238
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24128x24128, b=2048): 0.0374
Throughput (in TFLOP/s) for attention_linear_projection (4x24128x24128, b=2048): 255.139
Elapsed time for mlp_h_to_4h (4x24128x96512, b=2048): 0.1506
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24128x96512, b=2048): 253.355
Elapsed time for mlp_fused_gelu (2048x4x96512): 0.0026
Elapsed time for mlp_4h_to_h (4x96512x24128, b=2048): 0.1586
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96512x24128, b=2048): 240.540
Elapsed time for transformer_add_bias_dropout (2048x4x24128): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24128): 0.0010

Attention duration (in seconds): 0.2075
Attention throughput (in TFLOP/s): 191.691
MLP duration (in seconds): 0.3118
MLP throughput (in TFLOP/s): 244.686
Transformer duration (in seconds): 0.5248
Transformer throughput (in TFLOP/s): 221.163
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1850
Attention throughput (in TFLOP/s): 214.944
MLP duration (in seconds): 0.3185
MLP throughput (in TFLOP/s): 239.551
Transformer duration (in seconds): 0.5123
Transformer throughput (in TFLOP/s): 226.590
Transformer - MLP - Attention (in seconds): 0.0087
========================================================================================================================
num_attention_heads: 64, hidden_size: 24192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24192x72576, b=2048): 0.1132
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24192x72576, b=2048): 254.159
Elapsed time for attention_key_query_prob (256x2048x378x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x378x2048): 116.012
Elapsed time for attention_prob_times_values (256x2048x2048x378): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x378): 125.804
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24192x24192, b=2048): 0.0372
Throughput (in TFLOP/s) for attention_linear_projection (4x24192x24192, b=2048): 257.676
Elapsed time for mlp_h_to_4h (4x24192x96768, b=2048): 0.1508
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24192x96768, b=2048): 254.414
Elapsed time for mlp_fused_gelu (2048x4x96768): 0.0027
Elapsed time for mlp_4h_to_h (4x96768x24192, b=2048): 0.1582
Throughput (in TFLOP/s) for mlp_4h_to_h (4x96768x24192, b=2048): 242.433
Elapsed time for transformer_add_bias_dropout (2048x4x24192): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24192): 0.0010

Attention duration (in seconds): 0.1988
Attention throughput (in TFLOP/s): 201.057
MLP duration (in seconds): 0.3116
MLP throughput (in TFLOP/s): 246.163
Transformer duration (in seconds): 0.5160
Transformer throughput (in TFLOP/s): 226.159
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1726
Attention throughput (in TFLOP/s): 231.603
MLP duration (in seconds): 0.3170
MLP throughput (in TFLOP/s): 241.952
Transformer duration (in seconds): 0.5001
Transformer throughput (in TFLOP/s): 233.316
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================
num_attention_heads: 64, hidden_size: 24256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24256x72768, b=2048): 0.1147
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24256x72768, b=2048): 252.036
Elapsed time for attention_key_query_prob (256x2048x379x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x379x2048): 76.221
Elapsed time for attention_prob_times_values (256x2048x2048x379): 0.0105
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x379): 77.183
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24256x24256, b=2048): 0.0392
Throughput (in TFLOP/s) for attention_linear_projection (4x24256x24256, b=2048): 246.103
Elapsed time for mlp_h_to_4h (4x24256x97024, b=2048): 0.1521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24256x97024, b=2048): 253.579
Elapsed time for mlp_fused_gelu (2048x4x97024): 0.0027
Elapsed time for mlp_4h_to_h (4x97024x24256, b=2048): 0.1597
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97024x24256, b=2048): 241.467
Elapsed time for transformer_add_bias_dropout (2048x4x24256): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24256): 0.0010

Attention duration (in seconds): 0.2101
Attention throughput (in TFLOP/s): 191.255
MLP duration (in seconds): 0.3144
MLP throughput (in TFLOP/s): 245.277
Transformer duration (in seconds): 0.5301
Transformer throughput (in TFLOP/s): 221.293
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1884
Attention throughput (in TFLOP/s): 213.277
MLP duration (in seconds): 0.3210
MLP throughput (in TFLOP/s): 240.270
Transformer duration (in seconds): 0.5185
Transformer throughput (in TFLOP/s): 226.237
Transformer - MLP - Attention (in seconds): 0.0091
========================================================================================================================
num_attention_heads: 64, hidden_size: 24320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24320x72960, b=2048): 0.1146
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24320x72960, b=2048): 253.626
Elapsed time for attention_key_query_prob (256x2048x380x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x380x2048): 116.830
Elapsed time for attention_prob_times_values (256x2048x2048x380): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x380): 126.822
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24320x24320, b=2048): 0.0393
Throughput (in TFLOP/s) for attention_linear_projection (4x24320x24320, b=2048): 246.686
Elapsed time for mlp_h_to_4h (4x24320x97280, b=2048): 0.1521
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24320x97280, b=2048): 254.870
Elapsed time for mlp_fused_gelu (2048x4x97280): 0.0027
Elapsed time for mlp_4h_to_h (4x97280x24320, b=2048): 0.1604
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97280x24320, b=2048): 241.601
Elapsed time for transformer_add_bias_dropout (2048x4x24320): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24320): 0.0010

Attention duration (in seconds): 0.2023
Attention throughput (in TFLOP/s): 199.647
MLP duration (in seconds): 0.3152
MLP throughput (in TFLOP/s): 245.955
Transformer duration (in seconds): 0.5230
Transformer throughput (in TFLOP/s): 225.445
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1771
Attention throughput (in TFLOP/s): 228.073
MLP duration (in seconds): 0.3225
MLP throughput (in TFLOP/s): 240.378
Transformer duration (in seconds): 0.5083
Transformer throughput (in TFLOP/s): 231.996
Transformer - MLP - Attention (in seconds): 0.0087
========================================================================================================================
num_attention_heads: 64, hidden_size: 24384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24384x73152, b=2048): 0.1159
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24384x73152, b=2048): 252.221
Elapsed time for attention_key_query_prob (256x2048x381x2048): 0.0107
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x381x2048): 76.808
Elapsed time for attention_prob_times_values (256x2048x2048x381): 0.0106
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x381): 77.328
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24384x24384, b=2048): 0.0382
Throughput (in TFLOP/s) for attention_linear_projection (4x24384x24384, b=2048): 254.834
Elapsed time for mlp_h_to_4h (4x24384x97536, b=2048): 0.1537
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24384x97536, b=2048): 253.532
Elapsed time for mlp_fused_gelu (2048x4x97536): 0.0027
Elapsed time for mlp_4h_to_h (4x97536x24384, b=2048): 0.1619
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97536x24384, b=2048): 240.675
Elapsed time for transformer_add_bias_dropout (2048x4x24384): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24384): 0.0010

Attention duration (in seconds): 0.2103
Attention throughput (in TFLOP/s): 193.054
MLP duration (in seconds): 0.3183
MLP throughput (in TFLOP/s): 244.857
Transformer duration (in seconds): 0.5342
Transformer throughput (in TFLOP/s): 221.905
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1882
Attention throughput (in TFLOP/s): 215.748
MLP duration (in seconds): 0.3249
MLP throughput (in TFLOP/s): 239.862
Transformer duration (in seconds): 0.5209
Transformer throughput (in TFLOP/s): 227.551
Transformer - MLP - Attention (in seconds): 0.0078
========================================================================================================================
num_attention_heads: 64, hidden_size: 24448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24448x73344, b=2048): 0.1149
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24448x73344, b=2048): 255.636
Elapsed time for attention_key_query_prob (256x2048x382x2048): 0.0070
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x382x2048): 117.415
Elapsed time for attention_prob_times_values (256x2048x2048x382): 0.0064
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x382): 128.242
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24448x24448, b=2048): 0.0381
Throughput (in TFLOP/s) for attention_linear_projection (4x24448x24448, b=2048): 257.088
Elapsed time for mlp_h_to_4h (4x24448x97792, b=2048): 0.1536
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24448x97792, b=2048): 254.972
Elapsed time for mlp_fused_gelu (2048x4x97792): 0.0027
Elapsed time for mlp_4h_to_h (4x97792x24448, b=2048): 0.1617
Throughput (in TFLOP/s) for mlp_4h_to_h (4x97792x24448, b=2048): 242.274
Elapsed time for transformer_add_bias_dropout (2048x4x24448): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24448): 0.0010

Attention duration (in seconds): 0.2014
Attention throughput (in TFLOP/s): 202.650
MLP duration (in seconds): 0.3180
MLP throughput (in TFLOP/s): 246.361
Transformer duration (in seconds): 0.5249
Transformer throughput (in TFLOP/s): 226.988
Transformer - MLP - Attention (in seconds): 0.0055


Actual
------
Attention duration (in seconds): 0.1760
Attention throughput (in TFLOP/s): 231.861
MLP duration (in seconds): 0.3236
MLP throughput (in TFLOP/s): 242.124
Transformer duration (in seconds): 0.5099
Transformer throughput (in TFLOP/s): 233.683
Transformer - MLP - Attention (in seconds): 0.0103
========================================================================================================================
num_attention_heads: 64, hidden_size: 24512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24512x73536, b=2048): 0.1168
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24512x73536, b=2048): 252.757
Elapsed time for attention_key_query_prob (256x2048x383x2048): 0.0106
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x383x2048): 77.749
Elapsed time for attention_prob_times_values (256x2048x2048x383): 0.0107
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x383): 77.218
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24512x24512, b=2048): 0.0383
Throughput (in TFLOP/s) for attention_linear_projection (4x24512x24512, b=2048): 257.257
Elapsed time for mlp_h_to_4h (4x24512x98048, b=2048): 0.1543
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24512x98048, b=2048): 255.192
Elapsed time for mlp_fused_gelu (2048x4x98048): 0.0027
Elapsed time for mlp_4h_to_h (4x98048x24512, b=2048): 0.1631
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98048x24512, b=2048): 241.473
Elapsed time for transformer_add_bias_dropout (2048x4x24512): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24512): 0.0010

Attention duration (in seconds): 0.2114
Attention throughput (in TFLOP/s): 194.087
MLP duration (in seconds): 0.3201
MLP throughput (in TFLOP/s): 246.055
Transformer duration (in seconds): 0.5370
Transformer throughput (in TFLOP/s): 223.034
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1900
Attention throughput (in TFLOP/s): 215.943
MLP duration (in seconds): 0.3284
MLP throughput (in TFLOP/s): 239.789
Transformer duration (in seconds): 0.5232
Transformer throughput (in TFLOP/s): 228.934
Transformer - MLP - Attention (in seconds): 0.0048
========================================================================================================================
num_attention_heads: 64, hidden_size: 24576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24576x73728, b=2048): 0.1163
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24576x73728, b=2048): 255.237
Elapsed time for attention_key_query_prob (256x2048x384x2048): 0.0041
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x384x2048): 198.729
Elapsed time for attention_prob_times_values (256x2048x2048x384): 0.0044
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x384): 188.419
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24576x24576, b=2048): 0.0384
Throughput (in TFLOP/s) for attention_linear_projection (4x24576x24576, b=2048): 257.386
Elapsed time for mlp_h_to_4h (4x24576x98304, b=2048): 0.1561
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24576x98304, b=2048): 253.593
Elapsed time for mlp_fused_gelu (2048x4x98304): 0.0027
Elapsed time for mlp_4h_to_h (4x98304x24576, b=2048): 0.1642
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98304x24576, b=2048): 241.131
Elapsed time for transformer_add_bias_dropout (2048x4x24576): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24576): 0.0010

Attention duration (in seconds): 0.1983
Attention throughput (in TFLOP/s): 207.943
MLP duration (in seconds): 0.3229
MLP throughput (in TFLOP/s): 245.138
Transformer duration (in seconds): 0.5268
Transformer throughput (in TFLOP/s): 228.540
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1789
Attention throughput (in TFLOP/s): 230.475
MLP duration (in seconds): 0.3266
MLP throughput (in TFLOP/s): 242.418
Transformer duration (in seconds): 0.5127
Transformer throughput (in TFLOP/s): 234.834
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 24640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24640x73920, b=2048): 0.1177
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24640x73920, b=2048): 253.568
Elapsed time for attention_key_query_prob (256x2048x385x2048): 0.0108
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x385x2048): 76.245
Elapsed time for attention_prob_times_values (256x2048x2048x385): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x385): 76.894
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x24640x24640, b=2048): 0.0402
Throughput (in TFLOP/s) for attention_linear_projection (4x24640x24640, b=2048): 247.603
Elapsed time for mlp_h_to_4h (4x24640x98560, b=2048): 0.1570
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24640x98560, b=2048): 253.394
Elapsed time for mlp_fused_gelu (2048x4x98560): 0.0027
Elapsed time for mlp_4h_to_h (4x98560x24640, b=2048): 0.1658
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98560x24640, b=2048): 240.027
Elapsed time for transformer_add_bias_dropout (2048x4x24640): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24640): 0.0010

Attention duration (in seconds): 0.2145
Attention throughput (in TFLOP/s): 193.225
MLP duration (in seconds): 0.3255
MLP throughput (in TFLOP/s): 244.479
Transformer duration (in seconds): 0.5456
Transformer throughput (in TFLOP/s): 221.799
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1949
Attention throughput (in TFLOP/s): 212.648
MLP duration (in seconds): 0.3318
MLP throughput (in TFLOP/s): 239.814
Transformer duration (in seconds): 0.5348
Transformer throughput (in TFLOP/s): 226.289
Transformer - MLP - Attention (in seconds): 0.0081
========================================================================================================================
num_attention_heads: 64, hidden_size: 24704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24704x74112, b=2048): 0.1178
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24704x74112, b=2048): 254.535
Elapsed time for attention_key_query_prob (256x2048x386x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x386x2048): 110.047
Elapsed time for attention_prob_times_values (256x2048x2048x386): 0.0065
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x386): 127.939
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24704x24704, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_linear_projection (4x24704x24704, b=2048): 246.565
Elapsed time for mlp_h_to_4h (4x24704x98816, b=2048): 0.1582
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24704x98816, b=2048): 252.766
Elapsed time for mlp_fused_gelu (2048x4x98816): 0.0027
Elapsed time for mlp_4h_to_h (4x98816x24704, b=2048): 0.1659
Throughput (in TFLOP/s) for mlp_4h_to_h (4x98816x24704, b=2048): 241.028
Elapsed time for transformer_add_bias_dropout (2048x4x24704): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24704): 0.0010

Attention duration (in seconds): 0.2074
Attention throughput (in TFLOP/s): 200.847
MLP duration (in seconds): 0.3269
MLP throughput (in TFLOP/s): 244.708
Transformer duration (in seconds): 0.5399
Transformer throughput (in TFLOP/s): 225.310
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1836
Attention throughput (in TFLOP/s): 226.928
MLP duration (in seconds): 0.3330
MLP throughput (in TFLOP/s): 240.182
Transformer duration (in seconds): 0.5246
Transformer throughput (in TFLOP/s): 231.862
Transformer - MLP - Attention (in seconds): 0.0080
========================================================================================================================
num_attention_heads: 64, hidden_size: 24768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24768x74304, b=2048): 0.1191
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24768x74304, b=2048): 253.255
Elapsed time for attention_key_query_prob (256x2048x387x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x387x2048): 75.471
Elapsed time for attention_prob_times_values (256x2048x2048x387): 0.0108
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x387): 77.302
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24768x24768, b=2048): 0.0395
Throughput (in TFLOP/s) for attention_linear_projection (4x24768x24768, b=2048): 254.742
Elapsed time for mlp_h_to_4h (4x24768x99072, b=2048): 0.1591
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24768x99072, b=2048): 252.633
Elapsed time for mlp_fused_gelu (2048x4x99072): 0.0027
Elapsed time for mlp_4h_to_h (4x99072x24768, b=2048): 0.1671
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99072x24768, b=2048): 240.596
Elapsed time for transformer_add_bias_dropout (2048x4x24768): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24768): 0.0010

Attention duration (in seconds): 0.2153
Attention throughput (in TFLOP/s): 194.476
MLP duration (in seconds): 0.3290
MLP throughput (in TFLOP/s): 244.429
Transformer duration (in seconds): 0.5499
Transformer throughput (in TFLOP/s): 222.350
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1942
Attention throughput (in TFLOP/s): 215.632
MLP duration (in seconds): 0.3352
MLP throughput (in TFLOP/s): 239.882
Transformer duration (in seconds): 0.5392
Transformer throughput (in TFLOP/s): 226.746
Transformer - MLP - Attention (in seconds): 0.0099
========================================================================================================================
num_attention_heads: 64, hidden_size: 24832, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24832x74496, b=2048): 0.1202
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24832x74496, b=2048): 252.156
Elapsed time for attention_key_query_prob (256x2048x388x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x388x2048): 109.905
Elapsed time for attention_prob_times_values (256x2048x2048x388): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x388): 126.551
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24832x24832, b=2048): 0.0397
Throughput (in TFLOP/s) for attention_linear_projection (4x24832x24832, b=2048): 254.194
Elapsed time for mlp_h_to_4h (4x24832x99328, b=2048): 0.1606
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24832x99328, b=2048): 251.578
Elapsed time for mlp_fused_gelu (2048x4x99328): 0.0027
Elapsed time for mlp_4h_to_h (4x99328x24832, b=2048): 0.1679
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99328x24832, b=2048): 240.680
Elapsed time for transformer_add_bias_dropout (2048x4x24832): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24832): 0.0010

Attention duration (in seconds): 0.2091
Attention throughput (in TFLOP/s): 201.270
MLP duration (in seconds): 0.3313
MLP throughput (in TFLOP/s): 243.985
Transformer duration (in seconds): 0.5460
Transformer throughput (in TFLOP/s): 225.109
Transformer - MLP - Attention (in seconds): 0.0056


Actual
------
Attention duration (in seconds): 0.1843
Attention throughput (in TFLOP/s): 228.316
MLP duration (in seconds): 0.3380
MLP throughput (in TFLOP/s): 239.144
Transformer duration (in seconds): 0.5305
Transformer throughput (in TFLOP/s): 231.669
Transformer - MLP - Attention (in seconds): 0.0082
========================================================================================================================
num_attention_heads: 64, hidden_size: 24896, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24896x74688, b=2048): 0.1205
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24896x74688, b=2048): 252.784
Elapsed time for attention_key_query_prob (256x2048x389x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x389x2048): 75.916
Elapsed time for attention_prob_times_values (256x2048x2048x389): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x389): 76.961
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24896x24896, b=2048): 0.0396
Throughput (in TFLOP/s) for attention_linear_projection (4x24896x24896, b=2048): 256.502
Elapsed time for mlp_h_to_4h (4x24896x99584, b=2048): 0.1601
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24896x99584, b=2048): 253.753
Elapsed time for mlp_fused_gelu (2048x4x99584): 0.0027
Elapsed time for mlp_4h_to_h (4x99584x24896, b=2048): 0.1682
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99584x24896, b=2048): 241.492
Elapsed time for transformer_add_bias_dropout (2048x4x24896): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24896): 0.0010

Attention duration (in seconds): 0.2170
Attention throughput (in TFLOP/s): 194.932
MLP duration (in seconds): 0.3310
MLP throughput (in TFLOP/s): 245.426
Transformer duration (in seconds): 0.5537
Transformer throughput (in TFLOP/s): 223.120
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1959
Attention throughput (in TFLOP/s): 215.857
MLP duration (in seconds): 0.3385
MLP throughput (in TFLOP/s): 239.979
Transformer duration (in seconds): 0.5440
Transformer throughput (in TFLOP/s): 227.069
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 64, hidden_size: 24960, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x24960x74880, b=2048): 0.1213
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x24960x74880, b=2048): 252.536
Elapsed time for attention_key_query_prob (256x2048x390x2048): 0.0075
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x390x2048): 111.045
Elapsed time for attention_prob_times_values (256x2048x2048x390): 0.0066
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x390): 126.491
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x24960x24960, b=2048): 0.0398
Throughput (in TFLOP/s) for attention_linear_projection (4x24960x24960, b=2048): 256.397
Elapsed time for mlp_h_to_4h (4x24960x99840, b=2048): 0.1610
Throughput (in TFLOP/s) for mlp_h_to_4h (4x24960x99840, b=2048): 253.637
Elapsed time for mlp_fused_gelu (2048x4x99840): 0.0027
Elapsed time for mlp_4h_to_h (4x99840x24960, b=2048): 0.1686
Throughput (in TFLOP/s) for mlp_4h_to_h (4x99840x24960, b=2048): 242.109
Elapsed time for transformer_add_bias_dropout (2048x4x24960): 0.0018
Elapsed time for transformer_layer_norm (2048x4x24960): 0.0010

Attention duration (in seconds): 0.2102
Attention throughput (in TFLOP/s): 202.211
MLP duration (in seconds): 0.3324
MLP throughput (in TFLOP/s): 245.695
Transformer duration (in seconds): 0.5482
Transformer throughput (in TFLOP/s): 226.486
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1853
Attention throughput (in TFLOP/s): 229.343
MLP duration (in seconds): 0.3415
MLP throughput (in TFLOP/s): 239.150
Transformer duration (in seconds): 0.5340
Transformer throughput (in TFLOP/s): 232.502
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 25024, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25024x75072, b=2048): 0.1221
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25024x75072, b=2048): 252.103
Elapsed time for attention_key_query_prob (256x2048x391x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x391x2048): 75.777
Elapsed time for attention_prob_times_values (256x2048x2048x391): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x391): 75.609
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25024x25024, b=2048): 0.0416
Throughput (in TFLOP/s) for attention_linear_projection (4x25024x25024, b=2048): 246.520
Elapsed time for mlp_h_to_4h (4x25024x100096, b=2048): 0.1632
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25024x100096, b=2048): 251.418
Elapsed time for mlp_fused_gelu (2048x4x100096): 0.0027
Elapsed time for mlp_4h_to_h (4x100096x25024, b=2048): 0.1708
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100096x25024, b=2048): 240.213
Elapsed time for transformer_add_bias_dropout (2048x4x25024): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25024): 0.0010

Attention duration (in seconds): 0.2209
Attention throughput (in TFLOP/s): 193.385
MLP duration (in seconds): 0.3368
MLP throughput (in TFLOP/s): 243.683
Transformer duration (in seconds): 0.5634
Transformer throughput (in TFLOP/s): 221.498
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1999
Attention throughput (in TFLOP/s): 213.745
MLP duration (in seconds): 0.3425
MLP throughput (in TFLOP/s): 239.618
Transformer duration (in seconds): 0.5509
Transformer throughput (in TFLOP/s): 226.542
Transformer - MLP - Attention (in seconds): 0.0085
========================================================================================================================
num_attention_heads: 64, hidden_size: 25088, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25088x75264, b=2048): 0.1220
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25088x75264, b=2048): 253.648
Elapsed time for attention_key_query_prob (256x2048x392x2048): 0.0046
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x392x2048): 182.032
Elapsed time for attention_prob_times_values (256x2048x2048x392): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x392): 186.467
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x25088x25088, b=2048): 0.0418
Throughput (in TFLOP/s) for attention_linear_projection (4x25088x25088, b=2048): 246.881
Elapsed time for mlp_h_to_4h (4x25088x100352, b=2048): 0.1638
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25088x100352, b=2048): 251.778
Elapsed time for mlp_fused_gelu (2048x4x100352): 0.0028
Elapsed time for mlp_4h_to_h (4x100352x25088, b=2048): 0.1707
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100352x25088, b=2048): 241.697
Elapsed time for transformer_add_bias_dropout (2048x4x25088): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25088): 0.0010

Attention duration (in seconds): 0.2079
Attention throughput (in TFLOP/s): 206.543
MLP duration (in seconds): 0.3372
MLP throughput (in TFLOP/s): 244.619
Transformer duration (in seconds): 0.5508
Transformer throughput (in TFLOP/s): 227.725
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1875
Attention throughput (in TFLOP/s): 228.958
MLP duration (in seconds): 0.3420
MLP throughput (in TFLOP/s): 241.221
Transformer duration (in seconds): 0.5389
Transformer throughput (in TFLOP/s): 232.772
Transformer - MLP - Attention (in seconds): 0.0093
========================================================================================================================
num_attention_heads: 64, hidden_size: 25152, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25152x75456, b=2048): 0.1227
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25152x75456, b=2048): 253.423
Elapsed time for attention_key_query_prob (256x2048x393x2048): 0.0111
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x393x2048): 76.100
Elapsed time for attention_prob_times_values (256x2048x2048x393): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x393): 75.128
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25152x25152, b=2048): 0.0407
Throughput (in TFLOP/s) for attention_linear_projection (4x25152x25152, b=2048): 254.925
Elapsed time for mlp_h_to_4h (4x25152x100608, b=2048): 0.1637
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25152x100608, b=2048): 253.206
Elapsed time for mlp_fused_gelu (2048x4x100608): 0.0028
Elapsed time for mlp_4h_to_h (4x100608x25152, b=2048): 0.1727
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100608x25152, b=2048): 240.000
Elapsed time for transformer_add_bias_dropout (2048x4x25152): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25152): 0.0010

Attention duration (in seconds): 0.2207
Attention throughput (in TFLOP/s): 195.544
MLP duration (in seconds): 0.3393
MLP throughput (in TFLOP/s): 244.419
Transformer duration (in seconds): 0.5657
Transformer throughput (in TFLOP/s): 222.865
Transformer - MLP - Attention (in seconds): 0.0058


Actual
------
Attention duration (in seconds): 0.2000
Attention throughput (in TFLOP/s): 215.707
MLP duration (in seconds): 0.3452
MLP throughput (in TFLOP/s): 240.237
Transformer duration (in seconds): 0.5548
Transformer throughput (in TFLOP/s): 227.248
Transformer - MLP - Attention (in seconds): 0.0096
========================================================================================================================
num_attention_heads: 64, hidden_size: 25216, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25216x75648, b=2048): 0.1236
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25216x75648, b=2048): 252.930
Elapsed time for attention_key_query_prob (256x2048x394x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x394x2048): 110.946
Elapsed time for attention_prob_times_values (256x2048x2048x394): 0.0067
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x394): 125.376
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25216x25216, b=2048): 0.0406
Throughput (in TFLOP/s) for attention_linear_projection (4x25216x25216, b=2048): 256.336
Elapsed time for mlp_h_to_4h (4x25216x100864, b=2048): 0.1653
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25216x100864, b=2048): 252.092
Elapsed time for mlp_fused_gelu (2048x4x100864): 0.0028
Elapsed time for mlp_4h_to_h (4x100864x25216, b=2048): 0.1727
Throughput (in TFLOP/s) for mlp_4h_to_h (4x100864x25216, b=2048): 241.357
Elapsed time for transformer_add_bias_dropout (2048x4x25216): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25216): 0.0010

Attention duration (in seconds): 0.2137
Attention throughput (in TFLOP/s): 202.958
MLP duration (in seconds): 0.3407
MLP throughput (in TFLOP/s): 244.604
Transformer duration (in seconds): 0.5601
Transformer throughput (in TFLOP/s): 226.215
Transformer - MLP - Attention (in seconds): 0.0057


Actual
------
Attention duration (in seconds): 0.1887
Attention throughput (in TFLOP/s): 229.844
MLP duration (in seconds): 0.3469
MLP throughput (in TFLOP/s): 240.266
Transformer duration (in seconds): 0.5461
Transformer throughput (in TFLOP/s): 232.026
Transformer - MLP - Attention (in seconds): 0.0105
========================================================================================================================[2023-06-12 21:40:57,642] [INFO] [comm.py:643:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...
[2023-06-12 21:40:58,435] [INFO] [comm.py:697:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.130.44, master_port=6000
[2023-06-12 21:40:58,435] [INFO] [comm.py:661:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-06-12 21:41:01,635] [INFO] [checkpointing.py:227:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 64, hidden_size: 25280, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25280x75840, b=2048): 0.1245
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25280x75840, b=2048): 252.304
Elapsed time for attention_key_query_prob (256x2048x395x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x395x2048): 77.148
Elapsed time for attention_prob_times_values (256x2048x2048x395): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x395): 77.081
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x25280x25280, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_linear_projection (4x25280x25280, b=2048): 255.687
Elapsed time for mlp_h_to_4h (4x25280x101120, b=2048): 0.1648
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25280x101120, b=2048): 254.105
Elapsed time for mlp_fused_gelu (2048x4x101120): 0.0028
Elapsed time for mlp_4h_to_h (4x101120x25280, b=2048): 0.1763
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101120x25280, b=2048): 237.517
Elapsed time for transformer_add_bias_dropout (2048x4x25280): 0.0018
Elapsed time for transformer_layer_norm (2048x4x25280): 0.0010

Attention duration (in seconds): 0.2224
Attention throughput (in TFLOP/s): 195.910
MLP duration (in seconds): 0.3439
MLP throughput (in TFLOP/s): 243.554
Transformer duration (in seconds): 0.5721
Transformer throughput (in TFLOP/s): 222.575
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 25344, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25344x76032, b=2048): 0.1238
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25344x76032, b=2048): 255.079
Elapsed time for attention_key_query_prob (256x2048x396x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x396x2048): 112.226
Elapsed time for attention_prob_times_values (256x2048x2048x396): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x396): 125.469
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x25344x25344, b=2048): 0.0409
Throughput (in TFLOP/s) for attention_linear_projection (4x25344x25344, b=2048): 257.338
Elapsed time for mlp_h_to_4h (4x25344x101376, b=2048): 0.1659
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25344x101376, b=2048): 253.786
Elapsed time for mlp_fused_gelu (2048x4x101376): 0.0028
Elapsed time for mlp_4h_to_h (4x101376x25344, b=2048): 0.1767
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101376x25344, b=2048): 238.288
Elapsed time for transformer_add_bias_dropout (2048x4x25344): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25344): 0.0010

Attention duration (in seconds): 0.2140
Attention throughput (in TFLOP/s): 204.657
MLP duration (in seconds): 0.3453
MLP throughput (in TFLOP/s): 243.816
Transformer duration (in seconds): 0.5650
Transformer throughput (in TFLOP/s): 226.508
Transformer - MLP - Attention (in seconds): 0.0057
========================================================================================================================
num_attention_heads: 64, hidden_size: 25408, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25408x76224, b=2048): 0.1246
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25408x76224, b=2048): 254.614
Elapsed time for attention_key_query_prob (256x2048x397x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x397x2048): 77.462
Elapsed time for attention_prob_times_values (256x2048x2048x397): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x397): 77.662
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x25408x25408, b=2048): 0.0410
Throughput (in TFLOP/s) for attention_linear_projection (4x25408x25408, b=2048): 258.087
Elapsed time for mlp_h_to_4h (4x25408x101632, b=2048): 0.1668
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25408x101632, b=2048): 253.676
Elapsed time for mlp_fused_gelu (2048x4x101632): 0.0028
Elapsed time for mlp_4h_to_h (4x101632x25408, b=2048): 0.1786
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101632x25408, b=2048): 236.937
Elapsed time for transformer_add_bias_dropout (2048x4x25408): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25408): 0.0010

Attention duration (in seconds): 0.2226
Attention throughput (in TFLOP/s): 197.743
MLP duration (in seconds): 0.3481
MLP throughput (in TFLOP/s): 243.059
Transformer duration (in seconds): 0.5765
Transformer throughput (in TFLOP/s): 223.116
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 25472, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25472x76416, b=2048): 0.1253
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25472x76416, b=2048): 254.555
Elapsed time for attention_key_query_prob (256x2048x398x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x398x2048): 112.448
Elapsed time for attention_prob_times_values (256x2048x2048x398): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x398): 125.806
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x25472x25472, b=2048): 0.0411
Throughput (in TFLOP/s) for attention_linear_projection (4x25472x25472, b=2048): 258.780
Elapsed time for mlp_h_to_4h (4x25472x101888, b=2048): 0.1673
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25472x101888, b=2048): 254.129
Elapsed time for mlp_fused_gelu (2048x4x101888): 0.0028
Elapsed time for mlp_4h_to_h (4x101888x25472, b=2048): 0.1784
Throughput (in TFLOP/s) for mlp_4h_to_h (4x101888x25472, b=2048): 238.330
Elapsed time for transformer_add_bias_dropout (2048x4x25472): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25472): 0.0010

Attention duration (in seconds): 0.2157
Attention throughput (in TFLOP/s): 205.018
MLP duration (in seconds): 0.3485
MLP throughput (in TFLOP/s): 244.005
Transformer duration (in seconds): 0.5701
Transformer throughput (in TFLOP/s): 226.773
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 25536, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25536x76608, b=2048): 0.1262
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25536x76608, b=2048): 253.957
Elapsed time for attention_key_query_prob (256x2048x399x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x399x2048): 78.254
Elapsed time for attention_prob_times_values (256x2048x2048x399): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x399): 77.399
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x25536x25536, b=2048): 0.0417
Throughput (in TFLOP/s) for attention_linear_projection (4x25536x25536, b=2048): 256.026
Elapsed time for mlp_h_to_4h (4x25536x102144, b=2048): 0.1680
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25536x102144, b=2048): 254.356
Elapsed time for mlp_fused_gelu (2048x4x102144): 0.0028
Elapsed time for mlp_4h_to_h (4x102144x25536, b=2048): 0.1790
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102144x25536, b=2048): 238.801
Elapsed time for transformer_add_bias_dropout (2048x4x25536): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25536): 0.0010

Attention duration (in seconds): 0.2249
Attention throughput (in TFLOP/s): 197.603
MLP duration (in seconds): 0.3498
MLP throughput (in TFLOP/s): 244.361
Transformer duration (in seconds): 0.5805
Transformer throughput (in TFLOP/s): 223.793
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 25600, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25600x76800, b=2048): 0.1259
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25600x76800, b=2048): 255.879
Elapsed time for attention_key_query_prob (256x2048x400x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x400x2048): 193.163
Elapsed time for attention_prob_times_values (256x2048x2048x400): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x400): 191.953
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25600x25600, b=2048): 0.0419
Throughput (in TFLOP/s) for attention_linear_projection (4x25600x25600, b=2048): 256.080
Elapsed time for mlp_h_to_4h (4x25600x102400, b=2048): 0.1686
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25600x102400, b=2048): 254.737
Elapsed time for mlp_fused_gelu (2048x4x102400): 0.0028
Elapsed time for mlp_4h_to_h (4x102400x25600, b=2048): 0.1798
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102400x25600, b=2048): 238.913
Elapsed time for transformer_add_bias_dropout (2048x4x25600): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25600): 0.0010

Attention duration (in seconds): 0.2117
Attention throughput (in TFLOP/s): 210.976
MLP duration (in seconds): 0.3512
MLP throughput (in TFLOP/s): 244.601
Transformer duration (in seconds): 0.5687
Transformer throughput (in TFLOP/s): 229.586
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 25664, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25664x76992, b=2048): 0.1270
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25664x76992, b=2048): 254.941
Elapsed time for attention_key_query_prob (256x2048x401x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x401x2048): 78.925
Elapsed time for attention_prob_times_values (256x2048x2048x401): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x401): 77.418
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25664x25664, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_linear_projection (4x25664x25664, b=2048): 255.705
Elapsed time for mlp_h_to_4h (4x25664x102656, b=2048): 0.1697
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25664x102656, b=2048): 254.348
Elapsed time for mlp_fused_gelu (2048x4x102656): 0.0028
Elapsed time for mlp_4h_to_h (4x102656x25664, b=2048): 0.1813
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102656x25664, b=2048): 238.113
Elapsed time for transformer_add_bias_dropout (2048x4x25664): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25664): 0.0011

Attention duration (in seconds): 0.2262
Attention throughput (in TFLOP/s): 198.441
MLP duration (in seconds): 0.3538
MLP throughput (in TFLOP/s): 244.006
Transformer duration (in seconds): 0.5859
Transformer throughput (in TFLOP/s): 223.971
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25728, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25728x77184, b=2048): 0.1278
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25728x77184, b=2048): 254.609
Elapsed time for attention_key_query_prob (256x2048x402x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x402x2048): 113.271
Elapsed time for attention_prob_times_values (256x2048x2048x402): 0.0068
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x402): 126.351
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25728x25728, b=2048): 0.0422
Throughput (in TFLOP/s) for attention_linear_projection (4x25728x25728, b=2048): 257.186
Elapsed time for mlp_h_to_4h (4x25728x102912, b=2048): 0.1704
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25728x102912, b=2048): 254.591
Elapsed time for mlp_fused_gelu (2048x4x102912): 0.0028
Elapsed time for mlp_4h_to_h (4x102912x25728, b=2048): 0.1817
Throughput (in TFLOP/s) for mlp_4h_to_h (4x102912x25728, b=2048): 238.746
Elapsed time for transformer_add_bias_dropout (2048x4x25728): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25728): 0.0010

Attention duration (in seconds): 0.2194
Attention throughput (in TFLOP/s): 205.609
MLP duration (in seconds): 0.3549
MLP throughput (in TFLOP/s): 244.456
Transformer duration (in seconds): 0.5801
Transformer throughput (in TFLOP/s): 227.304
Transformer - MLP - Attention (in seconds): 0.0058
========================================================================================================================
num_attention_heads: 64, hidden_size: 25792, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25792x77376, b=2048): 0.1280
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25792x77376, b=2048): 255.364
Elapsed time for attention_key_query_prob (256x2048x403x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x403x2048): 79.181
Elapsed time for attention_prob_times_values (256x2048x2048x403): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x403): 78.875
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25792x25792, b=2048): 0.0423
Throughput (in TFLOP/s) for attention_linear_projection (4x25792x25792, b=2048): 257.584
Elapsed time for mlp_h_to_4h (4x25792x103168, b=2048): 0.1717
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25792x103168, b=2048): 253.961
Elapsed time for mlp_fused_gelu (2048x4x103168): 0.0028
Elapsed time for mlp_4h_to_h (4x103168x25792, b=2048): 0.1832
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103168x25792, b=2048): 238.001
Elapsed time for transformer_add_bias_dropout (2048x4x25792): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25792): 0.0011

Attention duration (in seconds): 0.2272
Attention throughput (in TFLOP/s): 199.470
MLP duration (in seconds): 0.3577
MLP throughput (in TFLOP/s): 243.779
Transformer duration (in seconds): 0.5908
Transformer throughput (in TFLOP/s): 224.302
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25856, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25856x77568, b=2048): 0.1291
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25856x77568, b=2048): 254.556
Elapsed time for attention_key_query_prob (256x2048x404x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x404x2048): 113.906
Elapsed time for attention_prob_times_values (256x2048x2048x404): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x404): 126.327
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25856x25856, b=2048): 0.0424
Throughput (in TFLOP/s) for attention_linear_projection (4x25856x25856, b=2048): 258.348
Elapsed time for mlp_h_to_4h (4x25856x103424, b=2048): 0.1724
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25856x103424, b=2048): 254.114
Elapsed time for mlp_fused_gelu (2048x4x103424): 0.0028
Elapsed time for mlp_4h_to_h (4x103424x25856, b=2048): 0.1831
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103424x25856, b=2048): 239.266
Elapsed time for transformer_add_bias_dropout (2048x4x25856): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25856): 0.0010

Attention duration (in seconds): 0.2210
Attention throughput (in TFLOP/s): 206.140
MLP duration (in seconds): 0.3584
MLP throughput (in TFLOP/s): 244.516
Transformer duration (in seconds): 0.5852
Transformer throughput (in TFLOP/s): 227.581
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25920, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25920x77760, b=2048): 0.1293
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25920x77760, b=2048): 255.493
Elapsed time for attention_key_query_prob (256x2048x405x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x405x2048): 79.647
Elapsed time for attention_prob_times_values (256x2048x2048x405): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x405): 79.175
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x25920x25920, b=2048): 0.0432
Throughput (in TFLOP/s) for attention_linear_projection (4x25920x25920, b=2048): 255.087
Elapsed time for mlp_h_to_4h (4x25920x103680, b=2048): 0.1732
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25920x103680, b=2048): 254.277
Elapsed time for mlp_fused_gelu (2048x4x103680): 0.0028
Elapsed time for mlp_4h_to_h (4x103680x25920, b=2048): 0.1852
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103680x25920, b=2048): 237.742
Elapsed time for transformer_add_bias_dropout (2048x4x25920): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25920): 0.0011

Attention duration (in seconds): 0.2293
Attention throughput (in TFLOP/s): 199.609
MLP duration (in seconds): 0.3612
MLP throughput (in TFLOP/s): 243.800
Transformer duration (in seconds): 0.5964
Transformer throughput (in TFLOP/s): 224.395
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 25984, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x25984x77952, b=2048): 0.1304
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x25984x77952, b=2048): 254.501
Elapsed time for attention_key_query_prob (256x2048x406x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x406x2048): 114.144
Elapsed time for attention_prob_times_values (256x2048x2048x406): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x406): 126.820
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x25984x25984, b=2048): 0.0432
Throughput (in TFLOP/s) for attention_linear_projection (4x25984x25984, b=2048): 256.037
Elapsed time for mlp_h_to_4h (4x25984x103936, b=2048): 0.1742
Throughput (in TFLOP/s) for mlp_h_to_4h (4x25984x103936, b=2048): 253.974
Elapsed time for mlp_fused_gelu (2048x4x103936): 0.0029
Elapsed time for mlp_4h_to_h (4x103936x25984, b=2048): 0.1867
Throughput (in TFLOP/s) for mlp_4h_to_h (4x103936x25984, b=2048): 237.004
Elapsed time for transformer_add_bias_dropout (2048x4x25984): 0.0019
Elapsed time for transformer_layer_norm (2048x4x25984): 0.0010

Attention duration (in seconds): 0.2231
Attention throughput (in TFLOP/s): 206.193
MLP duration (in seconds): 0.3638
MLP throughput (in TFLOP/s): 243.274
Transformer duration (in seconds): 0.5927
Transformer throughput (in TFLOP/s): 226.898
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 26048, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26048x78144, b=2048): 0.1306
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26048x78144, b=2048): 255.345
Elapsed time for attention_key_query_prob (256x2048x407x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x407x2048): 80.088
Elapsed time for attention_prob_times_values (256x2048x2048x407): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x407): 78.219
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26048x26048, b=2048): 0.0434
Throughput (in TFLOP/s) for attention_linear_projection (4x26048x26048, b=2048): 256.015
Elapsed time for mlp_h_to_4h (4x26048x104192, b=2048): 0.1745
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26048x104192, b=2048): 254.766
Elapsed time for mlp_fused_gelu (2048x4x104192): 0.0029
Elapsed time for mlp_4h_to_h (4x104192x26048, b=2048): 0.1866
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104192x26048, b=2048): 238.298
Elapsed time for transformer_add_bias_dropout (2048x4x26048): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26048): 0.0011

Attention duration (in seconds): 0.2311
Attention throughput (in TFLOP/s): 200.013
MLP duration (in seconds): 0.3640
MLP throughput (in TFLOP/s): 244.324
Transformer duration (in seconds): 0.6010
Transformer throughput (in TFLOP/s): 224.869
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26112, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26112x78336, b=2048): 0.1318
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26112x78336, b=2048): 254.304
Elapsed time for attention_key_query_prob (256x2048x408x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x408x2048): 186.135
Elapsed time for attention_prob_times_values (256x2048x2048x408): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x408): 192.154
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26112x26112, b=2048): 0.0435
Throughput (in TFLOP/s) for attention_linear_projection (4x26112x26112, b=2048): 256.740
Elapsed time for mlp_h_to_4h (4x26112x104448, b=2048): 0.1759
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26112x104448, b=2048): 254.020
Elapsed time for mlp_fused_gelu (2048x4x104448): 0.0029
Elapsed time for mlp_4h_to_h (4x104448x26112, b=2048): 0.1867
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104448x26112, b=2048): 239.282
Elapsed time for transformer_add_bias_dropout (2048x4x26112): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26112): 0.0010

Attention duration (in seconds): 0.2195
Attention throughput (in TFLOP/s): 211.562
MLP duration (in seconds): 0.3655
MLP throughput (in TFLOP/s): 244.501
Transformer duration (in seconds): 0.5909
Transformer throughput (in TFLOP/s): 229.816
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 26176, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26176x78528, b=2048): 0.1318
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26176x78528, b=2048): 255.579
Elapsed time for attention_key_query_prob (256x2048x409x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x409x2048): 80.305
Elapsed time for attention_prob_times_values (256x2048x2048x409): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x409): 78.143
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26176x26176, b=2048): 0.0438
Throughput (in TFLOP/s) for attention_linear_projection (4x26176x26176, b=2048): 256.246
Elapsed time for mlp_h_to_4h (4x26176x104704, b=2048): 0.1762
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26176x104704, b=2048): 254.893
Elapsed time for mlp_fused_gelu (2048x4x104704): 0.0029
Elapsed time for mlp_4h_to_h (4x104704x26176, b=2048): 0.1882
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104704x26176, b=2048): 238.568
Elapsed time for transformer_add_bias_dropout (2048x4x26176): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26176): 0.0011

Attention duration (in seconds): 0.2327
Attention throughput (in TFLOP/s): 200.517
MLP duration (in seconds): 0.3673
MLP throughput (in TFLOP/s): 244.535
Transformer duration (in seconds): 0.6060
Transformer throughput (in TFLOP/s): 225.212
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26240, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26240x78720, b=2048): 0.1324
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26240x78720, b=2048): 255.647
Elapsed time for attention_key_query_prob (256x2048x410x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x410x2048): 115.282
Elapsed time for attention_prob_times_values (256x2048x2048x410): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x410): 127.542
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x26240x26240, b=2048): 0.0437
Throughput (in TFLOP/s) for attention_linear_projection (4x26240x26240, b=2048): 258.171
Elapsed time for mlp_h_to_4h (4x26240x104960, b=2048): 0.1776
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26240x104960, b=2048): 254.131
Elapsed time for mlp_fused_gelu (2048x4x104960): 0.0029
Elapsed time for mlp_4h_to_h (4x104960x26240, b=2048): 0.1892
Throughput (in TFLOP/s) for mlp_4h_to_h (4x104960x26240, b=2048): 238.513
Elapsed time for transformer_add_bias_dropout (2048x4x26240): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26240): 0.0011

Attention duration (in seconds): 0.2256
Attention throughput (in TFLOP/s): 207.796
MLP duration (in seconds): 0.3696
MLP throughput (in TFLOP/s): 244.159
Transformer duration (in seconds): 0.6012
Transformer throughput (in TFLOP/s): 228.097
Transformer - MLP - Attention (in seconds): 0.0059
========================================================================================================================
num_attention_heads: 64, hidden_size: 26304, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26304x78912, b=2048): 0.1334
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26304x78912, b=2048): 254.909
Elapsed time for attention_key_query_prob (256x2048x411x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x411x2048): 80.269
Elapsed time for attention_prob_times_values (256x2048x2048x411): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x411): 80.567
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x26304x26304, b=2048): 0.0443
Throughput (in TFLOP/s) for attention_linear_projection (4x26304x26304, b=2048): 256.016
Elapsed time for mlp_h_to_4h (4x26304x105216, b=2048): 0.1785
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26304x105216, b=2048): 254.094
Elapsed time for mlp_fused_gelu (2048x4x105216): 0.0029
Elapsed time for mlp_4h_to_h (4x105216x26304, b=2048): 0.1900
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105216x26304, b=2048): 238.606
Elapsed time for transformer_add_bias_dropout (2048x4x26304): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26304): 0.0011

Attention duration (in seconds): 0.2346
Attention throughput (in TFLOP/s): 200.789
MLP duration (in seconds): 0.3714
MLP throughput (in TFLOP/s): 244.195
Transformer duration (in seconds): 0.6120
Transformer throughput (in TFLOP/s): 225.163
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26368, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26368x79104, b=2048): 0.1337
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26368x79104, b=2048): 255.601
Elapsed time for attention_key_query_prob (256x2048x412x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x412x2048): 115.088
Elapsed time for attention_prob_times_values (256x2048x2048x412): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x412): 128.150
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x26368x26368, b=2048): 0.0444
Throughput (in TFLOP/s) for attention_linear_projection (4x26368x26368, b=2048): 256.622
Elapsed time for mlp_h_to_4h (4x26368x105472, b=2048): 0.1796
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26368x105472, b=2048): 253.738
Elapsed time for mlp_fused_gelu (2048x4x105472): 0.0029
Elapsed time for mlp_4h_to_h (4x105472x26368, b=2048): 0.1912
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105472x26368, b=2048): 238.338
Elapsed time for transformer_add_bias_dropout (2048x4x26368): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26368): 0.0011

Attention duration (in seconds): 0.2277
Attention throughput (in TFLOP/s): 207.922
MLP duration (in seconds): 0.3736
MLP throughput (in TFLOP/s): 243.895
Transformer duration (in seconds): 0.6073
Transformer throughput (in TFLOP/s): 228.018
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26432, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26432x79296, b=2048): 0.1351
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26432x79296, b=2048): 254.144
Elapsed time for attention_key_query_prob (256x2048x413x2048): 0.0110
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x413x2048): 80.585
Elapsed time for attention_prob_times_values (256x2048x2048x413): 0.0109
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x413): 81.010
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x26432x26432, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_linear_projection (4x26432x26432, b=2048): 255.679
Elapsed time for mlp_h_to_4h (4x26432x105728, b=2048): 0.1808
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26432x105728, b=2048): 253.194
Elapsed time for mlp_fused_gelu (2048x4x105728): 0.0029
Elapsed time for mlp_4h_to_h (4x105728x26432, b=2048): 0.1918
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105728x26432, b=2048): 238.768
Elapsed time for transformer_add_bias_dropout (2048x4x26432): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26432): 0.0011

Attention duration (in seconds): 0.2368
Attention throughput (in TFLOP/s): 200.832
MLP duration (in seconds): 0.3755
MLP throughput (in TFLOP/s): 243.872
Transformer duration (in seconds): 0.6184
Transformer throughput (in TFLOP/s): 224.998
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26496, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26496x79488, b=2048): 0.1352
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26496x79488, b=2048): 255.207
Elapsed time for attention_key_query_prob (256x2048x414x2048): 0.0076
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x414x2048): 116.295
Elapsed time for attention_prob_times_values (256x2048x2048x414): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x414): 129.209
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x26496x26496, b=2048): 0.0448
Throughput (in TFLOP/s) for attention_linear_projection (4x26496x26496, b=2048): 256.751
Elapsed time for mlp_h_to_4h (4x26496x105984, b=2048): 0.1810
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26496x105984, b=2048): 254.127
Elapsed time for mlp_fused_gelu (2048x4x105984): 0.0029
Elapsed time for mlp_4h_to_h (4x105984x26496, b=2048): 0.1930
Throughput (in TFLOP/s) for mlp_4h_to_h (4x105984x26496, b=2048): 238.346
Elapsed time for transformer_add_bias_dropout (2048x4x26496): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26496): 0.0011

Attention duration (in seconds): 0.2295
Attention throughput (in TFLOP/s): 208.214
MLP duration (in seconds): 0.3770
MLP throughput (in TFLOP/s): 244.090
Transformer duration (in seconds): 0.6125
Transformer throughput (in TFLOP/s): 228.255
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26560, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26560x79680, b=2048): 0.1364
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26560x79680, b=2048): 254.143
Elapsed time for attention_key_query_prob (256x2048x415x2048): 0.0109
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x415x2048): 81.433
Elapsed time for attention_prob_times_values (256x2048x2048x415): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x415): 80.624
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26560x26560, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_linear_projection (4x26560x26560, b=2048): 256.241
Elapsed time for mlp_h_to_4h (4x26560x106240, b=2048): 0.1828
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26560x106240, b=2048): 252.937
Elapsed time for mlp_fused_gelu (2048x4x106240): 0.0029
Elapsed time for mlp_4h_to_h (4x106240x26560, b=2048): 0.1944
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106240x26560, b=2048): 237.814
Elapsed time for transformer_add_bias_dropout (2048x4x26560): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26560): 0.0011

Attention duration (in seconds): 0.2385
Attention throughput (in TFLOP/s): 201.315
MLP duration (in seconds): 0.3801
MLP throughput (in TFLOP/s): 243.264
Transformer duration (in seconds): 0.6247
Transformer throughput (in TFLOP/s): 224.888
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26624, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26624x79872, b=2048): 0.1371
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26624x79872, b=2048): 254.213
Elapsed time for attention_key_query_prob (256x2048x416x2048): 0.0044
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x416x2048): 201.393
Elapsed time for attention_prob_times_values (256x2048x2048x416): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x416): 199.782
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26624x26624, b=2048): 0.0451
Throughput (in TFLOP/s) for attention_linear_projection (4x26624x26624, b=2048): 257.717
Elapsed time for mlp_h_to_4h (4x26624x106496, b=2048): 0.1832
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26624x106496, b=2048): 253.596
Elapsed time for mlp_fused_gelu (2048x4x106496): 0.0029
Elapsed time for mlp_4h_to_h (4x106496x26624, b=2048): 0.1944
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106496x26624, b=2048): 238.984
Elapsed time for transformer_add_bias_dropout (2048x4x26624): 0.0019
Elapsed time for transformer_layer_norm (2048x4x26624): 0.0011

Attention duration (in seconds): 0.2260
Attention throughput (in TFLOP/s): 213.468
MLP duration (in seconds): 0.3805
MLP throughput (in TFLOP/s): 244.187
Transformer duration (in seconds): 0.6125
Transformer throughput (in TFLOP/s): 230.449
Transformer - MLP - Attention (in seconds): 0.0060
========================================================================================================================
num_attention_heads: 64, hidden_size: 26688, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26688x80064, b=2048): 0.1380
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26688x80064, b=2048): 253.774
Elapsed time for attention_key_query_prob (256x2048x417x2048): 0.0113
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x417x2048): 79.501
Elapsed time for attention_prob_times_values (256x2048x2048x417): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x417): 80.233
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26688x26688, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_linear_projection (4x26688x26688, b=2048): 257.771
Elapsed time for mlp_h_to_4h (4x26688x106752, b=2048): 0.1847
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26688x106752, b=2048): 252.709
Elapsed time for mlp_fused_gelu (2048x4x106752): 0.0029
Elapsed time for mlp_4h_to_h (4x106752x26688, b=2048): 0.1962
Throughput (in TFLOP/s) for mlp_4h_to_h (4x106752x26688, b=2048): 237.952
Elapsed time for transformer_add_bias_dropout (2048x4x26688): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26688): 0.0011

Attention duration (in seconds): 0.2406
Attention throughput (in TFLOP/s): 201.431
MLP duration (in seconds): 0.3838
MLP throughput (in TFLOP/s): 243.236
Transformer duration (in seconds): 0.6306
Transformer throughput (in TFLOP/s): 224.920
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26752, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26752x80256, b=2048): 0.1383
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26752x80256, b=2048): 254.351
Elapsed time for attention_key_query_prob (256x2048x418x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x418x2048): 117.196
Elapsed time for attention_prob_times_values (256x2048x2048x418): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x418): 130.149
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26752x26752, b=2048): 0.0453
Throughput (in TFLOP/s) for attention_linear_projection (4x26752x26752, b=2048): 258.599
Elapsed time for mlp_h_to_4h (4x26752x107008, b=2048): 0.1857
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26752x107008, b=2048): 252.512
Elapsed time for mlp_fused_gelu (2048x4x107008): 0.0029
Elapsed time for mlp_4h_to_h (4x107008x26752, b=2048): 0.1961
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107008x26752, b=2048): 239.163
Elapsed time for transformer_add_bias_dropout (2048x4x26752): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26752): 0.0011

Attention duration (in seconds): 0.2332
Attention throughput (in TFLOP/s): 208.851
MLP duration (in seconds): 0.3848
MLP throughput (in TFLOP/s): 243.784
Transformer duration (in seconds): 0.6240
Transformer throughput (in TFLOP/s): 228.360
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26816, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26816x80448, b=2048): 0.1390
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26816x80448, b=2048): 254.339
Elapsed time for attention_key_query_prob (256x2048x419x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x419x2048): 78.486
Elapsed time for attention_prob_times_values (256x2048x2048x419): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x419): 81.462
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26816x26816, b=2048): 0.0461
Throughput (in TFLOP/s) for attention_linear_projection (4x26816x26816, b=2048): 255.632
Elapsed time for mlp_h_to_4h (4x26816x107264, b=2048): 0.1856
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26816x107264, b=2048): 253.913
Elapsed time for mlp_fused_gelu (2048x4x107264): 0.0029
Elapsed time for mlp_4h_to_h (4x107264x26816, b=2048): 0.1978
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107264x26816, b=2048): 238.199
Elapsed time for transformer_add_bias_dropout (2048x4x26816): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26816): 0.0011

Attention duration (in seconds): 0.2425
Attention throughput (in TFLOP/s): 201.738
MLP duration (in seconds): 0.3864
MLP throughput (in TFLOP/s): 243.934
Transformer duration (in seconds): 0.6350
Transformer throughput (in TFLOP/s): 225.465
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26880, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26880x80640, b=2048): 0.1398
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26880x80640, b=2048): 254.014
Elapsed time for attention_key_query_prob (256x2048x420x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x420x2048): 117.443
Elapsed time for attention_prob_times_values (256x2048x2048x420): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x420): 129.477
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x26880x26880, b=2048): 0.0464
Throughput (in TFLOP/s) for attention_linear_projection (4x26880x26880, b=2048): 254.888
Elapsed time for mlp_h_to_4h (4x26880x107520, b=2048): 0.1872
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26880x107520, b=2048): 252.925
Elapsed time for mlp_fused_gelu (2048x4x107520): 0.0029
Elapsed time for mlp_4h_to_h (4x107520x26880, b=2048): 0.1988
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107520x26880, b=2048): 238.156
Elapsed time for transformer_add_bias_dropout (2048x4x26880): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26880): 0.0011

Attention duration (in seconds): 0.2359
Attention throughput (in TFLOP/s): 208.387
MLP duration (in seconds): 0.3890
MLP throughput (in TFLOP/s): 243.460
Transformer duration (in seconds): 0.6310
Transformer throughput (in TFLOP/s): 227.999
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 26944, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x26944x80832, b=2048): 0.1402
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x26944x80832, b=2048): 254.561
Elapsed time for attention_key_query_prob (256x2048x421x2048): 0.0115
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x421x2048): 78.451
Elapsed time for attention_prob_times_values (256x2048x2048x421): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x421): 81.600
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x26944x26944, b=2048): 0.0463
Throughput (in TFLOP/s) for attention_linear_projection (4x26944x26944, b=2048): 256.944
Elapsed time for mlp_h_to_4h (4x26944x107776, b=2048): 0.1882
Throughput (in TFLOP/s) for mlp_h_to_4h (4x26944x107776, b=2048): 252.757
Elapsed time for mlp_fused_gelu (2048x4x107776): 0.0030
Elapsed time for mlp_4h_to_h (4x107776x26944, b=2048): 0.1993
Throughput (in TFLOP/s) for mlp_4h_to_h (4x107776x26944, b=2048): 238.724
Elapsed time for transformer_add_bias_dropout (2048x4x26944): 0.0020
Elapsed time for transformer_layer_norm (2048x4x26944): 0.0011

Attention duration (in seconds): 0.2441
Attention throughput (in TFLOP/s): 202.357
MLP duration (in seconds): 0.3905
MLP throughput (in TFLOP/s): 243.684
Transformer duration (in seconds): 0.6407
Transformer throughput (in TFLOP/s): 225.604
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 27008, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27008x81024, b=2048): 0.1410
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27008x81024, b=2048): 254.275
Elapsed time for attention_key_query_prob (256x2048x422x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x422x2048): 118.062
Elapsed time for attention_prob_times_values (256x2048x2048x422): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x422): 130.580
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27008x27008, b=2048): 0.0467
Throughput (in TFLOP/s) for attention_linear_projection (4x27008x27008, b=2048): 256.026
Elapsed time for mlp_h_to_4h (4x27008x108032, b=2048): 0.1890
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27008x108032, b=2048): 252.895
Elapsed time for mlp_fused_gelu (2048x4x108032): 0.0030
Elapsed time for mlp_4h_to_h (4x108032x27008, b=2048): 0.2002
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108032x27008, b=2048): 238.799
Elapsed time for transformer_add_bias_dropout (2048x4x27008): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27008): 0.0011

Attention duration (in seconds): 0.2373
Attention throughput (in TFLOP/s): 209.106
MLP duration (in seconds): 0.3922
MLP throughput (in TFLOP/s): 243.790
Transformer duration (in seconds): 0.6356
Transformer throughput (in TFLOP/s): 228.496
Transformer - MLP - Attention (in seconds): 0.0061
========================================================================================================================
num_attention_heads: 64, hidden_size: 27072, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27072x81216, b=2048): 0.1416
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27072x81216, b=2048): 254.355
Elapsed time for attention_key_query_prob (256x2048x423x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x423x2048): 78.079
Elapsed time for attention_prob_times_values (256x2048x2048x423): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x423): 80.404
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27072x27072, b=2048): 0.0466
Throughput (in TFLOP/s) for attention_linear_projection (4x27072x27072, b=2048): 257.519
Elapsed time for mlp_h_to_4h (4x27072x108288, b=2048): 0.1900
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27072x108288, b=2048): 252.812
Elapsed time for mlp_fused_gelu (2048x4x108288): 0.0030
Elapsed time for mlp_4h_to_h (4x108288x27072, b=2048): 0.2017
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108288x27072, b=2048): 238.097
Elapsed time for transformer_add_bias_dropout (2048x4x27072): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27072): 0.0011

Attention duration (in seconds): 0.2462
Attention throughput (in TFLOP/s): 202.484
MLP duration (in seconds): 0.3947
MLP throughput (in TFLOP/s): 243.391
Transformer duration (in seconds): 0.6470
Transformer throughput (in TFLOP/s): 225.505
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27136, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27136x81408, b=2048): 0.1424
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27136x81408, b=2048): 254.140
Elapsed time for attention_key_query_prob (256x2048x424x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x424x2048): 183.705
Elapsed time for attention_prob_times_values (256x2048x2048x424): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x424): 198.643
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x27136x27136, b=2048): 0.0468
Throughput (in TFLOP/s) for attention_linear_projection (4x27136x27136, b=2048): 257.801
Elapsed time for mlp_h_to_4h (4x27136x108544, b=2048): 0.1912
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27136x108544, b=2048): 252.417
Elapsed time for mlp_fused_gelu (2048x4x108544): 0.0030
Elapsed time for mlp_4h_to_h (4x108544x27136, b=2048): 0.2020
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108544x27136, b=2048): 238.960
Elapsed time for transformer_add_bias_dropout (2048x4x27136): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27136): 0.0011

Attention duration (in seconds): 0.2337
Attention throughput (in TFLOP/s): 214.253
MLP duration (in seconds): 0.3961
MLP throughput (in TFLOP/s): 243.658
Transformer duration (in seconds): 0.6360
Transformer throughput (in TFLOP/s): 230.489
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27200, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27200x81600, b=2048): 0.1425
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27200x81600, b=2048): 255.187
Elapsed time for attention_key_query_prob (256x2048x425x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x425x2048): 78.050
Elapsed time for attention_prob_times_values (256x2048x2048x425): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x425): 80.436
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27200x27200, b=2048): 0.0472
Throughput (in TFLOP/s) for attention_linear_projection (4x27200x27200, b=2048): 256.585
Elapsed time for mlp_h_to_4h (4x27200x108800, b=2048): 0.1915
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27200x108800, b=2048): 253.172
Elapsed time for mlp_fused_gelu (2048x4x108800): 0.0030
Elapsed time for mlp_4h_to_h (4x108800x27200, b=2048): 0.2036
Throughput (in TFLOP/s) for mlp_4h_to_h (4x108800x27200, b=2048): 238.156
Elapsed time for transformer_add_bias_dropout (2048x4x27200): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27200): 0.0011

Attention duration (in seconds): 0.2477
Attention throughput (in TFLOP/s): 203.087
MLP duration (in seconds): 0.3981
MLP throughput (in TFLOP/s): 243.593
Transformer duration (in seconds): 0.6521
Transformer throughput (in TFLOP/s): 225.878
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27264, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27264x81792, b=2048): 0.1436
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27264x81792, b=2048): 254.389
Elapsed time for attention_key_query_prob (256x2048x426x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x426x2048): 118.375
Elapsed time for attention_prob_times_values (256x2048x2048x426): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x426): 131.522
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27264x27264, b=2048): 0.0475
Throughput (in TFLOP/s) for attention_linear_projection (4x27264x27264, b=2048): 256.550
Elapsed time for mlp_h_to_4h (4x27264x109056, b=2048): 0.1923
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27264x109056, b=2048): 253.307
Elapsed time for mlp_fused_gelu (2048x4x109056): 0.0030
Elapsed time for mlp_4h_to_h (4x109056x27264, b=2048): 0.2036
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109056x27264, b=2048): 239.255
Elapsed time for transformer_add_bias_dropout (2048x4x27264): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27264): 0.0011

Attention duration (in seconds): 0.2407
Attention throughput (in TFLOP/s): 209.960
MLP duration (in seconds): 0.3989
MLP throughput (in TFLOP/s): 244.233
Transformer duration (in seconds): 0.6458
Transformer throughput (in TFLOP/s): 229.115
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27328, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27328x81984, b=2048): 0.1441
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27328x81984, b=2048): 254.760
Elapsed time for attention_key_query_prob (256x2048x427x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x427x2048): 78.265
Elapsed time for attention_prob_times_values (256x2048x2048x427): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x427): 83.052
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27328x27328, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_linear_projection (4x27328x27328, b=2048): 255.913
Elapsed time for mlp_h_to_4h (4x27328x109312, b=2048): 0.1934
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27328x109312, b=2048): 253.033
Elapsed time for mlp_fused_gelu (2048x4x109312): 0.0030
Elapsed time for mlp_4h_to_h (4x109312x27328, b=2048): 0.2056
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109312x27328, b=2048): 238.059
Elapsed time for transformer_add_bias_dropout (2048x4x27328): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27328): 0.0011

Attention duration (in seconds): 0.2496
Attention throughput (in TFLOP/s): 203.428
MLP duration (in seconds): 0.4020
MLP throughput (in TFLOP/s): 243.488
Transformer duration (in seconds): 0.6579
Transformer throughput (in TFLOP/s): 225.967
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27392, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27392x82176, b=2048): 0.1450
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27392x82176, b=2048): 254.280
Elapsed time for attention_key_query_prob (256x2048x428x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x428x2048): 117.925
Elapsed time for attention_prob_times_values (256x2048x2048x428): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x428): 132.125
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27392x27392, b=2048): 0.0478
Throughput (in TFLOP/s) for attention_linear_projection (4x27392x27392, b=2048): 257.001
Elapsed time for mlp_h_to_4h (4x27392x109568, b=2048): 0.1942
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27392x109568, b=2048): 253.151
Elapsed time for mlp_fused_gelu (2048x4x109568): 0.0030
Elapsed time for mlp_4h_to_h (4x109568x27392, b=2048): 0.2060
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109568x27392, b=2048): 238.658
Elapsed time for transformer_add_bias_dropout (2048x4x27392): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27392): 0.0011

Attention duration (in seconds): 0.2426
Attention throughput (in TFLOP/s): 210.254
MLP duration (in seconds): 0.4033
MLP throughput (in TFLOP/s): 243.862
Transformer duration (in seconds): 0.6522
Transformer throughput (in TFLOP/s): 229.016
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27456, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27456x82368, b=2048): 0.1457
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27456x82368, b=2048): 254.362
Elapsed time for attention_key_query_prob (256x2048x429x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x429x2048): 78.800
Elapsed time for attention_prob_times_values (256x2048x2048x429): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x429): 83.292
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x27456x27456, b=2048): 0.0481
Throughput (in TFLOP/s) for attention_linear_projection (4x27456x27456, b=2048): 256.541
Elapsed time for mlp_h_to_4h (4x27456x109824, b=2048): 0.1961
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27456x109824, b=2048): 251.983
Elapsed time for mlp_fused_gelu (2048x4x109824): 0.0030
Elapsed time for mlp_4h_to_h (4x109824x27456, b=2048): 0.2078
Throughput (in TFLOP/s) for mlp_4h_to_h (4x109824x27456, b=2048): 237.773
Elapsed time for transformer_add_bias_dropout (2048x4x27456): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27456): 0.0011

Attention duration (in seconds): 0.2516
Attention throughput (in TFLOP/s): 203.718
MLP duration (in seconds): 0.4068
MLP throughput (in TFLOP/s): 242.863
Transformer duration (in seconds): 0.6647
Transformer throughput (in TFLOP/s): 225.754
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27520, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27520x82560, b=2048): 0.1459
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27520x82560, b=2048): 255.193
Elapsed time for attention_key_query_prob (256x2048x430x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x430x2048): 119.922
Elapsed time for attention_prob_times_values (256x2048x2048x430): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x430): 132.957
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x27520x27520, b=2048): 0.0483
Throughput (in TFLOP/s) for attention_linear_projection (4x27520x27520, b=2048): 257.090
Elapsed time for mlp_h_to_4h (4x27520x110080, b=2048): 0.1955
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27520x110080, b=2048): 253.823
Elapsed time for mlp_fused_gelu (2048x4x110080): 0.0030
Elapsed time for mlp_4h_to_h (4x110080x27520, b=2048): 0.2081
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110080x27520, b=2048): 238.528
Elapsed time for transformer_add_bias_dropout (2048x4x27520): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27520): 0.0011

Attention duration (in seconds): 0.2438
Attention throughput (in TFLOP/s): 211.202
MLP duration (in seconds): 0.4066
MLP throughput (in TFLOP/s): 244.113
Transformer duration (in seconds): 0.6566
Transformer throughput (in TFLOP/s): 229.577
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27584, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27584x82752, b=2048): 0.1470
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27584x82752, b=2048): 254.408
Elapsed time for attention_key_query_prob (256x2048x431x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x431x2048): 78.993
Elapsed time for attention_prob_times_values (256x2048x2048x431): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x431): 82.210
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x27584x27584, b=2048): 0.0482
Throughput (in TFLOP/s) for attention_linear_projection (4x27584x27584, b=2048): 258.395
Elapsed time for mlp_h_to_4h (4x27584x110336, b=2048): 0.1964
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27584x110336, b=2048): 253.834
Elapsed time for mlp_fused_gelu (2048x4x110336): 0.0030
Elapsed time for mlp_4h_to_h (4x110336x27584, b=2048): 0.2095
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110336x27584, b=2048): 238.034
Elapsed time for transformer_add_bias_dropout (2048x4x27584): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27584): 0.0011

Attention duration (in seconds): 0.2532
Attention throughput (in TFLOP/s): 204.254
MLP duration (in seconds): 0.4090
MLP throughput (in TFLOP/s): 243.863
Transformer duration (in seconds): 0.6684
Transformer throughput (in TFLOP/s): 226.564
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27648, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27648x82944, b=2048): 0.1473
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27648x82944, b=2048): 255.131
Elapsed time for attention_key_query_prob (256x2048x432x2048): 0.0048
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x432x2048): 194.277
Elapsed time for attention_prob_times_values (256x2048x2048x432): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x432): 205.863
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27648x27648, b=2048): 0.0484
Throughput (in TFLOP/s) for attention_linear_projection (4x27648x27648, b=2048): 259.010
Elapsed time for mlp_h_to_4h (4x27648x110592, b=2048): 0.1974
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27648x110592, b=2048): 253.824
Elapsed time for mlp_fused_gelu (2048x4x110592): 0.0030
Elapsed time for mlp_4h_to_h (4x110592x27648, b=2048): 0.2096
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110592x27648, b=2048): 239.001
Elapsed time for transformer_add_bias_dropout (2048x4x27648): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27648): 0.0011

Attention duration (in seconds): 0.2399
Attention throughput (in TFLOP/s): 216.578
MLP duration (in seconds): 0.4100
MLP throughput (in TFLOP/s): 244.369
Transformer duration (in seconds): 0.6561
Transformer throughput (in TFLOP/s): 231.884
Transformer - MLP - Attention (in seconds): 0.0062
========================================================================================================================
num_attention_heads: 64, hidden_size: 27712, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27712x83136, b=2048): 0.1482
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27712x83136, b=2048): 254.649
Elapsed time for attention_key_query_prob (256x2048x433x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x433x2048): 79.523
Elapsed time for attention_prob_times_values (256x2048x2048x433): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x433): 82.396
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27712x27712, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_linear_projection (4x27712x27712, b=2048): 255.073
Elapsed time for mlp_h_to_4h (4x27712x110848, b=2048): 0.1985
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27712x110848, b=2048): 253.560
Elapsed time for mlp_fused_gelu (2048x4x110848): 0.0030
Elapsed time for mlp_4h_to_h (4x110848x27712, b=2048): 0.2114
Throughput (in TFLOP/s) for mlp_4h_to_h (4x110848x27712, b=2048): 238.049
Elapsed time for transformer_add_bias_dropout (2048x4x27712): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27712): 0.0011

Attention duration (in seconds): 0.2555
Attention throughput (in TFLOP/s): 204.245
MLP duration (in seconds): 0.4130
MLP throughput (in TFLOP/s): 243.752
Transformer duration (in seconds): 0.6748
Transformer throughput (in TFLOP/s): 226.500
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27776, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27776x83328, b=2048): 0.1489
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27776x83328, b=2048): 254.627
Elapsed time for attention_key_query_prob (256x2048x434x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x434x2048): 120.397
Elapsed time for attention_prob_times_values (256x2048x2048x434): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x434): 133.726
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27776x27776, b=2048): 0.0493
Throughput (in TFLOP/s) for attention_linear_projection (4x27776x27776, b=2048): 256.358
Elapsed time for mlp_h_to_4h (4x27776x111104, b=2048): 0.1988
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27776x111104, b=2048): 254.381
Elapsed time for mlp_fused_gelu (2048x4x111104): 0.0030
Elapsed time for mlp_4h_to_h (4x111104x27776, b=2048): 0.2114
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111104x27776, b=2048): 239.143
Elapsed time for transformer_add_bias_dropout (2048x4x27776): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27776): 0.0011

Attention duration (in seconds): 0.2479
Attention throughput (in TFLOP/s): 211.464
MLP duration (in seconds): 0.4132
MLP throughput (in TFLOP/s): 244.711
Transformer duration (in seconds): 0.6675
Transformer throughput (in TFLOP/s): 230.050
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27840, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27840x83520, b=2048): 0.1501
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27840x83520, b=2048): 253.860
Elapsed time for attention_key_query_prob (256x2048x435x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x435x2048): 79.868
Elapsed time for attention_prob_times_values (256x2048x2048x435): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x435): 84.632
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27840x27840, b=2048): 0.0494
Throughput (in TFLOP/s) for attention_linear_projection (4x27840x27840, b=2048): 256.949
Elapsed time for mlp_h_to_4h (4x27840x111360, b=2048): 0.2008
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27840x111360, b=2048): 252.964
Elapsed time for mlp_fused_gelu (2048x4x111360): 0.0031
Elapsed time for mlp_4h_to_h (4x111360x27840, b=2048): 0.2134
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111360x27840, b=2048): 237.979
Elapsed time for transformer_add_bias_dropout (2048x4x27840): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27840): 0.0012

Attention duration (in seconds): 0.2572
Attention throughput (in TFLOP/s): 204.762
MLP duration (in seconds): 0.4173
MLP throughput (in TFLOP/s): 243.450
Transformer duration (in seconds): 0.6809
Transformer throughput (in TFLOP/s): 226.547
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 27904, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27904x83712, b=2048): 0.1502
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27904x83712, b=2048): 254.867
Elapsed time for attention_key_query_prob (256x2048x436x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x436x2048): 121.545
Elapsed time for attention_prob_times_values (256x2048x2048x436): 0.0069
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x436): 134.750
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27904x27904, b=2048): 0.0497
Throughput (in TFLOP/s) for attention_linear_projection (4x27904x27904, b=2048): 256.582
Elapsed time for mlp_h_to_4h (4x27904x111616, b=2048): 0.2010
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27904x111616, b=2048): 253.830
Elapsed time for mlp_fused_gelu (2048x4x111616): 0.0031
Elapsed time for mlp_4h_to_h (4x111616x27904, b=2048): 0.2145
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111616x27904, b=2048): 237.912
Elapsed time for transformer_add_bias_dropout (2048x4x27904): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27904): 0.0011

Attention duration (in seconds): 0.2495
Attention throughput (in TFLOP/s): 212.022
MLP duration (in seconds): 0.4186
MLP throughput (in TFLOP/s): 243.819
Transformer duration (in seconds): 0.6744
Transformer throughput (in TFLOP/s): 229.762
Transformer - MLP - Attention (in seconds): 0.0063
========================================================================================================================
num_attention_heads: 64, hidden_size: 27968, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x27968x83904, b=2048): 0.1513
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x27968x83904, b=2048): 254.153
Elapsed time for attention_key_query_prob (256x2048x437x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x437x2048): 80.787
Elapsed time for attention_prob_times_values (256x2048x2048x437): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x437): 84.849
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x27968x27968, b=2048): 0.0500
Throughput (in TFLOP/s) for attention_linear_projection (4x27968x27968, b=2048): 256.259
Elapsed time for mlp_h_to_4h (4x27968x111872, b=2048): 0.2025
Throughput (in TFLOP/s) for mlp_h_to_4h (4x27968x111872, b=2048): 253.088
Elapsed time for mlp_fused_gelu (2048x4x111872): 0.0031
Elapsed time for mlp_4h_to_h (4x111872x27968, b=2048): 0.2153
Throughput (in TFLOP/s) for mlp_4h_to_h (4x111872x27968, b=2048): 238.075
Elapsed time for transformer_add_bias_dropout (2048x4x27968): 0.0020
Elapsed time for transformer_layer_norm (2048x4x27968): 0.0012

Attention duration (in seconds): 0.2589
Attention throughput (in TFLOP/s): 205.243
MLP duration (in seconds): 0.4209
MLP throughput (in TFLOP/s): 243.566
Transformer duration (in seconds): 0.6862
Transformer throughput (in TFLOP/s): 226.841
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28032, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28032x84096, b=2048): 0.1529
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28032x84096, b=2048): 252.631
Elapsed time for attention_key_query_prob (256x2048x438x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x438x2048): 121.134
Elapsed time for attention_prob_times_values (256x2048x2048x438): 0.0073
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x438): 129.135
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28032x28032, b=2048): 0.0498
Throughput (in TFLOP/s) for attention_linear_projection (4x28032x28032, b=2048): 258.689
Elapsed time for mlp_h_to_4h (4x28032x112128, b=2048): 0.2029
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28032x112128, b=2048): 253.776
Elapsed time for mlp_fused_gelu (2048x4x112128): 0.0031
Elapsed time for mlp_4h_to_h (4x112128x28032, b=2048): 0.2159
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112128x28032, b=2048): 238.558
Elapsed time for transformer_add_bias_dropout (2048x4x28032): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28032): 0.0011

Attention duration (in seconds): 0.2526
Attention throughput (in TFLOP/s): 211.283
MLP duration (in seconds): 0.4219
MLP throughput (in TFLOP/s): 244.141
Transformer duration (in seconds): 0.6809
Transformer throughput (in TFLOP/s): 229.661
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28096x84288, b=2048): 0.1520
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28096x84288, b=2048): 255.323
Elapsed time for attention_key_query_prob (256x2048x439x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x439x2048): 80.310
Elapsed time for attention_prob_times_values (256x2048x2048x439): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x439): 83.352
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28096x28096, b=2048): 0.0506
Throughput (in TFLOP/s) for attention_linear_projection (4x28096x28096, b=2048): 255.559
Elapsed time for mlp_h_to_4h (4x28096x112384, b=2048): 0.2038
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28096x112384, b=2048): 253.843
Elapsed time for mlp_fused_gelu (2048x4x112384): 0.0031
Elapsed time for mlp_4h_to_h (4x112384x28096, b=2048): 0.2171
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112384x28096, b=2048): 238.258
Elapsed time for transformer_add_bias_dropout (2048x4x28096): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28096): 0.0012

Attention duration (in seconds): 0.2606
Attention throughput (in TFLOP/s): 205.779
MLP duration (in seconds): 0.4240
MLP throughput (in TFLOP/s): 244.016
Transformer duration (in seconds): 0.6910
Transformer throughput (in TFLOP/s): 227.322
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28160, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28160x84480, b=2048): 0.1534
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28160x84480, b=2048): 254.075
Elapsed time for attention_key_query_prob (256x2048x440x2048): 0.0051
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x440x2048): 185.756
Elapsed time for attention_prob_times_values (256x2048x2048x440): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x440): 204.619
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x28160x28160, b=2048): 0.0507
Throughput (in TFLOP/s) for attention_linear_projection (4x28160x28160, b=2048): 256.169
Elapsed time for mlp_h_to_4h (4x28160x112640, b=2048): 0.2050
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28160x112640, b=2048): 253.483
Elapsed time for mlp_fused_gelu (2048x4x112640): 0.0031
Elapsed time for mlp_4h_to_h (4x112640x28160, b=2048): 0.2178
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112640x28160, b=2048): 238.580
Elapsed time for transformer_add_bias_dropout (2048x4x28160): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28160): 0.0011

Attention duration (in seconds): 0.2488
Attention throughput (in TFLOP/s): 216.451
MLP duration (in seconds): 0.4259
MLP throughput (in TFLOP/s): 244.026
Transformer duration (in seconds): 0.6812
Transformer throughput (in TFLOP/s): 231.656
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28224, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28224x84672, b=2048): 0.1534
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28224x84672, b=2048): 255.266
Elapsed time for attention_key_query_prob (256x2048x441x2048): 0.0117
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x441x2048): 80.818
Elapsed time for attention_prob_times_values (256x2048x2048x441): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x441): 83.488
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28224x28224, b=2048): 0.0509
Throughput (in TFLOP/s) for attention_linear_projection (4x28224x28224, b=2048): 256.611
Elapsed time for mlp_h_to_4h (4x28224x112896, b=2048): 0.2063
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28224x112896, b=2048): 252.996
Elapsed time for mlp_fused_gelu (2048x4x112896): 0.0031
Elapsed time for mlp_4h_to_h (4x112896x28224, b=2048): 0.2190
Throughput (in TFLOP/s) for mlp_4h_to_h (4x112896x28224, b=2048): 238.411
Elapsed time for transformer_add_bias_dropout (2048x4x28224): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28224): 0.0012

Attention duration (in seconds): 0.2623
Attention throughput (in TFLOP/s): 206.277
MLP duration (in seconds): 0.4284
MLP throughput (in TFLOP/s): 243.715
Transformer duration (in seconds): 0.6971
Transformer throughput (in TFLOP/s): 227.377
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28288, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28288x84864, b=2048): 0.1544
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28288x84864, b=2048): 254.805
Elapsed time for attention_key_query_prob (256x2048x442x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x442x2048): 123.088
Elapsed time for attention_prob_times_values (256x2048x2048x442): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x442): 136.061
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28288x28288, b=2048): 0.0510
Throughput (in TFLOP/s) for attention_linear_projection (4x28288x28288, b=2048): 257.145
Elapsed time for mlp_h_to_4h (4x28288x113152, b=2048): 0.2072
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28288x113152, b=2048): 253.137
Elapsed time for mlp_fused_gelu (2048x4x113152): 0.0031
Elapsed time for mlp_4h_to_h (4x113152x28288, b=2048): 0.2198
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113152x28288, b=2048): 238.603
Elapsed time for transformer_add_bias_dropout (2048x4x28288): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28288): 0.0011

Attention duration (in seconds): 0.2550
Attention throughput (in TFLOP/s): 213.111
MLP duration (in seconds): 0.4301
MLP throughput (in TFLOP/s): 243.884
Transformer duration (in seconds): 0.6915
Transformer throughput (in TFLOP/s): 230.277
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28352, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28352x85056, b=2048): 0.1559
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28352x85056, b=2048): 253.409
Elapsed time for attention_key_query_prob (256x2048x443x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x443x2048): 81.916
Elapsed time for attention_prob_times_values (256x2048x2048x443): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x443): 86.582
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0319
Elapsed time for attention_linear_projection (4x28352x28352, b=2048): 0.0512
Throughput (in TFLOP/s) for attention_linear_projection (4x28352x28352, b=2048): 256.988
Elapsed time for mlp_h_to_4h (4x28352x113408, b=2048): 0.2097
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28352x113408, b=2048): 251.269
Elapsed time for mlp_fused_gelu (2048x4x113408): 0.0031
Elapsed time for mlp_4h_to_h (4x113408x28352, b=2048): 0.2216
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113408x28352, b=2048): 237.701
Elapsed time for transformer_add_bias_dropout (2048x4x28352): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28352): 0.0012

Attention duration (in seconds): 0.2662
Attention throughput (in TFLOP/s): 205.055
MLP duration (in seconds): 0.4344
MLP throughput (in TFLOP/s): 242.551
Transformer duration (in seconds): 0.7070
Transformer throughput (in TFLOP/s): 226.219
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28416, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28416x85248, b=2048): 0.1561
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28416x85248, b=2048): 254.184
Elapsed time for attention_key_query_prob (256x2048x444x2048): 0.0077
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x444x2048): 123.589
Elapsed time for attention_prob_times_values (256x2048x2048x444): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x444): 137.172
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x28416x28416, b=2048): 0.0512
Throughput (in TFLOP/s) for attention_linear_projection (4x28416x28416, b=2048): 258.421
Elapsed time for mlp_h_to_4h (4x28416x113664, b=2048): 0.2092
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28416x113664, b=2048): 252.907
Elapsed time for mlp_fused_gelu (2048x4x113664): 0.0031
Elapsed time for mlp_4h_to_h (4x113664x28416, b=2048): 0.2220
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113664x28416, b=2048): 238.352
Elapsed time for transformer_add_bias_dropout (2048x4x28416): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28416): 0.0011

Attention duration (in seconds): 0.2570
Attention throughput (in TFLOP/s): 213.311
MLP duration (in seconds): 0.4344
MLP throughput (in TFLOP/s): 243.651
Transformer duration (in seconds): 0.6978
Transformer throughput (in TFLOP/s): 230.232
Transformer - MLP - Attention (in seconds): 0.0064
========================================================================================================================
num_attention_heads: 64, hidden_size: 28480, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28480x85440, b=2048): 0.1567
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28480x85440, b=2048): 254.342
Elapsed time for attention_key_query_prob (256x2048x445x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x445x2048): 82.682
Elapsed time for attention_prob_times_values (256x2048x2048x445): 0.0110
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x445): 87.007
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28480x28480, b=2048): 0.0520
Throughput (in TFLOP/s) for attention_linear_projection (4x28480x28480, b=2048): 255.449
Elapsed time for mlp_h_to_4h (4x28480x113920, b=2048): 0.2107
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28480x113920, b=2048): 252.286
Elapsed time for mlp_fused_gelu (2048x4x113920): 0.0031
Elapsed time for mlp_4h_to_h (4x113920x28480, b=2048): 0.2236
Throughput (in TFLOP/s) for mlp_4h_to_h (4x113920x28480, b=2048): 237.764
Elapsed time for transformer_add_bias_dropout (2048x4x28480): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28480): 0.0012

Attention duration (in seconds): 0.2663
Attention throughput (in TFLOP/s): 206.801
MLP duration (in seconds): 0.4374
MLP throughput (in TFLOP/s): 243.061
Transformer duration (in seconds): 0.7102
Transformer throughput (in TFLOP/s): 227.238
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28544, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28544x85632, b=2048): 0.1575
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28544x85632, b=2048): 254.340
Elapsed time for attention_key_query_prob (256x2048x446x2048): 0.0078
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x446x2048): 123.130
Elapsed time for attention_prob_times_values (256x2048x2048x446): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x446): 137.530
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28544x28544, b=2048): 0.0520
Throughput (in TFLOP/s) for attention_linear_projection (4x28544x28544, b=2048): 256.500
Elapsed time for mlp_h_to_4h (4x28544x114176, b=2048): 0.2117
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28544x114176, b=2048): 252.223
Elapsed time for mlp_fused_gelu (2048x4x114176): 0.0031
Elapsed time for mlp_4h_to_h (4x114176x28544, b=2048): 0.2240
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114176x28544, b=2048): 238.376
Elapsed time for transformer_add_bias_dropout (2048x4x28544): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28544): 0.0011

Attention duration (in seconds): 0.2592
Attention throughput (in TFLOP/s): 213.378
MLP duration (in seconds): 0.4388
MLP throughput (in TFLOP/s): 243.355
Transformer duration (in seconds): 0.7045
Transformer throughput (in TFLOP/s): 230.090
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28608, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28608x85824, b=2048): 0.1580
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28608x85824, b=2048): 254.610
Elapsed time for attention_key_query_prob (256x2048x447x2048): 0.0116
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x447x2048): 82.857
Elapsed time for attention_prob_times_values (256x2048x2048x447): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x447): 86.110
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28608x28608, b=2048): 0.0525
Throughput (in TFLOP/s) for attention_linear_projection (4x28608x28608, b=2048): 255.364
Elapsed time for mlp_h_to_4h (4x28608x114432, b=2048): 0.2110
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28608x114432, b=2048): 254.211
Elapsed time for mlp_fused_gelu (2048x4x114432): 0.0031
Elapsed time for mlp_4h_to_h (4x114432x28608, b=2048): 0.2250
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114432x28608, b=2048): 238.398
Elapsed time for transformer_add_bias_dropout (2048x4x28608): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28608): 0.0012

Attention duration (in seconds): 0.2682
Attention throughput (in TFLOP/s): 207.134
MLP duration (in seconds): 0.4391
MLP throughput (in TFLOP/s): 244.293
Transformer duration (in seconds): 0.7139
Transformer throughput (in TFLOP/s): 228.092
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28672, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28672x86016, b=2048): 0.1585
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28672x86016, b=2048): 254.894
Elapsed time for attention_key_query_prob (256x2048x448x2048): 0.0047
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x448x2048): 203.001
Elapsed time for attention_prob_times_values (256x2048x2048x448): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x448): 212.502
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28672x28672, b=2048): 0.0524
Throughput (in TFLOP/s) for attention_linear_projection (4x28672x28672, b=2048): 256.956
Elapsed time for mlp_h_to_4h (4x28672x114688, b=2048): 0.2127
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28672x114688, b=2048): 253.324
Elapsed time for mlp_fused_gelu (2048x4x114688): 0.0031
Elapsed time for mlp_4h_to_h (4x114688x28672, b=2048): 0.2249
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114688x28672, b=2048): 239.514
Elapsed time for transformer_add_bias_dropout (2048x4x28672): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28672): 0.0012

Attention duration (in seconds): 0.2552
Attention throughput (in TFLOP/s): 218.658
MLP duration (in seconds): 0.4408
MLP throughput (in TFLOP/s): 244.471
Transformer duration (in seconds): 0.7024
Transformer throughput (in TFLOP/s): 232.832
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28736, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28736x86208, b=2048): 0.1594
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28736x86208, b=2048): 254.679
Elapsed time for attention_key_query_prob (256x2048x449x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x449x2048): 81.352
Elapsed time for attention_prob_times_values (256x2048x2048x449): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x449): 85.440
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28736x28736, b=2048): 0.0528
Throughput (in TFLOP/s) for attention_linear_projection (4x28736x28736, b=2048): 256.288
Elapsed time for mlp_h_to_4h (4x28736x114944, b=2048): 0.2134
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28736x114944, b=2048): 253.548
Elapsed time for mlp_fused_gelu (2048x4x114944): 0.0031
Elapsed time for mlp_4h_to_h (4x114944x28736, b=2048): 0.2263
Throughput (in TFLOP/s) for mlp_4h_to_h (4x114944x28736, b=2048): 239.152
Elapsed time for transformer_add_bias_dropout (2048x4x28736): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28736): 0.0012

Attention duration (in seconds): 0.2703
Attention throughput (in TFLOP/s): 207.367
MLP duration (in seconds): 0.4429
MLP throughput (in TFLOP/s): 244.390
Transformer duration (in seconds): 0.7197
Transformer throughput (in TFLOP/s): 228.256
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28800, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28800x86400, b=2048): 0.1597
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28800x86400, b=2048): 255.210
Elapsed time for attention_key_query_prob (256x2048x450x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x450x2048): 117.030
Elapsed time for attention_prob_times_values (256x2048x2048x450): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x450): 138.155
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28800x28800, b=2048): 0.0529
Throughput (in TFLOP/s) for attention_linear_projection (4x28800x28800, b=2048): 256.837
Elapsed time for mlp_h_to_4h (4x28800x115200, b=2048): 0.2147
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28800x115200, b=2048): 253.179
Elapsed time for mlp_fused_gelu (2048x4x115200): 0.0032
Elapsed time for mlp_4h_to_h (4x115200x28800, b=2048): 0.2267
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115200x28800, b=2048): 239.732
Elapsed time for transformer_add_bias_dropout (2048x4x28800): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28800): 0.0012

Attention duration (in seconds): 0.2629
Attention throughput (in TFLOP/s): 214.130
MLP duration (in seconds): 0.4446
MLP throughput (in TFLOP/s): 244.525
Transformer duration (in seconds): 0.7140
Transformer throughput (in TFLOP/s): 231.097
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28864, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28864x86592, b=2048): 0.1614
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28864x86592, b=2048): 253.726
Elapsed time for attention_key_query_prob (256x2048x451x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x451x2048): 81.506
Elapsed time for attention_prob_times_values (256x2048x2048x451): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x451): 87.429
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28864x28864, b=2048): 0.0529
Throughput (in TFLOP/s) for attention_linear_projection (4x28864x28864, b=2048): 257.823
Elapsed time for mlp_h_to_4h (4x28864x115456, b=2048): 0.2164
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28864x115456, b=2048): 252.288
Elapsed time for mlp_fused_gelu (2048x4x115456): 0.0032
Elapsed time for mlp_4h_to_h (4x115456x28864, b=2048): 0.2292
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115456x28864, b=2048): 238.244
Elapsed time for transformer_add_bias_dropout (2048x4x28864): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28864): 0.0012

Attention duration (in seconds): 0.2723
Attention throughput (in TFLOP/s): 207.648
MLP duration (in seconds): 0.4488
MLP throughput (in TFLOP/s): 243.338
Transformer duration (in seconds): 0.7276
Transformer throughput (in TFLOP/s): 227.779
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 28928, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28928x86784, b=2048): 0.1618
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28928x86784, b=2048): 254.237
Elapsed time for attention_key_query_prob (256x2048x452x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x452x2048): 117.302
Elapsed time for attention_prob_times_values (256x2048x2048x452): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x452): 137.671
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28928x28928, b=2048): 0.0533
Throughput (in TFLOP/s) for attention_linear_projection (4x28928x28928, b=2048): 257.330
Elapsed time for mlp_h_to_4h (4x28928x115712, b=2048): 0.2170
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28928x115712, b=2048): 252.686
Elapsed time for mlp_fused_gelu (2048x4x115712): 0.0032
Elapsed time for mlp_4h_to_h (4x115712x28928, b=2048): 0.2305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115712x28928, b=2048): 237.892
Elapsed time for transformer_add_bias_dropout (2048x4x28928): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28928): 0.0012

Attention duration (in seconds): 0.2653
Attention throughput (in TFLOP/s): 214.001
MLP duration (in seconds): 0.4507
MLP throughput (in TFLOP/s): 243.344
Transformer duration (in seconds): 0.7226
Transformer throughput (in TFLOP/s): 230.365
Transformer - MLP - Attention (in seconds): 0.0065
========================================================================================================================
num_attention_heads: 64, hidden_size: 28992, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x28992x86976, b=2048): 0.1631
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x28992x86976, b=2048): 253.287
Elapsed time for attention_key_query_prob (256x2048x453x2048): 0.0119
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x453x2048): 81.645
Elapsed time for attention_prob_times_values (256x2048x2048x453): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x453): 87.638
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x28992x28992, b=2048): 0.0541
Throughput (in TFLOP/s) for attention_linear_projection (4x28992x28992, b=2048): 254.775
Elapsed time for mlp_h_to_4h (4x28992x115968, b=2048): 0.2182
Throughput (in TFLOP/s) for mlp_h_to_4h (4x28992x115968, b=2048): 252.473
Elapsed time for mlp_fused_gelu (2048x4x115968): 0.0032
Elapsed time for mlp_4h_to_h (4x115968x28992, b=2048): 0.2305
Throughput (in TFLOP/s) for mlp_4h_to_h (4x115968x28992, b=2048): 239.023
Elapsed time for transformer_add_bias_dropout (2048x4x28992): 0.0021
Elapsed time for transformer_layer_norm (2048x4x28992): 0.0012

Attention duration (in seconds): 0.2751
Attention throughput (in TFLOP/s): 207.279
MLP duration (in seconds): 0.4518
MLP throughput (in TFLOP/s): 243.839
Transformer duration (in seconds): 0.7336
Transformer throughput (in TFLOP/s): 227.929
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29056, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29056x87168, b=2048): 0.1627
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29056x87168, b=2048): 254.981
Elapsed time for attention_key_query_prob (256x2048x454x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x454x2048): 117.920
Elapsed time for attention_prob_times_values (256x2048x2048x454): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x454): 137.240
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29056x29056, b=2048): 0.0539
Throughput (in TFLOP/s) for attention_linear_projection (4x29056x29056, b=2048): 256.483
Elapsed time for mlp_h_to_4h (4x29056x116224, b=2048): 0.2194
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29056x116224, b=2048): 252.127
Elapsed time for mlp_fused_gelu (2048x4x116224): 0.0032
Elapsed time for mlp_4h_to_h (4x116224x29056, b=2048): 0.2314
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116224x29056, b=2048): 239.098
Elapsed time for transformer_add_bias_dropout (2048x4x29056): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29056): 0.0012

Attention duration (in seconds): 0.2670
Attention throughput (in TFLOP/s): 214.523
MLP duration (in seconds): 0.4540
MLP throughput (in TFLOP/s): 243.718
Transformer duration (in seconds): 0.7276
Transformer throughput (in TFLOP/s): 230.804
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29120, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29120x87360, b=2048): 0.1635
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29120x87360, b=2048): 254.851
Elapsed time for attention_key_query_prob (256x2048x455x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x455x2048): 80.940
Elapsed time for attention_prob_times_values (256x2048x2048x455): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x455): 85.150
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29120x29120, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_linear_projection (4x29120x29120, b=2048): 255.745
Elapsed time for mlp_h_to_4h (4x29120x116480, b=2048): 0.2190
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29120x116480, b=2048): 253.764
Elapsed time for mlp_fused_gelu (2048x4x116480): 0.0032
Elapsed time for mlp_4h_to_h (4x116480x29120, b=2048): 0.2333
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116480x29120, b=2048): 238.234
Elapsed time for transformer_add_bias_dropout (2048x4x29120): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29120): 0.0012

Attention duration (in seconds): 0.2764
Attention throughput (in TFLOP/s): 208.150
MLP duration (in seconds): 0.4555
MLP throughput (in TFLOP/s): 244.034
Transformer duration (in seconds): 0.7385
Transformer throughput (in TFLOP/s): 228.405
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29184, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29184x87552, b=2048): 0.1647
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29184x87552, b=2048): 254.139
Elapsed time for attention_key_query_prob (256x2048x456x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x456x2048): 186.052
Elapsed time for attention_prob_times_values (256x2048x2048x456): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x456): 209.552
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29184x29184, b=2048): 0.0543
Throughput (in TFLOP/s) for attention_linear_projection (4x29184x29184, b=2048): 257.162
Elapsed time for mlp_h_to_4h (4x29184x116736, b=2048): 0.2212
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29184x116736, b=2048): 252.303
Elapsed time for mlp_fused_gelu (2048x4x116736): 0.0032
Elapsed time for mlp_4h_to_h (4x116736x29184, b=2048): 0.2337
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116736x29184, b=2048): 238.837
Elapsed time for transformer_add_bias_dropout (2048x4x29184): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29184): 0.0012

Attention duration (in seconds): 0.2639
Attention throughput (in TFLOP/s): 218.968
MLP duration (in seconds): 0.4581
MLP throughput (in TFLOP/s): 243.674
Transformer duration (in seconds): 0.7286
Transformer throughput (in TFLOP/s): 232.512
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29248, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29248x87744, b=2048): 0.1650
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29248x87744, b=2048): 254.757
Elapsed time for attention_key_query_prob (256x2048x457x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x457x2048): 80.968
Elapsed time for attention_prob_times_values (256x2048x2048x457): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x457): 84.595
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29248x29248, b=2048): 0.0544
Throughput (in TFLOP/s) for attention_linear_projection (4x29248x29248, b=2048): 257.695
Elapsed time for mlp_h_to_4h (4x29248x116992, b=2048): 0.2216
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29248x116992, b=2048): 252.976
Elapsed time for mlp_fused_gelu (2048x4x116992): 0.0032
Elapsed time for mlp_4h_to_h (4x116992x29248, b=2048): 0.2358
Throughput (in TFLOP/s) for mlp_4h_to_h (4x116992x29248, b=2048): 237.783
Elapsed time for transformer_add_bias_dropout (2048x4x29248): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29248): 0.0012

Attention duration (in seconds): 0.2781
Attention throughput (in TFLOP/s): 208.618
MLP duration (in seconds): 0.4606
MLP throughput (in TFLOP/s): 243.439
Transformer duration (in seconds): 0.7454
Transformer throughput (in TFLOP/s): 228.269
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29312, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29312x87936, b=2048): 0.1664
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29312x87936, b=2048): 253.770
Elapsed time for attention_key_query_prob (256x2048x458x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x458x2048): 118.076
Elapsed time for attention_prob_times_values (256x2048x2048x458): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x458): 136.741
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29312x29312, b=2048): 0.0546
Throughput (in TFLOP/s) for attention_linear_projection (4x29312x29312, b=2048): 257.631
Elapsed time for mlp_h_to_4h (4x29312x117248, b=2048): 0.2239
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29312x117248, b=2048): 251.535
Elapsed time for mlp_fused_gelu (2048x4x117248): 0.0032
Elapsed time for mlp_4h_to_h (4x117248x29312, b=2048): 0.2359
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117248x29312, b=2048): 238.715
Elapsed time for transformer_add_bias_dropout (2048x4x29312): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29312): 0.0012

Attention duration (in seconds): 0.2715
Attention throughput (in TFLOP/s): 214.603
MLP duration (in seconds): 0.4629
MLP throughput (in TFLOP/s): 243.258
Transformer duration (in seconds): 0.7411
Transformer throughput (in TFLOP/s): 230.581
Transformer - MLP - Attention (in seconds): 0.0066
========================================================================================================================
num_attention_heads: 64, hidden_size: 29376, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29376x88128, b=2048): 0.1667
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29376x88128, b=2048): 254.434
Elapsed time for attention_key_query_prob (256x2048x459x2048): 0.0120
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x459x2048): 82.017
Elapsed time for attention_prob_times_values (256x2048x2048x459): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x459): 88.131
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29376x29376, b=2048): 0.0553
Throughput (in TFLOP/s) for attention_linear_projection (4x29376x29376, b=2048): 255.697
Elapsed time for mlp_h_to_4h (4x29376x117504, b=2048): 0.2228
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29376x117504, b=2048): 253.812
Elapsed time for mlp_fused_gelu (2048x4x117504): 0.0032
Elapsed time for mlp_4h_to_h (4x117504x29376, b=2048): 0.2370
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117504x29376, b=2048): 238.633
Elapsed time for transformer_add_bias_dropout (2048x4x29376): 0.0021
Elapsed time for transformer_layer_norm (2048x4x29376): 0.0012

Attention duration (in seconds): 0.2802
Attention throughput (in TFLOP/s): 208.877
MLP duration (in seconds): 0.4630
MLP throughput (in TFLOP/s): 244.281
Transformer duration (in seconds): 0.7499
Transformer throughput (in TFLOP/s): 228.870
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29440, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29440x88320, b=2048): 0.1680
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29440x88320, b=2048): 253.510
Elapsed time for attention_key_query_prob (256x2048x460x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x460x2048): 118.930
Elapsed time for attention_prob_times_values (256x2048x2048x460): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x460): 138.112
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29440x29440, b=2048): 0.0554
Throughput (in TFLOP/s) for attention_linear_projection (4x29440x29440, b=2048): 256.376
Elapsed time for mlp_h_to_4h (4x29440x117760, b=2048): 0.2249
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29440x117760, b=2048): 252.615
Elapsed time for mlp_fused_gelu (2048x4x117760): 0.0032
Elapsed time for mlp_4h_to_h (4x117760x29440, b=2048): 0.2381
Throughput (in TFLOP/s) for mlp_4h_to_h (4x117760x29440, b=2048): 238.576
Elapsed time for transformer_add_bias_dropout (2048x4x29440): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29440): 0.0012

Attention duration (in seconds): 0.2738
Attention throughput (in TFLOP/s): 214.643
MLP duration (in seconds): 0.4662
MLP throughput (in TFLOP/s): 243.698
Transformer duration (in seconds): 0.7467
Transformer throughput (in TFLOP/s): 230.867
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29504, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29504x88512, b=2048): 0.1681
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29504x88512, b=2048): 254.571
Elapsed time for attention_key_query_prob (256x2048x461x2048): 0.0121
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x461x2048): 81.699
Elapsed time for attention_prob_times_values (256x2048x2048x461): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x461): 88.278
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29504x29504, b=2048): 0.0559
Throughput (in TFLOP/s) for attention_linear_projection (4x29504x29504, b=2048): 255.296
Elapsed time for mlp_h_to_4h (4x29504x118016, b=2048): 0.2256
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29504x118016, b=2048): 252.819
Elapsed time for mlp_fused_gelu (2048x4x118016): 0.0032
Elapsed time for mlp_4h_to_h (4x118016x29504, b=2048): 0.2402
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118016x29504, b=2048): 237.510
Elapsed time for transformer_add_bias_dropout (2048x4x29504): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29504): 0.0012

Attention duration (in seconds): 0.2823
Attention throughput (in TFLOP/s): 209.130
MLP duration (in seconds): 0.4691
MLP throughput (in TFLOP/s): 243.238
Transformer duration (in seconds): 0.7581
Transformer throughput (in TFLOP/s): 228.375
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29568, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29568x88704, b=2048): 0.1690
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29568x88704, b=2048): 254.218
Elapsed time for attention_key_query_prob (256x2048x462x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x462x2048): 119.132
Elapsed time for attention_prob_times_values (256x2048x2048x462): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x462): 138.253
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29568x29568, b=2048): 0.0560
Throughput (in TFLOP/s) for attention_linear_projection (4x29568x29568, b=2048): 255.748
Elapsed time for mlp_h_to_4h (4x29568x118272, b=2048): 0.2277
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29568x118272, b=2048): 251.581
Elapsed time for mlp_fused_gelu (2048x4x118272): 0.0032
Elapsed time for mlp_4h_to_h (4x118272x29568, b=2048): 0.2401
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118272x29568, b=2048): 238.679
Elapsed time for transformer_add_bias_dropout (2048x4x29568): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29568): 0.0012

Attention duration (in seconds): 0.2755
Attention throughput (in TFLOP/s): 215.149
MLP duration (in seconds): 0.4710
MLP throughput (in TFLOP/s): 243.277
Transformer duration (in seconds): 0.7533
Transformer throughput (in TFLOP/s): 230.813
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29632, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29632x88896, b=2048): 0.1695
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29632x88896, b=2048): 254.597
Elapsed time for attention_key_query_prob (256x2048x463x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x463x2048): 77.823
Elapsed time for attention_prob_times_values (256x2048x2048x463): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x463): 86.618
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29632x29632, b=2048): 0.0562
Throughput (in TFLOP/s) for attention_linear_projection (4x29632x29632, b=2048): 256.085
Elapsed time for mlp_h_to_4h (4x29632x118528, b=2048): 0.2268
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29632x118528, b=2048): 253.776
Elapsed time for mlp_fused_gelu (2048x4x118528): 0.0032
Elapsed time for mlp_4h_to_h (4x118528x29632, b=2048): 0.2407
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118528x29632, b=2048): 239.063
Elapsed time for transformer_add_bias_dropout (2048x4x29632): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29632): 0.0012

Attention duration (in seconds): 0.2849
Attention throughput (in TFLOP/s): 208.944
MLP duration (in seconds): 0.4707
MLP throughput (in TFLOP/s): 244.503
Transformer duration (in seconds): 0.7624
Transformer throughput (in TFLOP/s): 229.048
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29696, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29696x89088, b=2048): 0.1704
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29696x89088, b=2048): 254.428
Elapsed time for attention_key_query_prob (256x2048x464x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x464x2048): 198.854
Elapsed time for attention_prob_times_values (256x2048x2048x464): 0.0045
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x464): 219.561
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29696x29696, b=2048): 0.0563
Throughput (in TFLOP/s) for attention_linear_projection (4x29696x29696, b=2048): 256.442
Elapsed time for mlp_h_to_4h (4x29696x118784, b=2048): 0.2288
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29696x118784, b=2048): 252.625
Elapsed time for mlp_fused_gelu (2048x4x118784): 0.0033
Elapsed time for mlp_4h_to_h (4x118784x29696, b=2048): 0.2423
Throughput (in TFLOP/s) for mlp_4h_to_h (4x118784x29696, b=2048): 238.508
Elapsed time for transformer_add_bias_dropout (2048x4x29696): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29696): 0.0012

Attention duration (in seconds): 0.2712
Attention throughput (in TFLOP/s): 220.446
MLP duration (in seconds): 0.4743
MLP throughput (in TFLOP/s): 243.680
Transformer duration (in seconds): 0.7523
Transformer throughput (in TFLOP/s): 233.122
Transformer - MLP - Attention (in seconds): 0.0067
========================================================================================================================
num_attention_heads: 64, hidden_size: 29760, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29760x89280, b=2048): 0.1711
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29760x89280, b=2048): 254.494
Elapsed time for attention_key_query_prob (256x2048x465x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x465x2048): 81.877
Elapsed time for attention_prob_times_values (256x2048x2048x465): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x465): 86.771
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29760x29760, b=2048): 0.0567
Throughput (in TFLOP/s) for attention_linear_projection (4x29760x29760, b=2048): 256.016
Elapsed time for mlp_h_to_4h (4x29760x119040, b=2048): 0.2303
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29760x119040, b=2048): 252.008
Elapsed time for mlp_fused_gelu (2048x4x119040): 0.0033
Elapsed time for mlp_4h_to_h (4x119040x29760, b=2048): 0.2428
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119040x29760, b=2048): 239.017
Elapsed time for transformer_add_bias_dropout (2048x4x29760): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29760): 0.0012

Attention duration (in seconds): 0.2864
Attention throughput (in TFLOP/s): 209.633
MLP duration (in seconds): 0.4764
MLP throughput (in TFLOP/s): 243.660
Transformer duration (in seconds): 0.7696
Transformer throughput (in TFLOP/s): 228.844
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29824, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29824x89472, b=2048): 0.1713
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29824x89472, b=2048): 255.260
Elapsed time for attention_key_query_prob (256x2048x466x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x466x2048): 119.972
Elapsed time for attention_prob_times_values (256x2048x2048x466): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x466): 139.812
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29824x29824, b=2048): 0.0568
Throughput (in TFLOP/s) for attention_linear_projection (4x29824x29824, b=2048): 256.761
Elapsed time for mlp_h_to_4h (4x29824x119296, b=2048): 0.2306
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29824x119296, b=2048): 252.755
Elapsed time for mlp_fused_gelu (2048x4x119296): 0.0033
Elapsed time for mlp_4h_to_h (4x119296x29824, b=2048): 0.2445
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119296x29824, b=2048): 238.404
Elapsed time for transformer_add_bias_dropout (2048x4x29824): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29824): 0.0012

Attention duration (in seconds): 0.2785
Attention throughput (in TFLOP/s): 216.498
MLP duration (in seconds): 0.4784
MLP throughput (in TFLOP/s): 243.693
Transformer duration (in seconds): 0.7637
Transformer throughput (in TFLOP/s): 231.614
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29888, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29888x89664, b=2048): 0.1731
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29888x89664, b=2048): 253.624
Elapsed time for attention_key_query_prob (256x2048x467x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x467x2048): 82.055
Elapsed time for attention_prob_times_values (256x2048x2048x467): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x467): 89.173
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29888x29888, b=2048): 0.0573
Throughput (in TFLOP/s) for attention_linear_projection (4x29888x29888, b=2048): 255.510
Elapsed time for mlp_h_to_4h (4x29888x119552, b=2048): 0.2332
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29888x119552, b=2048): 250.995
Elapsed time for mlp_fused_gelu (2048x4x119552): 0.0033
Elapsed time for mlp_4h_to_h (4x119552x29888, b=2048): 0.2448
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119552x29888, b=2048): 239.188
Elapsed time for transformer_add_bias_dropout (2048x4x29888): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29888): 0.0012

Attention duration (in seconds): 0.2888
Attention throughput (in TFLOP/s): 209.626
MLP duration (in seconds): 0.4813
MLP throughput (in TFLOP/s): 243.286
Transformer duration (in seconds): 0.7769
Transformer throughput (in TFLOP/s): 228.638
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 29952, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x29952x89856, b=2048): 0.1741
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x29952x89856, b=2048): 253.231
Elapsed time for attention_key_query_prob (256x2048x468x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x468x2048): 120.948
Elapsed time for attention_prob_times_values (256x2048x2048x468): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x468): 139.887
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x29952x29952, b=2048): 0.0573
Throughput (in TFLOP/s) for attention_linear_projection (4x29952x29952, b=2048): 256.674
Elapsed time for mlp_h_to_4h (4x29952x119808, b=2048): 0.2341
Throughput (in TFLOP/s) for mlp_h_to_4h (4x29952x119808, b=2048): 251.153
Elapsed time for mlp_fused_gelu (2048x4x119808): 0.0033
Elapsed time for mlp_4h_to_h (4x119808x29952, b=2048): 0.2459
Throughput (in TFLOP/s) for mlp_4h_to_h (4x119808x29952, b=2048): 239.120
Elapsed time for transformer_add_bias_dropout (2048x4x29952): 0.0022
Elapsed time for transformer_layer_norm (2048x4x29952): 0.0012

Attention duration (in seconds): 0.2818
Attention throughput (in TFLOP/s): 215.733
MLP duration (in seconds): 0.4833
MLP throughput (in TFLOP/s): 243.326
Transformer duration (in seconds): 0.7719
Transformer throughput (in TFLOP/s): 231.115
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30016, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30016x90048, b=2048): 0.1742
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30016x90048, b=2048): 254.192
Elapsed time for attention_key_query_prob (256x2048x469x2048): 0.0122
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x469x2048): 82.284
Elapsed time for attention_prob_times_values (256x2048x2048x469): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x469): 89.275
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0308
Elapsed time for attention_linear_projection (4x30016x30016, b=2048): 0.0574
Throughput (in TFLOP/s) for attention_linear_projection (4x30016x30016, b=2048): 257.065
Elapsed time for mlp_h_to_4h (4x30016x120064, b=2048): 0.2340
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30016x120064, b=2048): 252.311
Elapsed time for mlp_fused_gelu (2048x4x120064): 0.0033
Elapsed time for mlp_4h_to_h (4x120064x30016, b=2048): 0.2478
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120064x30016, b=2048): 238.326
Elapsed time for transformer_add_bias_dropout (2048x4x30016): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30016): 0.0012

Attention duration (in seconds): 0.2905
Attention throughput (in TFLOP/s): 210.153
MLP duration (in seconds): 0.4851
MLP throughput (in TFLOP/s): 243.457
Transformer duration (in seconds): 0.7824
Transformer throughput (in TFLOP/s): 228.962
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30080, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30080x90240, b=2048): 0.1752
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30080x90240, b=2048): 253.863
Elapsed time for attention_key_query_prob (256x2048x470x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x470x2048): 121.095
Elapsed time for attention_prob_times_values (256x2048x2048x470): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x470): 140.638
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30080x30080, b=2048): 0.0578
Throughput (in TFLOP/s) for attention_linear_projection (4x30080x30080, b=2048): 256.338
Elapsed time for mlp_h_to_4h (4x30080x120320, b=2048): 0.2346
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30080x120320, b=2048): 252.803
Elapsed time for mlp_fused_gelu (2048x4x120320): 0.0033
Elapsed time for mlp_4h_to_h (4x120320x30080, b=2048): 0.2482
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120320x30080, b=2048): 238.876
Elapsed time for transformer_add_bias_dropout (2048x4x30080): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30080): 0.0012

Attention duration (in seconds): 0.2835
Attention throughput (in TFLOP/s): 216.252
MLP duration (in seconds): 0.4861
MLP throughput (in TFLOP/s): 243.977
Transformer duration (in seconds): 0.7764
Transformer throughput (in TFLOP/s): 231.715
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30144, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30144x90432, b=2048): 0.1763
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30144x90432, b=2048): 253.373
Elapsed time for attention_key_query_prob (256x2048x471x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x471x2048): 82.107
Elapsed time for attention_prob_times_values (256x2048x2048x471): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x471): 87.576
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0319
Elapsed time for attention_linear_projection (4x30144x30144, b=2048): 0.0577
Throughput (in TFLOP/s) for attention_linear_projection (4x30144x30144, b=2048): 257.837
Elapsed time for mlp_h_to_4h (4x30144x120576, b=2048): 0.2345
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30144x120576, b=2048): 253.897
Elapsed time for mlp_fused_gelu (2048x4x120576): 0.0033
Elapsed time for mlp_4h_to_h (4x120576x30144, b=2048): 0.2489
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120576x30144, b=2048): 239.236
Elapsed time for transformer_add_bias_dropout (2048x4x30144): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30144): 0.0012

Attention duration (in seconds): 0.2943
Attention throughput (in TFLOP/s): 209.214
MLP duration (in seconds): 0.4868
MLP throughput (in TFLOP/s): 244.679
Transformer duration (in seconds): 0.7880
Transformer throughput (in TFLOP/s): 229.293
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30208, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30208x90624, b=2048): 0.1764
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30208x90624, b=2048): 254.257
Elapsed time for attention_key_query_prob (256x2048x472x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x472x2048): 189.876
Elapsed time for attention_prob_times_values (256x2048x2048x472): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x472): 216.180
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30208x30208, b=2048): 0.0579
Throughput (in TFLOP/s) for attention_linear_projection (4x30208x30208, b=2048): 258.159
Elapsed time for mlp_h_to_4h (4x30208x120832, b=2048): 0.2362
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30208x120832, b=2048): 253.159
Elapsed time for mlp_fused_gelu (2048x4x120832): 0.0033
Elapsed time for mlp_4h_to_h (4x120832x30208, b=2048): 0.2502
Throughput (in TFLOP/s) for mlp_4h_to_h (4x120832x30208, b=2048): 239.036
Elapsed time for transformer_add_bias_dropout (2048x4x30208): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30208): 0.0012

Attention duration (in seconds): 0.2793
Attention throughput (in TFLOP/s): 221.356
MLP duration (in seconds): 0.4897
MLP throughput (in TFLOP/s): 244.236
Transformer duration (in seconds): 0.7759
Transformer throughput (in TFLOP/s): 233.843
Transformer - MLP - Attention (in seconds): 0.0068
========================================================================================================================
num_attention_heads: 64, hidden_size: 30272, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30272x90816, b=2048): 0.1773
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30272x90816, b=2048): 254.044
Elapsed time for attention_key_query_prob (256x2048x473x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x473x2048): 82.150
Elapsed time for attention_prob_times_values (256x2048x2048x473): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x473): 87.613
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30272x30272, b=2048): 0.0586
Throughput (in TFLOP/s) for attention_linear_projection (4x30272x30272, b=2048): 256.124
Elapsed time for mlp_h_to_4h (4x30272x121088, b=2048): 0.2388
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30272x121088, b=2048): 251.520
Elapsed time for mlp_fused_gelu (2048x4x121088): 0.0033
Elapsed time for mlp_4h_to_h (4x121088x30272, b=2048): 0.2524
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121088x30272, b=2048): 237.932
Elapsed time for transformer_add_bias_dropout (2048x4x30272): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30272): 0.0013

Attention duration (in seconds): 0.2948
Attention throughput (in TFLOP/s): 210.600
MLP duration (in seconds): 0.4945
MLP throughput (in TFLOP/s): 242.900
Transformer duration (in seconds): 0.7963
Transformer throughput (in TFLOP/s): 228.824
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30336, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30336x91008, b=2048): 0.1778
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30336x91008, b=2048): 254.356
Elapsed time for attention_key_query_prob (256x2048x474x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x474x2048): 121.554
Elapsed time for attention_prob_times_values (256x2048x2048x474): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x474): 141.243
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30336x30336, b=2048): 0.0588
Throughput (in TFLOP/s) for attention_linear_projection (4x30336x30336, b=2048): 256.417
Elapsed time for mlp_h_to_4h (4x30336x121344, b=2048): 0.2399
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30336x121344, b=2048): 251.386
Elapsed time for mlp_fused_gelu (2048x4x121344): 0.0033
Elapsed time for mlp_4h_to_h (4x121344x30336, b=2048): 0.2527
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121344x30336, b=2048): 238.665
Elapsed time for transformer_add_bias_dropout (2048x4x30336): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30336): 0.0012

Attention duration (in seconds): 0.2872
Attention throughput (in TFLOP/s): 217.120
MLP duration (in seconds): 0.4959
MLP throughput (in TFLOP/s): 243.221
Transformer duration (in seconds): 0.7899
Transformer throughput (in TFLOP/s): 231.622
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30400, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30400x91200, b=2048): 0.1784
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30400x91200, b=2048): 254.583
Elapsed time for attention_key_query_prob (256x2048x475x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x475x2048): 82.464
Elapsed time for attention_prob_times_values (256x2048x2048x475): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x475): 90.694
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30400x30400, b=2048): 0.0592
Throughput (in TFLOP/s) for attention_linear_projection (4x30400x30400, b=2048): 255.589
Elapsed time for mlp_h_to_4h (4x30400x121600, b=2048): 0.2406
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30400x121600, b=2048): 251.693
Elapsed time for mlp_fused_gelu (2048x4x121600): 0.0033
Elapsed time for mlp_4h_to_h (4x121600x30400, b=2048): 0.2560
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121600x30400, b=2048): 236.542
Elapsed time for transformer_add_bias_dropout (2048x4x30400): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30400): 0.0012

Attention duration (in seconds): 0.2962
Attention throughput (in TFLOP/s): 211.351
MLP duration (in seconds): 0.5000
MLP throughput (in TFLOP/s): 242.261
Transformer duration (in seconds): 0.8031
Transformer throughput (in TFLOP/s): 228.771
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30464, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30464x91392, b=2048): 0.1797
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30464x91392, b=2048): 253.848
Elapsed time for attention_key_query_prob (256x2048x476x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x476x2048): 122.124
Elapsed time for attention_prob_times_values (256x2048x2048x476): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x476): 142.086
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30464x30464, b=2048): 0.0590
Throughput (in TFLOP/s) for attention_linear_projection (4x30464x30464, b=2048): 257.514
Elapsed time for mlp_h_to_4h (4x30464x121856, b=2048): 0.2421
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30464x121856, b=2048): 251.223
Elapsed time for mlp_fused_gelu (2048x4x121856): 0.0033
Elapsed time for mlp_4h_to_h (4x121856x30464, b=2048): 0.2544
Throughput (in TFLOP/s) for mlp_4h_to_h (4x121856x30464, b=2048): 239.060
Elapsed time for transformer_add_bias_dropout (2048x4x30464): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30464): 0.0012

Attention duration (in seconds): 0.2892
Attention throughput (in TFLOP/s): 217.342
MLP duration (in seconds): 0.4999
MLP throughput (in TFLOP/s): 243.355
Transformer duration (in seconds): 0.7960
Transformer throughput (in TFLOP/s): 231.794
Transformer - MLP - Attention (in seconds): 0.0069
========================================================================================================================
num_attention_heads: 64, hidden_size: 30528, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30528x91584, b=2048): 0.1804
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30528x91584, b=2048): 253.990
Elapsed time for attention_key_query_prob (256x2048x477x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x477x2048): 82.793
Elapsed time for attention_prob_times_values (256x2048x2048x477): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x477): 91.189
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30528x30528, b=2048): 0.0594
Throughput (in TFLOP/s) for attention_linear_projection (4x30528x30528, b=2048): 257.070
Elapsed time for mlp_h_to_4h (4x30528x122112, b=2048): 0.2442
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30528x122112, b=2048): 250.080
Elapsed time for mlp_fused_gelu (2048x4x122112): 0.0033
Elapsed time for mlp_4h_to_h (4x122112x30528, b=2048): 0.2566
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122112x30528, b=2048): 238.002
Elapsed time for transformer_add_bias_dropout (2048x4x30528): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30528): 0.0013

Attention duration (in seconds): 0.2984
Attention throughput (in TFLOP/s): 211.573
MLP duration (in seconds): 0.5042
MLP throughput (in TFLOP/s): 242.274
Transformer duration (in seconds): 0.8095
Transformer throughput (in TFLOP/s): 228.869
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30592, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30592x91776, b=2048): 0.1817
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30592x91776, b=2048): 253.128
Elapsed time for attention_key_query_prob (256x2048x478x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x478x2048): 122.085
Elapsed time for attention_prob_times_values (256x2048x2048x478): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x478): 142.748
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30592x30592, b=2048): 0.0596
Throughput (in TFLOP/s) for attention_linear_projection (4x30592x30592, b=2048): 257.373
Elapsed time for mlp_h_to_4h (4x30592x122368, b=2048): 0.2440
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30592x122368, b=2048): 251.322
Elapsed time for mlp_fused_gelu (2048x4x122368): 0.0034
Elapsed time for mlp_4h_to_h (4x122368x30592, b=2048): 0.2562
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122368x30592, b=2048): 239.442
Elapsed time for transformer_add_bias_dropout (2048x4x30592): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30592): 0.0012

Attention duration (in seconds): 0.2919
Attention throughput (in TFLOP/s): 217.139
MLP duration (in seconds): 0.5035
MLP throughput (in TFLOP/s): 243.603
Transformer duration (in seconds): 0.8024
Transformer throughput (in TFLOP/s): 231.858
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30656, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30656x91968, b=2048): 0.1815
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30656x91968, b=2048): 254.467
Elapsed time for attention_key_query_prob (256x2048x479x2048): 0.0123
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x479x2048): 83.530
Elapsed time for attention_prob_times_values (256x2048x2048x479): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x479): 90.075
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30656x30656, b=2048): 0.0600
Throughput (in TFLOP/s) for attention_linear_projection (4x30656x30656, b=2048): 256.478
Elapsed time for mlp_h_to_4h (4x30656x122624, b=2048): 0.2449
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30656x122624, b=2048): 251.464
Elapsed time for mlp_fused_gelu (2048x4x122624): 0.0034
Elapsed time for mlp_4h_to_h (4x122624x30656, b=2048): 0.2573
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122624x30656, b=2048): 239.330
Elapsed time for transformer_add_bias_dropout (2048x4x30656): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30656): 0.0013

Attention duration (in seconds): 0.3003
Attention throughput (in TFLOP/s): 211.939
MLP duration (in seconds): 0.5056
MLP throughput (in TFLOP/s): 243.617
Transformer duration (in seconds): 0.8130
Transformer throughput (in TFLOP/s): 229.808
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30720, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30720x92160, b=2048): 0.1824
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30720x92160, b=2048): 254.335
Elapsed time for attention_key_query_prob (256x2048x480x2048): 0.0050
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x480x2048): 204.364
Elapsed time for attention_prob_times_values (256x2048x2048x480): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x480): 224.365
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30720x30720, b=2048): 0.0602
Throughput (in TFLOP/s) for attention_linear_projection (4x30720x30720, b=2048): 256.864
Elapsed time for mlp_h_to_4h (4x30720x122880, b=2048): 0.2440
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30720x122880, b=2048): 253.444
Elapsed time for mlp_fused_gelu (2048x4x122880): 0.0034
Elapsed time for mlp_4h_to_h (4x122880x30720, b=2048): 0.2585
Throughput (in TFLOP/s) for mlp_4h_to_h (4x122880x30720, b=2048): 239.293
Elapsed time for transformer_add_bias_dropout (2048x4x30720): 0.0022
Elapsed time for transformer_layer_norm (2048x4x30720): 0.0012

Attention duration (in seconds): 0.2872
Attention throughput (in TFLOP/s): 222.534
MLP duration (in seconds): 0.5059
MLP throughput (in TFLOP/s): 244.529
Transformer duration (in seconds): 0.8000
Transformer throughput (in TFLOP/s): 234.505
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30784, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30784x92352, b=2048): 0.1835
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30784x92352, b=2048): 253.792
Elapsed time for attention_key_query_prob (256x2048x481x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x481x2048): 82.065
Elapsed time for attention_prob_times_values (256x2048x2048x481): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x481): 89.592
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30784x30784, b=2048): 0.0606
Throughput (in TFLOP/s) for attention_linear_projection (4x30784x30784, b=2048): 256.119
Elapsed time for mlp_h_to_4h (4x30784x123136, b=2048): 0.2453
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30784x123136, b=2048): 253.211
Elapsed time for mlp_fused_gelu (2048x4x123136): 0.0034
Elapsed time for mlp_4h_to_h (4x123136x30784, b=2048): 0.2596
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123136x30784, b=2048): 239.255
Elapsed time for transformer_add_bias_dropout (2048x4x30784): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30784): 0.0013

Attention duration (in seconds): 0.3032
Attention throughput (in TFLOP/s): 211.643
MLP duration (in seconds): 0.5082
MLP throughput (in TFLOP/s): 244.404
Transformer duration (in seconds): 0.8184
Transformer throughput (in TFLOP/s): 230.174
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30848, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30848x92544, b=2048): 0.1839
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30848x92544, b=2048): 254.301
Elapsed time for attention_key_query_prob (256x2048x482x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x482x2048): 123.703
Elapsed time for attention_prob_times_values (256x2048x2048x482): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x482): 144.417
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x30848x30848, b=2048): 0.0608
Throughput (in TFLOP/s) for attention_linear_projection (4x30848x30848, b=2048): 256.634
Elapsed time for mlp_h_to_4h (4x30848x123392, b=2048): 0.2472
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30848x123392, b=2048): 252.267
Elapsed time for mlp_fused_gelu (2048x4x123392): 0.0034
Elapsed time for mlp_4h_to_h (4x123392x30848, b=2048): 0.2604
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123392x30848, b=2048): 239.499
Elapsed time for transformer_add_bias_dropout (2048x4x30848): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30848): 0.0012

Attention duration (in seconds): 0.2951
Attention throughput (in TFLOP/s): 218.312
MLP duration (in seconds): 0.5110
MLP throughput (in TFLOP/s): 244.093
Transformer duration (in seconds): 0.8131
Transformer throughput (in TFLOP/s): 232.645
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30912, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30912x92736, b=2048): 0.1860
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30912x92736, b=2048): 252.472
Elapsed time for attention_key_query_prob (256x2048x483x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x483x2048): 81.441
Elapsed time for attention_prob_times_values (256x2048x2048x483): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x483): 91.983
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x30912x30912, b=2048): 0.0607
Throughput (in TFLOP/s) for attention_linear_projection (4x30912x30912, b=2048): 257.709
Elapsed time for mlp_h_to_4h (4x30912x123648, b=2048): 0.2502
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30912x123648, b=2048): 250.306
Elapsed time for mlp_fused_gelu (2048x4x123648): 0.0034
Elapsed time for mlp_4h_to_h (4x123648x30912, b=2048): 0.2626
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123648x30912, b=2048): 238.505
Elapsed time for transformer_add_bias_dropout (2048x4x30912): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30912): 0.0013

Attention duration (in seconds): 0.3058
Attention throughput (in TFLOP/s): 211.586
MLP duration (in seconds): 0.5161
MLP throughput (in TFLOP/s): 242.661
Transformer duration (in seconds): 0.8289
Transformer throughput (in TFLOP/s): 229.139
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 30976, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x30976x92928, b=2048): 0.1867
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x30976x92928, b=2048): 252.674
Elapsed time for attention_key_query_prob (256x2048x484x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x484x2048): 123.960
Elapsed time for attention_prob_times_values (256x2048x2048x484): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x484): 144.916
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x30976x30976, b=2048): 0.0613
Throughput (in TFLOP/s) for attention_linear_projection (4x30976x30976, b=2048): 256.318
Elapsed time for mlp_h_to_4h (4x30976x123904, b=2048): 0.2513
Throughput (in TFLOP/s) for mlp_h_to_4h (4x30976x123904, b=2048): 250.216
Elapsed time for mlp_fused_gelu (2048x4x123904): 0.0034
Elapsed time for mlp_4h_to_h (4x123904x30976, b=2048): 0.2616
Throughput (in TFLOP/s) for mlp_4h_to_h (4x123904x30976, b=2048): 240.420
Elapsed time for transformer_add_bias_dropout (2048x4x30976): 0.0023
Elapsed time for transformer_layer_norm (2048x4x30976): 0.0012

Attention duration (in seconds): 0.2986
Attention throughput (in TFLOP/s): 217.580
MLP duration (in seconds): 0.5163
MLP throughput (in TFLOP/s): 243.608
Transformer duration (in seconds): 0.8218
Transformer throughput (in TFLOP/s): 232.082
Transformer - MLP - Attention (in seconds): 0.0070
========================================================================================================================
num_attention_heads: 64, hidden_size: 31040, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31040x93120, b=2048): 0.1869
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31040x93120, b=2048): 253.358
Elapsed time for attention_key_query_prob (256x2048x485x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x485x2048): 81.812
Elapsed time for attention_prob_times_values (256x2048x2048x485): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x485): 92.221
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31040x31040, b=2048): 0.0615
Throughput (in TFLOP/s) for attention_linear_projection (4x31040x31040, b=2048): 256.809
Elapsed time for mlp_h_to_4h (4x31040x124160, b=2048): 0.2495
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31040x124160, b=2048): 253.028
Elapsed time for mlp_fused_gelu (2048x4x124160): 0.0034
Elapsed time for mlp_4h_to_h (4x124160x31040, b=2048): 0.2641
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124160x31040, b=2048): 239.093
Elapsed time for transformer_add_bias_dropout (2048x4x31040): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31040): 0.0013

Attention duration (in seconds): 0.3074
Attention throughput (in TFLOP/s): 212.174
MLP duration (in seconds): 0.5170
MLP throughput (in TFLOP/s): 244.248
Transformer duration (in seconds): 0.8315
Transformer throughput (in TFLOP/s): 230.311
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31104, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31104x93312, b=2048): 0.1865
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31104x93312, b=2048): 254.978
Elapsed time for attention_key_query_prob (256x2048x486x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x486x2048): 124.657
Elapsed time for attention_prob_times_values (256x2048x2048x486): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x486): 145.659
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31104x31104, b=2048): 0.0613
Throughput (in TFLOP/s) for attention_linear_projection (4x31104x31104, b=2048): 258.607
Elapsed time for mlp_h_to_4h (4x31104x124416, b=2048): 0.2511
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31104x124416, b=2048): 252.519
Elapsed time for mlp_fused_gelu (2048x4x124416): 0.0034
Elapsed time for mlp_4h_to_h (4x124416x31104, b=2048): 0.2641
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124416x31104, b=2048): 240.055
Elapsed time for transformer_add_bias_dropout (2048x4x31104): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31104): 0.0013

Attention duration (in seconds): 0.2983
Attention throughput (in TFLOP/s): 219.563
MLP duration (in seconds): 0.5186
MLP throughput (in TFLOP/s): 244.515
Transformer duration (in seconds): 0.8240
Transformer throughput (in TFLOP/s): 233.384
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31168, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31168x93504, b=2048): 0.1876
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31168x93504, b=2048): 254.462
Elapsed time for attention_key_query_prob (256x2048x487x2048): 0.0131
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x487x2048): 79.535
Elapsed time for attention_prob_times_values (256x2048x2048x487): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x487): 90.172
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31168x31168, b=2048): 0.0621
Throughput (in TFLOP/s) for attention_linear_projection (4x31168x31168, b=2048): 256.307
Elapsed time for mlp_h_to_4h (4x31168x124672, b=2048): 0.2522
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31168x124672, b=2048): 252.452
Elapsed time for mlp_fused_gelu (2048x4x124672): 0.0034
Elapsed time for mlp_4h_to_h (4x124672x31168, b=2048): 0.2654
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124672x31168, b=2048): 239.845
Elapsed time for transformer_add_bias_dropout (2048x4x31168): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31168): 0.0013

Attention duration (in seconds): 0.3095
Attention throughput (in TFLOP/s): 212.478
MLP duration (in seconds): 0.5210
MLP throughput (in TFLOP/s): 244.377
Transformer duration (in seconds): 0.8376
Transformer throughput (in TFLOP/s): 230.522
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31232, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31232x93696, b=2048): 0.1879
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31232x93696, b=2048): 255.111
Elapsed time for attention_key_query_prob (256x2048x488x2048): 0.0055
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x488x2048): 189.780
Elapsed time for attention_prob_times_values (256x2048x2048x488): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x488): 223.177
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31232x31232, b=2048): 0.0624
Throughput (in TFLOP/s) for attention_linear_projection (4x31232x31232, b=2048): 256.231
Elapsed time for mlp_h_to_4h (4x31232x124928, b=2048): 0.2536
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31232x124928, b=2048): 252.040
Elapsed time for mlp_fused_gelu (2048x4x124928): 0.0034
Elapsed time for mlp_4h_to_h (4x124928x31232, b=2048): 0.2661
Throughput (in TFLOP/s) for mlp_4h_to_h (4x124928x31232, b=2048): 240.201
Elapsed time for transformer_add_bias_dropout (2048x4x31232): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31232): 0.0013

Attention duration (in seconds): 0.2955
Attention throughput (in TFLOP/s): 223.416
MLP duration (in seconds): 0.5232
MLP throughput (in TFLOP/s): 244.373
Transformer duration (in seconds): 0.8258
Transformer throughput (in TFLOP/s): 234.778
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31296, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31296x93888, b=2048): 0.1892
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31296x93888, b=2048): 254.496
Elapsed time for attention_key_query_prob (256x2048x489x2048): 0.0128
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x489x2048): 81.891
Elapsed time for attention_prob_times_values (256x2048x2048x489): 0.0116
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x489): 90.201
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31296x31296, b=2048): 0.0627
Throughput (in TFLOP/s) for attention_linear_projection (4x31296x31296, b=2048): 256.017
Elapsed time for mlp_h_to_4h (4x31296x125184, b=2048): 0.2531
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31296x125184, b=2048): 253.572
Elapsed time for mlp_fused_gelu (2048x4x125184): 0.0034
Elapsed time for mlp_4h_to_h (4x125184x31296, b=2048): 0.2679
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125184x31296, b=2048): 239.631
Elapsed time for transformer_add_bias_dropout (2048x4x31296): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31296): 0.0013

Attention duration (in seconds): 0.3113
Attention throughput (in TFLOP/s): 212.953
MLP duration (in seconds): 0.5244
MLP throughput (in TFLOP/s): 244.794
Transformer duration (in seconds): 0.8428
Transformer throughput (in TFLOP/s): 230.969
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31360, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31360x94080, b=2048): 0.1898
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31360x94080, b=2048): 254.641
Elapsed time for attention_key_query_prob (256x2048x490x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x490x2048): 125.484
Elapsed time for attention_prob_times_values (256x2048x2048x490): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x490): 146.669
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31360x31360, b=2048): 0.0626
Throughput (in TFLOP/s) for attention_linear_projection (4x31360x31360, b=2048): 257.420
Elapsed time for mlp_h_to_4h (4x31360x125440, b=2048): 0.2546
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31360x125440, b=2048): 253.157
Elapsed time for mlp_fused_gelu (2048x4x125440): 0.0034
Elapsed time for mlp_4h_to_h (4x125440x31360, b=2048): 0.2693
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125440x31360, b=2048): 239.334
Elapsed time for transformer_add_bias_dropout (2048x4x31360): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31360): 0.0012

Attention duration (in seconds): 0.3030
Attention throughput (in TFLOP/s): 219.669
MLP duration (in seconds): 0.5273
MLP throughput (in TFLOP/s): 244.448
Transformer duration (in seconds): 0.8374
Transformer throughput (in TFLOP/s): 233.415
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31424, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31424x94272, b=2048): 0.1909
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31424x94272, b=2048): 254.258
Elapsed time for attention_key_query_prob (256x2048x491x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x491x2048): 82.941
Elapsed time for attention_prob_times_values (256x2048x2048x491): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x491): 93.477
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31424x31424, b=2048): 0.0629
Throughput (in TFLOP/s) for attention_linear_projection (4x31424x31424, b=2048): 257.393
Elapsed time for mlp_h_to_4h (4x31424x125696, b=2048): 0.2554
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31424x125696, b=2048): 253.414
Elapsed time for mlp_fused_gelu (2048x4x125696): 0.0034
Elapsed time for mlp_4h_to_h (4x125696x31424, b=2048): 0.2697
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125696x31424, b=2048): 239.962
Elapsed time for transformer_add_bias_dropout (2048x4x31424): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31424): 0.0013

Attention duration (in seconds): 0.3127
Attention throughput (in TFLOP/s): 213.665
MLP duration (in seconds): 0.5285
MLP throughput (in TFLOP/s): 244.901
Transformer duration (in seconds): 0.8484
Transformer throughput (in TFLOP/s): 231.322
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31488, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31488x94464, b=2048): 0.1919
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31488x94464, b=2048): 253.906
Elapsed time for attention_key_query_prob (256x2048x492x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x492x2048): 126.578
Elapsed time for attention_prob_times_values (256x2048x2048x492): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x492): 147.883
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31488x31488, b=2048): 0.0632
Throughput (in TFLOP/s) for attention_linear_projection (4x31488x31488, b=2048): 257.037
Elapsed time for mlp_h_to_4h (4x31488x125952, b=2048): 0.2591
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31488x125952, b=2048): 250.821
Elapsed time for mlp_fused_gelu (2048x4x125952): 0.0034
Elapsed time for mlp_4h_to_h (4x125952x31488, b=2048): 0.2713
Throughput (in TFLOP/s) for mlp_4h_to_h (4x125952x31488, b=2048): 239.546
Elapsed time for transformer_add_bias_dropout (2048x4x31488): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31488): 0.0013

Attention duration (in seconds): 0.3056
Attention throughput (in TFLOP/s): 219.564
MLP duration (in seconds): 0.5338
MLP throughput (in TFLOP/s): 243.473
Transformer duration (in seconds): 0.8464
Transformer throughput (in TFLOP/s): 232.797
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31552, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31552x94656, b=2048): 0.1923
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31552x94656, b=2048): 254.395
Elapsed time for attention_key_query_prob (256x2048x493x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x493x2048): 83.733
Elapsed time for attention_prob_times_values (256x2048x2048x493): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x493): 93.593
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31552x31552, b=2048): 0.0639
Throughput (in TFLOP/s) for attention_linear_projection (4x31552x31552, b=2048): 255.128
Elapsed time for mlp_h_to_4h (4x31552x126208, b=2048): 0.2592
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31552x126208, b=2048): 251.672
Elapsed time for mlp_fused_gelu (2048x4x126208): 0.0035
Elapsed time for mlp_4h_to_h (4x126208x31552, b=2048): 0.2731
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126208x31552, b=2048): 238.890
Elapsed time for transformer_add_bias_dropout (2048x4x31552): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31552): 0.0013

Attention duration (in seconds): 0.3152
Attention throughput (in TFLOP/s): 213.702
MLP duration (in seconds): 0.5358
MLP throughput (in TFLOP/s): 243.535
Transformer duration (in seconds): 0.8582
Transformer throughput (in TFLOP/s): 230.534
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31616, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31616x94848, b=2048): 0.1933
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31616x94848, b=2048): 254.222
Elapsed time for attention_key_query_prob (256x2048x494x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x494x2048): 126.502
Elapsed time for attention_prob_times_values (256x2048x2048x494): 0.0072
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x494): 147.612
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31616x31616, b=2048): 0.0639
Throughput (in TFLOP/s) for attention_linear_projection (4x31616x31616, b=2048): 256.213
Elapsed time for mlp_h_to_4h (4x31616x126464, b=2048): 0.2597
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31616x126464, b=2048): 252.213
Elapsed time for mlp_fused_gelu (2048x4x126464): 0.0035
Elapsed time for mlp_4h_to_h (4x126464x31616, b=2048): 0.2743
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126464x31616, b=2048): 238.860
Elapsed time for transformer_add_bias_dropout (2048x4x31616): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31616): 0.0013

Attention duration (in seconds): 0.3077
Attention throughput (in TFLOP/s): 219.774
MLP duration (in seconds): 0.5374
MLP throughput (in TFLOP/s): 243.776
Transformer duration (in seconds): 0.8523
Transformer throughput (in TFLOP/s): 233.070
Transformer - MLP - Attention (in seconds): 0.0071
========================================================================================================================
num_attention_heads: 64, hidden_size: 31680, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31680x95040, b=2048): 0.1944
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31680x95040, b=2048): 253.778
Elapsed time for attention_key_query_prob (256x2048x495x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x495x2048): 83.568
Elapsed time for attention_prob_times_values (256x2048x2048x495): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x495): 92.208
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31680x31680, b=2048): 0.0641
Throughput (in TFLOP/s) for attention_linear_projection (4x31680x31680, b=2048): 256.574
Elapsed time for mlp_h_to_4h (4x31680x126720, b=2048): 0.2603
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31680x126720, b=2048): 252.682
Elapsed time for mlp_fused_gelu (2048x4x126720): 0.0035
Elapsed time for mlp_4h_to_h (4x126720x31680, b=2048): 0.2754
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126720x31680, b=2048): 238.857
Elapsed time for transformer_add_bias_dropout (2048x4x31680): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31680): 0.0013

Attention duration (in seconds): 0.3177
Attention throughput (in TFLOP/s): 213.744
MLP duration (in seconds): 0.5391
MLP throughput (in TFLOP/s): 243.995
Transformer duration (in seconds): 0.8640
Transformer throughput (in TFLOP/s): 230.839
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31744, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31744x95232, b=2048): 0.1959
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31744x95232, b=2048): 252.802
Elapsed time for attention_key_query_prob (256x2048x496x2048): 0.0053
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x496x2048): 200.438
Elapsed time for attention_prob_times_values (256x2048x2048x496): 0.0047
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x496): 228.623
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31744x31744, b=2048): 0.0643
Throughput (in TFLOP/s) for attention_linear_projection (4x31744x31744, b=2048): 256.708
Elapsed time for mlp_h_to_4h (4x31744x126976, b=2048): 0.2627
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31744x126976, b=2048): 251.345
Elapsed time for mlp_fused_gelu (2048x4x126976): 0.0035
Elapsed time for mlp_4h_to_h (4x126976x31744, b=2048): 0.2752
Throughput (in TFLOP/s) for mlp_4h_to_h (4x126976x31744, b=2048): 240.009
Elapsed time for transformer_add_bias_dropout (2048x4x31744): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31744): 0.0013

Attention duration (in seconds): 0.3052
Attention throughput (in TFLOP/s): 223.354
MLP duration (in seconds): 0.5414
MLP throughput (in TFLOP/s): 243.970
Transformer duration (in seconds): 0.8538
Transformer throughput (in TFLOP/s): 234.548
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 31808, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31808x95424, b=2048): 0.1957
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31808x95424, b=2048): 254.093
Elapsed time for attention_key_query_prob (256x2048x497x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x497x2048): 84.098
Elapsed time for attention_prob_times_values (256x2048x2048x497): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x497): 92.611
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31808x31808, b=2048): 0.0646
Throughput (in TFLOP/s) for attention_linear_projection (4x31808x31808, b=2048): 256.590
Elapsed time for mlp_h_to_4h (4x31808x127232, b=2048): 0.2618
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31808x127232, b=2048): 253.271
Elapsed time for mlp_fused_gelu (2048x4x127232): 0.0035
Elapsed time for mlp_4h_to_h (4x127232x31808, b=2048): 0.2763
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127232x31808, b=2048): 240.015
Elapsed time for transformer_add_bias_dropout (2048x4x31808): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31808): 0.0013

Attention duration (in seconds): 0.3195
Attention throughput (in TFLOP/s): 214.187
MLP duration (in seconds): 0.5415
MLP throughput (in TFLOP/s): 244.879
Transformer duration (in seconds): 0.8683
Transformer throughput (in TFLOP/s): 231.539
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 31872, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31872x95616, b=2048): 0.1960
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31872x95616, b=2048): 254.789
Elapsed time for attention_key_query_prob (256x2048x498x2048): 0.0084
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x498x2048): 127.952
Elapsed time for attention_prob_times_values (256x2048x2048x498): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x498): 149.945
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31872x31872, b=2048): 0.0649
Throughput (in TFLOP/s) for attention_linear_projection (4x31872x31872, b=2048): 256.496
Elapsed time for mlp_h_to_4h (4x31872x127488, b=2048): 0.2632
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31872x127488, b=2048): 252.922
Elapsed time for mlp_fused_gelu (2048x4x127488): 0.0035
Elapsed time for mlp_4h_to_h (4x127488x31872, b=2048): 0.2786
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127488x31872, b=2048): 238.933
Elapsed time for transformer_add_bias_dropout (2048x4x31872): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31872): 0.0013

Attention duration (in seconds): 0.3113
Attention throughput (in TFLOP/s): 220.720
MLP duration (in seconds): 0.5453
MLP throughput (in TFLOP/s): 244.156
Transformer duration (in seconds): 0.8639
Transformer throughput (in TFLOP/s): 233.654
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 31936, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x31936x95808, b=2048): 0.1970
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x31936x95808, b=2048): 254.447
Elapsed time for attention_key_query_prob (256x2048x499x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x499x2048): 85.426
Elapsed time for attention_prob_times_values (256x2048x2048x499): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x499): 95.301
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x31936x31936, b=2048): 0.0652
Throughput (in TFLOP/s) for attention_linear_projection (4x31936x31936, b=2048): 256.327
Elapsed time for mlp_h_to_4h (4x31936x127744, b=2048): 0.2639
Throughput (in TFLOP/s) for mlp_h_to_4h (4x31936x127744, b=2048): 253.278
Elapsed time for mlp_fused_gelu (2048x4x127744): 0.0035
Elapsed time for mlp_4h_to_h (4x127744x31936, b=2048): 0.2805
Throughput (in TFLOP/s) for mlp_4h_to_h (4x127744x31936, b=2048): 238.313
Elapsed time for transformer_add_bias_dropout (2048x4x31936): 0.0023
Elapsed time for transformer_layer_norm (2048x4x31936): 0.0013

Attention duration (in seconds): 0.3210
Attention throughput (in TFLOP/s): 214.913
MLP duration (in seconds): 0.5479
MLP throughput (in TFLOP/s): 244.003
Transformer duration (in seconds): 0.8761
Transformer throughput (in TFLOP/s): 231.318
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32000, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32000x96000, b=2048): 0.1979
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32000x96000, b=2048): 254.354
Elapsed time for attention_key_query_prob (256x2048x500x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x500x2048): 128.858
Elapsed time for attention_prob_times_values (256x2048x2048x500): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x500): 151.121
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32000x32000, b=2048): 0.0654
Throughput (in TFLOP/s) for attention_linear_projection (4x32000x32000, b=2048): 256.723
Elapsed time for mlp_h_to_4h (4x32000x128000, b=2048): 0.2657
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32000x128000, b=2048): 252.550
Elapsed time for mlp_fused_gelu (2048x4x128000): 0.0035
Elapsed time for mlp_4h_to_h (4x128000x32000, b=2048): 0.2787
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128000x32000, b=2048): 240.828
Elapsed time for transformer_add_bias_dropout (2048x4x32000): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32000): 0.0013

Attention duration (in seconds): 0.3137
Attention throughput (in TFLOP/s): 220.798
MLP duration (in seconds): 0.5479
MLP throughput (in TFLOP/s): 244.975
Transformer duration (in seconds): 0.8688
Transformer throughput (in TFLOP/s): 234.202
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32064, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32064x96192, b=2048): 0.1985
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32064x96192, b=2048): 254.531
Elapsed time for attention_key_query_prob (256x2048x501x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x501x2048): 86.282
Elapsed time for attention_prob_times_values (256x2048x2048x501): 0.0113
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x501): 95.488
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32064x32064, b=2048): 0.0657
Throughput (in TFLOP/s) for attention_linear_projection (4x32064x32064, b=2048): 256.520
Elapsed time for mlp_h_to_4h (4x32064x128256, b=2048): 0.2665
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32064x128256, b=2048): 252.846
Elapsed time for mlp_fused_gelu (2048x4x128256): 0.0035
Elapsed time for mlp_4h_to_h (4x128256x32064, b=2048): 0.2819
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128256x32064, b=2048): 238.996
Elapsed time for transformer_add_bias_dropout (2048x4x32064): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32064): 0.0013

Attention duration (in seconds): 0.3229
Attention throughput (in TFLOP/s): 215.330
MLP duration (in seconds): 0.5519
MLP throughput (in TFLOP/s): 244.163
Transformer duration (in seconds): 0.8821
Transformer throughput (in TFLOP/s): 231.584
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32128, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32128x96384, b=2048): 0.1988
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32128x96384, b=2048): 255.193
Elapsed time for attention_key_query_prob (256x2048x502x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x502x2048): 129.329
Elapsed time for attention_prob_times_values (256x2048x2048x502): 0.0071
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x502): 152.227
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32128x32128, b=2048): 0.0660
Throughput (in TFLOP/s) for attention_linear_projection (4x32128x32128, b=2048): 256.308
Elapsed time for mlp_h_to_4h (4x32128x128512, b=2048): 0.2671
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32128x128512, b=2048): 253.254
Elapsed time for mlp_fused_gelu (2048x4x128512): 0.0035
Elapsed time for mlp_4h_to_h (4x128512x32128, b=2048): 0.2823
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128512x32128, b=2048): 239.658
Elapsed time for transformer_add_bias_dropout (2048x4x32128): 0.0023
Elapsed time for transformer_layer_norm (2048x4x32128): 0.0013

Attention duration (in seconds): 0.3152
Attention throughput (in TFLOP/s): 221.454
MLP duration (in seconds): 0.5529
MLP throughput (in TFLOP/s): 244.701
Transformer duration (in seconds): 0.8753
Transformer throughput (in TFLOP/s): 234.305
Transformer - MLP - Attention (in seconds): 0.0072
========================================================================================================================
num_attention_heads: 64, hidden_size: 32192, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32192x96576, b=2048): 0.2009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32192x96576, b=2048): 253.501
Elapsed time for attention_key_query_prob (256x2048x503x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x503x2048): 84.938
Elapsed time for attention_prob_times_values (256x2048x2048x503): 0.0115
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x503): 94.296
Elapsed time for attention_dropout (4x64x2048x2048): 0.0046
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32192x32192, b=2048): 0.0662
Throughput (in TFLOP/s) for attention_linear_projection (4x32192x32192, b=2048): 256.341
Elapsed time for mlp_h_to_4h (4x32192x128768, b=2048): 0.2694
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32192x128768, b=2048): 252.115
Elapsed time for mlp_fused_gelu (2048x4x128768): 0.0035
Elapsed time for mlp_4h_to_h (4x128768x32192, b=2048): 0.2839
Throughput (in TFLOP/s) for mlp_4h_to_h (4x128768x32192, b=2048): 239.217
Elapsed time for transformer_add_bias_dropout (2048x4x32192): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32192): 0.0013

Attention duration (in seconds): 0.3263
Attention throughput (in TFLOP/s): 214.734
MLP duration (in seconds): 0.5568
MLP throughput (in TFLOP/s): 243.943
Transformer duration (in seconds): 0.8905
Transformer throughput (in TFLOP/s): 231.230
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32256x96768, b=2048): 0.2009
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32256x96768, b=2048): 254.591
Elapsed time for attention_key_query_prob (256x2048x504x2048): 0.0057
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x504x2048): 191.327
Elapsed time for attention_prob_times_values (256x2048x2048x504): 0.0048
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x504): 227.617
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32256x32256, b=2048): 0.0664
Throughput (in TFLOP/s) for attention_linear_projection (4x32256x32256, b=2048): 256.872
Elapsed time for mlp_h_to_4h (4x32256x129024, b=2048): 0.2696
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32256x129024, b=2048): 252.958
Elapsed time for mlp_fused_gelu (2048x4x129024): 0.0035
Elapsed time for mlp_4h_to_h (4x129024x32256, b=2048): 0.2850
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129024x32256, b=2048): 239.226
Elapsed time for transformer_add_bias_dropout (2048x4x32256): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32256): 0.0013

Attention duration (in seconds): 0.3126
Attention throughput (in TFLOP/s): 225.049
MLP duration (in seconds): 0.5581
MLP throughput (in TFLOP/s): 244.348
Transformer duration (in seconds): 0.8780
Transformer throughput (in TFLOP/s): 235.449
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32320, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32320x96960, b=2048): 0.2024
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32320x96960, b=2048): 253.659
Elapsed time for attention_key_query_prob (256x2048x505x2048): 0.0127
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x505x2048): 85.316
Elapsed time for attention_prob_times_values (256x2048x2048x505): 0.0114
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x505): 94.741
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32320x32320, b=2048): 0.0666
Throughput (in TFLOP/s) for attention_linear_projection (4x32320x32320, b=2048): 256.889
Elapsed time for mlp_h_to_4h (4x32320x129280, b=2048): 0.2712
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32320x129280, b=2048): 252.469
Elapsed time for mlp_fused_gelu (2048x4x129280): 0.0035
Elapsed time for mlp_4h_to_h (4x129280x32320, b=2048): 0.2866
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129280x32320, b=2048): 238.864
Elapsed time for transformer_add_bias_dropout (2048x4x32320): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32320): 0.0013

Attention duration (in seconds): 0.3282
Attention throughput (in TFLOP/s): 215.224
MLP duration (in seconds): 0.5613
MLP throughput (in TFLOP/s): 243.932
Transformer duration (in seconds): 0.8968
Transformer throughput (in TFLOP/s): 231.428
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32384, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32384x97152, b=2048): 0.2023
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32384x97152, b=2048): 254.820
Elapsed time for attention_key_query_prob (256x2048x506x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x506x2048): 130.690
Elapsed time for attention_prob_times_values (256x2048x2048x506): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x506): 154.457
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32384x32384, b=2048): 0.0664
Throughput (in TFLOP/s) for attention_linear_projection (4x32384x32384, b=2048): 258.880
Elapsed time for mlp_h_to_4h (4x32384x129536, b=2048): 0.2719
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32384x129536, b=2048): 252.740
Elapsed time for mlp_fused_gelu (2048x4x129536): 0.0035
Elapsed time for mlp_4h_to_h (4x129536x32384, b=2048): 0.2875
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129536x32384, b=2048): 239.068
Elapsed time for transformer_add_bias_dropout (2048x4x32384): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32384): 0.0013

Attention duration (in seconds): 0.3190
Attention throughput (in TFLOP/s): 222.277
MLP duration (in seconds): 0.5630
MLP throughput (in TFLOP/s): 244.168
Transformer duration (in seconds): 0.8893
Transformer throughput (in TFLOP/s): 234.303
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32448, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32448x97344, b=2048): 0.2038
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32448x97344, b=2048): 253.879
Elapsed time for attention_key_query_prob (256x2048x507x2048): 0.0125
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x507x2048): 87.003
Elapsed time for attention_prob_times_values (256x2048x2048x507): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x507): 97.651
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32448x32448, b=2048): 0.0674
Throughput (in TFLOP/s) for attention_linear_projection (4x32448x32448, b=2048): 255.841
Elapsed time for mlp_h_to_4h (4x32448x129792, b=2048): 0.2745
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32448x129792, b=2048): 251.375
Elapsed time for mlp_fused_gelu (2048x4x129792): 0.0035
Elapsed time for mlp_4h_to_h (4x129792x32448, b=2048): 0.2878
Throughput (in TFLOP/s) for mlp_4h_to_h (4x129792x32448, b=2048): 239.719
Elapsed time for transformer_add_bias_dropout (2048x4x32448): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32448): 0.0013

Attention duration (in seconds): 0.3299
Attention throughput (in TFLOP/s): 215.774
MLP duration (in seconds): 0.5659
MLP throughput (in TFLOP/s): 243.869
Transformer duration (in seconds): 0.9031
Transformer throughput (in TFLOP/s): 231.615
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32512, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32512x97536, b=2048): 0.2044
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32512x97536, b=2048): 254.217
Elapsed time for attention_key_query_prob (256x2048x508x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x508x2048): 131.694
Elapsed time for attention_prob_times_values (256x2048x2048x508): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x508): 155.977
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32512x32512, b=2048): 0.0675
Throughput (in TFLOP/s) for attention_linear_projection (4x32512x32512, b=2048): 256.706
Elapsed time for mlp_h_to_4h (4x32512x130048, b=2048): 0.2754
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32512x130048, b=2048): 251.536
Elapsed time for mlp_fused_gelu (2048x4x130048): 0.0036
Elapsed time for mlp_4h_to_h (4x130048x32512, b=2048): 0.2884
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130048x32512, b=2048): 240.208
Elapsed time for transformer_add_bias_dropout (2048x4x32512): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32512): 0.0013

Attention duration (in seconds): 0.3221
Attention throughput (in TFLOP/s): 221.844
MLP duration (in seconds): 0.5673
MLP throughput (in TFLOP/s): 244.200
Transformer duration (in seconds): 0.8968
Transformer throughput (in TFLOP/s): 234.172
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32576, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32576x97728, b=2048): 0.2063
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32576x97728, b=2048): 252.894
Elapsed time for attention_key_query_prob (256x2048x509x2048): 0.0124
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x509x2048): 87.892
Elapsed time for attention_prob_times_values (256x2048x2048x509): 0.0111
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x509): 98.264
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0304
Elapsed time for attention_linear_projection (4x32576x32576, b=2048): 0.0679
Throughput (in TFLOP/s) for attention_linear_projection (4x32576x32576, b=2048): 256.146
Elapsed time for mlp_h_to_4h (4x32576x130304, b=2048): 0.2764
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32576x130304, b=2048): 251.577
Elapsed time for mlp_fused_gelu (2048x4x130304): 0.0036
Elapsed time for mlp_4h_to_h (4x130304x32576, b=2048): 0.2921
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130304x32576, b=2048): 238.067
Elapsed time for transformer_add_bias_dropout (2048x4x32576): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32576): 0.0013

Attention duration (in seconds): 0.3327
Attention throughput (in TFLOP/s): 215.631
MLP duration (in seconds): 0.5721
MLP throughput (in TFLOP/s): 243.112
Transformer duration (in seconds): 0.9122
Transformer throughput (in TFLOP/s): 231.113
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32640, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32640x97920, b=2048): 0.2061
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32640x97920, b=2048): 254.082
Elapsed time for attention_key_query_prob (256x2048x510x2048): 0.0083
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x510x2048): 131.553
Elapsed time for attention_prob_times_values (256x2048x2048x510): 0.0070
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x510): 156.721
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0306
Elapsed time for attention_linear_projection (4x32640x32640, b=2048): 0.0681
Throughput (in TFLOP/s) for attention_linear_projection (4x32640x32640, b=2048): 256.407
Elapsed time for mlp_h_to_4h (4x32640x130560, b=2048): 0.2753
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32640x130560, b=2048): 253.591
Elapsed time for mlp_fused_gelu (2048x4x130560): 0.0036
Elapsed time for mlp_4h_to_h (4x130560x32640, b=2048): 0.2918
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130560x32640, b=2048): 239.291
Elapsed time for transformer_add_bias_dropout (2048x4x32640): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32640): 0.0013

Attention duration (in seconds): 0.3246
Attention throughput (in TFLOP/s): 221.816
MLP duration (in seconds): 0.5707
MLP throughput (in TFLOP/s): 244.694
Transformer duration (in seconds): 0.9027
Transformer throughput (in TFLOP/s): 234.475
Transformer - MLP - Attention (in seconds): 0.0073
========================================================================================================================
num_attention_heads: 64, hidden_size: 32704, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32704x98112, b=2048): 0.2056
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32704x98112, b=2048): 255.708
Elapsed time for attention_key_query_prob (256x2048x511x2048): 0.0126
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x511x2048): 87.301
Elapsed time for attention_prob_times_values (256x2048x2048x511): 0.0112
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x511): 97.840
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0305
Elapsed time for attention_linear_projection (4x32704x32704, b=2048): 0.0683
Throughput (in TFLOP/s) for attention_linear_projection (4x32704x32704, b=2048): 256.388
Elapsed time for mlp_h_to_4h (4x32704x130816, b=2048): 0.2779
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32704x130816, b=2048): 252.268
Elapsed time for mlp_fused_gelu (2048x4x130816): 0.0036
Elapsed time for mlp_4h_to_h (4x130816x32704, b=2048): 0.2949
Throughput (in TFLOP/s) for mlp_4h_to_h (4x130816x32704, b=2048): 237.705
Elapsed time for transformer_add_bias_dropout (2048x4x32704): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32704): 0.0013

Attention duration (in seconds): 0.3328
Attention throughput (in TFLOP/s): 217.243
MLP duration (in seconds): 0.5763
MLP throughput (in TFLOP/s): 243.251
Transformer duration (in seconds): 0.9165
Transformer throughput (in TFLOP/s): 231.838
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
num_attention_heads: 64, hidden_size: 32768, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Estimate
--------
Elapsed time for attention_key_value_query_transform (4x32768x98304, b=2048): 0.2071
Throughput (in TFLOP/s) for attention_key_value_query_transform (4x32768x98304, b=2048): 254.847
Elapsed time for attention_key_query_prob (256x2048x512x2048): 0.0052
Throughput (in TFLOP/s) for attention_key_query_prob (256x2048x512x2048): 209.487
Elapsed time for attention_prob_times_values (256x2048x2048x512): 0.0046
Throughput (in TFLOP/s) for attention_prob_times_values (256x2048x2048x512): 238.144
Elapsed time for attention_dropout (4x64x2048x2048): 0.0045
Elapsed time for attention_softmax (4x64x2048x2048): 0.0317
Elapsed time for attention_linear_projection (4x32768x32768, b=2048): 0.0683
Throughput (in TFLOP/s) for attention_linear_projection (4x32768x32768, b=2048): 257.512
Elapsed time for mlp_h_to_4h (4x32768x131072, b=2048): 0.2824
Throughput (in TFLOP/s) for mlp_h_to_4h (4x32768x131072, b=2048): 249.185
Elapsed time for mlp_fused_gelu (2048x4x131072): 0.0036
Elapsed time for mlp_4h_to_h (4x131072x32768, b=2048): 0.2943
Throughput (in TFLOP/s) for mlp_4h_to_h (4x131072x32768, b=2048): 239.119
Elapsed time for transformer_add_bias_dropout (2048x4x32768): 0.0024
Elapsed time for transformer_layer_norm (2048x4x32768): 0.0013

Attention duration (in seconds): 0.3215
Attention throughput (in TFLOP/s): 225.696
MLP duration (in seconds): 0.5803
MLP throughput (in TFLOP/s): 242.541
Transformer duration (in seconds): 0.9092
Transformer throughput (in TFLOP/s): 234.611
Transformer - MLP - Attention (in seconds): 0.0074
========================================================================================================================
