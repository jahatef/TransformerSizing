1.13.1 

[2023-11-22 16:00:08,667] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-22 16:00:09,456] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.155.251, master_port=6000
[2023-11-22 16:00:09,456] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-22 16:00:12,642] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 16, hidden_size: 256, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0023
Attention throughput (in TFLOP/s): 9.531
MLP duration (in seconds): 0.0001
MLP throughput (in TFLOP/s): 64.108
Transformer duration (in seconds): 0.0027
Transformer throughput (in TFLOP/s): 11.282
========================================================================================================================
1.13.1 

[2023-11-22 16:04:33,314] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-22 16:04:34,100] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.147.1, master_port=6000
[2023-11-22 16:04:34,100] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-22 16:04:37,353] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 16, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
1.13.1 

[2023-11-22 16:06:23,354] [INFO] [distributed.py:36:init_distributed] Not using the DeepSpeed or torch.distributed launchers, attempting to detect MPI environment...
[2023-11-22 16:06:23,816] [INFO] [distributed.py:83:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=26.0.147.1, master_port=6000
[2023-11-22 16:06:23,816] [INFO] [distributed.py:46:init_distributed] Initializing torch distributed with backend: nccl
[2023-11-22 16:06:25,056] [INFO] [checkpointing.py:223:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
num_attention_heads: 16, hidden_size: 4096, train_micro_batch_size_per_gpu: 4, tensor_mp_size: 1, pipeline_mp_size: 1, dp_size: 1


Actual
------
Attention duration (in seconds): 0.0333
Attention throughput (in TFLOP/s): 41.325
MLP duration (in seconds): 0.0095
MLP throughput (in TFLOP/s): 230.440
Transformer duration (in seconds): 0.0216
Transformer throughput (in TFLOP/s): 165.741
========================================================================================================================
